


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._tensor &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/_tensor.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/_tensor.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
          <li><a href="../torch.html">torch</a> &gt;</li>
        
      <li>torch._tensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch._tensor</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copyreg</span>
<span class="kn">import</span> <span class="nn">enum</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">numbers</span> <span class="kn">import</span> <span class="n">Number</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C</span>
<span class="kn">import</span> <span class="nn">torch.utils.hooks</span> <span class="k">as</span> <span class="nn">hooks</span>
<span class="kn">from</span> <span class="nn">torch._namedtensor_internals</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_serializing_named_tensor</span><span class="p">,</span>
    <span class="n">is_ellipsis</span><span class="p">,</span>
    <span class="n">resolve_ellipsis</span><span class="p">,</span>
    <span class="n">single_ellipsis_index</span><span class="p">,</span>
    <span class="n">unzip_namedshape</span><span class="p">,</span>
    <span class="n">update_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_default_nowrap_functions</span><span class="p">,</span>
    <span class="n">handle_torch_function</span><span class="p">,</span>
    <span class="n">has_torch_function</span><span class="p">,</span>
    <span class="n">has_torch_function_unary</span><span class="p">,</span>
    <span class="n">has_torch_function_variadic</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.utils.dlpack</span> <span class="kn">import</span> <span class="n">DLDeviceType</span>


<span class="k">def</span> <span class="nf">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="n">assigned</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">WRAPPER_ASSIGNMENTS</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">assigned</span><span class="o">=</span><span class="n">assigned</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># See https://github.com/pytorch/pytorch/issues/75462</span>
            <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">wrapped</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

    <span class="k">return</span> <span class="n">wrapped</span>


<span class="c1"># Should not be used, this is kept only for BC of loading old serialized Tensor subclasses</span>
<span class="k">def</span> <span class="nf">_rebuild_from_type</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>
    <span class="n">ret</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="nb">dict</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">_rebuild_from_type_v2</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">new_type</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">new_type</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="n">new_type</span><span class="p">)</span>
    <span class="c1"># Tensor does define __setstate__ even though it doesn&#39;t define</span>
    <span class="c1"># __getstate__. So only use __setstate__ if it is NOT the one defined</span>
    <span class="c1"># on Tensor</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">ret</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="s2">&quot;__setstate__&quot;</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">)</span>
        <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span>
    <span class="p">):</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_set_obj_state</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="c1"># NB: If you subclass Tensor, and want to share the subclassed class</span>
<span class="c1"># across processes, you must also update torch/multiprocessing/reductions.py</span>
<span class="c1"># to define a ForkingPickler serialization mode for the class.</span>
<span class="c1">#</span>
<span class="c1"># NB: If you add a new method to Tensor, you must update</span>
<span class="c1"># torch/__init__.py.in to add a type annotation for your method;</span>
<span class="c1"># otherwise, it will not show up in autocomplete.</span>
<span class="k">class</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__deepcopy__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Only Tensors created explicitly by the user &quot;</span>
                <span class="s2">&quot;(graph leaves) support the deepcopy protocol at the moment.  &quot;</span>
                <span class="s2">&quot;If you were attempting to deepcopy a module, this may be because &quot;</span>
                <span class="s2">&quot;of a torch.nn.utils.weight_norm usage, &quot;</span>
                <span class="s2">&quot;see https://github.com/pytorch/pytorch/pull/103001&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># TODO: skipping storage copy is wrong for meta, as meta</span>
            <span class="c1"># does accurate alias tracking; however, the code below</span>
            <span class="c1"># doesn&#39;t work because of</span>
            <span class="c1"># https://github.com/pytorch/pytorch/issues/47442</span>
            <span class="c1"># Update the test in test_serialization if you remove &#39;meta&#39; from here</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lazy&quot;</span><span class="p">,</span> <span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="s2">&quot;mps&quot;</span><span class="p">,</span> <span class="s2">&quot;ort&quot;</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">,</span> <span class="s2">&quot;ipu&quot;</span><span class="p">]</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;The default implementation of __deepcopy__() for wrapper subclasses &quot;</span>
                        <span class="s2">&quot;only works for subclass types that implement clone() and for which &quot;</span>
                        <span class="s2">&quot;cloning returns another instance of the same subclass. You should either &quot;</span>
                        <span class="s2">&quot;properly implement clone() for your subclass or override __deepcopy__() &quot;</span>
                        <span class="s2">&quot;if it is intended behavior for clone() to return an instance of a &quot;</span>
                        <span class="s2">&quot;different type.&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_deepcopy</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
                    <span class="c1"># quantizer_params can be different type based on torch attribute</span>
                    <span class="n">quantizer_params</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
                        <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                        <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                    <span class="p">]</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
                        <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span>
                    <span class="p">):</span>
                        <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_scales</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_zero_points</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_axis</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Unsupported qscheme </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span><span class="si">}</span><span class="s2"> in deepcopy&quot;</span>
                        <span class="p">)</span>
                    <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
                    <span class="c1"># need to wrap with TypedStorage</span>
                    <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_qtensor</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">new_storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                            <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                        <span class="n">quantizer_params</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;The default implementation of __deepcopy__() for quantized tensors &quot;</span>
                            <span class="s2">&quot;expects the tensor returned by torch._utils._rebuild_qtensor() to &quot;</span>
                            <span class="s2">&quot;match the type of the instance being copied. If you encounter this, &quot;</span>
                            <span class="s2">&quot;please open an issue on PyTorch&#39;s GitHub.&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_empty</span><span class="p">([])</span>
                    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;The default implementation of __deepcopy__() for non-wrapper subclasses &quot;</span>
                            <span class="s2">&quot;only works for subclass types that implement new_empty() and for which &quot;</span>
                            <span class="s2">&quot;that function returns another instance of the same subclass. You should &quot;</span>
                            <span class="s2">&quot;either properly implement new_empty() for your subclass or override &quot;</span>
                            <span class="s2">&quot;__deepcopy__() if it is intended behavior for new_empty() to return &quot;</span>
                            <span class="s2">&quot;an instance of a different type.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">new_tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span>
                        <span class="n">new_storage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_conj</span><span class="p">():</span>
                        <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">new_tensor</span><span class="o">.</span><span class="n">conj_physical</span><span class="p">()</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_neg</span><span class="p">():</span>
                        <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">new_tensor</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">new_tensor</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">new_tensor</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">__deepcopy__</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Type of deepcopy result does not match the type of the source tensor. &quot;</span>
                        <span class="s2">&quot;If you encounter this, please open an issue on PyTorch&#39;s GitHub.&quot;</span>
                    <span class="p">)</span>

                <span class="c1"># Plain Tensors don&#39;t have slots</span>
                <span class="n">slots_to_save</span> <span class="o">=</span> <span class="n">copyreg</span><span class="o">.</span><span class="n">_slotnames</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">for</span> <span class="n">slot</span> <span class="ow">in</span> <span class="n">slots_to_save</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">):</span>
                        <span class="nb">setattr</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">,</span> <span class="n">slot</span><span class="p">,</span> <span class="n">deepcopy</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">),</span> <span class="n">memo</span><span class="p">))</span>

            <span class="n">new_tensor</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span> <span class="n">memo</span><span class="p">)</span>

            <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_tensor</span>
            <span class="k">return</span> <span class="n">new_tensor</span>

    <span class="k">def</span> <span class="nf">__reduce_ex__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_get_obj_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">state</span><span class="p">:</span>
            <span class="c1"># Fast path for regular tensor without Python state.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_ex_internal</span><span class="p">(</span><span class="n">proto</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__reduce_ex__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">)</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_ex_internal</span><span class="p">(</span><span class="n">proto</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">_rebuild_from_type_v2</span><span class="p">,</span> <span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">))</span>

<div class="viewcode-block" id="Tensor.storage"><a class="viewcode-back" href="../../generated/torch.Tensor.storage.html#torch.Tensor.storage">[docs]</a>    <span class="k">def</span> <span class="nf">storage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        storage() -&gt; torch.TypedStorage</span>

<span class="sd">        Returns the underlying :class:`TypedStorage`.</span>

<span class="sd">        .. warning::</span>

<span class="sd">            :class:`TypedStorage` is deprecated. It will be removed in the future, and</span>
<span class="sd">            :class:`UntypedStorage` will be the only storage class. To access the</span>
<span class="sd">            :class:`UntypedStorage` directly, use :attr:`Tensor.untyped_storage()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">storage</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span></div>

    <span class="c1"># For internal use only, to avoid raising deprecation warning</span>
    <span class="k">def</span> <span class="nf">_typed_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">untyped_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_reduce_ex_internal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">):</span>
        <span class="n">check_serializing_named_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># See Note [Don&#39;t serialize hooks]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">hooks</span><span class="o">.</span><span class="n">warn_if_has_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">backward_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="c1"># Note: Numpy array is chosen to be the rebuild component for XLA, ORT Tensors.</span>
        <span class="c1"># We considered a few options:</span>
        <span class="c1"># 1. CPU tensor can&#39;t be used here.</span>
        <span class="c1">#    Otherwise in torch.load CPU storage is reconstructed with randomly</span>
        <span class="c1">#    initialized data, moved onto backend device, and then storage is updated</span>
        <span class="c1">#    to the serialized content. This works perfectly for CPU/CUDA but not these backends;</span>
        <span class="c1">#    their tensors are disconnected with storage so they don&#39;t get the update.</span>
        <span class="c1"># 2. Python list is not a good fit due to performance reason.</span>
        <span class="c1">#    `tolist()` converts every single element in the tensor into python objects</span>
        <span class="c1">#    and serialize them one by one.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="s2">&quot;ort&quot;</span><span class="p">]</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Convert BFloat16 tesors to Float32 before conversion to numpy, as numpy doesn&#39;t</span>
            <span class="c1"># support BFloat16. The rebuild tensor from numpy takes in the original self.dtype,</span>
            <span class="c1"># this would reconstruct the BFloat16 tensor from numpy.</span>
            <span class="n">numpy_tensor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_device_tensor_from_numpy</span><span class="p">,</span>
                <span class="p">(</span><span class="n">numpy_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;meta&quot;</span><span class="p">:</span>
            <span class="c1"># NB: This implementation BREAKS storage sharing.  Current</span>
            <span class="c1"># hypothesis is that no one cares for meta tensors.</span>
            <span class="n">arg_meta</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_meta_tensor_no_storage</span><span class="p">,</span> <span class="n">arg_meta</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
            <span class="c1"># quantizer_params can be different type based on torch attribute</span>
            <span class="n">quantizer_params</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
                <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
                <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="c1"># convert scales and zero points to tuple to avoid recursive calls</span>
                <span class="c1"># when/if we get multi-axis quantized tensors in the future, the shape</span>
                <span class="c1"># is recoverable from the main tensor shape</span>
                <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_scales</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_zero_points</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_axis</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Serialization is not supported for tensors of type </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
            <span class="c1"># need to wrap with TypedStorage</span>
            <span class="n">args_qtensor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="n">quantizer_params</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                <span class="n">backward_hooks</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_qtensor</span><span class="p">,</span> <span class="n">args_qtensor</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
                <span class="n">args_sparse</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">()),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="s2">&quot;sparse tensor __reduce_ex__ for layout `</span><span class="si">%s</span><span class="s2">`&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_sparse_tensor</span><span class="p">,</span> <span class="n">args_sparse</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csc</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsc</span><span class="p">,</span>
        <span class="p">}:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="ow">in</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr</span><span class="p">}:</span>
                <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">crow_indices</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">col_indices</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ccol_indices</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">row_indices</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="n">args_sparse_compressed</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="n">compressed_indices</span><span class="p">,</span>
                    <span class="n">plain_indices</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_sparse_tensor</span><span class="p">,</span> <span class="n">args_sparse_compressed</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__torch_dispatch__</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__torch_dispatch__</span>
        <span class="p">):</span>
            <span class="n">arg_wrapper_subclass</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_wrapper_subclass</span><span class="p">,</span> <span class="n">arg_wrapper_subclass</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: Once we decide to break serialization FC, no longer</span>
            <span class="c1"># need to wrap with TypedStorage</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                <span class="n">backward_hooks</span><span class="p">,</span>
            <span class="p">)</span>  <span class="c1"># previously was self._backward_hooks</span>

            <span class="n">metadata</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">get_tensor_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">metadata</span><span class="p">:</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="n">metadata</span><span class="p">,)</span>  <span class="c1"># type: ignore[assignment]</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_tensor_v2</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># Warning: this method is NOT called when you torch.load() a tensor;</span>
        <span class="c1"># that is managed by _rebuild_tensor_v2</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;__setstate__ can be only called on leaf Tensors&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="c1"># legacy serialization of Tensor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="o">*</span><span class="n">state</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="c1"># legacy serialization of Variable</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="c1"># The setting of _backward_hooks is expected to be a no-op.</span>
        <span class="c1"># See Note [Don&#39;t serialize hooks]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="n">tensor_contents</span>
            <span class="p">)</span>
        <span class="c1"># All strings are unicode in Python 3.</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_tensor_str</span><span class="o">.</span><span class="n">_str</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_contents</span><span class="o">=</span><span class="n">tensor_contents</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.backward"><a class="viewcode-back" href="../../generated/torch.Tensor.backward.html#torch.Tensor.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the gradient of current tensor wrt graph leaves.</span>

<span class="sd">        The graph is differentiated using the chain rule. If the tensor is</span>
<span class="sd">        non-scalar (i.e. its data has more than one element) and requires</span>
<span class="sd">        gradient, the function additionally requires specifying ``gradient``.</span>
<span class="sd">        It should be a tensor of matching type and location, that contains</span>
<span class="sd">        the gradient of the differentiated function w.r.t. ``self``.</span>

<span class="sd">        This function accumulates gradients in the leaves - you might need to zero</span>
<span class="sd">        ``.grad`` attributes or set them to ``None`` before calling it.</span>
<span class="sd">        See :ref:`Default gradient layouts&lt;default-grad-layouts&gt;`</span>
<span class="sd">        for details on the memory layout of accumulated gradients.</span>

<span class="sd">        .. note::</span>

<span class="sd">            If you run any forward ops, create ``gradient``, and/or call ``backward``</span>
<span class="sd">            in a user-specified CUDA stream context, see</span>
<span class="sd">            :ref:`Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;`.</span>

<span class="sd">        .. note::</span>

<span class="sd">            When ``inputs`` are provided and a given input is not a leaf,</span>
<span class="sd">            the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).</span>
<span class="sd">            It is an implementation detail on which the user should not rely.</span>
<span class="sd">            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.</span>

<span class="sd">        Args:</span>
<span class="sd">            gradient (Tensor or None): Gradient w.r.t. the</span>
<span class="sd">                tensor. If it is a tensor, it will be automatically converted</span>
<span class="sd">                to a Tensor that does not require grad unless ``create_graph`` is True.</span>
<span class="sd">                None values can be specified for scalar Tensors or ones that</span>
<span class="sd">                don&#39;t require grad. If a None value would be acceptable then</span>
<span class="sd">                this argument is optional.</span>
<span class="sd">            retain_graph (bool, optional): If ``False``, the graph used to compute</span>
<span class="sd">                the grads will be freed. Note that in nearly all cases setting</span>
<span class="sd">                this option to True is not needed and often can be worked around</span>
<span class="sd">                in a much more efficient way. Defaults to the value of</span>
<span class="sd">                ``create_graph``.</span>
<span class="sd">            create_graph (bool, optional): If ``True``, graph of the derivative will</span>
<span class="sd">                be constructed, allowing to compute higher order derivative</span>
<span class="sd">                products. Defaults to ``False``.</span>
<span class="sd">            inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be</span>
<span class="sd">                accumulated into ``.grad``. All other Tensors will be ignored. If not</span>
<span class="sd">                provided, the gradient is accumulated into all the leaf Tensors that were</span>
<span class="sd">                used to compute the attr::tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">gradient</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span>
                <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
                <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.register_hook"><a class="viewcode-back" href="../../generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook">[docs]</a>    <span class="k">def</span> <span class="nf">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook.</span>

<span class="sd">        The hook will be called every time a gradient with respect to the</span>
<span class="sd">        Tensor is computed. The hook should have the following signature::</span>

<span class="sd">            hook(grad) -&gt; Tensor or None</span>


<span class="sd">        The hook should not modify its argument, but it can optionally return</span>
<span class="sd">        a new gradient which will be used in place of :attr:`grad`.</span>

<span class="sd">        This function returns a handle with a method ``handle.remove()``</span>
<span class="sd">        that removes the hook from the module.</span>

<span class="sd">        .. note::</span>
<span class="sd">            See :ref:`backward-hooks-execution` for more information on how when this hook</span>
<span class="sd">            is executed, and how its execution is ordered relative to other hooks.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient</span>
<span class="sd">            &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))</span>
<span class="sd">            &gt;&gt;&gt; v.grad</span>

<span class="sd">             2</span>
<span class="sd">             4</span>
<span class="sd">             6</span>
<span class="sd">            [torch.FloatTensor of size (3,)]</span>

<span class="sd">            &gt;&gt;&gt; h.remove()  # removes the hook</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">register_hook</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;cannot register a hook on a tensor that &quot;</span> <span class="s2">&quot;doesn&#39;t require gradient&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_register_hook_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">return</span> <span class="n">handle</span></div>

    <span class="k">def</span> <span class="nf">reinforce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">trim</span><span class="p">(</span><span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)])</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="n">trim</span><span class="p">(</span>
                <span class="sa">r</span><span class="sd">&quot;&quot;&quot;reinforce() was removed.</span>
<span class="sd">            Use torch.distributions instead.</span>
<span class="sd">            See https://pytorch.org/docs/master/distributions.html</span>

<span class="sd">            Instead of:</span>

<span class="sd">            probs = policy_network(state)</span>
<span class="sd">            action = probs.multinomial()</span>
<span class="sd">            next_state, reward = env.step(action)</span>
<span class="sd">            action.reinforce(reward)</span>
<span class="sd">            action.backward()</span>

<span class="sd">            Use:</span>

<span class="sd">            probs = policy_network(state)</span>
<span class="sd">            # NOTE: categorical is equivalent to what used to be called multinomial</span>
<span class="sd">            m = torch.distributions.Categorical(probs)</span>
<span class="sd">            action = m.sample()</span>
<span class="sd">            next_state, reward = env.step(action)</span>
<span class="sd">            loss = -m.log_prob(action) * reward</span>
<span class="sd">            loss.backward()</span>
<span class="sd">        &quot;&quot;&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">detach</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_add_docstr</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">detach</span><span class="p">,</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new Tensor, detached from the current graph.</span>

<span class="sd">    The result will never require gradient.</span>

<span class="sd">    This method also affects forward mode AD gradients and the result will never</span>
<span class="sd">    have forward mode AD gradients.</span>

<span class="sd">    .. note::</span>

<span class="sd">      Returned Tensor shares the same storage with the original one.</span>
<span class="sd">      In-place modifications on either of them will be seen, and may trigger</span>
<span class="sd">      errors in correctness checks.</span>
<span class="sd">      IMPORTANT NOTE: Previously, in-place size / stride / storage changes</span>
<span class="sd">      (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor</span>
<span class="sd">      also update the original tensor. Now, these in-place changes will not update the</span>
<span class="sd">      original tensor anymore, and will instead trigger an error.</span>
<span class="sd">      For sparse tensors:</span>
<span class="sd">      In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the</span>
<span class="sd">      returned tensor will not update the original tensor anymore, and will instead</span>
<span class="sd">      trigger an error.</span>
<span class="sd">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">detach_</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_add_docstr</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">detach_</span><span class="p">,</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Detaches the Tensor from the graph that created it, making it a leaf.</span>
<span class="sd">    Views cannot be detached in-place.</span>

<span class="sd">    This method also affects forward mode AD gradients and the result will never</span>
<span class="sd">    have forward mode AD gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>

<div class="viewcode-block" id="Tensor.is_shared"><a class="viewcode-back" href="../../generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared">[docs]</a>    <span class="k">def</span> <span class="nf">is_shared</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Checks if tensor is in shared memory.</span>

<span class="sd">        This is always ``True`` for CUDA tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">is_shared</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_is_shared</span><span class="p">()</span></div>

<div class="viewcode-block" id="Tensor.share_memory_"><a class="viewcode-back" href="../../generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_">[docs]</a>    <span class="k">def</span> <span class="nf">share_memory_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the underlying storage to shared memory.</span>

<span class="sd">        This is a no-op if the underlying storage is already in shared memory</span>
<span class="sd">        and for CUDA tensors. Tensors in shared memory cannot be resized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_share_memory_</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="fm">__reversed__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reverses the tensor along dimension 0.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__reversed__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.norm"><a class="viewcode-back" href="../../generated/torch.Tensor.norm.html#torch.Tensor.norm">[docs]</a>    <span class="k">def</span> <span class="nf">norm</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;fro&quot;</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.norm`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="n">solve</span>

        <span class="k">return</span> <span class="n">solve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">lstsq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="n">lstsq</span>

        <span class="k">return</span> <span class="n">lstsq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">eig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="n">eig</span>

        <span class="k">return</span> <span class="n">eig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="n">eigenvectors</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">symeig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="n">_symeig</span>

        <span class="k">return</span> <span class="n">_symeig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="n">eigenvectors</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.lu"><a class="viewcode-back" href="../../generated/torch.Tensor.lu.html#torch.Tensor.lu">[docs]</a>    <span class="k">def</span> <span class="nf">lu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">get_infos</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.lu`&quot;&quot;&quot;</span>
        <span class="c1"># If get_infos is True, then we don&#39;t need to check for errors and vice versa</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">lu</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="n">pivot</span><span class="p">,</span> <span class="n">get_infos</span><span class="o">=</span><span class="n">get_infos</span>
            <span class="p">)</span>

        <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_lu_with_info</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="n">pivot</span><span class="p">,</span> <span class="n">check_errors</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="n">get_infos</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">get_infos</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">infos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span></div>

<div class="viewcode-block" id="Tensor.stft"><a class="viewcode-back" href="../../generated/torch.Tensor.stft.html#torch.Tensor.stft">[docs]</a>    <span class="k">def</span> <span class="nf">stft</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_fft</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hop_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">win_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">window</span><span class="p">:</span> <span class="s2">&quot;Optional[Tensor]&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">center</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reflect&quot;</span><span class="p">,</span>
        <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">onesided</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_complex</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.stft`</span>

<span class="sd">        .. warning::</span>
<span class="sd">          This function changed signature at version 0.4.1. Calling with</span>
<span class="sd">          the previous signature may cause error or return incorrect result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">stft</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">n_fft</span><span class="p">,</span>
                <span class="n">hop_length</span><span class="o">=</span><span class="n">hop_length</span><span class="p">,</span>
                <span class="n">win_length</span><span class="o">=</span><span class="n">win_length</span><span class="p">,</span>
                <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">,</span>
                <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span>
                <span class="n">pad_mode</span><span class="o">=</span><span class="n">pad_mode</span><span class="p">,</span>
                <span class="n">normalized</span><span class="o">=</span><span class="n">normalized</span><span class="p">,</span>
                <span class="n">onesided</span><span class="o">=</span><span class="n">onesided</span><span class="p">,</span>
                <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stft</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_fft</span><span class="p">,</span>
            <span class="n">hop_length</span><span class="p">,</span>
            <span class="n">win_length</span><span class="p">,</span>
            <span class="n">window</span><span class="p">,</span>
            <span class="n">center</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">normalized</span><span class="p">,</span>
            <span class="n">onesided</span><span class="p">,</span>
            <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.istft"><a class="viewcode-back" href="../../generated/torch.Tensor.istft.html#torch.Tensor.istft">[docs]</a>    <span class="k">def</span> <span class="nf">istft</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_fft</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hop_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">win_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">window</span><span class="p">:</span> <span class="s2">&quot;Optional[Tensor]&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">center</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">onesided</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_complex</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.istft`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">istft</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">n_fft</span><span class="p">,</span>
                <span class="n">hop_length</span><span class="o">=</span><span class="n">hop_length</span><span class="p">,</span>
                <span class="n">win_length</span><span class="o">=</span><span class="n">win_length</span><span class="p">,</span>
                <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">,</span>
                <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span>
                <span class="n">normalized</span><span class="o">=</span><span class="n">normalized</span><span class="p">,</span>
                <span class="n">onesided</span><span class="o">=</span><span class="n">onesided</span><span class="p">,</span>
                <span class="n">length</span><span class="o">=</span><span class="n">length</span><span class="p">,</span>
                <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">istft</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">n_fft</span><span class="p">,</span>
            <span class="n">hop_length</span><span class="p">,</span>
            <span class="n">win_length</span><span class="p">,</span>
            <span class="n">window</span><span class="p">,</span>
            <span class="n">center</span><span class="p">,</span>
            <span class="n">normalized</span><span class="p">,</span>
            <span class="n">onesided</span><span class="p">,</span>
            <span class="n">length</span><span class="p">,</span>
            <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">resize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">sizes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">resize</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">sizes</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;non-inplace resize is deprecated&quot;</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.autograd._functions</span> <span class="kn">import</span> <span class="n">Resize</span>

        <span class="k">return</span> <span class="n">Resize</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resize_as</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">resize_as</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;non-inplace resize_as is deprecated&quot;</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.autograd._functions</span> <span class="kn">import</span> <span class="n">Resize</span>

        <span class="k">return</span> <span class="n">Resize</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<div class="viewcode-block" id="Tensor.split"><a class="viewcode-back" href="../../generated/torch.Tensor.split.html#torch.Tensor.split">[docs]</a>    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.split`&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">split</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">split_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">split_size</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_VF</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_VF</span><span class="o">.</span><span class="n">split_with_sizes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unique"><a class="viewcode-back" href="../../generated/torch.Tensor.unique.html#torch.Tensor.unique">[docs]</a>    <span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the unique elements of the input tensor.</span>

<span class="sd">        See :func:`torch.unique`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">unique</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
                <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
                <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
                <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
            <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
            <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unique_consecutive"><a class="viewcode-back" href="../../generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive">[docs]</a>    <span class="k">def</span> <span class="nf">unique_consecutive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Eliminates all but the first element from every consecutive group of equivalent elements.</span>

<span class="sd">        See :func:`torch.unique_consecutive`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
                <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span>
                <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
        <span class="p">)</span></div>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">rsub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="nf">__rdiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">()</span> <span class="o">*</span> <span class="n">other</span>

    <span class="fm">__rtruediv__</span> <span class="o">=</span> <span class="n">__rdiv__</span>
    <span class="fm">__itruediv__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">__idiv__</span>

    <span class="fm">__pow__</span> <span class="o">=</span> <span class="n">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">pow</span>
    <span class="p">)</span>
    <span class="fm">__ipow__</span> <span class="o">=</span> <span class="n">_handle_torch_function_and_wrap_type_error_to_not_implemented</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">pow_</span>
    <span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rmod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__format__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_meta</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="n">format_spec</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">result_type</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rfloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rlshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_left_shift</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rrshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bitwise_right_shift</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="fm">__pos__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">positive</span>
    <span class="fm">__neg__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">neg</span>
    <span class="fm">__abs__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">abs</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__len__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;len() of a 0-d tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Using len to get tensor shape might cause the trace to be incorrect. &quot;</span>
                <span class="s2">&quot;Recommended usage would be tensor.shape[0]. &quot;</span>
                <span class="s2">&quot;Passing a tensor of different shape might lead to errors or silently give &quot;</span>
                <span class="s2">&quot;incorrect results.&quot;</span><span class="p">,</span>
                <span class="n">category</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">TracerWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># NB: we use &#39;imap&#39; and not &#39;map&#39; here, so that in Python 2 we get a</span>
        <span class="c1"># generator and don&#39;t eagerly perform all the indexes.  This could</span>
        <span class="c1"># save us work, and also helps keep trace ordering deterministic</span>
        <span class="c1"># (e.g., if you zip(*hiddens), the eager map will force all the</span>
        <span class="c1"># indexes of hiddens[0] before hiddens[1], while the generator</span>
        <span class="c1"># map will interleave them.)</span>
        <span class="c1"># NB: We have intentionally skipped __torch_function__ dispatch here.</span>
        <span class="c1"># See gh-54457</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;iteration over a 0-d tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Iterating over a tensor might cause the trace to be incorrect. &quot;</span>
                <span class="s2">&quot;Passing a tensor of different shape won&#39;t change the number of &quot;</span>
                <span class="s2">&quot;iterations executed (and might lead to errors or silently give &quot;</span>
                <span class="s2">&quot;incorrect results).&quot;</span><span class="p">,</span>
                <span class="n">category</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">TracerWarning</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Do NOT handle __torch_function__ here as user&#39;s default</span>
        <span class="c1"># implementation that handle most functions will most likely do it wrong.</span>
        <span class="c1"># It can be easily overridden by defining this method on the user</span>
        <span class="c1"># subclass if needed.</span>
        <span class="k">return</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">tensor_methods</span> <span class="o">=</span> <span class="nb">dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
        <span class="n">tensor_methods</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;volatile&quot;</span><span class="p">)</span>  <span class="c1"># deprecated</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">tensor_methods</span> <span class="o">+</span> <span class="n">attrs</span>

        <span class="c1"># property only available dense, cuda tensors</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="n">keys</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;__cuda_array_interface__&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>

    <span class="c1"># Numpy array interface, to support `numpy.asarray(tensor) -&gt; ndarray`</span>
    <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># prefer Tensor ops over numpy ones</span>

    <span class="k">def</span> <span class="nf">__array__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__array__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Wrap Numpy array again in a suitable tensor when done, to support e.g.</span>
    <span class="c1"># `numpy.sin(tensor) -&gt; tensor` or `numpy.greater(tensor, 0) -&gt; ByteTensor`</span>
    <span class="k">def</span> <span class="nf">__array_wrap__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">array</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">__array_wrap__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">array</span><span class="o">=</span><span class="n">array</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">bool</span><span class="p">:</span>
            <span class="c1"># Workaround, torch has no built-in bool tensor</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">element</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check if `element` is present in tensor</span>

<span class="sd">        Args:</span>
<span class="sd">            element (Tensor or scalar): element to be checked</span>
<span class="sd">                for presence in current tensor&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__contains__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">element</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">element</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Number</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymFloat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymBool</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="c1"># type hint doesn&#39;t understand the __contains__ result array</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">element</span> <span class="o">==</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># type: ignore[union-attr]</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Tensor.__contains__ only supports Tensor or scalar, but you passed in a </span><span class="si">%s</span><span class="s2">.&quot;</span>
            <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">__cuda_array_interface__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Array view description for cuda tensors.</span>

<span class="sd">        See:</span>
<span class="sd">        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="c1"># TODO mypy doesn&#39;t support @property, see: https://github.com/python/mypy/issues/6185</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__cuda_array_interface__</span><span class="o">.</span><span class="fm">__get__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

        <span class="c1"># raise AttributeError for unsupported tensors, so that</span>
        <span class="c1"># hasattr(cpu_tensor, &quot;__cuda_array_interface__&quot;) is False.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on non-CUDA tensor type: </span><span class="si">%s</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;If CUDA data is required use tensor.cuda() to copy tensor to device memory.&quot;</span>
                <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on sparse type: </span><span class="si">%s</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;Use Tensor.to_dense() to convert to a dense tensor first.&quot;</span>
                <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="c1"># RuntimeError, matching tensor.__array__() behavior.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on Variable that requires grad. &quot;</span>
                <span class="s2">&quot;If gradients aren&#39;t required, use var.detach() to get Variable that doesn&#39;t require grad.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># CUDA devices are little-endian and tensors are stored in native byte</span>
        <span class="c1"># order. 1-byte entries are endian-agnostic.</span>
        <span class="n">typestr</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span> <span class="s2">&quot;&lt;c8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span> <span class="s2">&quot;&lt;c16&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="s2">&quot;&lt;f2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="s2">&quot;&lt;f4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="s2">&quot;&lt;f8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="s2">&quot;|u1&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="s2">&quot;|i1&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="s2">&quot;&lt;i2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="s2">&quot;&lt;i4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="s2">&quot;&lt;i8&quot;</span><span class="p">,</span>
        <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>

        <span class="n">itemsize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
            <span class="c1"># __cuda_array_interface__ v2 requires the strides to be omitted</span>
            <span class="c1"># (either not set or set to None) for C-contiguous arrays.</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n">itemsize</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
        <span class="n">data_ptr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_ptr</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>  <span class="c1"># read-only is false</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">typestr</span><span class="o">=</span><span class="n">typestr</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.storage_type"><a class="viewcode-back" href="../../generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type">[docs]</a>    <span class="k">def</span> <span class="nf">storage_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;storage_type() -&gt; type</span>

<span class="sd">        Returns the type of the underlying storage.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">storage_type</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_warn_typed_storage_removal</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_get_legacy_storage_class</span><span class="p">()</span></div>

<div class="viewcode-block" id="Tensor.refine_names"><a class="viewcode-back" href="../../named_tensor.html#torch.Tensor.refine_names">[docs]</a>    <span class="k">def</span> <span class="nf">refine_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Refines the dimension names of :attr:`self` according to :attr:`names`.</span>

<span class="sd">        Refining is a special case of renaming that &quot;lifts&quot; unnamed dimensions.</span>
<span class="sd">        A ``None`` dim can be refined to have any name; a named dim can only be</span>
<span class="sd">        refined to have the same name.</span>

<span class="sd">        Because named tensors can coexist with unnamed tensors, refining names</span>
<span class="sd">        gives a nice way to write named-tensor-aware code that works with both</span>
<span class="sd">        named and unnamed tensors.</span>

<span class="sd">        :attr:`names` may contain up to one Ellipsis (``...``).</span>
<span class="sd">        The Ellipsis is expanded greedily; it is expanded in-place to fill</span>
<span class="sd">        :attr:`names` to the same length as ``self.dim()`` using names from the</span>
<span class="sd">        corresponding indices of ``self.names``.</span>

<span class="sd">        Python 2 does not support Ellipsis but one may use a string literal</span>
<span class="sd">        instead (``&#39;...&#39;``).</span>

<span class="sd">        Args:</span>
<span class="sd">            names (iterable of str): The desired names of the output tensor. May</span>
<span class="sd">                contain up to one Ellipsis.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; imgs = torch.randn(32, 3, 128, 128)</span>
<span class="sd">            &gt;&gt;&gt; named_imgs = imgs.refine_names(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>
<span class="sd">            &gt;&gt;&gt; named_imgs.names</span>
<span class="sd">            (&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="sd">            &gt;&gt;&gt; tensor = torch.randn(2, 3, 5, 7, 11)</span>
<span class="sd">            &gt;&gt;&gt; tensor = tensor.refine_names(&#39;A&#39;, ..., &#39;B&#39;, &#39;C&#39;)</span>
<span class="sd">            &gt;&gt;&gt; tensor.names</span>
<span class="sd">            (&#39;A&#39;, None, None, &#39;B&#39;, &#39;C&#39;)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">)</span>
        <span class="n">names</span> <span class="o">=</span> <span class="n">resolve_ellipsis</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">names</span><span class="p">,</span> <span class="s2">&quot;refine_names&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="n">names</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.align_to"><a class="viewcode-back" href="../../named_tensor.html#torch.Tensor.align_to">[docs]</a>    <span class="k">def</span> <span class="nf">align_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Permutes the dimensions of the :attr:`self` tensor to match the order</span>
<span class="sd">        specified in :attr:`names`, adding size-one dims for any new names.</span>

<span class="sd">        All of the dims of :attr:`self` must be named in order to use this method.</span>
<span class="sd">        The resulting tensor is a view on the original tensor.</span>

<span class="sd">        All dimension names of :attr:`self` must be present in :attr:`names`.</span>
<span class="sd">        :attr:`names` may contain additional names that are not in ``self.names``;</span>
<span class="sd">        the output tensor has a size-one dimension for each of those new names.</span>

<span class="sd">        :attr:`names` may contain up to one Ellipsis (``...``).</span>
<span class="sd">        The Ellipsis is expanded to be equal to all dimension names of :attr:`self`</span>
<span class="sd">        that are not mentioned in :attr:`names`, in the order that they appear</span>
<span class="sd">        in :attr:`self`.</span>

<span class="sd">        Python 2 does not support Ellipsis but one may use a string literal</span>
<span class="sd">        instead (``&#39;...&#39;``).</span>

<span class="sd">        Args:</span>
<span class="sd">            names (iterable of str): The desired dimension ordering of the</span>
<span class="sd">                output tensor. May contain up to one Ellipsis that is expanded</span>
<span class="sd">                to all unmentioned dim names of :attr:`self`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; tensor = torch.randn(2, 2, 2, 2, 2, 2)</span>
<span class="sd">            &gt;&gt;&gt; named_tensor = tensor.refine_names(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;)</span>

<span class="sd">            # Move the F and E dims to the front while keeping the rest in order</span>
<span class="sd">            &gt;&gt;&gt; named_tensor.align_to(&#39;F&#39;, &#39;E&#39;, ...)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">align_to</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">)</span>
        <span class="n">ellipsis_idx</span> <span class="o">=</span> <span class="n">single_ellipsis_index</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="s2">&quot;align_to&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ellipsis_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span>
            <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_ellipsis</span><span class="p">(</span><span class="n">name</span><span class="p">)],</span> <span class="n">ellipsis_idx</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unflatten"><a class="viewcode-back" href="../../generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten">[docs]</a>    <span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        unflatten(dim, sizes) -&gt; Tensor</span>

<span class="sd">        See :func:`torch.unflatten`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">unflatten</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;unflatten: sizes must be non-empty&quot;</span><span class="p">)</span>

        <span class="n">names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">OrderedDict</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="n">names</span><span class="p">,</span> <span class="n">sizes</span> <span class="o">=</span> <span class="n">unzip_namedshape</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.rename_"><a class="viewcode-back" href="../../named_tensor.html#torch.Tensor.rename_">[docs]</a>    <span class="k">def</span> <span class="nf">rename_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;In-place version of :meth:`~Tensor.rename`.&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">rename_</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span>
            <span class="p">)</span>

        <span class="c1"># Note [rename_ / rename API]</span>
        <span class="c1"># The Python API for these is different from the C++ API. In Python:</span>
        <span class="c1"># 1) tensor.rename(*names) takes a vararglist of names</span>
        <span class="c1"># 2) tensor.rename(**rename_map) takes a map of names to rename.</span>
        <span class="c1"># C++ is static, making it difficult to implement similar behavior.</span>
        <span class="k">return</span> <span class="n">update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">rename_map</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.rename"><a class="viewcode-back" href="../../named_tensor.html#torch.Tensor.rename">[docs]</a>    <span class="k">def</span> <span class="nf">rename</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Renames dimension names of :attr:`self`.</span>

<span class="sd">        There are two main usages:</span>

<span class="sd">        ``self.rename(**rename_map)`` returns a view on tensor that has dims</span>
<span class="sd">        renamed as specified in the mapping :attr:`rename_map`.</span>

<span class="sd">        ``self.rename(*names)`` returns a view on tensor, renaming all</span>
<span class="sd">        dimensions positionally using :attr:`names`.</span>
<span class="sd">        Use ``self.rename(None)`` to drop names on a tensor.</span>

<span class="sd">        One cannot specify both positional args :attr:`names` and keyword args</span>
<span class="sd">        :attr:`rename_map`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; imgs = torch.rand(2, 3, 5, 7, names=(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;))</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(N=&#39;batch&#39;, C=&#39;channels&#39;)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (&#39;batch&#39;, &#39;channels&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(None)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (None, None, None, None)</span>

<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">rename</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span>
            <span class="p">)</span>

        <span class="c1"># See Note [rename_ / rename API]</span>
        <span class="k">return</span> <span class="n">update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">rename_map</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.to_sparse_coo"><a class="viewcode-back" href="../../generated/torch.Tensor.to_sparse_coo.html#torch.Tensor.to_sparse_coo">[docs]</a>    <span class="k">def</span> <span class="nf">to_sparse_coo</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Convert a tensor to :ref:`coordinate format &lt;sparse-coo-docs&gt;`.</span>

<span class="sd">        Examples::</span>

<span class="sd">             &gt;&gt;&gt; dense = torch.randn(5, 5)</span>
<span class="sd">             &gt;&gt;&gt; sparse = dense.to_sparse_coo()</span>
<span class="sd">             &gt;&gt;&gt; sparse._nnz()</span>
<span class="sd">             25</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span></div>

    <span class="k">def</span> <span class="nf">_update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">inplace</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">_update_names</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">inplace</span>
            <span class="p">)</span>

        <span class="c1"># See Note [rename_ / rename API]</span>
        <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">rename_</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_function__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This __torch_function__ implementation wraps subclasses such that</span>
<span class="sd">        methods called on subclasses return a subclass instance instead of</span>
<span class="sd">        a ``torch.Tensor`` instance.</span>

<span class="sd">        One corollary to this is that you need coverage for torch.Tensor</span>
<span class="sd">        methods if implementing __torch_function__ for subclasses.</span>

<span class="sd">        We recommend always calling ``super().__torch_function__`` as the base</span>
<span class="sd">        case when doing the above.</span>

<span class="sd">        While not mandatory, we recommend making `__torch_function__` a classmethod.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">types</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

        <span class="k">with</span> <span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">get_default_nowrap_functions</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">ret</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span>

    <span class="n">__torch_dispatch__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_disabled_torch_dispatch_impl</span>

    <span class="k">def</span> <span class="nf">__dlpack__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_</span>
<span class="sd">        of the current tensor to be exported to other libraries.</span>

<span class="sd">        This function will be called from the `from_dlpack` method</span>
<span class="sd">        of the library that will consume the capsule. `from_dlpack` passes the current</span>
<span class="sd">        stream to this method as part of the specification.</span>

<span class="sd">        Args:</span>
<span class="sd">            stream (integer or None): An optional Python integer representing a</span>
<span class="sd">            pointer to a CUDA stream. The current stream is synchronized with</span>
<span class="sd">            this stream before the capsule is created, and since the capsule</span>
<span class="sd">            shares its storage with the tensor this make it safe to access from</span>
<span class="sd">            both streams.  If None or -1 is passed then no synchronization is performed.</span>
<span class="sd">            If 1 (on CUDA) or 0 (on ROCM) then the default stream is used for</span>
<span class="sd">            synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__dlpack__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>

        <span class="c1"># DLPack capsules can&#39;t capture all of PyTorch&#39;s semantics,</span>
        <span class="c1"># so we prohibit exporting tensors that would lose their properties like</span>
        <span class="c1"># requires_grad and having the conjugate bit set.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t export tensors that require gradient, use tensor.detach()&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_conj</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t export tensors with the conjugate bit set&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t export tensors with layout other than torch.strided&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">int</span><span class="p">:</span>
            <span class="c1"># Stream pointers in CUDA/ROCm are uniquely numbered and can</span>
            <span class="c1"># be retrieved from their integer value.</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;stream must be ``int`` or ``none``&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">stream</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
                <span class="c1"># NB: This logic handles the special case values for default</span>
                <span class="c1"># streams and must be kept in sync with from_dlpack in</span>
                <span class="c1"># torch/utils/dlpack.py</span>
                <span class="k">if</span> <span class="n">stream</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">()</span>
                <span class="k">elif</span> <span class="n">stream</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ExternalStream</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
                <span class="c1"># Only synchronize on different streams</span>
                <span class="n">sync_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">stream</span> <span class="o">!=</span> <span class="n">sync_stream</span><span class="p">:</span>
                    <span class="n">event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>
                    <span class="n">event</span><span class="o">.</span><span class="n">record</span><span class="p">(</span><span class="n">sync_stream</span><span class="p">)</span>
                    <span class="n">stream</span><span class="o">.</span><span class="n">wait_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__dlpack_device__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__dlpack_device__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">torch_device_type</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span>
        <span class="k">if</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLROCM</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_pinned</span><span class="p">():</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLCPUPinned</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLGPU</span>
        <span class="k">elif</span> <span class="n">torch_device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLCPU</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;xpu&quot;</span><span class="p">:</span>
            <span class="n">device_type</span> <span class="o">=</span> <span class="n">DLDeviceType</span><span class="o">.</span><span class="n">kDLOneAPI</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Unknown device type </span><span class="si">{}</span><span class="s2"> for Dlpack&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch_device_type</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch&quot;</span>


<span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">cls</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="c1"># Also handles things like namedtuples</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">ret</span><span class="p">)(</span><span class="n">_convert</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ret</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/sphinx_highlight.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.serialization &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/serialization.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/serialization.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
          <li><a href="../torch.html">torch</a> &gt;</li>
        
      <li>torch.serialization</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.serialization</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">difflib</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">struct</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">tarfile</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">closing</span><span class="p">,</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_import_dotted_name</span>
<span class="kn">from</span> <span class="nn">torch._sources</span> <span class="kn">import</span> <span class="n">get_source_lines_and_file</span>
<span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">Storage</span>
<span class="kn">from</span> <span class="nn">torch.storage</span> <span class="kn">import</span> <span class="n">_get_dtype_from_pickle_storage_type</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">BinaryIO</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">cast</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">IO</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypeAlias</span>  <span class="c1"># Python 3.10+</span>
<span class="kn">import</span> <span class="nn">copyreg</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">torch._weights_only_unpickler</span> <span class="k">as</span> <span class="nn">_weights_only_unpickler</span>

<span class="n">DEFAULT_PROTOCOL</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">LONG_SIZE</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">Struct</span><span class="p">(</span><span class="s1">&#39;=l&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span>
<span class="n">INT_SIZE</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">Struct</span><span class="p">(</span><span class="s1">&#39;=i&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span>
<span class="n">SHORT_SIZE</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">Struct</span><span class="p">(</span><span class="s1">&#39;=h&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span>

<span class="n">MAGIC_NUMBER</span> <span class="o">=</span> <span class="mh">0x1950a86a20f9469cfc6c</span>
<span class="n">PROTOCOL_VERSION</span> <span class="o">=</span> <span class="mi">1001</span>
<span class="n">STORAGE_KEY_SEPARATOR</span> <span class="o">=</span> <span class="s1">&#39;,&#39;</span>

<span class="n">FILE_LIKE</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">,</span> <span class="n">BinaryIO</span><span class="p">,</span> <span class="n">IO</span><span class="p">[</span><span class="nb">bytes</span><span class="p">]]</span>
<span class="n">MAP_LOCATION</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span>
<span class="n">STORAGE</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Storage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">]</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;SourceChangeWarning&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mkdtemp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;register_package&#39;</span><span class="p">,</span>
    <span class="s1">&#39;check_module_version_greater_or_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;validate_cuda_device&#39;</span><span class="p">,</span>
    <span class="s1">&#39;validate_hpu_device&#39;</span><span class="p">,</span>
    <span class="s1">&#39;location_tag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;default_restore_location&#39;</span><span class="p">,</span>
    <span class="s1">&#39;normalize_storage_type&#39;</span><span class="p">,</span>
    <span class="s1">&#39;storage_to_tensor_type&#39;</span><span class="p">,</span>
    <span class="s1">&#39;save&#39;</span><span class="p">,</span>
    <span class="s1">&#39;load&#39;</span><span class="p">,</span>
    <span class="s1">&#39;StorageType&#39;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">class</span> <span class="nc">SourceChangeWarning</span><span class="p">(</span><span class="ne">Warning</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">mkdtemp</span><span class="p">():</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">path</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>


<span class="n">_package_registry</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">_is_zipfile</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># This is a stricter implementation than zipfile.is_zipfile().</span>
    <span class="c1"># zipfile.is_zipfile() is True if the magic number appears anywhere in the</span>
    <span class="c1"># binary. Since we expect the files here to be generated by torch.save or</span>
    <span class="c1"># torch.jit.save, it&#39;s safe to only check the start bytes and avoid</span>
    <span class="c1"># collisions and assume the zip has only 1 file.</span>
    <span class="c1"># See bugs.python.org/issue28494.</span>

    <span class="c1"># Read the first 4 bytes of the file</span>
    <span class="n">read_bytes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">tell</span><span class="p">()</span>

    <span class="n">byte</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">byte</span> <span class="o">!=</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="n">read_bytes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">byte</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">read_bytes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">byte</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>

    <span class="n">local_header_magic_number</span> <span class="o">=</span> <span class="p">[</span><span class="sa">b</span><span class="s1">&#39;P&#39;</span><span class="p">,</span> <span class="sa">b</span><span class="s1">&#39;K&#39;</span><span class="p">,</span> <span class="sa">b</span><span class="s1">&#39;</span><span class="se">\x03</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">b</span><span class="s1">&#39;</span><span class="se">\x04</span><span class="s1">&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">read_bytes</span> <span class="o">==</span> <span class="n">local_header_magic_number</span>


<div class="viewcode-block" id="register_package"><a class="viewcode-back" href="../../notes/serialization.html#torch.register_package">[docs]</a><span class="k">def</span> <span class="nf">register_package</span><span class="p">(</span>
    <span class="n">priority</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">tagger</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">STORAGE</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">deserializer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">STORAGE</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">STORAGE</span><span class="p">]]</span>
<span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Registers callables for tagging and deserializing storage objects with an associated priority.</span>
<span class="sd">    Tagging associates a device with a storage object at save time while deserializing moves a</span>
<span class="sd">    storage object to an appropriate device at load time. :attr:`tagger` and :attr:`deserializer`</span>
<span class="sd">    are run in the order given by their :attr:`priority` until a tagger/deserializer returns a</span>
<span class="sd">    value that is not `None`.</span>

<span class="sd">    To override the deserialization behavior for a device in the global registry, one can register a</span>
<span class="sd">    tagger with a higher priority than the existing tagger.</span>

<span class="sd">    This function can also be used to register a tagger and deserializer for new devices.</span>

<span class="sd">    Args:</span>
<span class="sd">        priority: Indicates the priority associated with the tagger and deserializer, where a lower</span>
<span class="sd">            value indicates higher priority.</span>
<span class="sd">        tagger: Callable that takes in a storage object and returns its tagged device as a string</span>
<span class="sd">            or None.</span>
<span class="sd">        deserializer: Callable that takes in storage object and a device string and returns a storage</span>
<span class="sd">            object on the appropriate device or None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `None`</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; def ipu_tag(obj):</span>
<span class="sd">        &gt;&gt;&gt;     if obj.device.type == &#39;ipu&#39;:</span>
<span class="sd">        &gt;&gt;&gt;         return &#39;ipu&#39;</span>
<span class="sd">        &gt;&gt;&gt; def ipu_deserialize(obj, location):</span>
<span class="sd">        &gt;&gt;&gt;     if location.startswith(&#39;ipu&#39;):</span>
<span class="sd">        &gt;&gt;&gt;         ipu = getattr(torch, &quot;ipu&quot;, None)</span>
<span class="sd">        &gt;&gt;&gt;         assert ipu is not None, &quot;IPU device module is not loaded&quot;</span>
<span class="sd">        &gt;&gt;&gt;         assert torch.ipu.is_available(), &quot;ipu is not available&quot;</span>
<span class="sd">        &gt;&gt;&gt;         return obj.ipu(location)</span>
<span class="sd">        &gt;&gt;&gt; torch.serialization.register_package(11, ipu_tag, ipu_deserialize)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">queue_elem</span> <span class="o">=</span> <span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">tagger</span><span class="p">,</span> <span class="n">deserializer</span><span class="p">)</span>
    <span class="n">_package_registry</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">queue_elem</span><span class="p">)</span>
    <span class="n">_package_registry</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">check_module_version_greater_or_equal</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">req_version_tuple</span><span class="p">,</span> <span class="n">error_if_malformed</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Check if a module&#39;s version satisfies requirements</span>

<span class="sd">    Usually, a module&#39;s version string will be like &#39;x.y.z&#39;, which would be represented</span>
<span class="sd">    as a tuple (x, y, z), but sometimes it could be an unexpected format. If the version</span>
<span class="sd">    string does not match the given tuple&#39;s format up to the length of the tuple, then</span>
<span class="sd">    error and exit or emit a warning.</span>

<span class="sd">    Args:</span>
<span class="sd">        module: the module to check the version of</span>
<span class="sd">        req_version_tuple: tuple (usually of ints) representing the required version</span>
<span class="sd">        error_if_malformed: whether we should exit if module version string is malformed</span>

<span class="sd">    Returns:</span>
<span class="sd">        requirement_is_met: bool</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">version_strs</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="c1"># Cast module version fields to match the types of the required version</span>
        <span class="n">module_version</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">req_field</span><span class="p">)(</span><span class="n">version_strs</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">req_field</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">req_version_tuple</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">requirement_is_met</span> <span class="o">=</span> <span class="n">module_version</span> <span class="o">&gt;=</span> <span class="n">req_version_tuple</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;&#39;</span><span class="si">%s</span><span class="s2">&#39; module version string is malformed &#39;</span><span class="si">%s</span><span class="s2">&#39; and cannot be compared&quot;</span>
            <span class="s2">&quot; with tuple </span><span class="si">%s</span><span class="s2">&quot;</span>
        <span class="p">)</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">module</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">req_version_tuple</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">error_if_malformed</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">message</span> <span class="o">+</span> <span class="s1">&#39;, but continuing assuming that requirement is met&#39;</span><span class="p">)</span>
            <span class="n">requirement_is_met</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="n">requirement_is_met</span>


<span class="k">def</span> <span class="nf">_cpu_tag</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cpu&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;cpu&#39;</span>


<span class="k">def</span> <span class="nf">_cuda_tag</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;cuda:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_hpu_tag</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;hpu&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;hpu:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_mps_tag</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;mps&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;mps&#39;</span>


<span class="k">def</span> <span class="nf">_meta_tag</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;meta&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;meta&#39;</span>


<span class="k">def</span> <span class="nf">_privateuse1_tag</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="n">backend_name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="n">backend_name</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">backend_name</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">backend_name</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_cpu_deserialize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">location</span> <span class="o">==</span> <span class="s1">&#39;cpu&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">obj</span>


<span class="k">def</span> <span class="nf">validate_cuda_device</span><span class="p">(</span><span class="n">location</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_get_device_index</span><span class="p">(</span><span class="n">location</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Attempting to deserialize object on a CUDA &#39;</span>
                           <span class="s1">&#39;device but torch.cuda.is_available() is False. &#39;</span>
                           <span class="s1">&#39;If you are running on a CPU-only machine, &#39;</span>
                           <span class="s1">&#39;please use torch.load with map_location=torch.device(</span><span class="se">\&#39;</span><span class="s1">cpu</span><span class="se">\&#39;</span><span class="s1">) &#39;</span>
                           <span class="s1">&#39;to map your storages to the CPU.&#39;</span><span class="p">)</span>
    <span class="n">device_count</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">&gt;=</span> <span class="n">device_count</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Attempting to deserialize object on CUDA device &#39;</span>
                           <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s1"> but torch.cuda.device_count() is </span><span class="si">{</span><span class="n">device_count</span><span class="si">}</span><span class="s1">. Please use &#39;</span>
                           <span class="s1">&#39;torch.load with map_location to map your storages &#39;</span>
                           <span class="s1">&#39;to an existing device.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">device</span>


<span class="k">def</span> <span class="nf">_cuda_deserialize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">location</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">validate_cuda_device</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="s2">&quot;_torch_load_uninitialized&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">nbytes</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">location</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">validate_hpu_device</span><span class="p">(</span><span class="n">location</span><span class="p">):</span>
    <span class="n">hpu</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;hpu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">hpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;HPU device module is not loaded&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">hpu</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_get_device_index</span><span class="p">(</span><span class="n">location</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">hpu</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Attempting to deserialize object on a HPU &#39;</span>
                           <span class="s1">&#39;device but torch.hpu.is_available() is False. &#39;</span>
                           <span class="s1">&#39;If you are running on a CPU-only machine, &#39;</span>
                           <span class="s1">&#39;please use torch.load with map_location=torch.device(</span><span class="se">\&#39;</span><span class="s1">cpu</span><span class="se">\&#39;</span><span class="s1">) &#39;</span>
                           <span class="s1">&#39;to map your storages to the CPU.&#39;</span><span class="p">)</span>
    <span class="n">device_count</span> <span class="o">=</span> <span class="n">hpu</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">&gt;=</span> <span class="n">device_count</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Attempting to deserialize object on HPU device &#39;</span>
                           <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s1"> but torch.hpu.device_count() is </span><span class="si">{</span><span class="n">device_count</span><span class="si">}</span><span class="s1">. Please use &#39;</span>
                           <span class="s1">&#39;torch.load with map_location to map your storages &#39;</span>
                           <span class="s1">&#39;to an existing device.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">device</span>


<span class="k">def</span> <span class="nf">_hpu_deserialize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
    <span class="n">hpu</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="s2">&quot;hpu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">hpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;HPU device module is not loaded&quot;</span>
    <span class="k">if</span> <span class="n">location</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;hpu&#39;</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">validate_hpu_device</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="s2">&quot;_torch_load_uninitialized&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">hpu</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">nbytes</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">location</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">hpu</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_mps_deserialize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">location</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">mps</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_meta_deserialize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">location</span> <span class="o">==</span> <span class="s1">&#39;meta&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">nbytes</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;meta&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_validate_privateuse1_device</span><span class="p">(</span><span class="n">location</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
    <span class="n">device_index</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The </span><span class="si">{</span><span class="n">backend_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s1"> device module is not registered. &#39;</span>
                           <span class="s1">&#39;If you are running on a CPU-only machine, &#39;</span>
                           <span class="s1">&#39;please use torch.load with map_location=torch.device(</span><span class="se">\&#39;</span><span class="s1">cpu</span><span class="se">\&#39;</span><span class="s1">) &#39;</span>
                           <span class="s1">&#39;to map your storages to the CPU.&#39;</span><span class="p">)</span>
    <span class="n">device_module</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">device_module</span><span class="p">,</span> <span class="s1">&#39;is_available&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">device_module</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Attempting to deserialize object on a </span><span class="si">{</span><span class="n">backend_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s1"> &#39;</span>
                           <span class="sa">f</span><span class="s1">&#39;device but torch.</span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s1">.is_available() is False. &#39;</span>
                           <span class="s1">&#39;If you are running on a CPU-only machine, &#39;</span>
                           <span class="s1">&#39;please use torch.load with map_location=torch.device(</span><span class="se">\&#39;</span><span class="s1">cpu</span><span class="se">\&#39;</span><span class="s1">) &#39;</span>
                           <span class="s1">&#39;to map your storages to the CPU.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">device_module</span><span class="p">,</span> <span class="s1">&#39;device_count&#39;</span><span class="p">):</span>
        <span class="n">device_count</span> <span class="o">=</span> <span class="n">device_module</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">device_index</span> <span class="o">&gt;=</span> <span class="n">device_count</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Attempting to deserialize object on </span><span class="si">{</span><span class="n">backend_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s1"> device &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">device_index</span><span class="si">}</span><span class="s1"> but torch.</span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s1">.device_count() is </span><span class="si">{</span><span class="n">device_count</span><span class="si">}</span><span class="s1">. &#39;</span>
                               <span class="s1">&#39;Please use torch.load with map_location to map your storages &#39;</span>
                               <span class="s1">&#39;to an existing device.&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">device_index</span>


<span class="k">def</span> <span class="nf">_privateuse1_deserialize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
    <span class="n">backend_name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">location</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">backend_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Attempting to load the storages to the </span><span class="si">{</span><span class="n">backend_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s1"> device &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;but torch.storage._StorageBase.</span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s1">() or &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;torch.storage.TypedStorage.</span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s1">() is not generated. &#39;</span>
                               <span class="s1">&#39;Please use torch.utils.generate_methods_for_privateuse1_backend &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;to generate storage.</span><span class="si">{</span><span class="n">backend_name</span><span class="si">}</span><span class="s1">() method first.&#39;</span><span class="p">)</span>
        <span class="n">device_index</span> <span class="o">=</span> <span class="n">_validate_privateuse1_device</span><span class="p">(</span><span class="n">location</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">backend_name</span><span class="p">)(</span><span class="n">device_index</span><span class="p">)</span>


<span class="n">register_package</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">_cpu_tag</span><span class="p">,</span> <span class="n">_cpu_deserialize</span><span class="p">)</span>
<span class="n">register_package</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">_cuda_tag</span><span class="p">,</span> <span class="n">_cuda_deserialize</span><span class="p">)</span>
<span class="n">register_package</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="n">_mps_tag</span><span class="p">,</span> <span class="n">_mps_deserialize</span><span class="p">)</span>
<span class="n">register_package</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="n">_meta_tag</span><span class="p">,</span> <span class="n">_meta_deserialize</span><span class="p">)</span>
<span class="n">register_package</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span> <span class="n">_privateuse1_tag</span><span class="p">,</span> <span class="n">_privateuse1_deserialize</span><span class="p">)</span>
<span class="n">register_package</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">_hpu_tag</span><span class="p">,</span> <span class="n">_hpu_deserialize</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">location_tag</span><span class="p">(</span><span class="n">storage</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Storage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">tagger</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">_package_registry</span><span class="p">:</span>
        <span class="n">location</span> <span class="o">=</span> <span class="n">tagger</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">location</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">location</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;don&#39;t know how to determine data location of &quot;</span>
                       <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">storage</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">default_restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">_package_registry</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">result</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;don&#39;t know how to restore data location of &quot;</span>
                       <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; (tagged with &quot;</span>
                       <span class="o">+</span> <span class="n">location</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">normalize_storage_type</span><span class="p">(</span><span class="n">storage_type</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">storage_type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">storage_to_tensor_type</span><span class="p">(</span><span class="n">storage</span><span class="p">):</span>
    <span class="n">storage_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">_import_dotted_name</span><span class="p">(</span><span class="n">storage_type</span><span class="o">.</span><span class="vm">__module__</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">storage_type</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;Storage&#39;</span><span class="p">,</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_is_path</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">_opener</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_like</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file_like</span> <span class="o">=</span> <span class="n">file_like</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">file_like</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">pass</span>


<span class="k">class</span> <span class="nc">_open_file</span><span class="p">(</span><span class="n">_opener</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">mode</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file_like</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_open_buffer_reader</span><span class="p">(</span><span class="n">_opener</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">buffer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
        <span class="n">_check_seekable</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_open_buffer_writer</span><span class="p">(</span><span class="n">_opener</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file_like</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_open_file_like</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">_is_path</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_open_file</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="s1">&#39;w&#39;</span> <span class="ow">in</span> <span class="n">mode</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_open_buffer_writer</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="s1">&#39;r&#39;</span> <span class="ow">in</span> <span class="n">mode</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_open_buffer_reader</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected &#39;r&#39; or &#39;w&#39; in mode but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_open_zipfile_reader</span><span class="p">(</span><span class="n">_opener</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name_or_buffer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">PyTorchFileReader</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">_open_zipfile_writer_file</span><span class="p">(</span><span class="n">_opener</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file_stream</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">UnicodeEncodeError</span><span class="p">:</span>
            <span class="c1"># PyTorchFileWriter only supports ascii filename.</span>
            <span class="c1"># For filenames with non-ascii characters, we rely on Python</span>
            <span class="c1"># for writing out the file.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">file_stream</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">FileIO</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">PyTorchFileWriter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">file_stream</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">PyTorchFileWriter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file_like</span><span class="o">.</span><span class="n">write_end_of_file</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">file_stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">file_stream</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_open_zipfile_writer_buffer</span><span class="p">(</span><span class="n">_opener</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">buffer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="s2">&quot;write&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Buffer of </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buffer</span><span class="p">))</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&lt;&gt;&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2"> has no callable attribute &#39;write&#39;&quot;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="s2">&quot;write&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">buffer</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">PyTorchFileWriter</span><span class="p">(</span><span class="n">buffer</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file_like</span><span class="o">.</span><span class="n">write_end_of_file</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_open_zipfile_writer</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">):</span>
    <span class="n">container</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">_opener</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">_is_path</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">):</span>
        <span class="n">container</span> <span class="o">=</span> <span class="n">_open_zipfile_writer_file</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">container</span> <span class="o">=</span> <span class="n">_open_zipfile_writer_buffer</span>
    <span class="k">return</span> <span class="n">container</span><span class="p">(</span><span class="n">name_or_buffer</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_is_compressed_file</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">compress_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gzip&#39;</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">f</span><span class="o">.</span><span class="vm">__module__</span> <span class="ow">in</span> <span class="n">compress_modules</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_should_read_directly</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if f is a file that should be read directly. It should be read</span>
<span class="sd">    directly if it is backed by a real file (has a fileno) and is not a</span>
<span class="sd">    a compressed file (e.g. gzip)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_is_compressed_file</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">f</span><span class="o">.</span><span class="n">fileno</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span>
    <span class="k">except</span> <span class="n">io</span><span class="o">.</span><span class="n">UnsupportedOperation</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_check_seekable</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">raise_err_msg</span><span class="p">(</span><span class="n">patterns</span><span class="p">,</span> <span class="n">e</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">patterns</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;. You can only torch.load from a file that is seekable.&quot;</span>
                                <span class="o">+</span> <span class="s2">&quot; Please pre-load the data into a buffer like io.BytesIO and&quot;</span>
                                <span class="o">+</span> <span class="s2">&quot; try to load from it instead.&quot;</span><span class="p">)</span>
                <span class="k">raise</span> <span class="nb">type</span><span class="p">(</span><span class="n">e</span><span class="p">)(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">e</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">tell</span><span class="p">())</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">except</span> <span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">UnsupportedOperation</span><span class="p">,</span> <span class="ne">AttributeError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">raise_err_msg</span><span class="p">([</span><span class="s2">&quot;seek&quot;</span><span class="p">,</span> <span class="s2">&quot;tell&quot;</span><span class="p">],</span> <span class="n">e</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_check_dill_version</span><span class="p">(</span><span class="n">pickle_module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&#39;&#39;&#39;Checks if using dill as the pickle module, and if so, checks if it is the correct version.</span>
<span class="sd">    If dill version is lower than 0.3.1, a ValueError is raised.</span>

<span class="sd">    Args:</span>
<span class="sd">        pickle_module: module used for pickling metadata and objects</span>

<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="n">pickle_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pickle_module</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;dill&#39;</span><span class="p">:</span>
        <span class="n">required_dill_version</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">check_module_version_greater_or_equal</span><span class="p">(</span><span class="n">pickle_module</span><span class="p">,</span> <span class="n">required_dill_version</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span>
                <span class="s2">&quot;&#39;torch&#39; supports dill &gt;= </span><span class="si">%s</span><span class="s2">, but you have dill </span><span class="si">%s</span><span class="s2">.&quot;</span>
                <span class="s2">&quot; Please upgrade dill or switch to &#39;pickle&#39;&quot;</span>
            <span class="p">)</span> <span class="o">%</span> <span class="p">(</span>
                <span class="s1">&#39;.&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">required_dill_version</span><span class="p">]),</span>
                <span class="n">pickle_module</span><span class="o">.</span><span class="n">__version__</span>
            <span class="p">))</span>


<span class="k">def</span> <span class="nf">_check_save_filelike</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s1">&#39;write&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">((</span>
            <span class="s2">&quot;expected &#39;f&#39; to be string, path, or a file-like object with &quot;</span>
            <span class="s2">&quot;a &#39;write&#39; attribute&quot;</span><span class="p">))</span>


<div class="viewcode-block" id="save"><a class="viewcode-back" href="../../generated/torch.save.html#torch.save">[docs]</a><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">FILE_LIKE</span><span class="p">,</span>
    <span class="n">pickle_module</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">,</span>
    <span class="n">pickle_protocol</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_PROTOCOL</span><span class="p">,</span>
    <span class="n">_use_new_zipfile_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">_disable_byteorder_record</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Reference: https://github.com/pytorch/pytorch/issues/54354</span>
    <span class="c1"># The first line of this docstring overrides the one Sphinx generates for the</span>
    <span class="c1"># documentation. We need it so that Sphinx doesn&#39;t leak `pickle`s path from</span>
    <span class="c1"># the build environment (e.g. `&lt;module &#39;pickle&#39; from &#39;/leaked/path&#39;).</span>

    <span class="sd">&quot;&quot;&quot;save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)</span>

<span class="sd">    Saves an object to a disk file.</span>

<span class="sd">    See also: :ref:`saving-loading-tensors`</span>

<span class="sd">    Args:</span>
<span class="sd">        obj: saved object</span>
<span class="sd">        f: a file-like object (has to implement write and flush) or a string or</span>
<span class="sd">           os.PathLike object containing a file name</span>
<span class="sd">        pickle_module: module used for pickling metadata and objects</span>
<span class="sd">        pickle_protocol: can be specified to override the default protocol</span>

<span class="sd">    .. note::</span>
<span class="sd">        A common PyTorch convention is to save tensors using .pt file extension.</span>

<span class="sd">    .. note::</span>
<span class="sd">        PyTorch preserves storage sharing across serialization. See</span>
<span class="sd">        :ref:`preserve-storage-sharing` for more details.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The 1.6 release of PyTorch switched ``torch.save`` to use a new</span>
<span class="sd">        zipfile-based file format. ``torch.load`` still retains the ability to</span>
<span class="sd">        load files in the old format. If for any reason you want ``torch.save``</span>
<span class="sd">        to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;makes cwd dirty&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Save to file</span>
<span class="sd">        &gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4])</span>
<span class="sd">        &gt;&gt;&gt; torch.save(x, &#39;tensor.pt&#39;)</span>
<span class="sd">        &gt;&gt;&gt; # Save to io.BytesIO buffer</span>
<span class="sd">        &gt;&gt;&gt; buffer = io.BytesIO()</span>
<span class="sd">        &gt;&gt;&gt; torch.save(x, buffer)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.save&quot;</span><span class="p">)</span>
    <span class="n">_check_dill_version</span><span class="p">(</span><span class="n">pickle_module</span><span class="p">)</span>
    <span class="n">_check_save_filelike</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_use_new_zipfile_serialization</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">_open_zipfile_writer</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">as</span> <span class="n">opened_zipfile</span><span class="p">:</span>
            <span class="n">_save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">opened_zipfile</span><span class="p">,</span> <span class="n">pickle_module</span><span class="p">,</span> <span class="n">pickle_protocol</span><span class="p">,</span> <span class="n">_disable_byteorder_record</span><span class="p">)</span>
            <span class="k">return</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">_open_file_like</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">opened_file</span><span class="p">:</span>
            <span class="n">_legacy_save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">opened_file</span><span class="p">,</span> <span class="n">pickle_module</span><span class="p">,</span> <span class="n">pickle_protocol</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_legacy_save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">pickle_module</span><span class="p">,</span> <span class="n">pickle_protocol</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
    <span class="n">serialized_container_types</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">serialized_storages</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Since loading storages that view the same data with different dtypes is</span>
    <span class="c1"># not supported, we need to keep track of the dtype associated with each</span>
    <span class="c1"># storage data_ptr and throw an error if the dtype is ever different.</span>
    <span class="c1"># TODO: This feature could be added in the future</span>
    <span class="n">storage_dtypes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">persistent_id</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]:</span>
        <span class="c1"># FIXME: the docs say that persistent_id should only return a string</span>
        <span class="c1"># but torch store returns tuples. This works only in the binary protocol</span>
        <span class="c1"># see</span>
        <span class="c1"># https://docs.python.org/2/library/pickle.html#pickling-and-unpickling-external-objects</span>
        <span class="c1"># https://github.com/python/cpython/blob/master/Lib/pickle.py#L527-L537</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">serialized_container_types</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>
            <span class="n">serialized_container_types</span><span class="p">[</span><span class="n">obj</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">source_file</span> <span class="o">=</span> <span class="n">source</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">source_lines</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">source_file</span> <span class="o">=</span> <span class="n">get_source_lines_and_file</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
                <span class="n">source</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">source_lines</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># saving the source is optional, so we can ignore any errors</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Couldn&#39;t retrieve source code for container of &quot;</span>
                              <span class="s2">&quot;type &quot;</span> <span class="o">+</span> <span class="n">obj</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;. It won&#39;t be checked &quot;</span>
                              <span class="s2">&quot;for correctness upon loading.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="s1">&#39;module&#39;</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">source_file</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">)</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_storage</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
            <span class="n">storage</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">):</span>
                <span class="c1"># TODO: Once we decide to break serialization FC, this case</span>
                <span class="c1"># can be deleted</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">_untyped_storage</span>
                <span class="n">storage_dtype</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">dtype</span>
                <span class="n">storage_type_str</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">_pickle_storage_type</span><span class="p">()</span>
                <span class="n">storage_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">storage_type_str</span><span class="p">)</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">dtype</span>
                <span class="n">storage_numel</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">_size</span><span class="p">()</span>

            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">):</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="n">obj</span>
                <span class="n">storage_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
                <span class="n">storage_type</span> <span class="o">=</span> <span class="n">normalize_storage_type</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
                <span class="n">storage_numel</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;type not recognized: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="c1"># If storage is allocated, ensure that any other saved storages</span>
            <span class="c1"># pointing to the same data all have the same dtype. If storage is</span>
            <span class="c1"># not allocated, don&#39;t perform this check</span>
            <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="ow">in</span> <span class="n">storage_dtypes</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">storage_dtype</span> <span class="o">!=</span> <span class="n">storage_dtypes</span><span class="p">[</span><span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()]:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s1">&#39;Cannot save multiple tensors or storages that &#39;</span>
                            <span class="s1">&#39;view the same data as different types&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">storage_dtypes</span><span class="p">[</span><span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()]</span> <span class="o">=</span> <span class="n">storage_dtype</span>

            <span class="n">view_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span>

            <span class="c1"># Offset is always 0, but we keep it for backwards compatibility</span>
            <span class="c1"># with the old serialization format (which supported storage views)</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">storage_key</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">_cdata</span><span class="p">)</span>
            <span class="n">location</span> <span class="o">=</span> <span class="n">location_tag</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>

            <span class="c1"># TODO: There&#39;s an issue here with FC. It might be impossible to</span>
            <span class="c1"># solve, but it&#39;s worth noting. Imagine we save a list `[storage,</span>
            <span class="c1"># tensor]`, where `tensor.storage()` is the same as `storage`, and</span>
            <span class="c1"># `tensor.element_size() &gt; 1`. Let&#39;s say that `tensor.dtype ==</span>
            <span class="c1"># torch.float`.  The storage will be serialized with element size</span>
            <span class="c1"># of 1, since we&#39;re choosing to serialize the first occurance of</span>
            <span class="c1"># a duplicate storage. Since this legacy serialization format saves</span>
            <span class="c1"># the numel of the storage, rather than nbytes directly, we&#39;ll be</span>
            <span class="c1"># effectively saving nbytes in this case.  We&#39;ll be able to load it</span>
            <span class="c1"># and the tensor back up with no problems in _this_ and future</span>
            <span class="c1"># versions of pytorch, but in older versions, here&#39;s the problem:</span>
            <span class="c1"># the storage will be loaded up as a UntypedStorage, and then the</span>
            <span class="c1"># FloatTensor will loaded and the UntypedStorage will be assigned to</span>
            <span class="c1"># it. Since the storage dtype does not match the tensor dtype, this</span>
            <span class="c1"># will cause an error.  If we reverse the list, like `[tensor,</span>
            <span class="c1"># storage]`, then we will save the `tensor.storage()` as a faked</span>
            <span class="c1"># `FloatStorage`, and the saved size will be the correct</span>
            <span class="c1"># dtype-specific numel count that old versions expect. `tensor`</span>
            <span class="c1"># will be able to load up properly in old versions, pointing to</span>
            <span class="c1"># a FloatStorage. However, `storage` is still being translated to</span>
            <span class="c1"># a UntypedStorage, and it will try to resolve to the same</span>
            <span class="c1"># FloatStorage that `tensor` contains. This will also cause an</span>
            <span class="c1"># error. It doesn&#39;t seem like there&#39;s any way around this.</span>
            <span class="c1"># Probably, we just cannot maintain FC for the legacy format if the</span>
            <span class="c1"># saved list contains both a tensor and a storage that point to the</span>
            <span class="c1"># same data.  We should still be able to maintain FC for lists of</span>
            <span class="c1"># just tensors, as long as all views share the same dtype as the</span>
            <span class="c1"># tensor they are viewing.</span>

            <span class="k">if</span> <span class="n">storage_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">serialized_storages</span><span class="p">:</span>
                <span class="n">serialized_storages</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
            <span class="n">is_view</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">_cdata</span> <span class="o">!=</span> <span class="n">storage</span><span class="o">.</span><span class="n">_cdata</span>
            <span class="k">if</span> <span class="n">is_view</span><span class="p">:</span>
                <span class="n">view_metadata</span> <span class="o">=</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">_cdata</span><span class="p">),</span> <span class="n">offset</span><span class="p">,</span> <span class="n">storage</span><span class="o">.</span><span class="n">nbytes</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">view_metadata</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;storage&#39;</span><span class="p">,</span>
                   <span class="n">storage_type</span><span class="p">,</span>
                   <span class="n">storage_key</span><span class="p">,</span>
                   <span class="n">location</span><span class="p">,</span>
                   <span class="n">storage_numel</span><span class="p">,</span>
                   <span class="n">view_metadata</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">res</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">sys_info</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">protocol_version</span><span class="o">=</span><span class="n">PROTOCOL_VERSION</span><span class="p">,</span>
        <span class="n">little_endian</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">byteorder</span> <span class="o">==</span> <span class="s1">&#39;little&#39;</span><span class="p">,</span>
        <span class="n">type_sizes</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">short</span><span class="o">=</span><span class="n">SHORT_SIZE</span><span class="p">,</span>
            <span class="nb">int</span><span class="o">=</span><span class="n">INT_SIZE</span><span class="p">,</span>
            <span class="n">long</span><span class="o">=</span><span class="n">LONG_SIZE</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="n">pickle_module</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">MAGIC_NUMBER</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle_protocol</span><span class="p">)</span>
    <span class="n">pickle_module</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">PROTOCOL_VERSION</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle_protocol</span><span class="p">)</span>
    <span class="n">pickle_module</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">sys_info</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle_protocol</span><span class="p">)</span>
    <span class="n">pickler</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">Pickler</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle_protocol</span><span class="p">)</span>
    <span class="n">pickler</span><span class="o">.</span><span class="n">persistent_id</span> <span class="o">=</span> <span class="n">persistent_id</span>
    <span class="n">pickler</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>

    <span class="n">serialized_storage_keys</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">serialized_storages</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">pickle_module</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">serialized_storage_keys</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle_protocol</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">serialized_storage_keys</span><span class="p">:</span>
        <span class="n">storage</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">serialized_storages</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="n">storage</span><span class="o">.</span><span class="n">_write_file</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">_should_read_directly</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">zip_file</span><span class="p">,</span> <span class="n">pickle_module</span><span class="p">,</span> <span class="n">pickle_protocol</span><span class="p">,</span> <span class="n">_disable_byteorder_record</span><span class="p">):</span>
    <span class="n">serialized_storages</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">id_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Since loading storages that view the same data with different dtypes is</span>
    <span class="c1"># not supported, we need to keep track of the dtype associated with each</span>
    <span class="c1"># storage data_ptr and throw an error if the dtype is ever different.</span>
    <span class="c1"># TODO: This feature could be added in the future</span>
    <span class="n">storage_dtypes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">persistent_id</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
        <span class="c1"># FIXME: the docs say that persistent_id should only return a string</span>
        <span class="c1"># but torch store returns tuples. This works only in the binary protocol</span>
        <span class="c1"># see</span>
        <span class="c1"># https://docs.python.org/2/library/pickle.html#pickling-and-unpickling-external-objects</span>
        <span class="c1"># https://github.com/python/cpython/blob/master/Lib/pickle.py#L527-L537</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">)</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_storage</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">):</span>
                <span class="c1"># TODO: Once we decide to break serialization FC, this case</span>
                <span class="c1"># can be deleted</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">_untyped_storage</span>
                <span class="n">storage_dtype</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">dtype</span>
                <span class="n">storage_type_str</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">_pickle_storage_type</span><span class="p">()</span>
                <span class="n">storage_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">storage_type_str</span><span class="p">)</span>
                <span class="n">storage_numel</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">_size</span><span class="p">()</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="n">obj</span>
                <span class="n">storage_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
                <span class="n">storage_type</span> <span class="o">=</span> <span class="n">normalize_storage_type</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">))</span>
                <span class="n">storage_numel</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span>

            <span class="c1"># If storage is allocated, ensure that any other saved storages</span>
            <span class="c1"># pointing to the same data all have the same dtype. If storage is</span>
            <span class="c1"># not allocated, don&#39;t perform this check</span>
            <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="ow">in</span> <span class="n">storage_dtypes</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">storage_dtype</span> <span class="o">!=</span> <span class="n">storage_dtypes</span><span class="p">[</span><span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()]:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s1">&#39;Cannot save multiple tensors or storages that &#39;</span>
                            <span class="s1">&#39;view the same data as different types&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">storage_dtypes</span><span class="p">[</span><span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()]</span> <span class="o">=</span> <span class="n">storage_dtype</span>

            <span class="n">storage_key</span> <span class="o">=</span> <span class="n">id_map</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">_cdata</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">id_map</span><span class="p">)))</span>
            <span class="n">location</span> <span class="o">=</span> <span class="n">location_tag</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
            <span class="n">serialized_storages</span><span class="p">[</span><span class="n">storage_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">storage</span>

            <span class="k">return</span> <span class="p">(</span><span class="s1">&#39;storage&#39;</span><span class="p">,</span>
                    <span class="n">storage_type</span><span class="p">,</span>
                    <span class="n">storage_key</span><span class="p">,</span>
                    <span class="n">location</span><span class="p">,</span>
                    <span class="n">storage_numel</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># Write the pickle data for `obj`</span>
    <span class="n">data_buf</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
    <span class="n">pickler</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">Pickler</span><span class="p">(</span><span class="n">data_buf</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="n">pickle_protocol</span><span class="p">)</span>
    <span class="n">pickler</span><span class="o">.</span><span class="n">persistent_id</span> <span class="o">=</span> <span class="n">persistent_id</span>
    <span class="n">pickler</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="n">data_value</span> <span class="o">=</span> <span class="n">data_buf</span><span class="o">.</span><span class="n">getvalue</span><span class="p">()</span>
    <span class="n">zip_file</span><span class="o">.</span><span class="n">write_record</span><span class="p">(</span><span class="s1">&#39;data.pkl&#39;</span><span class="p">,</span> <span class="n">data_value</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_value</span><span class="p">))</span>

    <span class="c1"># Write byte order marker</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_disable_byteorder_record</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">byteorder</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;little&#39;</span><span class="p">,</span> <span class="s1">&#39;big&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown endianness type: &#39;</span> <span class="o">+</span> <span class="n">sys</span><span class="o">.</span><span class="n">byteorder</span><span class="p">)</span>

        <span class="n">zip_file</span><span class="o">.</span><span class="n">write_record</span><span class="p">(</span><span class="s1">&#39;byteorder&#39;</span><span class="p">,</span> <span class="n">sys</span><span class="o">.</span><span class="n">byteorder</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">byteorder</span><span class="p">))</span>

    <span class="c1"># Write each tensor to a file named tensor/the_tensor_key in the zip archive</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">serialized_storages</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="n">name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;data/</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">storage</span> <span class="o">=</span> <span class="n">serialized_storages</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="c1"># given that we copy things around anyway, we might use storage.cpu()</span>
        <span class="c1"># this means to that to get tensors serialized, you need to implement</span>
        <span class="c1"># .cpu() on the underlying Storage</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">:</span>
            <span class="n">storage</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="c1"># Now that it is on the CPU we can directly copy it into the zip file</span>
        <span class="n">num_bytes</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span>
        <span class="n">zip_file</span><span class="o">.</span><span class="n">write_record</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span> <span class="n">num_bytes</span><span class="p">)</span>


<div class="viewcode-block" id="load"><a class="viewcode-back" href="../../generated/torch.load.html#torch.load">[docs]</a><span class="k">def</span> <span class="nf">load</span><span class="p">(</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">FILE_LIKE</span><span class="p">,</span>
    <span class="n">map_location</span><span class="p">:</span> <span class="n">MAP_LOCATION</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pickle_module</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">weights_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mmap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">:</span> <span class="n">Any</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="c1"># Reference: https://github.com/pytorch/pytorch/issues/54354</span>
    <span class="c1"># The first line of this docstring overrides the one Sphinx generates for the</span>
    <span class="c1"># documentation. We need it so that Sphinx doesn&#39;t leak `pickle`s path from</span>
    <span class="c1"># the build environment (e.g. `&lt;module &#39;pickle&#39; from &#39;/leaked/path&#39;).</span>

    <span class="sd">&quot;&quot;&quot;load(f, map_location=None, pickle_module=pickle, *, weights_only=False, **pickle_load_args)</span>

<span class="sd">    Loads an object saved with :func:`torch.save` from a file.</span>

<span class="sd">    :func:`torch.load` uses Python&#39;s unpickling facilities but treats storages,</span>
<span class="sd">    which underlie tensors, specially. They are first deserialized on the</span>
<span class="sd">    CPU and are then moved to the device they were saved from. If this fails</span>
<span class="sd">    (e.g. because the run time system doesn&#39;t have certain devices), an exception</span>
<span class="sd">    is raised. However, storages can be dynamically remapped to an alternative</span>
<span class="sd">    set of devices using the :attr:`map_location` argument.</span>

<span class="sd">    If :attr:`map_location` is a callable, it will be called once for each serialized</span>
<span class="sd">    storage with two arguments: storage and location. The storage argument</span>
<span class="sd">    will be the initial deserialization of the storage, residing on the CPU.</span>
<span class="sd">    Each serialized storage has a location tag associated with it which</span>
<span class="sd">    identifies the device it was saved from, and this tag is the second</span>
<span class="sd">    argument passed to :attr:`map_location`. The builtin location tags are ``&#39;cpu&#39;``</span>
<span class="sd">    for CPU tensors and ``&#39;cuda:device_id&#39;`` (e.g. ``&#39;cuda:2&#39;``) for CUDA tensors.</span>
<span class="sd">    :attr:`map_location` should return either ``None`` or a storage. If</span>
<span class="sd">    :attr:`map_location` returns a storage, it will be used as the final deserialized</span>
<span class="sd">    object, already moved to the right device. Otherwise, :func:`torch.load` will</span>
<span class="sd">    fall back to the default behavior, as if :attr:`map_location` wasn&#39;t specified.</span>

<span class="sd">    If :attr:`map_location` is a :class:`torch.device` object or a string containing</span>
<span class="sd">    a device tag, it indicates the location where all tensors should be loaded.</span>

<span class="sd">    Otherwise, if :attr:`map_location` is a dict, it will be used to remap location tags</span>
<span class="sd">    appearing in the file (keys), to ones that specify where to put the</span>
<span class="sd">    storages (values).</span>

<span class="sd">    User extensions can register their own location tags and tagging and</span>
<span class="sd">    deserialization methods using :func:`torch.serialization.register_package`.</span>

<span class="sd">    Args:</span>
<span class="sd">        f: a file-like object (has to implement :meth:`read`, :meth:`readline`, :meth:`tell`, and :meth:`seek`),</span>
<span class="sd">            or a string or os.PathLike object containing a file name</span>
<span class="sd">        map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage</span>
<span class="sd">            locations</span>
<span class="sd">        pickle_module: module used for unpickling metadata and objects (has to</span>
<span class="sd">            match the :attr:`pickle_module` used to serialize file)</span>
<span class="sd">        weights_only: Indicates whether unpickler should be restricted to</span>
<span class="sd">            loading only tensors, primitive types and dictionaries</span>
<span class="sd">        mmap: Indicates whether the file should be mmaped rather than loading all the storages into memory.</span>
<span class="sd">            Typically, tensor storages in the file will first be moved from disk to CPU memory, after which they</span>
<span class="sd">            are moved to the location that they were tagged with when saving, or specified by `map_location`. This</span>
<span class="sd">            second step is a no-op if the final location is CPU. When the `mmap` flag is set, instead of copying the</span>
<span class="sd">            tensor storages from disk to CPU memory in the first step, f is mmaped.</span>
<span class="sd">        pickle_load_args: (Python 3 only) optional keyword arguments passed over to</span>
<span class="sd">            :func:`pickle_module.load` and :func:`pickle_module.Unpickler`, e.g.,</span>
<span class="sd">            :attr:`errors=...`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`torch.load()` unless `weights_only` parameter is set to `True`,</span>
<span class="sd">        uses ``pickle`` module implicitly, which is known to be insecure.</span>
<span class="sd">        It is possible to construct malicious pickle data which will execute arbitrary code</span>
<span class="sd">        during unpickling. Never load data that could have come from an untrusted</span>
<span class="sd">        source in an unsafe mode, or that could have been tampered with. **Only load data you trust**.</span>

<span class="sd">    .. note::</span>
<span class="sd">        When you call :func:`torch.load()` on a file which contains GPU tensors, those tensors</span>
<span class="sd">        will be loaded to GPU by default. You can call ``torch.load(.., map_location=&#39;cpu&#39;)``</span>
<span class="sd">        and then :meth:`load_state_dict` to avoid GPU RAM surge when loading a model checkpoint.</span>

<span class="sd">    .. note::</span>
<span class="sd">        By default, we decode byte strings as ``utf-8``.  This is to avoid a common error</span>
<span class="sd">        case ``UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0x...``</span>
<span class="sd">        when loading files saved by Python 2 in Python 3.  If this default</span>
<span class="sd">        is incorrect, you may use an extra :attr:`encoding` keyword argument to specify how</span>
<span class="sd">        these objects should be loaded, e.g., :attr:`encoding=&#39;latin1&#39;` decodes them</span>
<span class="sd">        to strings using ``latin1`` encoding, and :attr:`encoding=&#39;bytes&#39;` keeps them</span>
<span class="sd">        as byte arrays which can be decoded later with ``byte_array.decode(...)``.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined filepaths&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;)</span>
<span class="sd">        # Load all tensors onto the CPU</span>
<span class="sd">        &gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;, map_location=torch.device(&#39;cpu&#39;))</span>
<span class="sd">        # Load all tensors onto the CPU, using a function</span>
<span class="sd">        &gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;, map_location=lambda storage, loc: storage)</span>
<span class="sd">        # Load all tensors onto GPU 1</span>
<span class="sd">        &gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;, map_location=lambda storage, loc: storage.cuda(1))</span>
<span class="sd">        # Map tensors from GPU 1 to GPU 0</span>
<span class="sd">        &gt;&gt;&gt; torch.load(&#39;tensors.pt&#39;, map_location={&#39;cuda:1&#39;: &#39;cuda:0&#39;})</span>
<span class="sd">        # Load tensor from io.BytesIO object</span>
<span class="sd">        &gt;&gt;&gt; with open(&#39;tensor.pt&#39;, &#39;rb&#39;) as f:</span>
<span class="sd">        ...     buffer = io.BytesIO(f.read())</span>
<span class="sd">        &gt;&gt;&gt; torch.load(buffer)</span>
<span class="sd">        # Load a module with &#39;ascii&#39; encoding for unpickling</span>
<span class="sd">        &gt;&gt;&gt; torch.load(&#39;module.pt&#39;, encoding=&#39;ascii&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.load&quot;</span><span class="p">)</span>
    <span class="n">UNSAFE_MESSAGE</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;Weights only load failed. Re-running `torch.load` with `weights_only` set to `False`&quot;</span>
        <span class="s2">&quot; will likely succeed, but it can result in arbitrary code execution.&quot;</span>
        <span class="s2">&quot;Do it only if you get the file from a trusted source. WeightsUnpickler error: &quot;</span>
    <span class="p">)</span>
    <span class="c1"># Add ability to force safe only weight loads via environment variable</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TORCH_FORCE_WEIGHTS_ONLY_LOAD&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;true&#39;</span><span class="p">]:</span>
        <span class="n">weights_only</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">if</span> <span class="n">weights_only</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pickle_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Can not safely load weights when explicit pickle_module is specified&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pickle_module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pickle_module</span> <span class="o">=</span> <span class="n">pickle</span>

    <span class="c1"># make flipping default BC-compatible</span>
    <span class="k">if</span> <span class="n">mmap</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mmap</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">_check_dill_version</span><span class="p">(</span><span class="n">pickle_module</span><span class="p">)</span>

    <span class="k">if</span> <span class="s1">&#39;encoding&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pickle_load_args</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">pickle_load_args</span><span class="p">[</span><span class="s1">&#39;encoding&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;utf-8&#39;</span>

    <span class="k">with</span> <span class="n">_open_file_like</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">opened_file</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_is_zipfile</span><span class="p">(</span><span class="n">opened_file</span><span class="p">):</span>
            <span class="c1"># The zipfile reader is going to advance the current file position.</span>
            <span class="c1"># If we want to actually tail call to torch.jit.load, we need to</span>
            <span class="c1"># reset back to the original position.</span>
            <span class="n">orig_position</span> <span class="o">=</span> <span class="n">opened_file</span><span class="o">.</span><span class="n">tell</span><span class="p">()</span>
            <span class="n">overall_storage</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">with</span> <span class="n">_open_zipfile_reader</span><span class="p">(</span><span class="n">opened_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">opened_zipfile</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">_is_torchscript_zip</span><span class="p">(</span><span class="n">opened_zipfile</span><span class="p">):</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;&#39;torch.load&#39; received a zip file that looks like a TorchScript archive&quot;</span>
                                  <span class="s2">&quot; dispatching to &#39;torch.jit.load&#39; (call &#39;torch.jit.load&#39; directly to&quot;</span>
                                  <span class="s2">&quot; silence this warning)&quot;</span><span class="p">,</span> <span class="ne">UserWarning</span><span class="p">)</span>
                    <span class="n">opened_file</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="n">orig_position</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">opened_file</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">mmap</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;f must be a string filename in order to use mmap argument&quot;</span><span class="p">)</span>
                    <span class="n">size</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getsize</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                    <span class="n">overall_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">weights_only</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">_load</span><span class="p">(</span><span class="n">opened_zipfile</span><span class="p">,</span>
                                     <span class="n">map_location</span><span class="p">,</span>
                                     <span class="n">_weights_only_unpickler</span><span class="p">,</span>
                                     <span class="n">overall_storage</span><span class="o">=</span><span class="n">overall_storage</span><span class="p">,</span>
                                     <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">pickle</span><span class="o">.</span><span class="n">UnpicklingError</span><span class="p">(</span><span class="n">UNSAFE_MESSAGE</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span> <span class="kn">from</span> <span class="bp">None</span>
                <span class="k">return</span> <span class="n">_load</span><span class="p">(</span><span class="n">opened_zipfile</span><span class="p">,</span>
                             <span class="n">map_location</span><span class="p">,</span>
                             <span class="n">pickle_module</span><span class="p">,</span>
                             <span class="n">overall_storage</span><span class="o">=</span><span class="n">overall_storage</span><span class="p">,</span>
                             <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mmap</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;mmap can only be used with files saved with &quot;</span><span class="p">,</span>
                               <span class="s2">&quot;`torch.save(_use_new_zipfile_serialization=True), &quot;</span>
                               <span class="s2">&quot;please torch.save your checkpoint with this option in order to use mmap.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weights_only</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">_legacy_load</span><span class="p">(</span><span class="n">opened_file</span><span class="p">,</span> <span class="n">map_location</span><span class="p">,</span> <span class="n">_weights_only_unpickler</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">pickle</span><span class="o">.</span><span class="n">UnpicklingError</span><span class="p">(</span><span class="n">UNSAFE_MESSAGE</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span> <span class="kn">from</span> <span class="bp">None</span>
        <span class="k">return</span> <span class="n">_legacy_load</span><span class="p">(</span><span class="n">opened_file</span><span class="p">,</span> <span class="n">map_location</span><span class="p">,</span> <span class="n">pickle_module</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span></div>


<span class="c1"># Register pickling support for layout instances such as</span>
<span class="c1"># torch.sparse_coo, etc</span>
<span class="k">def</span> <span class="nf">_get_layout</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get layout extension object from its string representation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="n">_get_layout</span><span class="o">.</span><span class="n">cache</span>   <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cache</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">layout</span><span class="p">):</span>
                <span class="n">cache</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">return</span> <span class="n">cache</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

<span class="c1"># There are yet not good way to type annotate function attributes https://github.com/python/mypy/issues/2087</span>
<span class="n">_get_layout</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="p">{}</span>   <span class="c1"># type: ignore[attr-defined]</span>
<span class="n">copyreg</span><span class="o">.</span><span class="n">pickle</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">obj</span><span class="p">:</span> <span class="p">(</span><span class="n">_get_layout</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">obj</span><span class="p">),)))</span>


<span class="k">def</span> <span class="nf">_legacy_load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">map_location</span><span class="p">,</span> <span class="n">pickle_module</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">):</span>
    <span class="n">deserialized_objects</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">restore_location</span> <span class="o">=</span> <span class="n">_get_restore_location</span><span class="p">(</span><span class="n">map_location</span><span class="p">)</span>

    <span class="k">class</span> <span class="nc">UnpicklerWrapper</span><span class="p">(</span><span class="n">pickle_module</span><span class="o">.</span><span class="n">Unpickler</span><span class="p">):</span>  <span class="c1"># type: ignore[name-defined]</span>

        <span class="k">def</span> <span class="nf">find_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_name</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span> <span class="ow">and</span> <span class="s1">&#39;Storage&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">StorageType</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="k">pass</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">find_class</span><span class="p">(</span><span class="n">mod_name</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_container_source</span><span class="p">(</span><span class="n">container_type</span><span class="p">,</span> <span class="n">source_file</span><span class="p">,</span> <span class="n">original_source</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">current_source</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">get_source_lines_and_file</span><span class="p">(</span><span class="n">container_type</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  <span class="c1"># saving the source is optional, so we can ignore any errors</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Couldn&#39;t retrieve source code for container of &quot;</span>
                          <span class="s2">&quot;type &quot;</span> <span class="o">+</span> <span class="n">container_type</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;. It won&#39;t be checked &quot;</span>
                          <span class="s2">&quot;for correctness upon loading.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="n">original_source</span> <span class="o">!=</span> <span class="n">current_source</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">container_type</span><span class="o">.</span><span class="n">dump_patches</span><span class="p">:</span>
                <span class="n">file_name</span> <span class="o">=</span> <span class="n">container_type</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;.patch&#39;</span>
                <span class="n">diff</span> <span class="o">=</span> <span class="n">difflib</span><span class="o">.</span><span class="n">unified_diff</span><span class="p">(</span><span class="n">current_source</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">),</span>
                                            <span class="n">original_source</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">),</span>
                                            <span class="n">source_file</span><span class="p">,</span>
                                            <span class="n">source_file</span><span class="p">,</span> <span class="n">lineterm</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
                <span class="n">lines</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                        <span class="n">file_size</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                        <span class="n">f</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">file_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
                        <span class="k">elif</span> <span class="n">file_size</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span> <span class="ow">or</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span> <span class="o">!=</span> <span class="n">lines</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">IOError</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Saved a reverse patch to &quot;</span> <span class="o">+</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s2">&quot;. &quot;</span>
                           <span class="s2">&quot;Run `patch -p0 &lt; &quot;</span> <span class="o">+</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s2">&quot;` to revert your &quot;</span>
                           <span class="s2">&quot;changes.&quot;</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">IOError</span><span class="p">:</span>
                    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Tried to save a patch, but couldn&#39;t create a &quot;</span>
                           <span class="s2">&quot;writable file &quot;</span> <span class="o">+</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s2">&quot;. Make sure it &quot;</span>
                           <span class="s2">&quot;doesn&#39;t exist and your working directory is &quot;</span>
                           <span class="s2">&quot;writable.&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;you can retrieve the original source code by &quot;</span>
                       <span class="s2">&quot;accessing the object&#39;s source attribute or set &quot;</span>
                       <span class="s2">&quot;`torch.nn.Module.dump_patches = True` and use the &quot;</span>
                       <span class="s2">&quot;patch tool to revert the changes.&quot;</span><span class="p">)</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;source code of class &#39;</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">container_type</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39; has changed. </span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">SourceChangeWarning</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">legacy_load</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
        <span class="n">deserialized_objects</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">def</span> <span class="nf">persistent_load</span><span class="p">(</span><span class="n">saved_id</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">saved_id</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="c1"># Ignore containers that don&#39;t have any sources saved</span>
                <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">saved_id</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                    <span class="n">_check_container_source</span><span class="p">(</span><span class="o">*</span><span class="n">saved_id</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">saved_id</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">deserialized_objects</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">saved_id</span><span class="p">)]</span>

        <span class="k">with</span> <span class="n">closing</span><span class="p">(</span><span class="n">tarfile</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">fileobj</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r:&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="n">tarfile</span><span class="o">.</span><span class="n">PAX_FORMAT</span><span class="p">))</span> <span class="k">as</span> <span class="n">tar</span><span class="p">,</span> \
                <span class="n">mkdtemp</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmpdir</span><span class="p">:</span>

            <span class="n">tar</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="s1">&#39;storages&#39;</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">tmpdir</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">,</span> <span class="s1">&#39;storages&#39;</span><span class="p">),</span> <span class="s1">&#39;rb&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">num_storages</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_storages</span><span class="p">):</span>
                    <span class="n">args</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
                    <span class="n">key</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">storage_type</span> <span class="o">=</span> <span class="n">args</span>
                    <span class="n">dtype</span> <span class="o">=</span> <span class="n">storage_type</span><span class="o">.</span><span class="n">_dtype</span>
                    <span class="n">obj</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Storage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">)</span><span class="o">.</span><span class="n">_new_with_file</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
                    <span class="n">obj</span> <span class="o">=</span> <span class="n">restore_location</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">)</span>
                    <span class="c1"># TODO: Once we decide to break serialization FC, we can</span>
                    <span class="c1"># stop wrapping with TypedStorage</span>
                    <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                        <span class="n">wrap_storage</span><span class="o">=</span><span class="n">obj</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="n">storage_views</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">target_cdata</span><span class="p">,</span> <span class="n">root_cdata</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">numel</span> <span class="ow">in</span> <span class="n">storage_views</span><span class="p">:</span>
                    <span class="n">root</span> <span class="o">=</span> <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">root_cdata</span><span class="p">]</span>
                    <span class="n">element_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">root</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">offset_bytes</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">*</span> <span class="n">element_size</span>
                    <span class="c1"># TODO: Once we decide to break serialization FC, we can</span>
                    <span class="c1"># stop wrapping with TypedStorage</span>
                    <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">target_cdata</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                        <span class="n">wrap_storage</span><span class="o">=</span><span class="n">root</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">[</span><span class="n">offset_bytes</span><span class="p">:</span><span class="n">offset_bytes</span> <span class="o">+</span> <span class="n">numel</span> <span class="o">*</span> <span class="n">element_size</span><span class="p">],</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">root</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">tar</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="s1">&#39;tensors&#39;</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">tmpdir</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">,</span> <span class="s1">&#39;tensors&#39;</span><span class="p">),</span> <span class="s1">&#39;rb&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">num_tensors</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tensors</span><span class="p">):</span>
                    <span class="n">args</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
                    <span class="n">key</span><span class="p">,</span> <span class="n">storage_id</span><span class="p">,</span> <span class="n">original_tensor_type</span> <span class="o">=</span> <span class="n">args</span>
                    <span class="n">storage</span> <span class="o">=</span> <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">storage_id</span><span class="p">]</span>
                    <span class="n">ndim</span><span class="p">,</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="s1">&#39;&lt;i&#39;</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
                    <span class="c1"># skip next 4 bytes; legacy encoding treated ndim as 8 bytes</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
                    <span class="n">numel</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&lt;</span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s1">q&#39;</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="n">ndim</span><span class="p">))</span>
                    <span class="n">stride</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&lt;</span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s1">q&#39;</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="n">ndim</span><span class="p">))</span>
                    <span class="n">storage_offset</span><span class="p">,</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="s1">&#39;&lt;q&#39;</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
                    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span>
                        <span class="n">storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span> <span class="n">storage_offset</span><span class="p">,</span> <span class="n">numel</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
                    <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>

            <span class="n">pickle_file</span> <span class="o">=</span> <span class="n">tar</span><span class="o">.</span><span class="n">extractfile</span><span class="p">(</span><span class="s1">&#39;pickle&#39;</span><span class="p">)</span>
            <span class="n">unpickler</span> <span class="o">=</span> <span class="n">UnpicklerWrapper</span><span class="p">(</span><span class="n">pickle_file</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
            <span class="n">unpickler</span><span class="o">.</span><span class="n">persistent_load</span> <span class="o">=</span> <span class="n">persistent_load</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">unpickler</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">result</span>

    <span class="n">deserialized_objects</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">persistent_load</span><span class="p">(</span><span class="n">saved_id</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">saved_id</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
        <span class="n">typename</span> <span class="o">=</span> <span class="n">_maybe_decode_ascii</span><span class="p">(</span><span class="n">saved_id</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">saved_id</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">if</span> <span class="n">typename</span> <span class="o">==</span> <span class="s1">&#39;module&#39;</span><span class="p">:</span>
            <span class="c1"># Ignore containers that don&#39;t have any sources saved</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="n">_check_container_source</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">typename</span> <span class="o">==</span> <span class="s1">&#39;storage&#39;</span><span class="p">:</span>
            <span class="n">storage_type</span><span class="p">,</span> <span class="n">root_key</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">numel</span><span class="p">,</span> <span class="n">view_metadata</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">location</span> <span class="o">=</span> <span class="n">_maybe_decode_ascii</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">storage_type</span><span class="o">.</span><span class="n">dtype</span>

            <span class="n">nbytes</span> <span class="o">=</span> <span class="n">numel</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">root_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">deserialized_objects</span><span class="p">:</span>
                <span class="n">obj</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Storage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span><span class="n">nbytes</span><span class="p">))</span>
                <span class="n">obj</span><span class="o">.</span><span class="n">_torch_load_uninitialized</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># TODO: Once we decide to break serialization FC, we can</span>
                <span class="c1"># stop wrapping with TypedStorage</span>
                <span class="n">typed_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="n">restore_location</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">location</span><span class="p">),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">root_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">typed_storage</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">typed_storage</span> <span class="o">=</span> <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">root_key</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">typed_storage</span><span class="o">.</span><span class="n">_data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">typed_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                        <span class="n">device</span><span class="o">=</span><span class="n">typed_storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">view_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">view_key</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">view_size</span> <span class="o">=</span> <span class="n">view_metadata</span>
                <span class="n">offset_bytes</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">view_size_bytes</span> <span class="o">=</span> <span class="n">view_size</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">view_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">deserialized_objects</span><span class="p">:</span>
                    <span class="c1"># TODO: Once we decide to break serialization FC, we can</span>
                    <span class="c1"># stop wrapping with TypedStorage</span>
                    <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">view_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
                        <span class="n">wrap_storage</span><span class="o">=</span><span class="n">typed_storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">[</span><span class="n">offset_bytes</span><span class="p">:</span><span class="n">offset_bytes</span> <span class="o">+</span> <span class="n">view_size_bytes</span><span class="p">],</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">view_key</span><span class="p">]</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">typed_storage</span>
            <span class="k">return</span> <span class="n">res</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Unknown saved id type: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">saved_id</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">_check_seekable</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">f_should_read_directly</span> <span class="o">=</span> <span class="n">_should_read_directly</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">f_should_read_directly</span> <span class="ow">and</span> <span class="n">f</span><span class="o">.</span><span class="n">tell</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># legacy_load requires that f has fileno()</span>
        <span class="c1"># only if offset is zero we can attempt the legacy tar file loader</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">legacy_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">tarfile</span><span class="o">.</span><span class="n">TarError</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_is_zipfile</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
                <span class="c1"># .zip is used for torch.jit.save and will throw an un-pickling error here</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> is a zip archive (did you mean to use torch.jit.load()?)&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="bp">None</span>
            <span class="c1"># if not a tarfile, reset file offset and proceed</span>
            <span class="n">f</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s1">&#39;readinto&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;torch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Received object of type </span><span class="se">\&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="si">}</span><span class="se">\&quot;</span><span class="s2">. Please update to Python 3.8.2 or newer to restore this &quot;</span>
            <span class="s2">&quot;functionality.&quot;</span><span class="p">)</span>

    <span class="n">magic_number</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">magic_number</span> <span class="o">!=</span> <span class="n">MAGIC_NUMBER</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid magic number; corrupt file?&quot;</span><span class="p">)</span>
    <span class="n">protocol_version</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">protocol_version</span> <span class="o">!=</span> <span class="n">PROTOCOL_VERSION</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid protocol version: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">protocol_version</span><span class="p">)</span>

    <span class="n">_sys_info</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
    <span class="n">unpickler</span> <span class="o">=</span> <span class="n">UnpicklerWrapper</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
    <span class="n">unpickler</span><span class="o">.</span><span class="n">persistent_load</span> <span class="o">=</span> <span class="n">persistent_load</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">unpickler</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

    <span class="n">deserialized_storage_keys</span> <span class="o">=</span> <span class="n">pickle_module</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>

    <span class="n">offset</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">tell</span><span class="p">()</span> <span class="k">if</span> <span class="n">f_should_read_directly</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">deserialized_storage_keys</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">deserialized_objects</span>
        <span class="n">typed_storage</span> <span class="o">=</span> <span class="n">deserialized_objects</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="n">typed_storage</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_set_from_file</span><span class="p">(</span>
            <span class="n">f</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">f_should_read_directly</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">typed_storage</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">offset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">tell</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_validate_loaded_sparse_tensors</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">_maybe_decode_ascii</span><span class="p">(</span><span class="n">bytes_str</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bytes</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># When using encoding=&#39;bytes&#39; in Py3, some **internal** keys stored as</span>
    <span class="c1"># strings in Py2 are loaded as bytes. This function decodes them with</span>
    <span class="c1"># ascii encoding, one that Py3 uses by default.</span>
    <span class="c1">#</span>
    <span class="c1"># NOTE: This should only be used on internal keys (e.g., `typename` and</span>
    <span class="c1">#       `location` in `persistent_load` below!</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bytes_str</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">bytes_str</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bytes_str</span>


<span class="k">def</span> <span class="nf">_get_restore_location</span><span class="p">(</span><span class="n">map_location</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">map_location</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">restore_location</span> <span class="o">=</span> <span class="n">default_restore_location</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">map_location</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
            <span class="n">location</span> <span class="o">=</span> <span class="n">map_location</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">location</span><span class="p">,</span> <span class="n">location</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">default_restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">map_location</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">bytes</span><span class="p">)):</span>
        <span class="k">def</span> <span class="nf">restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">default_restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">map_location</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">map_location</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">default_restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">map_location</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">map_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">default_restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span>
    <span class="k">return</span> <span class="n">restore_location</span>


<span class="k">class</span> <span class="nc">StorageType</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">_get_dtype_from_pickle_storage_type</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;StorageType(dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">)&#39;</span>


<span class="k">def</span> <span class="nf">_load</span><span class="p">(</span><span class="n">zip_file</span><span class="p">,</span> <span class="n">map_location</span><span class="p">,</span> <span class="n">pickle_module</span><span class="p">,</span> <span class="n">pickle_file</span><span class="o">=</span><span class="s1">&#39;data.pkl&#39;</span><span class="p">,</span> <span class="n">overall_storage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">):</span>
    <span class="n">restore_location</span> <span class="o">=</span> <span class="n">_get_restore_location</span><span class="p">(</span><span class="n">map_location</span><span class="p">)</span>

    <span class="n">loaded_storages</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># check if byteswapping is needed</span>
    <span class="n">byteordername</span> <span class="o">=</span> <span class="s1">&#39;byteorder&#39;</span>
    <span class="n">byteorderdata</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">has_record</span><span class="p">(</span><span class="n">byteordername</span><span class="p">):</span>
        <span class="n">byteorderdata</span> <span class="o">=</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">get_record</span><span class="p">(</span><span class="n">byteordername</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">byteorderdata</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="sa">b</span><span class="s1">&#39;little&#39;</span><span class="p">,</span> <span class="sa">b</span><span class="s1">&#39;big&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown endianness type: &#39;</span> <span class="o">+</span> <span class="n">byteorderdata</span><span class="o">.</span><span class="n">decode</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">load_tensor</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">numel</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">location</span><span class="p">):</span>
        <span class="n">name</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;data/</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="n">overall_storage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">storage_offset</span> <span class="o">=</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">get_record_offset</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="n">storage</span> <span class="o">=</span> <span class="n">overall_storage</span><span class="p">[</span><span class="n">storage_offset</span><span class="p">:</span><span class="n">storage_offset</span> <span class="o">+</span> <span class="n">numel</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">storage</span> <span class="o">=</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">get_storage_from_record</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">numel</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">)</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_untyped_storage</span>
        <span class="c1"># swap here if byteswapping is needed</span>
        <span class="k">if</span> <span class="n">byteorderdata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">byteorderdata</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span> <span class="o">!=</span> <span class="n">sys</span><span class="o">.</span><span class="n">byteorder</span><span class="p">:</span>
                <span class="n">storage</span><span class="o">.</span><span class="n">byteswap</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># TODO: Once we decide to break serialization FC, we can</span>
        <span class="c1"># stop wrapping with TypedStorage</span>
        <span class="n">typed_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">TypedStorage</span><span class="p">(</span>
            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">restore_location</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">location</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">typed_storage</span><span class="o">.</span><span class="n">_data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loaded_storages</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">typed_storage</span>

        <span class="k">return</span> <span class="n">typed_storage</span>

    <span class="k">def</span> <span class="nf">persistent_load</span><span class="p">(</span><span class="n">saved_id</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">saved_id</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
        <span class="n">typename</span> <span class="o">=</span> <span class="n">_maybe_decode_ascii</span><span class="p">(</span><span class="n">saved_id</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">saved_id</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">assert</span> <span class="n">typename</span> <span class="o">==</span> <span class="s1">&#39;storage&#39;</span><span class="p">,</span> \
            <span class="sa">f</span><span class="s2">&quot;Unknown typename for persistent_load, expected &#39;storage&#39; but got &#39;</span><span class="si">{</span><span class="n">typename</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
        <span class="n">storage_type</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">numel</span> <span class="o">=</span> <span class="n">data</span>
        <span class="k">if</span> <span class="n">storage_type</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">storage_type</span><span class="o">.</span><span class="n">dtype</span>

        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">loaded_storages</span><span class="p">:</span>
            <span class="n">typed_storage</span> <span class="o">=</span> <span class="n">loaded_storages</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nbytes</span> <span class="o">=</span> <span class="n">numel</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">typed_storage</span> <span class="o">=</span> <span class="n">load_tensor</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">nbytes</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">_maybe_decode_ascii</span><span class="p">(</span><span class="n">location</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">typed_storage</span>

    <span class="n">load_module_mapping</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/pull/51633</span>
        <span class="s1">&#39;torch.tensor&#39;</span><span class="p">:</span> <span class="s1">&#39;torch._tensor&#39;</span>
    <span class="p">}</span>

    <span class="c1"># Need to subclass Unpickler instead of directly monkey-patching the find_class method</span>
    <span class="c1"># because it&#39;s marked readonly in pickle.</span>
    <span class="c1"># The type: ignore is because mypy can&#39;t statically determine the type of this class.</span>
    <span class="k">class</span> <span class="nc">UnpicklerWrapper</span><span class="p">(</span><span class="n">pickle_module</span><span class="o">.</span><span class="n">Unpickler</span><span class="p">):</span>  <span class="c1"># type: ignore[name-defined]</span>
        <span class="c1"># from https://stackoverflow.com/questions/13398462/unpickling-python-objects-with-a-changed-module-path/13405732</span>
        <span class="c1"># Lets us override the imports that pickle uses when unpickling an object.</span>
        <span class="c1"># This is useful for maintaining BC if we change a module path that tensor instantiation relies on.</span>
        <span class="k">def</span> <span class="nf">find_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_name</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span> <span class="ow">and</span> <span class="s1">&#39;Storage&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">StorageType</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="k">pass</span>
            <span class="n">mod_name</span> <span class="o">=</span> <span class="n">load_module_mapping</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">mod_name</span><span class="p">,</span> <span class="n">mod_name</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">find_class</span><span class="p">(</span><span class="n">mod_name</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># Load the data (which may in turn use `persistent_load` to load tensors)</span>
    <span class="n">data_file</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">zip_file</span><span class="o">.</span><span class="n">get_record</span><span class="p">(</span><span class="n">pickle_file</span><span class="p">))</span>

    <span class="n">unpickler</span> <span class="o">=</span> <span class="n">UnpicklerWrapper</span><span class="p">(</span><span class="n">data_file</span><span class="p">,</span> <span class="o">**</span><span class="n">pickle_load_args</span><span class="p">)</span>
    <span class="n">unpickler</span><span class="o">.</span><span class="n">persistent_load</span> <span class="o">=</span> <span class="n">persistent_load</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">unpickler</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_validate_loaded_sparse_tensors</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_metadata</span><span class="p">(</span>
        <span class="s2">&quot;torch.load.metadata&quot;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;serialization_id&quot;</span><span class="p">:</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">serialization_id</span><span class="p">()}</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">_is_torchscript_zip</span><span class="p">(</span><span class="n">zip_file</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;constants.pkl&#39;</span> <span class="ow">in</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">get_all_records</span><span class="p">()</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/sphinx_highlight.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
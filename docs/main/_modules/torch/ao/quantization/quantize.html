


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.ao.quantization.quantize &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/quantize.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/quantize.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../quantization.html">torch.ao.quantization</a> &gt;</li>
        
      <li>torch.ao.quantization.quantize</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.ao.quantization.quantize</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.ao.nn.quantized</span> <span class="k">as</span> <span class="nn">nnq</span>
<span class="kn">from</span> <span class="nn">torch.ao.nn.intrinsic</span> <span class="kn">import</span> <span class="n">_FusedModule</span>

<span class="kn">from</span> <span class="nn">torch.ao.quantization.quantization_mappings</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_default_dynamic_quant_module_mappings</span><span class="p">,</span>
    <span class="n">get_default_static_quant_module_mappings</span><span class="p">,</span>
    <span class="n">get_default_static_quant_reference_module_mappings</span><span class="p">,</span>
    <span class="n">get_default_qat_module_mappings</span><span class="p">,</span>
    <span class="n">get_default_qconfig_propagation_list</span><span class="p">,</span>
    <span class="n">no_observer_set</span><span class="p">,</span>
    <span class="n">_has_special_act_post_process</span><span class="p">,</span>
    <span class="n">_get_special_act_post_process</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">get_qparam_dict</span><span class="p">,</span> <span class="n">has_no_children_ignoring_parametrizations</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.stubs</span> <span class="kn">import</span> <span class="n">DeQuantStub</span><span class="p">,</span> <span class="n">QuantWrapper</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.qconfig</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_add_module_to_qconfig_obs_ctr</span><span class="p">,</span>
    <span class="n">default_dynamic_qconfig</span><span class="p">,</span>
    <span class="n">float16_dynamic_qconfig</span><span class="p">,</span>
    <span class="n">float_qparams_weight_only_qconfig</span><span class="p">,</span>
    <span class="n">float_qparams_weight_only_qconfig_4bit</span><span class="p">,</span>
    <span class="n">_activation_is_memoryless</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.parametrize</span> <span class="kn">import</span> <span class="n">type_before_parametrizations</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.observer</span> <span class="kn">import</span> <span class="n">_is_activation_post_process</span>

<span class="c1"># TODO remove this once BC is no longer required to avoid a SEV</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.observer</span> <span class="kn">import</span> <span class="p">(</span>   <span class="c1"># noqa: F401</span>
    <span class="n">_is_activation_post_process</span> <span class="k">as</span> <span class="n">is_activation_post_process</span>
<span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;get_default_custom_config_dict&quot;</span><span class="p">,</span>
    <span class="s2">&quot;propagate_qconfig_&quot;</span><span class="p">,</span>
    <span class="s2">&quot;add_quant_dequant&quot;</span><span class="p">,</span>
    <span class="s2">&quot;prepare&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_dynamic&quot;</span><span class="p">,</span>
    <span class="s2">&quot;prepare_qat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;quantize_qat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;convert&quot;</span><span class="p">,</span>
    <span class="s2">&quot;swap_module&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">_DEFAULT_CUSTOM_CONFIG_DICT</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;float_to_observed_custom_module_class&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">quantizable</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">quantizable</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s1">&#39;observed_to_quantized_custom_module_class&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">quantizable</span><span class="o">.</span><span class="n">LSTM</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">quantizable</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">get_default_custom_config_dict</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Defines the default custom config dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_DEFAULT_CUSTOM_CONFIG_DICT</span>

<span class="k">def</span> <span class="nf">_propagate_qconfig_helper</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">,</span>
                              <span class="n">qconfig_parent</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">prepare_custom_config_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This is a helper function for `propagate_qconfig_`</span>

<span class="sd">    Args:</span>
<span class="sd">        module: input module</span>
<span class="sd">        qconfig_dict: dictionary that maps from name of submodule to quantization</span>
<span class="sd">                     configuration</span>
<span class="sd">        qconfig_parent: quantization config of parent module, we will fallback to</span>
<span class="sd">                       this config when there is no specified config for current</span>
<span class="sd">                       module</span>
<span class="sd">        prefix: corresponding prefix of the current module, used as key in</span>
<span class="sd">                qconfig_dict</span>
<span class="sd">        prepare_custom_config_dict: dictionary for custom handling of modules</span>
<span class="sd">                                    see docs for :func:`~torch.ao.quantization.prepare_fx`</span>

<span class="sd">    Return:</span>
<span class="sd">        None, module is modified inplace with qconfig attached</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">module_qconfig</span> <span class="o">=</span> <span class="n">qconfig_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">module</span><span class="p">),</span> <span class="n">qconfig_parent</span><span class="p">)</span>
    <span class="n">module_qconfig</span> <span class="o">=</span> <span class="n">qconfig_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">module_qconfig</span><span class="p">)</span>
    <span class="n">module_qconfig</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">,</span> <span class="n">module_qconfig</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">qconfig</span><span class="o">.</span><span class="n">_assert_valid_qconfig</span><span class="p">(</span><span class="n">module_qconfig</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>

    <span class="n">qconfig_with_device_check</span> <span class="o">=</span> <span class="n">_add_module_to_qconfig_obs_ctr</span><span class="p">(</span><span class="n">module_qconfig</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">qconfig_with_device_check</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="n">module_prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> <span class="n">name</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="n">name</span>
        <span class="c1">#  do no not propagate qconfig to child if child is non traceable</span>
        <span class="k">if</span> <span class="n">prepare_custom_config_dict</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="n">name</span> <span class="ow">in</span> <span class="n">prepare_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;non_traceable_module_name&quot;</span><span class="p">,</span> <span class="p">[])</span>
            <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">child</span><span class="p">)</span> <span class="ow">in</span> <span class="n">prepare_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;non_traceable_module_class&quot;</span><span class="p">,</span> <span class="p">[])</span>
        <span class="p">):</span>
            <span class="n">_propagate_qconfig_helper</span><span class="p">(</span>
                <span class="n">child</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">,</span> <span class="n">qconfig_with_device_check</span><span class="p">,</span> <span class="n">module_prefix</span>
            <span class="p">)</span>

<div class="viewcode-block" id="propagate_qconfig_"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.propagate_qconfig_.html#torch.ao.quantization.propagate_qconfig_">[docs]</a><span class="k">def</span> <span class="nf">propagate_qconfig_</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prepare_custom_config_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Propagate qconfig through the module hierarchy and assign `qconfig`</span>
<span class="sd">    attribute on each leaf module</span>

<span class="sd">    Args:</span>
<span class="sd">        module: input module</span>
<span class="sd">        qconfig_dict: dictionary that maps from name or type of submodule to</span>
<span class="sd">            quantization configuration, qconfig applies to all submodules of a</span>
<span class="sd">            given module unless qconfig for the submodules are specified (when</span>
<span class="sd">            the submodule already has qconfig attribute)</span>
<span class="sd">        prepare_custom_config_dict: dictionary for custom handling of modules</span>
<span class="sd">            see docs for :func:`~torch.ao.quantization.prepare_fx`</span>

<span class="sd">    Return:</span>
<span class="sd">        None, module is modified inplace with qconfig attached</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">qconfig_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">prepare_custom_config_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prepare_custom_config_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">_propagate_qconfig_helper</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">,</span> <span class="n">prepare_custom_config_dict</span><span class="o">=</span><span class="n">prepare_custom_config_dict</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_observer_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward hook that calls observer on the output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_post_process</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_observer_forward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward pre hook that calls observer on the output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_post_process</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">_register_activation_post_process_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">pre_hook</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;activation_post_process&#39;</span><span class="p">),</span> \
        <span class="s1">&#39;Expect activation_post_process attribute already attached to the module&#39;</span>
    <span class="k">if</span> <span class="n">pre_hook</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span>
            <span class="n">_observer_forward_pre_hook</span><span class="p">,</span> <span class="n">prepend</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span>
            <span class="n">_observer_forward_hook</span><span class="p">,</span> <span class="n">prepend</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_add_observer_</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">qconfig_propagation_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_leaf_module_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_module_class_mapping</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Add observer for the leaf child of the module.</span>

<span class="sd">    This function insert observer module to all leaf child module that</span>
<span class="sd">    has a valid qconfig attribute.</span>

<span class="sd">    Args:</span>
<span class="sd">        module: input module with qconfig attributes for all the leaf modules that we want to quantize</span>
<span class="sd">        qconfig_propagation_list: a list of quantizable modules that will have observers added to them</span>
<span class="sd">            if they are leaf nodes</span>
<span class="sd">        device: parent device, if any</span>
<span class="sd">        non_leaf_module_list: list of non-leaf modules we want to add observer</span>

<span class="sd">    Return:</span>
<span class="sd">        None, module is modified inplace with added observer modules and forward_hooks</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">qconfig_propagation_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qconfig_propagation_list</span> <span class="o">=</span> <span class="n">get_default_qconfig_propagation_list</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">custom_module_class_mapping</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">custom_module_class_mapping</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># respect device affinity when adding observers</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="n">_get_unique_devices_</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;_add_observer_ only works with cpu or single-device CUDA modules, &quot;</span>
            <span class="s2">&quot;but got devices </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">devices</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">get_activation_post_process</span><span class="p">(</span><span class="n">qconfig</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">special_act_post_process</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">()</span> <span class="k">if</span> <span class="n">special_act_post_process</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">special_act_post_process</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">activation</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">activation</span>

    <span class="k">def</span> <span class="nf">needs_observation</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">qconfig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">insert_activation_post_process</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">special_act_post_process</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Adds an activation post process module and register</span>
<span class="sd">        a pre or post hook that calls the module</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We don&#39;t insert observer/fake_quantize for DeQuantStub</span>
        <span class="k">if</span> <span class="n">needs_observation</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">DeQuantStub</span><span class="p">):</span>
            <span class="c1"># observer and hook will be gone after we swap the module</span>
            <span class="n">m</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;activation_post_process&#39;</span><span class="p">,</span> <span class="n">get_activation_post_process</span><span class="p">(</span>
                <span class="n">m</span><span class="o">.</span><span class="n">qconfig</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">special_act_post_process</span><span class="p">))</span>
            <span class="c1"># Register observer as the first entry in the hook list</span>
            <span class="c1"># All post forward hooks are preserved and will be executed after the observer before convert</span>
            <span class="n">_register_activation_post_process_hook</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">pre_hook</span><span class="o">=</span><span class="n">_activation_is_memoryless</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">qconfig</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="c1"># TODO remove Dropout special after codebase stable</span>
        <span class="k">if</span> <span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">child</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">]:</span>
            <span class="k">continue</span>
        <span class="k">elif</span> <span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">child</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="n">nnq</span><span class="o">.</span><span class="n">FloatFunctional</span><span class="p">,</span> <span class="n">nnq</span><span class="o">.</span><span class="n">QFunctional</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">needs_observation</span><span class="p">(</span><span class="n">child</span><span class="p">):</span>
                <span class="n">child</span><span class="o">.</span><span class="n">activation_post_process</span> <span class="o">=</span> <span class="n">get_activation_post_process</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">qconfig</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">_FusedModule</span><span class="p">):</span>
            <span class="c1"># activation_post_process are now added directly to nn.Sequential/_FusedModule</span>
            <span class="k">if</span> <span class="n">needs_observation</span><span class="p">(</span><span class="n">child</span><span class="p">):</span>
                <span class="n">insert_activation_post_process</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">non_leaf_module_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">child</span><span class="p">)</span> <span class="ow">in</span> <span class="n">non_leaf_module_list</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">needs_observation</span><span class="p">(</span><span class="n">child</span><span class="p">):</span>
                <span class="n">insert_activation_post_process</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_has_special_act_post_process</span><span class="p">(</span><span class="n">child</span><span class="p">):</span>
            <span class="n">special_act_post_process</span> <span class="o">=</span> <span class="n">_get_special_act_post_process</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
            <span class="n">insert_activation_post_process</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">special_act_post_process</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">needs_observation</span><span class="p">(</span><span class="n">child</span><span class="p">)</span> <span class="ow">and</span> <span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">child</span><span class="p">)</span> <span class="ow">in</span> <span class="n">custom_module_class_mapping</span><span class="p">:</span>
            <span class="n">observed_child</span> <span class="o">=</span> <span class="n">custom_module_class_mapping</span><span class="p">[</span><span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">child</span><span class="p">)]</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">observed_child</span><span class="p">)</span>
            <span class="c1"># TODO: These are the modules that cannot be observed</span>
            <span class="c1">#       Once there are more, we should move them to a separate list</span>
            <span class="k">if</span> <span class="n">custom_module_class_mapping</span><span class="p">[</span><span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">child</span><span class="p">)]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">no_observer_set</span><span class="p">():</span>
                <span class="n">insert_activation_post_process</span><span class="p">(</span><span class="n">observed_child</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_add_observer_</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">qconfig_propagation_list</span><span class="p">,</span> <span class="n">non_leaf_module_list</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">custom_module_class_mapping</span><span class="p">)</span>

    <span class="c1"># Insert observers only for leaf nodes, note that this observer is for</span>
    <span class="c1"># the output of the module, for input QuantStub will observe them</span>
    <span class="k">if</span> <span class="n">has_no_children_ignoring_parametrizations</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span> \
       <span class="ow">and</span> <span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="ow">in</span> <span class="n">qconfig_propagation_list</span><span class="p">:</span>
        <span class="n">insert_activation_post_process</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_get_unique_devices_</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span> <span class="o">|</span> \
        <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">()}</span>

<div class="viewcode-block" id="add_quant_dequant"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.add_quant_dequant.html#torch.ao.quantization.add_quant_dequant">[docs]</a><span class="k">def</span> <span class="nf">add_quant_dequant</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wrap the leaf child module in QuantWrapper if it has a valid qconfig</span>
<span class="sd">    Note that this function will modify the children of module inplace and it</span>
<span class="sd">    can return a new module which wraps the input module as well.</span>

<span class="sd">    Args:</span>
<span class="sd">        module: input module with qconfig attributes for all the leaf modules</span>
<span class="sd">        that we want to quantize</span>

<span class="sd">    Return:</span>
<span class="sd">        Either the inplace modified module with submodules wrapped in</span>
<span class="sd">        `QuantWrapper` based on qconfig or a new `QuantWrapper` module which</span>
<span class="sd">        wraps the input module, the latter case only happens when the input</span>
<span class="sd">        module is a leaf module and we want to quantize it.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">has_no_children_ignoring_parametrizations</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">qconfig</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">QuantWrapper</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">add_quant_dequant</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span></div>

<div class="viewcode-block" id="prepare"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.prepare.html#torch.ao.quantization.prepare">[docs]</a><span class="k">def</span> <span class="nf">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">allow_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">observer_non_leaf_module_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">prepare_custom_config_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Prepares a copy of the model for quantization calibration or quantization-aware training.</span>

<span class="sd">    Quantization configuration should be assigned preemptively</span>
<span class="sd">    to individual submodules in `.qconfig` attribute.</span>

<span class="sd">    The model will be attached with observer or fake quant modules, and qconfig</span>
<span class="sd">    will be propagated.</span>

<span class="sd">    Args:</span>
<span class="sd">        `model`: input model to be modified in-place</span>
<span class="sd">        `inplace`: carry out model transformations in-place, the original module is mutated</span>
<span class="sd">        `allow_list`: list of quantizable modules</span>
<span class="sd">        `observer_non_leaf_module_list`: list of non-leaf modules we want to add observer</span>
<span class="sd">        `prepare_custom_config_dict`: customization configuration dictionary for prepare function</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">       # Example of prepare_custom_config_dict:</span>
<span class="sd">       prepare_custom_config_dict = {</span>
<span class="sd">           # user will manually define the corresponding observed</span>
<span class="sd">           # module class which has a from_float class method that converts</span>
<span class="sd">           # float custom module to observed custom module</span>
<span class="sd">           &quot;float_to_observed_custom_module_class&quot;: {</span>
<span class="sd">               CustomModule: ObservedCustomModule</span>
<span class="sd">           }</span>
<span class="sd">        }</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize.prepare&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prepare_custom_config_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prepare_custom_config_dict</span> <span class="o">=</span> <span class="n">get_default_custom_config_dict</span><span class="p">()</span>
    <span class="n">custom_module_class_mapping</span> <span class="o">=</span> <span class="n">prepare_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;float_to_observed_custom_module_class&quot;</span><span class="p">,</span> <span class="p">{})</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># TODO: remove allow_list</span>
    <span class="n">qconfig_propagation_list</span> <span class="o">=</span> <span class="n">allow_list</span>
    <span class="k">if</span> <span class="n">allow_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qconfig_propagation_list</span> <span class="o">=</span> <span class="n">get_default_qconfig_propagation_list</span><span class="p">()</span>
    <span class="n">propagate_qconfig_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="c1"># sanity check common API misusage</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">qconfig</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;None of the submodule got qconfig applied. Make sure you &quot;</span>
                      <span class="s2">&quot;passed correct configuration through `qconfig_dict` or &quot;</span>
                      <span class="s2">&quot;by assigning the `.qconfig` attribute directly on submodules&quot;</span><span class="p">)</span>

    <span class="n">_add_observer_</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">qconfig_propagation_list</span><span class="p">,</span> <span class="n">observer_non_leaf_module_list</span><span class="p">,</span>
        <span class="n">custom_module_class_mapping</span><span class="o">=</span><span class="n">custom_module_class_mapping</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>

<span class="k">def</span> <span class="nf">_remove_activation_post_process</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="c1"># TODO: maybe we should change activation_post_process to _activation_post_process</span>
    <span class="c1"># to prevent it from being used by user</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;activation_post_process&#39;</span><span class="p">)</span> <span class="ow">and</span> \
       <span class="n">_is_activation_post_process</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">activation_post_process</span><span class="p">):</span>
        <span class="nb">delattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;activation_post_process&#39;</span><span class="p">)</span>

    <span class="c1"># remove activation_post_process pre and post hooks</span>
    <span class="k">def</span> <span class="nf">remove_hooks</span><span class="p">(</span><span class="n">pre_hook</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">hook_map</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="k">if</span> <span class="n">pre_hook</span> <span class="k">else</span> <span class="n">module</span><span class="o">.</span><span class="n">_forward_hooks</span>
        <span class="n">observer_hook</span> <span class="o">=</span> <span class="n">_observer_forward_pre_hook</span> <span class="k">if</span> <span class="n">pre_hook</span> <span class="k">else</span> <span class="n">_observer_forward_hook</span>
        <span class="n">handle_ids_to_remove</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">handle_id</span><span class="p">,</span> <span class="n">hook_fn</span> <span class="ow">in</span> <span class="n">hook_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">hook_fn</span> <span class="ow">is</span> <span class="n">observer_hook</span><span class="p">:</span>
                <span class="n">handle_ids_to_remove</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">handle_id</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">handle_id</span> <span class="ow">in</span> <span class="n">handle_ids_to_remove</span><span class="p">:</span>
            <span class="n">hook_map</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">handle_id</span><span class="p">)</span>

    <span class="n">remove_hooks</span><span class="p">(</span><span class="n">pre_hook</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">remove_hooks</span><span class="p">(</span><span class="n">pre_hook</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># TODO: rename to something more general</span>
<span class="k">def</span> <span class="nf">_remove_qconfig</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clean up the qconfig left in the module so that new qconfig can be</span>
<span class="sd">    propagated.</span>

<span class="sd">    Args:</span>
<span class="sd">        module: module to be cleaned up</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">_remove_qconfig</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;qconfig&quot;</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">module</span><span class="o">.</span><span class="n">qconfig</span>

    <span class="n">_remove_activation_post_process</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

<div class="viewcode-block" id="quantize"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.quantize.html#torch.ao.quantization.quantize">[docs]</a><span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">run_fn</span><span class="p">,</span> <span class="n">run_args</span><span class="p">,</span> <span class="n">mapping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Quantize the input float model with post training static quantization.</span>

<span class="sd">    First it will prepare the model for calibration, then it calls</span>
<span class="sd">    `run_fn` which will run the calibration step, after that we will</span>
<span class="sd">    convert the model to a quantized model.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: input float model</span>
<span class="sd">        run_fn: a calibration function for calibrating the prepared model</span>
<span class="sd">        run_args: positional arguments for `run_fn`</span>
<span class="sd">        inplace: carry out model transformations in-place, the original module is mutated</span>
<span class="sd">        mapping: correspondence between original module types and quantized counterparts</span>

<span class="sd">    Return:</span>
<span class="sd">        Quantized model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize.quantize&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mapping</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mapping</span> <span class="o">=</span> <span class="n">get_default_static_quant_module_mappings</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">run_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">run_args</span><span class="p">)</span>
    <span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mapping</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>

<div class="viewcode-block" id="quantize_dynamic"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.quantize_dynamic.html#torch.ao.quantization.quantize_dynamic">[docs]</a><span class="k">def</span> <span class="nf">quantize_dynamic</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig_spec</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                     <span class="n">mapping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Converts a float model to dynamic (i.e. weights-only) quantized model.</span>

<span class="sd">    Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.</span>

<span class="sd">    For simplest usage provide `dtype` argument that can be float16 or qint8. Weight-only quantization</span>
<span class="sd">    by default is performed for layers with large weights size - i.e. Linear and RNN variants.</span>

<span class="sd">    Fine grained control is possible with `qconfig` and `mapping` that act similarly to `quantize()`.</span>
<span class="sd">    If `qconfig` is provided, the `dtype` argument is ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: input model</span>
<span class="sd">        qconfig_spec: Either:</span>

<span class="sd">            - A dictionary that maps from name or type of submodule to quantization</span>
<span class="sd">              configuration, qconfig applies to all submodules of a given</span>
<span class="sd">              module unless qconfig for the submodules are specified (when the</span>
<span class="sd">              submodule already has qconfig attribute). Entries in the dictionary</span>
<span class="sd">              need to be QConfig instances.</span>

<span class="sd">            - A set of types and/or submodule names to apply dynamic quantization to,</span>
<span class="sd">              in which case the `dtype` argument is used to specify the bit-width</span>

<span class="sd">        inplace: carry out model transformations in-place, the original module is mutated</span>
<span class="sd">        mapping: maps type of a submodule to a type of corresponding dynamically quantized version</span>
<span class="sd">            with which the submodule needs to be replaced</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize.quantize_dynamic&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">qconfig_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">:</span>
            <span class="n">qconfig_spec</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="p">:</span> <span class="n">default_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span> <span class="p">:</span> <span class="n">default_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span> <span class="p">:</span> <span class="n">default_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span> <span class="p">:</span> <span class="n">default_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span> <span class="p">:</span> <span class="n">default_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span> <span class="p">:</span> <span class="n">default_dynamic_qconfig</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">qconfig_spec</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="p">:</span> <span class="n">float16_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span> <span class="p">:</span> <span class="n">float16_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span> <span class="p">:</span> <span class="n">float16_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span> <span class="p">:</span> <span class="n">float16_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span> <span class="p">:</span> <span class="n">float16_dynamic_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span> <span class="p">:</span> <span class="n">float16_dynamic_qconfig</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">:</span>
            <span class="n">qconfig_spec</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span> <span class="p">:</span> <span class="n">float_qparams_weight_only_qconfig</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span> <span class="p">:</span> <span class="n">float_qparams_weight_only_qconfig</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">:</span>
            <span class="n">qconfig_spec</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span> <span class="p">:</span> <span class="n">float_qparams_weight_only_qconfig_4bit</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Don&#39;t know how to quantize with default settings for </span><span class="si">{}</span><span class="s2">. Provide full qconfig please&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">qconfig_spec</span><span class="p">,</span> <span class="nb">set</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">:</span>
            <span class="n">default_qconfig</span> <span class="o">=</span> <span class="n">default_dynamic_qconfig</span>
        <span class="k">elif</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">default_qconfig</span> <span class="o">=</span> <span class="n">float16_dynamic_qconfig</span>
        <span class="k">elif</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">:</span>
            <span class="n">default_qconfig</span> <span class="o">=</span> <span class="n">float_qparams_weight_only_qconfig</span>
        <span class="k">elif</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">:</span>
            <span class="n">default_qconfig</span> <span class="o">=</span> <span class="n">float_qparams_weight_only_qconfig_4bit</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown dtype specified for quantize_dynamic: &#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
        <span class="n">qconfig_spec</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">qconfig_spec</span><span class="p">,</span> <span class="n">itertools</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">default_qconfig</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">mapping</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mapping</span> <span class="o">=</span> <span class="n">get_default_dynamic_quant_module_mappings</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">propagate_qconfig_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig_spec</span><span class="p">)</span>
    <span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mapping</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>

<div class="viewcode-block" id="prepare_qat"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.prepare_qat.html#torch.ao.quantization.prepare_qat">[docs]</a><span class="k">def</span> <span class="nf">prepare_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mapping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepares a copy of the model for quantization calibration or</span>
<span class="sd">    quantization-aware training and converts it to quantized version.</span>

<span class="sd">    Quantization configuration should be assigned preemptively</span>
<span class="sd">    to individual submodules in `.qconfig` attribute.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: input model to be modified in-place</span>
<span class="sd">        mapping: dictionary that maps float modules to quantized modules to be</span>
<span class="sd">                 replaced.</span>
<span class="sd">        inplace: carry out model transformations in-place, the original module</span>
<span class="sd">                 is mutated</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize.prepare_qat&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="s2">&quot;prepare_qat only works on models in training mode&quot;</span>
    <span class="k">if</span> <span class="n">mapping</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mapping</span> <span class="o">=</span> <span class="n">get_default_qat_module_mappings</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="n">propagate_qconfig_</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mapping</span><span class="o">=</span><span class="n">mapping</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_qconfig</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">observer_non_leaf_module_list</span><span class="o">=</span><span class="nb">set</span><span class="p">(</span><span class="n">mapping</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>

<div class="viewcode-block" id="quantize_qat"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.quantize_qat.html#torch.ao.quantization.quantize_qat">[docs]</a><span class="k">def</span> <span class="nf">quantize_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">run_fn</span><span class="p">,</span> <span class="n">run_args</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Do quantization aware training and output a quantized model</span>

<span class="sd">    Args:</span>
<span class="sd">        model: input model</span>
<span class="sd">        run_fn: a function for evaluating the prepared model, can be a</span>
<span class="sd">                function that simply runs the prepared model or a training</span>
<span class="sd">                loop</span>
<span class="sd">        run_args: positional arguments for `run_fn`</span>

<span class="sd">    Return:</span>
<span class="sd">        Quantized model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize.quantize_qat&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">prepare_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">run_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">run_args</span><span class="p">)</span>
    <span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span></div>

<div class="viewcode-block" id="convert"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.convert.html#torch.ao.quantization.convert">[docs]</a><span class="k">def</span> <span class="nf">convert</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span> <span class="n">mapping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">remove_qconfig</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">convert_custom_config_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Converts submodules in input module to a different module according to `mapping`</span>
<span class="sd">    by calling `from_float` method on the target module class. And remove qconfig at the</span>
<span class="sd">    end if remove_qconfig is set to True.</span>

<span class="sd">    Args:</span>
<span class="sd">        `module`: prepared and calibrated module</span>
<span class="sd">        `mapping`: a dictionary that maps from source module type to target</span>
<span class="sd">                   module type, can be overwritten to allow swapping user defined</span>
<span class="sd">                   Modules</span>
<span class="sd">        `inplace`: carry out model transformations in-place, the original module</span>
<span class="sd">                   is mutated</span>
<span class="sd">        `convert_custom_config_dict`: custom configuration dictionary for convert function</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">       # Example of convert_custom_config_dict:</span>
<span class="sd">       convert_custom_config_dict = {</span>
<span class="sd">           # user will manually define the corresponding quantized</span>
<span class="sd">           # module class which has a from_observed class method that converts</span>
<span class="sd">           # observed custom module to quantized custom module</span>
<span class="sd">           &quot;observed_to_quantized_custom_module_class&quot;: {</span>
<span class="sd">               ObservedCustomModule: QuantizedCustomModule</span>
<span class="sd">           }</span>
<span class="sd">       }</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize.convert&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
    <span class="n">_convert</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span> <span class="n">mapping</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_reference</span><span class="o">=</span><span class="n">is_reference</span><span class="p">,</span>
        <span class="n">convert_custom_config_dict</span><span class="o">=</span><span class="n">convert_custom_config_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">remove_qconfig</span><span class="p">:</span>
        <span class="n">_remove_qconfig</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span></div>

<span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span> <span class="n">mapping</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">convert_custom_config_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Converts submodules in input module to a different module according to `mapping`</span>
<span class="sd">    by calling `from_float` method on the target module class</span>

<span class="sd">    Args:</span>
<span class="sd">        module: input module</span>
<span class="sd">        mapping: a dictionary that maps from source module type to target</span>
<span class="sd">                 module type, can be overwritten to allow swapping user defined</span>
<span class="sd">                 Modules</span>
<span class="sd">        inplace: carry out model transformations in-place, the original module</span>
<span class="sd">                 is mutated</span>
<span class="sd">        is_reference: a flag to enable quantized reference module</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mapping</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mapping</span> <span class="o">=</span> <span class="n">get_default_static_quant_reference_module_mappings</span><span class="p">()</span> <span class="k">if</span> <span class="n">is_reference</span> \
            <span class="k">else</span> <span class="n">get_default_static_quant_module_mappings</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">convert_custom_config_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">convert_custom_config_dict</span> <span class="o">=</span> <span class="n">get_default_custom_config_dict</span><span class="p">()</span>
    <span class="n">custom_module_class_mapping</span> <span class="o">=</span> <span class="n">convert_custom_config_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;observed_to_quantized_custom_module_class&quot;</span><span class="p">,</span> <span class="p">{})</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
    <span class="n">reassign</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="c1"># both fused modules and observed custom modules are</span>
        <span class="c1"># swapped as one unit</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">_FusedModule</span><span class="p">)</span> <span class="ow">and</span> \
           <span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">custom_module_class_mapping</span><span class="p">:</span>
            <span class="n">_convert</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">mapping</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># inplace</span>
                     <span class="n">is_reference</span><span class="p">,</span> <span class="n">convert_custom_config_dict</span><span class="p">)</span>
        <span class="n">reassign</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">swap_module</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">mapping</span><span class="p">,</span> <span class="n">custom_module_class_mapping</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">reassign</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">return</span> <span class="n">module</span>

<div class="viewcode-block" id="swap_module"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.swap_module.html#torch.ao.quantization.swap_module">[docs]</a><span class="k">def</span> <span class="nf">swap_module</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">mapping</span><span class="p">,</span> <span class="n">custom_module_class_mapping</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Swaps the module if it has a quantized counterpart and it has an</span>
<span class="sd">    `observer` attached.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod: input module</span>
<span class="sd">        mapping: a dictionary that maps from nn module to nnq module</span>

<span class="sd">    Return:</span>
<span class="sd">        The corresponding quantized module of `mod`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">new_mod</span> <span class="o">=</span> <span class="n">mod</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">mod</span><span class="o">.</span><span class="n">qconfig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">swapped</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="ow">in</span> <span class="n">custom_module_class_mapping</span><span class="p">:</span>
            <span class="n">new_mod</span> <span class="o">=</span> <span class="n">custom_module_class_mapping</span><span class="p">[</span><span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">mod</span><span class="p">)]</span><span class="o">.</span><span class="n">from_observed</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
            <span class="n">swapped</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="ow">in</span> <span class="n">mapping</span><span class="p">:</span>
            <span class="n">qmod</span> <span class="o">=</span> <span class="n">mapping</span><span class="p">[</span><span class="n">type_before_parametrizations</span><span class="p">(</span><span class="n">mod</span><span class="p">)]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">qmod</span><span class="p">,</span> <span class="s1">&#39;_IS_REFERENCE&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">qmod</span><span class="o">.</span><span class="n">_IS_REFERENCE</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">mod</span><span class="o">.</span><span class="n">qconfig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="n">weight_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">()</span>
                <span class="n">weight_post_process</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
                <span class="n">weight_qparams</span> <span class="o">=</span> <span class="n">get_qparam_dict</span><span class="p">(</span><span class="n">weight_post_process</span><span class="p">)</span>
                <span class="n">new_mod</span> <span class="o">=</span> <span class="n">qmod</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">weight_qparams</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_mod</span> <span class="o">=</span> <span class="n">qmod</span><span class="o">.</span><span class="n">from_float</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
            <span class="n">swapped</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">swapped</span><span class="p">:</span>
            <span class="c1"># Preserve module&#39;s pre forward hooks. They&#39;ll be called on quantized input</span>
            <span class="k">for</span> <span class="n">pre_hook_fn</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">new_mod</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">pre_hook_fn</span><span class="p">)</span>
            <span class="c1"># Preserve module&#39;s post forward hooks except _observer_forward_hook</span>
            <span class="c1"># After convert they&#39;ll work with quantized output</span>
            <span class="k">for</span> <span class="n">hook_fn</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">hook_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">_observer_forward_hook</span><span class="p">:</span>
                    <span class="n">new_mod</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">)</span>

            <span class="c1"># respect device affinity when swapping modules</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">_get_unique_devices_</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;swap_module only works with cpu or single-device CUDA modules, &quot;</span>
                <span class="s2">&quot;but got devices </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">devices</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">device</span><span class="p">:</span>
                <span class="n">new_mod</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_mod</span></div>

<span class="k">def</span> <span class="nf">_get_observer_dict</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Traverse the modules and save all observers into dict.</span>
<span class="sd">    This is mainly used for quantization accuracy debug</span>
<span class="sd">    Args:</span>
<span class="sd">        mod: the top module we want to save all observers</span>
<span class="sd">        prefix: the prefix for the current module</span>
<span class="sd">        target_dict: the dictionary used to save all the observers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">get_prefix</span><span class="p">(</span><span class="n">prefix</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">prefix</span> <span class="k">if</span> <span class="n">prefix</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span> <span class="k">else</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s1">&#39;activation_post_process&#39;</span><span class="p">):</span>
        <span class="n">target_dict</span><span class="p">[</span><span class="n">get_prefix</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;activation_post_process&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">activation_post_process</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="n">module_prefix</span> <span class="o">=</span> <span class="n">get_prefix</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span> <span class="k">if</span> <span class="n">prefix</span> <span class="k">else</span> <span class="n">name</span>
        <span class="n">_get_observer_dict</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">target_dict</span><span class="p">,</span> <span class="n">module_prefix</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.ao.quantization.qconfig &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/qconfig.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/qconfig.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../quantization.html">torch.ao.quantization</a> &gt;</li>
        
      <li>torch.ao.quantization.qconfig</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.ao.quantization.qconfig</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.fake_quantize</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FakeQuantize</span><span class="p">,</span>
    <span class="n">FakeQuantizeBase</span><span class="p">,</span>
    <span class="n">default_fake_quant</span><span class="p">,</span>
    <span class="n">default_dynamic_fake_quant</span><span class="p">,</span>
    <span class="n">default_per_channel_weight_fake_quant</span><span class="p">,</span>
    <span class="n">default_weight_fake_quant</span><span class="p">,</span>
    <span class="n">default_fused_act_fake_quant</span><span class="p">,</span>
    <span class="n">default_fused_wt_fake_quant</span><span class="p">,</span>
    <span class="n">FusedMovingAvgObsFakeQuantize</span><span class="p">,</span>
    <span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">,</span>
    <span class="n">default_embedding_fake_quant</span><span class="p">,</span>
    <span class="n">default_embedding_fake_quant_4bit</span><span class="p">,</span>
    <span class="n">fused_wt_fake_quant_range_neg_127_to_127</span><span class="p">,</span>
    <span class="n">fused_per_channel_wt_fake_quant_range_neg_127_to_127</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">.observer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_PartialWrapper</span><span class="p">,</span>
    <span class="n">MinMaxObserver</span><span class="p">,</span>
    <span class="n">HistogramObserver</span><span class="p">,</span>
    <span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
    <span class="n">NoopObserver</span><span class="p">,</span>
    <span class="n">PlaceholderObserver</span><span class="p">,</span>
    <span class="n">ReuseInputObserver</span><span class="p">,</span>
    <span class="n">default_debug_observer</span><span class="p">,</span>
    <span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
    <span class="n">default_float_qparams_observer</span><span class="p">,</span>
    <span class="n">default_float_qparams_observer_4bit</span><span class="p">,</span>
    <span class="n">default_observer</span><span class="p">,</span>
    <span class="n">default_per_channel_weight_observer</span><span class="p">,</span>
    <span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">default_weight_observer</span><span class="p">,</span>
    <span class="n">weight_observer_range_neg_127_to_127</span><span class="p">,</span>
    <span class="n">per_channel_weight_observer_range_neg_127_to_127</span><span class="p">,</span>
    <span class="n">default_reuse_input_observer</span><span class="p">,</span>
    <span class="n">ObserverBase</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;QConfig&quot;</span><span class="p">,</span>
    <span class="c1"># TODO: deprecated, remove</span>
    <span class="s2">&quot;QConfigDynamic&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_debug_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_per_channel_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_dynamic_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float16_dynamic_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float16_static_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;per_channel_dynamic_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float_qparams_weight_only_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;float_qparams_weight_only_qconfig_4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_quint8_weight_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_qat_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_dynamic_qat_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_weight_only_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_activation_only_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_qat_qconfig_v2&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_reuse_input_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_symmetric_qnnpack_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_per_channel_symmetric_qnnpack_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_symmetric_qnnpack_qat_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_per_channel_symmetric_qnnpack_qat_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_embedding_qat_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_embedding_qat_qconfig_4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_default_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_default_qat_qconfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_default_qconfig_dict&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_default_qat_qconfig_dict&quot;</span><span class="p">,</span>
    <span class="s2">&quot;QConfigAny&quot;</span><span class="p">,</span>
    <span class="s2">&quot;qconfig_equals&quot;</span><span class="p">,</span>

<span class="p">]</span>

<div class="viewcode-block" id="QConfig"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig">[docs]</a><span class="k">class</span> <span class="nc">QConfig</span><span class="p">(</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;QConfig&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">])):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Describes how to quantize a layer or a part of the network by providing</span>
<span class="sd">    settings (observer classes) for activations and weights respectively.</span>


<span class="sd">    Note that QConfig needs to contain observer **classes** (like MinMaxObserver) or a callable that returns</span>
<span class="sd">    instances on invocation, not the concrete observer instances themselves.</span>
<span class="sd">    Quantization preparation function will instantiate observers multiple times for each of the layers.</span>


<span class="sd">    Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`</span>
<span class="sd">    method (that behaves like functools.partial)::</span>

<span class="sd">      my_qconfig = QConfig(</span>
<span class="sd">          activation=MinMaxObserver.with_args(dtype=torch.qint8),</span>
<span class="sd">          weight=default_observer.with_args(dtype=torch.qint8))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="c1"># catch common mistakes</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;QConfig received observer instance, please pass observer class instead. &quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;Use MyObserver.with_args(x=1) to override arguments to constructor if needed&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">QConfig</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">QConfigDynamic</span><span class="p">(</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;QConfigDynamic&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">])):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Describes how to dynamically quantize a layer or a part of the network by providing</span>
<span class="sd">    settings (observer classes) for weights.</span>

<span class="sd">    It&#39;s like QConfig, but for dynamic quantization.</span>

<span class="sd">    Note that QConfigDynamic needs to contain observer **classes** (like MinMaxObserver) or a callable that returns</span>
<span class="sd">    instances on invocation, not the concrete observer instances themselves.</span>
<span class="sd">    Quantization function will instantiate observers multiple times for each of the layers.</span>

<span class="sd">    Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`</span>
<span class="sd">    method (that behaves like functools.partial)::</span>

<span class="sd">      my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">):</span>
        <span class="c1"># catch common mistakes</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;QConfigDynamic received observer instance, please pass observer class instead. &quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;Use MyObserver.with_args(x=1) to override arguments to constructor if needed&quot;</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">QConfigDynamic</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="n">default_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_observer</span><span class="p">,</span>
                          <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_debug_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">,</span>
                                <span class="n">activation</span><span class="o">=</span><span class="n">default_debug_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration for debugging.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_per_channel_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig configuration for per channel weight quantization.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
                                  <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default dynamic qconfig.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float16_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                                  <span class="n">weight</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized to `torch.float16`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float16_static_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span>
                                 <span class="n">weight</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with both activations and weights quantized to `torch.float16`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">per_channel_dynamic_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_quant_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized per channel.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float_qparams_weight_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">default_float_qparams_observer</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Dynamic qconfig with weights quantized with a floating point zero_point.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">float_qparams_weight_only_qconfig_4bit</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">default_placeholder_observer</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">default_float_qparams_observer_4bit</span><span class="p">)</span>

<span class="n">default_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fake_quant</span><span class="p">,</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for QAT.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_dynamic_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_dynamic_fake_quant</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for dynamic QAT.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_weight_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for quantizing weights only.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_activation_only_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fake_quant</span><span class="p">,</span>
                                          <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for quantizing activations only.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># QAT config that uses a fused observer + fake quant modules for optimized training performance.</span>
<span class="c1"># to modify the activation/weight observers, the default entries in fake_quantize.py can be modified.</span>
<span class="n">default_qat_qconfig_v2</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_fused_act_fake_quant</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_wt_fake_quant</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Fused version of `default_qat_config`, has performance benefits.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_reuse_input_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">default_reuse_input_observer</span><span class="p">,</span>
                                      <span class="n">weight</span><span class="o">=</span><span class="n">NoopObserver</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default qconfig for operators that reuse the observers from input Tensor, e.g. reshape</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;x86&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the default PTQ qconfig for the specified backend.</span>

<span class="sd">    Args:</span>
<span class="sd">      * `backend` (str): a string representing the target backend. Currently supports</span>
<span class="sd">        `x86` (default), `fbgemm`, `qnnpack` and `onednn`.</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">supported_backends</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fbgemm&quot;</span><span class="p">,</span> <span class="s2">&quot;x86&quot;</span><span class="p">,</span> <span class="s2">&quot;qnnpack&quot;</span><span class="p">,</span> <span class="s2">&quot;onednn&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_backends</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="s2">&quot;backend: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span> <span class="o">+</span>
            <span class="s2">&quot; not supported. backend must be one of </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">supported_backends</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
            <span class="c1"># TODO: make this compatible with xnnpack constraints</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_observer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;onednn&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">_is_cpu_support_vnni</span><span class="p">():</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;Default qconfig of oneDNN backend with reduce_range of false may have accuracy issues &quot;</span>
                    <span class="s2">&quot;on CPU without Vector Neural Network Instruction support.&quot;</span><span class="p">)</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;x86&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_observer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># won&#39;t reach</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qconfig</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Version number: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">)</span> <span class="o">+</span>
                             <span class="s2">&quot; in get_default_qconfig is not supported. Version number must be 0&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">qconfig</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default, symmetric PTQ qconfig for the specified backend. And a per_channel</span>
<span class="sd">variant of the same.</span>

<span class="sd">Symmetric here applies to signed weights with zero point = 0, and additional</span>
<span class="sd">value restrictions. The activations are also signed 8-bit integers with this</span>
<span class="sd">qconfig.</span>

<span class="sd">    * Once this change is merged [as of 3/17/22], with backend or qengine =</span>
<span class="sd">    &#39;qnnpack&#39;, some quantized operators with this symmetric qconfig may use</span>
<span class="sd">    operators from xnnpack library.</span>

<span class="sd">        ** Support to use xnnpack ops with `qnnpack` backed for asymmetric</span>
<span class="sd">        qconfig (returned by get_default_qconfig()) is not available yet.</span>

<span class="sd">    * This qconfig uses signed activations and weights. Weights have added</span>
<span class="sd">    restrictions such as zero point is forced to be 0, making the weights</span>
<span class="sd">    symmetric, hence the name. And the 8-bit quantized values are</span>
<span class="sd">    restricting to to [-127, +127], excluding -128.</span>

<span class="sd">    * xnnpack has a requantization scale value restriction, 0x1p-32 &lt;=</span>
<span class="sd">    requantization_scale &lt; 256.0 where, `requantization_scale = (input_scale</span>
<span class="sd">    * kernel_scale) / (output_scale)`. Using this eps (w/ assumed max value</span>
<span class="sd">    of 256) is to prevent requantization_scale to go below xnnpack lower</span>
<span class="sd">    threshold.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">default_symmetric_qnnpack_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                                                                                   <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                                   <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">),</span>
                                            <span class="n">weight</span><span class="o">=</span><span class="n">weight_observer_range_neg_127_to_127</span><span class="p">)</span>

<span class="n">default_per_channel_symmetric_qnnpack_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                                                                                               <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                                               <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">),</span>
                                                        <span class="n">weight</span><span class="o">=</span><span class="n">per_channel_weight_observer_range_neg_127_to_127</span><span class="p">)</span>

<span class="n">default_embedding_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">NoopObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                        <span class="n">weight</span><span class="o">=</span><span class="n">default_embedding_fake_quant</span><span class="p">)</span>

<span class="n">default_embedding_qat_qconfig_4bit</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">NoopObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                             <span class="n">weight</span><span class="o">=</span><span class="n">default_embedding_fake_quant_4bit</span><span class="p">)</span>

<span class="n">default_quint8_weight_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">HistogramObserver</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">MinMaxObserver</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_default_qat_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;x86&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the default QAT qconfig for the specified backend.</span>

<span class="sd">    Args:</span>
<span class="sd">      * `backend` (str): a string representing the target backend. Currently supports</span>
<span class="sd">        `x86` (default), `fbgemm`, `qnnpack` and `onednn`.</span>
<span class="sd">      * `version`: version, for backwards compatibility. Can be `None` or `1`.</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">supported_backends</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;fbgemm&quot;</span><span class="p">,</span> <span class="s2">&quot;x86&quot;</span><span class="p">,</span> <span class="s2">&quot;qnnpack&quot;</span><span class="p">,</span> <span class="s2">&quot;onednn&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_backends</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="s2">&quot;backend: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span> <span class="o">+</span>
            <span class="s2">&quot; not supported. backend must be one of </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">supported_backends</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Histogram observer is too slow for quantization aware training</span>
    <span class="k">if</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_weight_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;onednn&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;x86&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_per_channel_weight_fake_quant</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qat_qconfig</span>
    <span class="c1"># Use the fused observe + fake_quant modules for doing QAT.</span>
    <span class="k">elif</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;fbgemm&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                                 <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;qnnpack&#39;</span><span class="p">:</span>
            <span class="c1"># TODO: make this compatible with xnnpack constraints</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                                 <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_wt_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;onednn&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s1">&#39;x86&#39;</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                                                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
                                                                                 <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                              <span class="n">weight</span><span class="o">=</span><span class="n">default_fused_per_channel_wt_fake_quant</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">qconfig</span> <span class="o">=</span> <span class="n">default_qat_qconfig_v2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Version number: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">)</span> <span class="o">+</span>
                             <span class="s2">&quot;in get_default_qat_qconfig is not supported. Version number must be 0 or 1&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">qconfig</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default symmetric QAT qconfig for qnnpack. And its per channel weight variant.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">default_symmetric_qnnpack_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                       <span class="n">quant_min</span><span class="o">=-</span><span class="mi">128</span><span class="p">,</span>
                                                       <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">,</span>
                                                       <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                                                       <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                       <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">fused_wt_fake_quant_range_neg_127_to_127</span><span class="p">)</span>

<span class="n">default_per_channel_symmetric_qnnpack_qat_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">FusedMovingAvgObsFakeQuantize</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">observer</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span>
                                                       <span class="n">quant_min</span><span class="o">=-</span><span class="mi">128</span><span class="p">,</span>
                                                       <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">,</span>
                                                       <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
                                                       <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                       <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">fused_per_channel_wt_fake_quant_range_neg_127_to_127</span><span class="p">)</span>

<span class="n">_default_fp32_placeholder_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">_default_quint8_placeholder_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">),</span>
    <span class="c1"># operators using this qconfig doesn&#39;t have weights</span>
    <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">get_default_qconfig_dict</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;x86&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.ao.quantization.get_default_qconfig_dict is deprecated and will be removed in &quot;</span>
        <span class="s2">&quot;a future version. Please use torch.ao.quantization.get_default_qconfig_mapping instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig_mapping</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">version</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">get_default_qat_qconfig_dict</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;x86&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.ao.quantization.get_default_qat_qconfig_dict is deprecated and will be removed in &quot;</span>
        <span class="s2">&quot;a future version. Please use torch.ao.quantization.get_default_qat_qconfig_mapping instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qat_qconfig_mapping</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">version</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">_assert_valid_qconfig</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">],</span>
                          <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Verifies that this `qconfig` is valid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">is_conv_transpose_mod</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">is_conv_transpose_mod</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># for now, we assume that any qconfig for ConvTranspose without a weight is valid</span>
            <span class="k">return</span>
        <span class="n">example_observer</span> <span class="o">=</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">()</span>
        <span class="n">is_per_channel</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_observer</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">PerChannelMinMaxObserver</span><span class="p">,</span>
                                          <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">MovingAveragePerChannelMinMaxObserver</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">is_per_channel</span><span class="p">,</span> \
            <span class="s1">&#39;Per channel weight observer is not supported yet for ConvTranspose</span><span class="si">{n}</span><span class="s1">d.&#39;</span>

<span class="n">QConfigAny</span> <span class="o">=</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">]</span>
<span class="n">QConfigAny</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch.ao.quantization.qconfig&quot;</span>

<span class="k">def</span> <span class="nf">_add_module_to_qconfig_obs_ctr</span><span class="p">(</span>
        <span class="n">qconfig</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This is a helper function for use in quantization prepare that updates a qconfig so that</span>
<span class="sd">    the constructors stored in the qconfig will create observers on the same device that</span>
<span class="sd">    &#39;module&#39; is on. This is intended to be used when the qconfigs are propagated to each</span>
<span class="sd">    module in order to avoid potential device alignment issues.</span>

<span class="sd">    Args:</span>
<span class="sd">        qconfig: QConfig with obs constructors stored in activation and weight</span>
<span class="sd">        module: module which the qconfig is related to</span>

<span class="sd">    Return:</span>
<span class="sd">        qconfig: configured so that obs constructors set to construct on the same device as module</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">_fields</span> <span class="o">!=</span> <span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">qconfig</span>

    <span class="k">def</span> <span class="nf">get_factory_kwargs_based_on_module_device</span><span class="p">():</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()}</span> <span class="o">|</span> \
            <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">()}</span>
        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">devices</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">original_constructor</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># check if constructor can accept factory_kwargs</span>
            <span class="n">check</span> <span class="o">=</span> <span class="n">original_constructor</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">check</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">original_constructor</span><span class="o">.</span><span class="n">with_callable_args</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="o">=</span><span class="n">get_factory_kwargs_based_on_module_device</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>  <span class="c1"># qconfig doesn&#39;t have activation or weight</span>
            <span class="k">return</span> <span class="n">original_constructor</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>  <span class="c1"># the class doesn&#39;t accept factory_kwargs argument</span>
            <span class="k">return</span> <span class="n">original_constructor</span>

    <span class="n">activation</span> <span class="o">=</span> <span class="n">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">configure_constructor_to_put_obs_on_module_device</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>

<span class="n">_ObserverOrFakeQuantizeConstructor</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">_PartialWrapper</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">ObserverBase</span><span class="p">],</span> <span class="n">Type</span><span class="p">[</span><span class="n">FakeQuantizeBase</span><span class="p">]]</span>

<span class="k">def</span> <span class="nf">_obs_or_fq_ctr_equals</span><span class="p">(</span><span class="n">obs_or_fq1</span><span class="p">:</span> <span class="n">_ObserverOrFakeQuantizeConstructor</span><span class="p">,</span> <span class="n">obs_or_fq2</span><span class="p">:</span> <span class="n">_ObserverOrFakeQuantizeConstructor</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obs_or_fq1</span><span class="p">,</span> <span class="n">_PartialWrapper</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obs_or_fq2</span><span class="p">,</span> <span class="n">_PartialWrapper</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_partial_wrapper_equals</span><span class="p">(</span><span class="n">obs_or_fq1</span><span class="p">,</span> <span class="n">obs_or_fq2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">obs_or_fq1</span> <span class="o">==</span> <span class="n">obs_or_fq2</span>

<span class="k">def</span> <span class="nf">_partial_wrapper_equals</span><span class="p">(</span><span class="n">obs_or_fq1</span><span class="p">:</span> <span class="n">_PartialWrapper</span><span class="p">,</span> <span class="n">obs_or_fq2</span><span class="p">:</span> <span class="n">_PartialWrapper</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return whether the two partial wrappers are equal,</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># functools.partial has no __eq__ operator defined so &#39;==&#39; defaults to &#39;is&#39;</span>
    <span class="n">obs_or_fq1_keywords</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">obs_or_fq1</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">keywords</span><span class="p">)</span>
    <span class="n">obs_or_fq2_keywords</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">obs_or_fq2</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">keywords</span><span class="p">)</span>
    <span class="n">keywords_equal</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># compare observer constructor with _obs_or_fq_ctr_equals since direct compare would fail</span>
    <span class="k">if</span> <span class="s2">&quot;observer&quot;</span> <span class="ow">in</span> <span class="n">obs_or_fq1_keywords</span> <span class="ow">and</span> <span class="s2">&quot;observer&quot;</span> <span class="ow">in</span> <span class="n">obs_or_fq2_keywords</span><span class="p">:</span>
        <span class="n">keywords_equal</span> <span class="o">=</span> <span class="n">keywords_equal</span> <span class="ow">and</span> <span class="n">_obs_or_fq_ctr_equals</span><span class="p">(</span><span class="n">obs_or_fq1_keywords</span><span class="p">[</span><span class="s2">&quot;observer&quot;</span><span class="p">],</span> <span class="n">obs_or_fq2_keywords</span><span class="p">[</span><span class="s2">&quot;observer&quot;</span><span class="p">])</span>
        <span class="n">obs_or_fq1_keywords</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;observer&quot;</span><span class="p">)</span>
        <span class="n">obs_or_fq2_keywords</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;observer&quot;</span><span class="p">)</span>
    <span class="n">keywords_equal</span> <span class="o">=</span> <span class="n">keywords_equal</span> <span class="ow">and</span> <span class="n">obs_or_fq1_keywords</span> <span class="o">==</span> <span class="n">obs_or_fq2_keywords</span>
    <span class="k">return</span> <span class="n">obs_or_fq1</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">func</span> <span class="o">==</span> <span class="n">obs_or_fq2</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">func</span> <span class="ow">and</span> <span class="n">obs_or_fq1</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">args</span> <span class="o">==</span> <span class="n">obs_or_fq2</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="n">args</span> <span class="ow">and</span> <span class="n">keywords_equal</span>

<span class="k">def</span> <span class="nf">qconfig_equals</span><span class="p">(</span><span class="n">q1</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">,</span> <span class="n">q2</span><span class="p">:</span> <span class="n">QConfigAny</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns `True` if `q1` equals `q2`, and `False` otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">q1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">q2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">q1</span> <span class="o">==</span> <span class="n">q2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">q1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">q2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Qconfig weight and activation can be either a partial wrapper,</span>
            <span class="c1"># or an observer class. Special handling is required (above) for</span>
            <span class="c1"># comparing partial wrappers.</span>
            <span class="n">activation_same</span> <span class="o">=</span> <span class="n">_obs_or_fq_ctr_equals</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="n">q2</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span>
            <span class="n">weight_same</span> <span class="o">=</span> <span class="n">_obs_or_fq_ctr_equals</span><span class="p">(</span><span class="n">q1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">q2</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">activation_same</span> <span class="ow">and</span> <span class="n">weight_same</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">q1</span> <span class="o">==</span> <span class="n">q2</span>

<span class="k">def</span> <span class="nf">_activation_is_memoryless</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">QConfig</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return whether the observer for activations defined in the given QConfig is memoryless.</span>
<span class="sd">    This means a MovingAverage observer with averaging constant equal to 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_is_memoryless</span><span class="p">(</span><span class="n">observer</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">observer</span><span class="p">,</span> <span class="s2">&quot;averaging_constant&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">observer</span><span class="o">.</span><span class="n">averaging_constant</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">FakeQuantizeBase</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="s2">&quot;activation_post_process&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_is_memoryless</span><span class="p">(</span><span class="n">act</span><span class="o">.</span><span class="n">activation_post_process</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_is_memoryless</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_is_reuse_input_qconfig</span><span class="p">(</span><span class="n">qconfig</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QConfig</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">qconfig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">activation</span><span class="p">(),</span> <span class="n">ReuseInputObserver</span><span class="p">)</span> <span class="ow">and</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">(),</span> <span class="n">NoopObserver</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
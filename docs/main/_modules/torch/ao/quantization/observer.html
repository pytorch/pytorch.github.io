


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.ao.quantization.observer &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/observer.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/observer.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../quantization.html">torch.ao.quantization</a> &gt;</li>
        
      <li>torch.ao.quantization.observer</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.ao.quantization.observer</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This module implements observers which are used to collect statistics about</span>
<span class="sd">the values observed during calibration (PTQ) or training (QAT).</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">check_min_max_valid</span><span class="p">,</span> <span class="n">calculate_qmin_qmax</span><span class="p">,</span> <span class="n">is_per_tensor</span><span class="p">,</span> <span class="n">is_per_channel</span><span class="p">,</span> <span class="n">validate_qmin_qmax</span><span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;default_affine_fixed_qparams_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_debug_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_dynamic_quant_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_fixed_qparams_range_0to1_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_fixed_qparams_range_neg1to1_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_float_qparams_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_float_qparams_observer_4bit&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_histogram_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_per_channel_weight_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_placeholder_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_reuse_input_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_symmetric_fixed_qparams_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default_weight_observer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;get_observer_state_dict&quot;</span><span class="p">,</span>
    <span class="s2">&quot;load_observer_state_dict&quot;</span><span class="p">,</span>
    <span class="s2">&quot;per_channel_weight_observer_range_neg_127_to_127&quot;</span><span class="p">,</span>
    <span class="s2">&quot;weight_observer_range_neg_127_to_127&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FixedQParamsObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;HistogramObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MinMaxObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MovingAverageMinMaxObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MovingAveragePerChannelMinMaxObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;NoopObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ObserverBase&quot;</span><span class="p">,</span>
    <span class="s2">&quot;PerChannelMinMaxObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;PlaceholderObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;RecordingObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ReuseInputObserver&quot;</span><span class="p">,</span>
    <span class="s2">&quot;UniformQuantizationObserverBase&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">class</span> <span class="nc">_PartialWrapper</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callable_args</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">keywords</span><span class="p">):</span>
        <span class="c1"># call each arg in callable_args and add them partial, then run with keywords</span>
        <span class="c1"># skip if arg_name in keywords so its possible to overwrite</span>
        <span class="k">for</span> <span class="n">arg_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callable_args</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">arg_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">:</span>
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">keywords</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="n">arg_name</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">callable_args</span><span class="p">[</span><span class="n">arg_name</span><span class="p">]()}}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">keywords</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">callable_args</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">with_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_with_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">with_callable_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_PartialWrapper</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
        <span class="n">result</span><span class="o">.</span><span class="n">callable_args</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">callable_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">_with_args</span><span class="p">(</span><span class="n">cls_or_self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wrapper that allows creation of class factories.</span>

<span class="sd">    This can be useful when there is a need to create classes with the same</span>
<span class="sd">    constructor arguments, but different instances. Can be used in conjunction with</span>
<span class="sd">    _callable_args</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Undefined vars&quot;)</span>
<span class="sd">        &gt;&gt;&gt; Foo.with_args = classmethod(_with_args)</span>
<span class="sd">        &gt;&gt;&gt; foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)</span>
<span class="sd">        &gt;&gt;&gt; foo_instance1 = foo_builder()</span>
<span class="sd">        &gt;&gt;&gt; foo_instance2 = foo_builder()</span>
<span class="sd">        &gt;&gt;&gt; id(foo_instance1) == id(foo_instance2)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">_PartialWrapper</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">cls_or_self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">r</span>

<span class="k">def</span> <span class="nf">_with_callable_args</span><span class="p">(</span><span class="n">cls_or_self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wrapper that allows creation of class factories args that need to be</span>
<span class="sd">    called at construction time.</span>

<span class="sd">    This can be useful when there is a need to create classes with the same</span>
<span class="sd">    constructor arguments, but different instances and those arguments should only</span>
<span class="sd">    be calculated at construction time. Can be used in conjunction with _with_args</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Undefined vars&quot;)</span>
<span class="sd">        &gt;&gt;&gt; Foo.with_callable_args = classmethod(_with_callable_args)</span>
<span class="sd">        &gt;&gt;&gt; Foo.with_args = classmethod(_with_args)</span>
<span class="sd">        &gt;&gt;&gt; foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=&quot;dan&quot;)</span>
<span class="sd">        &gt;&gt;&gt; foo_instance1 = foo_builder()</span>
<span class="sd">        &gt;&gt;&gt; # wait 50</span>
<span class="sd">        &gt;&gt;&gt; foo_instance2 = foo_builder()</span>
<span class="sd">        &gt;&gt;&gt; id(foo_instance1.creation_time) == id(foo_instance2.creation_time)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">_PartialWrapper</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">cls_or_self</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">r</span><span class="o">.</span><span class="n">with_callable_args</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="n">ABC</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">ABCMeta</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;ABC&quot;</span><span class="p">),</span> <span class="p">(</span><span class="nb">object</span><span class="p">,),</span> <span class="p">{})</span>  <span class="c1"># compatible with Python 2 *and* 3:</span>


<div class="viewcode-block" id="ObserverBase"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase">[docs]</a><span class="k">class</span> <span class="nc">ObserverBase</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Base observer Module.</span>
<span class="sd">    Any observer implementation should derive from this class.</span>

<span class="sd">    Concrete observers should follow the same API. In forward, they will update</span>
<span class="sd">    the statistics of the observed Tensor. And they should provide a</span>
<span class="sd">    `calculate_qparams` function that computes the quantization parameters given</span>
<span class="sd">    the collected statistics.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype: dtype argument to the `quantize` node needed to implement the</span>
<span class="sd">               reference model spec.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="n">with_args</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_with_args</span><span class="p">)</span>
    <span class="n">with_callable_args</span> <span class="o">=</span> <span class="nb">classmethod</span><span class="p">(</span><span class="n">_with_callable_args</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">UniformQuantizationObserverBase</span><span class="p">(</span><span class="n">ObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Common base for all observers using uniform quantization to calculate</span>
<span class="sd">    scale and zero_point.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype: dtype argument to the `quantize` node needed to implement the</span>
<span class="sd">               reference model spec.</span>
<span class="sd">        qscheme: Quantization scheme to be used.</span>
<span class="sd">        reduce_range: Reduces the range of the quantized data type by 1 bit.</span>
<span class="sd">                      This is sometimes required to avoid instruction overflow.</span>
<span class="sd">        quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        :attr:`dtype` can only take ``torch.qint8`` or ``torch.quint8``.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        :attr:`qscheme` can only take one of the following options:</span>

<span class="sd">        - ``torch.per_tensor_affine``</span>
<span class="sd">        - ``torch.per_tensor_symmetric``</span>
<span class="sd">        - ``torch.per_channel_affine``</span>
<span class="sd">        - ``torch.per_channel_symmetric``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Note: the version is shared by all observer types</span>
    <span class="c1">#</span>
    <span class="c1"># Version 1/None</span>
    <span class="c1">#   self</span>
    <span class="c1">#</span>
    <span class="c1"># Version 2 (base class only, does not include child class buffers)</span>
    <span class="c1">#   self</span>
    <span class="c1">#   |--- eps : Tensor</span>
    <span class="c1">#</span>
    <span class="c1"># Version 3</span>
    <span class="c1">#   for HistogramObserver only, changed the shape of uninitialized</span>
    <span class="c1">#   min_val and max_val buffers from torch.Size([0]) to torch.Size([])</span>
    <span class="c1">#   for PerChannelObservers, changed the name of the buffers from min_vals</span>
    <span class="c1">#   to min_val and from max_vals to max_val.</span>
    <span class="n">_version</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="n">eps</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
        <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">factory_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">factory_kwargs</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">=</span> <span class="n">qscheme</span>
        <span class="k">if</span> <span class="n">reduce_range</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Please use quant_min and quant_max to specify the range for observers. </span><span class="se">\</span>
<span class="s2">                    reduce_range will be deprecated in a future release of PyTorch.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_range</span> <span class="o">=</span> <span class="n">reduce_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">eps</span><span class="p">],</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_symmetric</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span>
        <span class="p">),</span> <span class="s2">&quot;Default Observer only works for per_tensor_affine, </span><span class="se">\</span>
<span class="s2">                per_tensor_symmetric, per_channel_affine, </span><span class="se">\</span>
<span class="s2">                per_channel_symmetric and per_channel_float_qparams quantization scheme&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">,</span>
        <span class="p">),</span> <span class="s2">&quot;Default Observer only works for qint8, quint8 and quint4x2 data type&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_customized_qrange</span> <span class="o">=</span> <span class="p">(</span><span class="n">quant_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">quant_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_customized_qrange</span><span class="p">:</span>
            <span class="n">validate_qmin_qmax</span><span class="p">(</span><span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span> <span class="o">=</span> \
            <span class="n">calculate_qmin_qmax</span><span class="p">(</span><span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_customized_qrange</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_range</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load_from_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">,</span>
        <span class="n">local_metadata</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">,</span>
        <span class="n">missing_keys</span><span class="p">,</span>
        <span class="n">unexpected_keys</span><span class="p">,</span>
        <span class="n">error_msgs</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="n">version</span> <span class="o">=</span> <span class="n">local_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;version&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">version</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># eps was moved to a buffer in version 2</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">])</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;eps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">prefix</span><span class="p">,</span>
            <span class="n">local_metadata</span><span class="p">,</span>
            <span class="n">strict</span><span class="p">,</span>
            <span class="n">missing_keys</span><span class="p">,</span>
            <span class="n">unexpected_keys</span><span class="p">,</span>
            <span class="n">error_msgs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">_validate_qmin_qmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Validates that the user-specified quantization range is properly initialized</span>
<span class="sd">        and within the given bound supported by the observer dtype.</span>

<span class="sd">        To accommodate lower-bit quantization with respect to the existing torch.qint8 and</span>
<span class="sd">        torch.quint8 datatypes, the user can choose to use dynamic quantization range by passing</span>
<span class="sd">        in a tuple of initial qmin and qmax values. One use case is these customized qmin and qmax</span>
<span class="sd">        values are used to calculate static estimates of the scale and zero point for aggressive lower-bit</span>
<span class="sd">        fake quantization. These estimates are compared against parameters learned through backpropagation.</span>
<span class="sd">        The related literatures for scale and zero point via backpropagation are as follows:</span>

<span class="sd">        Learned Step Size Quantization: https://openreview.net/pdf?id=rkgO66VKDS</span>
<span class="sd">        Trained Quantization Thresholds: https://arxiv.org/pdf/1903.08066.pdf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># The variable names are prefixed with &quot;initial&quot; because their values (qmin and qmax) might be adjusted</span>
        <span class="c1"># based on whether quantization range is reduced and the datatype (signed/unsigned) used by the observer.</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">quant_min</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">quant_max</span>
        <span class="p">),</span> <span class="s2">&quot;Used-specified quantization range must include 0.&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">quant_min</span> <span class="o">&lt;</span> <span class="n">quant_max</span>
        <span class="p">),</span> <span class="s2">&quot;qmin must be strictly less than qmax for user-specified quantization range.&quot;</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">_calculate_qparams</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">min_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">max_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculates the quantization parameters, given min and max</span>
<span class="sd">        value tensors. Works for both per tensor and per channel cases</span>

<span class="sd">        Args:</span>
<span class="sd">            min_val: Minimum values per channel</span>
<span class="sd">            max_val: Maximum values per channel</span>

<span class="sd">        Returns:</span>
<span class="sd">            scales: Scales tensor of shape (#channels,)</span>
<span class="sd">            zero_points: Zero points tensor of shape (#channels,)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Functionally equivalent to &#39;determine_qparams&#39; in utils.py. Observers must be torchscriptable however and qscheme</span>
        <span class="c1"># as far as I can tell is not allowed to passed as a parameter in torchscript functions. This makes refactoring observer</span>
        <span class="c1"># to use this utility a massive pain and very gross. For now Im opting just to duplicate as this code</span>
        <span class="c1"># seems unlikey to change (last update over 1 year ago) and when torchscript is fully deprecated we can refactor.</span>
        <span class="c1"># TODO(jakeszwe, jerryzh168)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">check_min_max_valid</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">min_val</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">min_val</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

        <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span>
        <span class="n">min_val_neg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">min_val</span><span class="p">))</span>
        <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">max_val</span><span class="p">))</span>

        <span class="n">device</span> <span class="o">=</span> <span class="n">min_val_neg</span><span class="o">.</span><span class="n">device</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">min_val_neg</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">min_val_neg</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_symmetric</span>
        <span class="p">):</span>
            <span class="n">max_val_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="n">min_val_neg</span><span class="p">,</span> <span class="n">max_val_pos</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">max_val_pos</span> <span class="o">/</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_customized_qrange</span><span class="p">:</span>
                    <span class="c1"># When customized quantization range is used, down-rounded midpoint of the range is chosen.</span>
                    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">new_full</span><span class="p">(</span>
                        <span class="n">zero_point</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="p">(</span><span class="n">quant_min</span> <span class="o">+</span> <span class="n">quant_max</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">zero_point</span><span class="o">.</span><span class="n">new_full</span><span class="p">(</span><span class="n">zero_point</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="mi">128</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scale</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">scale</span><span class="p">))</span>
            <span class="c1"># We use the quantize function</span>
            <span class="c1"># xq = Round(Xf * inv_scale + zero_point),</span>
            <span class="c1"># setting zero_point to (-1 * min *inv_scale) we get</span>
            <span class="c1"># Xq = Round((Xf - min) * inv_scale)</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">min_val</span> <span class="o">/</span> <span class="n">scale</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val_pos</span> <span class="o">-</span> <span class="n">min_val_neg</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">quant_max</span> <span class="o">-</span> <span class="n">quant_min</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">quant_min</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">min_val_neg</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">quant_min</span><span class="p">,</span> <span class="n">quant_max</span><span class="p">)</span>

        <span class="c1"># For scalar values, cast them to Tensors of size 1 to keep the shape</span>
        <span class="c1"># consistent with default values in FakeQuantize.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># TODO: switch to scale.item() after adding JIT support</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">float</span><span class="p">(</span><span class="n">scale</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scale</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">zero_point</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># TODO: switch to zero_point.item() after adding JIT support</span>
            <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">zero_point</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">zero_point</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">:</span>
                <span class="n">zero_point</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">zero_point</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">zero_point</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">reset_min_max_vals</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Cannot reset min/max values in the given observer.&quot;</span><span class="p">)</span>


<span class="c1"># Originally, this class was called `_ObserverBase`.  Keeping the old name around</span>
<span class="c1"># for backwards compatibility.</span>
<span class="c1"># TODO(after v1.13): delete this</span>
<span class="n">_ObserverBase</span> <span class="o">=</span> <span class="n">UniformQuantizationObserverBase</span>


<div class="viewcode-block" id="MinMaxObserver"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver">[docs]</a><span class="k">class</span> <span class="nc">MinMaxObserver</span><span class="p">(</span><span class="n">UniformQuantizationObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Observer module for computing the quantization parameters based on the</span>
<span class="sd">    running min and max values.</span>

<span class="sd">    This observer uses the tensor min/max statistics to compute the quantization</span>
<span class="sd">    parameters. The module records the running minimum and maximum of incoming</span>
<span class="sd">    tensors, and uses this statistic to compute the quantization parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype: dtype argument to the `quantize` node needed to implement the</span>
<span class="sd">               reference model spec.</span>
<span class="sd">        qscheme: Quantization scheme to be used</span>
<span class="sd">        reduce_range: Reduces the range of the quantized data type by 1 bit</span>
<span class="sd">        quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.</span>

<span class="sd">    Given running min/max as :math:`x_\text{min}` and :math:`x_\text{max}`,</span>
<span class="sd">    scale :math:`s` and zero point :math:`z` are computed as:</span>

<span class="sd">    The running minimum/maximum :math:`x_\text{min/max}` is computed as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll}</span>
<span class="sd">        x_\text{min} &amp;= \begin{cases}</span>
<span class="sd">            \min(X) &amp; \text{if~}x_\text{min} = \text{None} \\</span>
<span class="sd">            \min\left(x_\text{min}, \min(X)\right) &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}\\</span>
<span class="sd">        x_\text{max} &amp;= \begin{cases}</span>
<span class="sd">            \max(X) &amp; \text{if~}x_\text{max} = \text{None} \\</span>
<span class="sd">            \max\left(x_\text{max}, \max(X)\right) &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}\\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`X` is the observed tensor.</span>

<span class="sd">    The scale :math:`s` and zero point :math:`z` are then computed as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{aligned}</span>
<span class="sd">            \text{if Symmetric:}&amp;\\</span>
<span class="sd">            &amp;s = 2 \max(|x_\text{min}|, x_\text{max}) /</span>
<span class="sd">                \left( Q_\text{max} - Q_\text{min} \right) \\</span>
<span class="sd">            &amp;z = \begin{cases}</span>
<span class="sd">                0 &amp; \text{if dtype is qint8} \\</span>
<span class="sd">                128 &amp; \text{otherwise}</span>
<span class="sd">            \end{cases}\\</span>
<span class="sd">            \text{Otherwise:}&amp;\\</span>
<span class="sd">                &amp;s = \left( x_\text{max} - x_\text{min}  \right ) /</span>
<span class="sd">                    \left( Q_\text{max} - Q_\text{min} \right ) \\</span>
<span class="sd">                &amp;z = Q_\text{min} - \text{round}(x_\text{min} / s)</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    where :math:`Q_\text{min}` and :math:`Q_\text{max}` are the minimum and</span>
<span class="sd">    maximum of the quantized data type.</span>

<span class="sd">    .. warning:: :attr:`dtype` can only take ``torch.qint8`` or ``torch.quint8``.</span>

<span class="sd">    .. note:: If the running minimum equals to the running maximum, the scale</span>
<span class="sd">              and zero_point are set to 1.0 and 0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">min_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">max_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
        <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">factory_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_per_tensor</span><span class="p">(</span><span class="n">qscheme</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;MinMaxObserver&#39;s qscheme only support torch.per_tensor_symmetric </span><span class="se">\</span>
<span class="s2">                    and torch.per_tensor_affine.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># For x86 quantized kernels, we need to ensure that the vpmaddubsw</span>
        <span class="c1"># instruction does not overflow. We allow for a reduce_range argument to</span>
        <span class="c1"># observers that reduces the quantized range to (0,127) or (-64, 63).</span>
        <span class="c1"># For more details see aten/src/ATen/native/quantized/cpu/qconv.cpp</span>
        <span class="c1"># This is not an optimal choice for non x86 backends as it loses a bit</span>
        <span class="c1"># of precision for activations.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">qscheme</span><span class="o">=</span><span class="n">qscheme</span><span class="p">,</span>
            <span class="n">reduce_range</span><span class="o">=</span><span class="n">reduce_range</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">factory_kwargs</span><span class="o">=</span><span class="n">factory_kwargs</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">factory_kwargs</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;min_val&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;max_val&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_range</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot reduce range for symmetric </span><span class="se">\</span>
<span class="s2">                                       quantization for quint8&quot;</span>
            <span class="p">)</span>

<div class="viewcode-block" id="MinMaxObserver.forward"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_orig</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Records the running minimum and maximum of ``x``.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_orig</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># avoid keeping autograd tape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">min_val_cur</span><span class="p">,</span> <span class="n">max_val_cur</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">min_val_cur</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">)</span>
        <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_val_cur</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">min_val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">max_val</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_orig</span></div>

<div class="viewcode-block" id="MinMaxObserver.calculate_qparams"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.calculate_qparams">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculates the quantization parameters.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">)</span></div>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;min_val=</span><span class="si">{}</span><span class="s2">, max_val=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">)</span>

<div class="viewcode-block" id="MinMaxObserver.reset_min_max_vals"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">reset_min_max_vals</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Resets the min/max values.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)))</span></div></div>

<div class="viewcode-block" id="MovingAverageMinMaxObserver"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html#torch.ao.quantization.observer.MovingAverageMinMaxObserver">[docs]</a><span class="k">class</span> <span class="nc">MovingAverageMinMaxObserver</span><span class="p">(</span><span class="n">MinMaxObserver</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Observer module for computing the quantization parameters based on the</span>
<span class="sd">    moving average of the min and max values.</span>

<span class="sd">    This observer computes the quantization parameters based on the moving</span>
<span class="sd">    averages of minimums and maximums of the incoming tensors. The module</span>
<span class="sd">    records the average minimum and maximum of incoming tensors, and uses this</span>
<span class="sd">    statistic to compute the quantization parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        averaging_constant: Averaging constant for min/max.</span>
<span class="sd">        dtype: dtype argument to the `quantize` node needed to implement the</span>
<span class="sd">               reference model spec.</span>
<span class="sd">        qscheme: Quantization scheme to be used</span>
<span class="sd">        reduce_range: Reduces the range of the quantized data type by 1 bit</span>
<span class="sd">        quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.</span>

<span class="sd">    The moving average min/max is computed as follows</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll}</span>
<span class="sd">                x_\text{min} = \begin{cases}</span>
<span class="sd">                    \min(X) &amp; \text{if~}x_\text{min} = \text{None} \\</span>
<span class="sd">                    (1 - c) x_\text{min} + c \min(X) &amp; \text{otherwise}</span>
<span class="sd">                \end{cases}\\</span>
<span class="sd">                x_\text{max} = \begin{cases}</span>
<span class="sd">                    \max(X) &amp; \text{if~}x_\text{max} = \text{None} \\</span>
<span class="sd">                    (1 - c) x_\text{max} + c \max(X) &amp; \text{otherwise}</span>
<span class="sd">                \end{cases}\\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`x_\text{min/max}` is the running average min/max, :math:`X` is</span>
<span class="sd">    is the incoming tensor, and :math:`c` is the ``averaging_constant``.</span>

<span class="sd">    The scale and zero point are then computed as in</span>
<span class="sd">    :class:`~torch.ao.quantization.observer.MinMaxObserver`.</span>

<span class="sd">    .. note:: Only works with ``torch.per_tensor_affine`` quantization scheme.</span>

<span class="sd">    .. note:: If the running minimum equals to the running maximum, the scale</span>
<span class="sd">              and zero_point are set to 1.0 and 0.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">averaging_constant</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
        <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_per_tensor</span><span class="p">(</span><span class="n">qscheme</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;MovingAverageMinMaxObserver&#39;s qscheme only support </span><span class="se">\</span>
<span class="s2">                    torch.per_tensor_symmetric and torch.per_tensor_affine.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">averaging_constant</span> <span class="o">=</span> <span class="n">averaging_constant</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">qscheme</span><span class="o">=</span><span class="n">qscheme</span><span class="p">,</span>
            <span class="n">reduce_range</span><span class="o">=</span><span class="n">reduce_range</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_orig</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_orig</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># avoid keeping autograd tape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span>
        <span class="n">max_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span>
        <span class="k">if</span> <span class="n">min_val</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">max_val</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">):</span>
            <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_val_cur</span><span class="p">,</span> <span class="n">max_val_cur</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="n">min_val</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">averaging_constant</span> <span class="o">*</span> <span class="p">(</span><span class="n">min_val_cur</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span>
            <span class="n">max_val</span> <span class="o">=</span> <span class="n">max_val</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">averaging_constant</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_val_cur</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">min_val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">max_val</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_orig</span></div>


<div class="viewcode-block" id="PerChannelMinMaxObserver"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver">[docs]</a><span class="k">class</span> <span class="nc">PerChannelMinMaxObserver</span><span class="p">(</span><span class="n">UniformQuantizationObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Observer module for computing the quantization parameters based on the</span>
<span class="sd">    running per channel min and max values.</span>

<span class="sd">    This observer uses the tensor min/max statistics to compute the per channel</span>
<span class="sd">    quantization parameters. The module records the running minimum and maximum</span>
<span class="sd">    of incoming tensors, and uses this statistic to compute the quantization</span>
<span class="sd">    parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        ch_axis: Channel axis</span>
<span class="sd">        dtype: dtype argument to the `quantize` node needed to implement the</span>
<span class="sd">               reference model spec.</span>
<span class="sd">        qscheme: Quantization scheme to be used</span>
<span class="sd">        reduce_range: Reduces the range of the quantized data type by 1 bit</span>
<span class="sd">        quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.</span>

<span class="sd">    The quantization parameters are computed the same way as in</span>
<span class="sd">    :class:`~torch.ao.quantization.observer.MinMaxObserver`, with the difference</span>
<span class="sd">    that the running min/max values are stored per channel.</span>
<span class="sd">    Scales and zero points are thus computed per channel as well.</span>

<span class="sd">    .. note:: If the running minimum equals to the running maximum, the scales</span>
<span class="sd">              and zero_points are set to 1.0 and 0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">min_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">max_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
        <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">factory_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_per_channel</span><span class="p">(</span><span class="n">qscheme</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;PerChannelMinMaxObserver&#39;s qscheme only support </span><span class="se">\</span>
<span class="s2">                    torch.per_channel_symmetric, torch.per_channel_affine and torch.per_channel_affine_float_qparams.&quot;</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">qscheme</span><span class="o">=</span><span class="n">qscheme</span><span class="p">,</span>
            <span class="n">reduce_range</span><span class="o">=</span><span class="n">reduce_range</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">factory_kwargs</span><span class="o">=</span><span class="n">factory_kwargs</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">factory_kwargs</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ch_axis</span> <span class="o">=</span> <span class="n">ch_axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;min_val&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;max_val&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_symmetric</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_range</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot reduce range for symmetric quantization for quint8&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_orig</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">x_orig</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_orig</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_orig</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># avoid keeping autograd tape</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span>
        <span class="n">max_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span>
        <span class="n">x_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="n">new_axis_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_dim</span><span class="p">))]</span>  <span class="c1"># noqa: C416</span>
        <span class="n">new_axis_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ch_axis</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">new_axis_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ch_axis</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">new_axis_list</span><span class="p">)</span>
        <span class="c1"># Need to match dtype of min/max because the updates to buffers</span>
        <span class="c1"># are done in place and types need to match for comparisons</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_val</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">max_val</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_val_cur</span><span class="p">,</span> <span class="n">max_val_cur</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">min_val_cur</span><span class="p">,</span> <span class="n">min_val</span><span class="p">)</span>
            <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_val_cur</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">min_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">min_val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">max_val</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_orig</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;min_val=</span><span class="si">{}</span><span class="s2">, max_val=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load_from_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">local_metadata</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="n">version</span> <span class="o">=</span> <span class="n">local_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;version&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">version</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">local_state</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;min_vals&quot;</span><span class="p">,</span> <span class="s2">&quot;max_vals&quot;</span><span class="p">]</span>
            <span class="n">expected_min_name</span> <span class="o">=</span> <span class="s2">&quot;min_vals&quot;</span>
            <span class="n">expected_max_name</span> <span class="o">=</span> <span class="s2">&quot;max_vals&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">local_state</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;min_val&quot;</span><span class="p">,</span> <span class="s2">&quot;max_val&quot;</span><span class="p">]</span>
            <span class="n">expected_min_name</span> <span class="o">=</span> <span class="s2">&quot;min_val&quot;</span>
            <span class="n">expected_max_name</span> <span class="o">=</span> <span class="s2">&quot;max_val&quot;</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">local_state</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                <span class="n">val</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="c1"># Custom handling to allow loading min_val or max_val</span>
                <span class="c1"># of size N into uninitialized buffers of size 0. The</span>
                <span class="c1"># buffers are resized here, and the values are copied in</span>
                <span class="c1"># the default state_dict loading code of the parent.</span>
                <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="n">expected_min_name</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="n">expected_max_name</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Observer load_from_state_dict got unexpected name </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="c1"># For torchscript module we need to update the attributes here since we do not</span>
                <span class="c1"># call the `_load_from_state_dict` function defined module.py</span>
                <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="n">expected_min_name</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="n">expected_max_name</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Observer load_from_state_dict got unexpected name </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">strict</span><span class="p">:</span>
                <span class="n">missing_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">prefix</span><span class="p">,</span>
                <span class="n">local_metadata</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="n">missing_keys</span><span class="p">,</span>
                <span class="n">unexpected_keys</span><span class="p">,</span>
                <span class="n">error_msgs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_load_from_state_dict_script</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">local_metadata</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">error_msgs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">prefix</span><span class="p">,</span>
            <span class="n">local_metadata</span><span class="p">,</span>
            <span class="n">strict</span><span class="p">,</span>
            <span class="n">missing_keys</span><span class="p">,</span>
            <span class="n">unexpected_keys</span><span class="p">,</span>
            <span class="n">error_msgs</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="PerChannelMinMaxObserver.reset_min_max_vals"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">reset_min_max_vals</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Resets the min/max values.&quot;&quot;&quot;</span>
        <span class="c1"># This used to be torch.ones but that does not work because</span>
        <span class="c1"># JIT compiler can optimize it via common subexpression elimination</span>
        <span class="c1"># in which case both min_val and max_val point to the same tensor.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">)</span></div></div>


<div class="viewcode-block" id="MovingAveragePerChannelMinMaxObserver"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver">[docs]</a><span class="k">class</span> <span class="nc">MovingAveragePerChannelMinMaxObserver</span><span class="p">(</span><span class="n">PerChannelMinMaxObserver</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Observer module for computing the quantization parameters based on the</span>
<span class="sd">    running per channel min and max values.</span>

<span class="sd">    This observer uses the tensor min/max statistics to compute the per channel</span>
<span class="sd">    quantization parameters. The module records the running minimum and maximum</span>
<span class="sd">    of incoming tensors, and uses this statistic to compute the quantization</span>
<span class="sd">    parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        averaging_constant: Averaging constant for min/max.</span>
<span class="sd">        ch_axis: Channel axis</span>
<span class="sd">        dtype: Quantized data type</span>
<span class="sd">        qscheme: Quantization scheme to be used</span>
<span class="sd">        reduce_range: Reduces the range of the quantized data type by 1 bit</span>
<span class="sd">        quant_min: Minimum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        quant_max: Maximum quantization value. If unspecified, it will follow the 8-bit setup.</span>
<span class="sd">        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.</span>

<span class="sd">    The quantization parameters are computed the same way as in</span>
<span class="sd">    :class:`~torch.ao.quantization.observer.MovingAverageMinMaxObserver`, with the</span>
<span class="sd">    difference that the running min/max values are stored per channel.</span>
<span class="sd">    Scales and zero points are thus computed per channel as well.</span>

<span class="sd">    .. note:: If the running minimum equals to the running maximum, the scales</span>
<span class="sd">              and zero_points are set to 1.0 and 0.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">averaging_constant</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">ch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
        <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_per_channel</span><span class="p">(</span><span class="n">qscheme</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;MovingAveragePerChannelMinMaxObserver&#39;s qscheme only support </span><span class="se">\</span>
<span class="s2">                    torch.per_channel_symmetric, torch.per_channel_affine and torch.per_channel_affine_float_qparams.&quot;</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">ch_axis</span><span class="o">=</span><span class="n">ch_axis</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">qscheme</span><span class="o">=</span><span class="n">qscheme</span><span class="p">,</span>
            <span class="n">reduce_range</span><span class="o">=</span><span class="n">reduce_range</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">averaging_constant</span> <span class="o">=</span> <span class="n">averaging_constant</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_orig</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_orig</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># avoid keeping autograd tape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span>
        <span class="n">max_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span>
        <span class="n">x_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="n">new_axis_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_dim</span><span class="p">))]</span>  <span class="c1"># noqa: C416</span>
        <span class="n">new_axis_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ch_axis</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">new_axis_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ch_axis</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">new_axis_list</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_val</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">max_val</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_val_cur</span><span class="p">,</span> <span class="n">max_val_cur</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="n">min_val</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">averaging_constant</span> <span class="o">*</span> <span class="p">(</span><span class="n">min_val_cur</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span>
            <span class="n">max_val</span> <span class="o">=</span> <span class="n">max_val</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">averaging_constant</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_val_cur</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">min_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">min_val</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">max_val</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_orig</span></div>


<div class="viewcode-block" id="HistogramObserver"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.HistogramObserver.html#torch.ao.quantization.observer.HistogramObserver">[docs]</a><span class="k">class</span> <span class="nc">HistogramObserver</span><span class="p">(</span><span class="n">UniformQuantizationObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The module records the running histogram of tensor values along with</span>
<span class="sd">    min/max values. ``calculate_qparams`` will calculate scale and zero_point.</span>

<span class="sd">    Args:</span>
<span class="sd">        bins: Number of bins to use for the histogram</span>
<span class="sd">        upsample_rate: Factor by which the histograms are upsampled, this is</span>
<span class="sd">                       used to interpolate histograms with varying ranges across observations</span>
<span class="sd">        dtype: dtype argument to the `quantize` node needed to implement the</span>
<span class="sd">               reference model spec</span>
<span class="sd">        qscheme: Quantization scheme to be used</span>
<span class="sd">        reduce_range: Reduces the range of the quantized data type by 1 bit</span>
<span class="sd">        eps: Epsilon value for float32, Defaults to `torch.finfo(torch.float32).eps`.</span>

<span class="sd">    The scale and zero point are computed as follows:</span>

<span class="sd">    1. Create the histogram of the incoming inputs.</span>
<span class="sd">        The histogram is computed continuously, and the ranges per bin change</span>
<span class="sd">        with every new tensor observed.</span>
<span class="sd">    2. Search the distribution in the histogram for optimal min/max values.</span>
<span class="sd">        The search for the min/max values ensures the minimization of the</span>
<span class="sd">        quantization error with respect to the floating point model.</span>
<span class="sd">    3. Compute the scale and zero point the same way as in the</span>
<span class="sd">        :class:`~torch.ao.quantization.MinMaxObserver`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">histogram</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">min_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">max_val</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">upsample_rate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
        <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">factory_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_per_tensor</span><span class="p">(</span><span class="n">qscheme</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;HistogramObserver&#39;s qscheme only support torch.per_tensor_symmetric </span><span class="se">\</span>
<span class="s2">                    and torch.per_tensor_affine.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># bins: The number of bins used for histogram calculation.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">qscheme</span><span class="o">=</span><span class="n">qscheme</span><span class="p">,</span>
            <span class="n">reduce_range</span><span class="o">=</span><span class="n">reduce_range</span><span class="p">,</span>
            <span class="n">quant_min</span><span class="o">=</span><span class="n">quant_min</span><span class="p">,</span>
            <span class="n">quant_max</span><span class="o">=</span><span class="n">quant_max</span><span class="p">,</span>
            <span class="n">factory_kwargs</span><span class="o">=</span><span class="n">factory_kwargs</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">factory_kwargs</span><span class="p">(</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">=</span> <span class="n">bins</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;histogram&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;min_val&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;max_val&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_nbins</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">bits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsample_rate</span> <span class="o">=</span> <span class="n">upsample_rate</span>

    <span class="k">def</span> <span class="nf">_get_norm</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">delta_begin</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">delta_end</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">density</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the norm of the values uniformaly distributed between</span>
<span class="sd">        delta_begin and delta_end.</span>
<span class="sd">        Currently only L2 norm is supported.</span>

<span class="sd">        norm = density * (integral_{begin, end} x^2)</span>
<span class="sd">             = density * (end^3 - begin^3) / 3</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">delta_end</span> <span class="o">*</span> <span class="n">delta_end</span> <span class="o">*</span> <span class="n">delta_end</span> <span class="o">-</span> <span class="n">delta_begin</span> <span class="o">*</span> <span class="n">delta_begin</span> <span class="o">*</span> <span class="n">delta_begin</span>
        <span class="p">)</span> <span class="o">/</span> <span class="mi">3</span>
        <span class="k">return</span> <span class="n">density</span> <span class="o">*</span> <span class="n">norm</span>

    <span class="k">def</span> <span class="nf">_compute_quantization_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">next_start_bin</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">next_end_bin</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the quantization error if we use start_bin to end_bin as the</span>
<span class="sd">        min and max to do the quantization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bin_width</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span>

        <span class="n">dst_bin_width</span> <span class="o">=</span> <span class="n">bin_width</span> <span class="o">*</span> <span class="p">(</span><span class="n">next_end_bin</span> <span class="o">-</span> <span class="n">next_start_bin</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">dst_nbins</span>
        <span class="k">if</span> <span class="n">dst_bin_width</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">0.0</span>

        <span class="n">src_bin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># distances from the beginning of first dst_bin to the beginning and</span>
        <span class="c1"># end of src_bin</span>
        <span class="n">src_bin_begin</span> <span class="o">=</span> <span class="p">(</span><span class="n">src_bin</span> <span class="o">-</span> <span class="n">next_start_bin</span><span class="p">)</span> <span class="o">*</span> <span class="n">bin_width</span>
        <span class="n">src_bin_end</span> <span class="o">=</span> <span class="n">src_bin_begin</span> <span class="o">+</span> <span class="n">bin_width</span>

        <span class="c1"># which dst_bins the beginning and end of src_bin belong to?</span>
        <span class="n">dst_bin_of_begin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">src_bin_begin</span><span class="p">,</span> <span class="n">dst_bin_width</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s1">&#39;floor&#39;</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dst_nbins</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">dst_bin_of_begin_center</span> <span class="o">=</span> <span class="p">(</span><span class="n">dst_bin_of_begin</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">dst_bin_width</span>

        <span class="n">dst_bin_of_end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">src_bin_end</span><span class="p">,</span> <span class="n">dst_bin_width</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s1">&#39;floor&#39;</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dst_nbins</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">density</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">histogram</span> <span class="o">/</span> <span class="n">bin_width</span>

        <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">delta_begin</span> <span class="o">=</span> <span class="n">src_bin_begin</span> <span class="o">-</span> <span class="n">dst_bin_of_begin_center</span>
        <span class="n">delta_end</span> <span class="o">=</span> <span class="n">dst_bin_width</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">norm</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_norm</span><span class="p">(</span><span class="n">delta_begin</span><span class="p">,</span>
                               <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta_end</span><span class="p">,</span>
                               <span class="n">density</span><span class="p">)</span>

        <span class="n">norm</span> <span class="o">+=</span> <span class="p">(</span><span class="n">dst_bin_of_end</span> <span class="o">-</span> <span class="n">dst_bin_of_begin</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_norm</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="n">dst_bin_width</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dst_bin_width</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="n">density</span>
        <span class="p">)</span>

        <span class="n">dst_bin_of_end_center</span> <span class="o">=</span> <span class="n">dst_bin_of_end</span> <span class="o">*</span> <span class="n">dst_bin_width</span> <span class="o">+</span> <span class="n">dst_bin_width</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="n">delta_begin</span> <span class="o">=</span> <span class="o">-</span><span class="n">dst_bin_width</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">delta_end</span> <span class="o">=</span> <span class="n">src_bin_end</span> <span class="o">-</span> <span class="n">dst_bin_of_end_center</span>
        <span class="n">norm</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_norm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">delta_begin</span><span class="p">),</span> <span class="n">delta_end</span><span class="p">,</span> <span class="n">density</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_non_linear_param_search</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Non-linear parameter search.</span>

<span class="sd">        An approximation for L2 error minimization for selecting min/max.</span>
<span class="sd">        By selecting new min/max, we filter out outliers in input distribution.</span>
<span class="sd">        This follows the implementation of NormMinimization::NonlinearQuantizationParamsSearch in</span>
<span class="sd">        caffe2/quantization/server/norm_minimization.cc</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="s2">&quot;bins mismatch&quot;</span>
        <span class="n">bin_width</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span>

        <span class="c1"># cumulative sum</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">cSum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">stepsize</span> <span class="o">=</span> <span class="mf">1e-5</span>  <span class="c1"># granularity</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># lower bound</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># upper bound</span>
        <span class="n">start_bin</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">end_bin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">norm_min</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>

        <span class="k">while</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="n">beta</span><span class="p">:</span>
            <span class="c1"># Find the next step</span>
            <span class="n">next_alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">stepsize</span>
            <span class="n">next_beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">stepsize</span>

            <span class="c1"># find the left and right bins between the quantile bounds</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">start_bin</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">end_bin</span>
            <span class="k">while</span> <span class="n">l</span> <span class="o">&lt;</span> <span class="n">end_bin</span> <span class="ow">and</span> <span class="n">cSum</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">next_alpha</span> <span class="o">*</span> <span class="n">total</span><span class="p">:</span>
                <span class="n">l</span> <span class="o">=</span> <span class="n">l</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">while</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="n">start_bin</span> <span class="ow">and</span> <span class="n">cSum</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">next_beta</span> <span class="o">*</span> <span class="n">total</span><span class="p">:</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="c1"># decide the next move</span>
            <span class="n">next_start_bin</span> <span class="o">=</span> <span class="n">start_bin</span>
            <span class="n">next_end_bin</span> <span class="o">=</span> <span class="n">end_bin</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="n">start_bin</span><span class="p">)</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">end_bin</span> <span class="o">-</span> <span class="n">r</span><span class="p">):</span>
                <span class="c1"># move the start bin</span>
                <span class="n">next_start_bin</span> <span class="o">=</span> <span class="n">l</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="n">next_alpha</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># move the end bin</span>
                <span class="n">next_end_bin</span> <span class="o">=</span> <span class="n">r</span>
                <span class="n">beta</span> <span class="o">=</span> <span class="n">next_beta</span>

            <span class="k">if</span> <span class="n">next_start_bin</span> <span class="o">==</span> <span class="n">start_bin</span> <span class="ow">and</span> <span class="n">next_end_bin</span> <span class="o">==</span> <span class="n">end_bin</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="c1"># calculate the quantization error using next_start_bin and next_end_bin</span>
            <span class="n">norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_quantization_error</span><span class="p">(</span><span class="n">next_start_bin</span><span class="p">,</span> <span class="n">next_end_bin</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">norm</span> <span class="o">&gt;</span> <span class="n">norm_min</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">norm_min</span> <span class="o">=</span> <span class="n">norm</span>
            <span class="n">start_bin</span> <span class="o">=</span> <span class="n">next_start_bin</span>
            <span class="n">end_bin</span> <span class="o">=</span> <span class="n">next_end_bin</span>

        <span class="n">new_min</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span> <span class="o">+</span> <span class="n">bin_width</span> <span class="o">*</span> <span class="n">start_bin</span>
        <span class="n">new_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span> <span class="o">+</span> <span class="n">bin_width</span> <span class="o">*</span> <span class="p">(</span><span class="n">end_bin</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_min</span><span class="p">,</span> <span class="n">new_max</span>

    <span class="k">def</span> <span class="nf">_adjust_min_max</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">combined_min</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">combined_max</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">upsample_rate</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="c1"># We ensure that:</span>
        <span class="c1"># (combined_max - combined_min)/(downsample_rate*Nbins) = (max - min)/(upsample_rate*Nbins)</span>
        <span class="c1"># This allows us to have a common grid of resolution s, where we can align</span>
        <span class="c1"># the input histogram</span>
        <span class="c1"># start_idx maps min_val to the histogram bin index.</span>

        <span class="c1"># Compute the width of histogram bins is a straightforward solution, where</span>
        <span class="c1"># hist_bin_width = (self.max_val - self.min_val) / (self.bins * upsample_rate)</span>
        <span class="c1"># Underflow happens if the numerator is close to the smallest positive subnormal number of FP32</span>
        <span class="c1"># Therefore, we avoid such division operation.</span>
        <span class="n">downsample_rate</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
                <span class="p">(</span><span class="n">combined_max</span> <span class="o">-</span> <span class="n">combined_min</span><span class="p">)</span> <span class="o">*</span> <span class="n">upsample_rate</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">downsample_rate</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="n">upsample_rate</span> <span class="o">-</span> <span class="p">(</span><span class="n">combined_max</span> <span class="o">-</span> <span class="n">combined_min</span><span class="p">)</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span> <span class="o">-</span> <span class="n">combined_min</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">*</span> <span class="n">upsample_rate</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">combined_max</span> <span class="o">=</span> <span class="n">combined_max</span> <span class="o">+</span> <span class="n">e</span>
        <span class="n">combined_min</span> <span class="o">=</span> <span class="n">combined_min</span>
        <span class="k">return</span> <span class="n">combined_min</span><span class="p">,</span> <span class="n">combined_max</span><span class="p">,</span> <span class="n">downsample_rate</span><span class="p">,</span> <span class="n">start_idx</span>

    <span class="k">def</span> <span class="nf">_combine_histograms</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">orig_hist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">new_hist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">upsample_rate</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">downsample_rate</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">start_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">Nbins</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># First up-sample the histogram with new data by a factor of L</span>
        <span class="c1"># This creates an approximate probability density thats piecewise constant</span>
        <span class="n">upsampled_histogram</span> <span class="o">=</span> <span class="n">new_hist</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">upsample_rate</span><span class="p">)</span>
        <span class="c1"># Now insert the upsampled histogram into the output</span>
        <span class="c1"># histogram, which is initialized with zeros.</span>
        <span class="c1"># The offset at which the histogram is introduced is determined</span>
        <span class="c1"># by the start index as the output histogram can cover a wider range</span>
        <span class="n">histogram_with_output_range</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">Nbins</span> <span class="o">*</span> <span class="n">downsample_rate</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">orig_hist</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">histogram_with_output_range</span><span class="p">[</span>
            <span class="n">start_idx</span> <span class="p">:</span> <span class="n">Nbins</span> <span class="o">*</span> <span class="n">upsample_rate</span> <span class="o">+</span> <span class="n">start_idx</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">upsampled_histogram</span>
        <span class="c1"># Compute integral histogram, double precision is needed to ensure</span>
        <span class="c1"># that there are no overflows</span>
        <span class="n">integral_histogram</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span>
            <span class="n">histogram_with_output_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span>
        <span class="p">)[</span><span class="n">downsample_rate</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">::</span> <span class="n">downsample_rate</span><span class="p">]</span>
        <span class="c1"># Finally perform interpolation</span>
        <span class="n">shifted_integral_histogram</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Nbins</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">orig_hist</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">shifted_integral_histogram</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">Nbins</span><span class="p">]</span> <span class="o">=</span> <span class="n">integral_histogram</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">interpolated_histogram</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">integral_histogram</span> <span class="o">-</span> <span class="n">shifted_integral_histogram</span>
        <span class="p">)</span> <span class="o">/</span> <span class="n">upsample_rate</span>
        <span class="n">orig_hist</span> <span class="o">=</span> <span class="n">orig_hist</span> <span class="o">+</span> <span class="n">interpolated_histogram</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">orig_hist</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_orig</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_orig</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span>
        <span class="n">max_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span>
        <span class="n">same_values</span> <span class="o">=</span> <span class="n">min_val</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">max_val</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">is_uninitialized</span> <span class="o">=</span> <span class="n">min_val</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">max_val</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_uninitialized</span> <span class="ow">or</span> <span class="n">same_values</span><span class="p">:</span>
            <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">min_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">min_val</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">max_val</span><span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">min_val</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">max_val</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="p">),</span> <span class="s2">&quot;histogram min/max values must be scalar.&quot;</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">min_val</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">max_val</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">histogram</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_min</span><span class="p">,</span> <span class="n">new_max</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">aminmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">combined_min</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">new_min</span><span class="p">,</span> <span class="n">min_val</span><span class="p">)</span>
            <span class="n">combined_max</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">new_max</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
            <span class="c1"># combine the existing histogram and new histogram into 1 histogram</span>
            <span class="c1"># We do this by first upsampling the histogram to a dense grid</span>
            <span class="c1"># and then downsampling the histogram efficiently</span>
            <span class="p">(</span>
                <span class="n">combined_min</span><span class="p">,</span>
                <span class="n">combined_max</span><span class="p">,</span>
                <span class="n">downsample_rate</span><span class="p">,</span>
                <span class="n">start_idx</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjust_min_max</span><span class="p">(</span><span class="n">combined_min</span><span class="p">,</span> <span class="n">combined_max</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsample_rate</span><span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">combined_min</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">combined_max</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="p">),</span> <span class="s2">&quot;histogram min/max values must be scalar.&quot;</span>
            <span class="n">combined_histogram</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">combined_min</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">combined_max</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">combined_min</span> <span class="o">==</span> <span class="n">min_val</span> <span class="ow">and</span> <span class="n">combined_max</span> <span class="o">==</span> <span class="n">max_val</span><span class="p">:</span>
                <span class="n">combined_histogram</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">histogram</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">combined_histogram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_combine_histograms</span><span class="p">(</span>
                    <span class="n">combined_histogram</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">upsample_rate</span><span class="p">,</span>
                    <span class="n">downsample_rate</span><span class="p">,</span>
                    <span class="n">start_idx</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">combined_histogram</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">combined_histogram</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">combined_min</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">combined_min</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">combined_max</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">combined_max</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_orig</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">is_uninitialized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span>
            <span class="s2">&quot;-inf&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_uninitialized</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;must run observer before calling calculate_qparams.</span><span class="se">\</span>
<span class="s2">                                    Returning default scale and zero point &quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">histogram</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;The number of bins in histogram should be equal to the number of bins &quot;</span>
            <span class="s2">&quot;supplied while making this observer&quot;</span>
        <span class="p">)</span>

        <span class="n">new_min</span><span class="p">,</span> <span class="n">new_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_linear_param_search</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_qparams</span><span class="p">(</span><span class="n">new_min</span><span class="p">,</span> <span class="n">new_max</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_save_to_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
        <span class="n">destination</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;min_val&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span>
        <span class="n">destination</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;max_val&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span>

    <span class="k">def</span> <span class="nf">_load_from_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">,</span>
        <span class="n">local_metadata</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">,</span>
        <span class="n">missing_keys</span><span class="p">,</span>
        <span class="n">unexpected_keys</span><span class="p">,</span>
        <span class="n">error_msgs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">version</span> <span class="o">=</span> <span class="n">local_metadata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;version&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">version</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># if min_val and max_val are not initialized, update their shape</span>
            <span class="c1"># to account for the differences between v2 and v3</span>
            <span class="n">min_val_name</span><span class="p">,</span> <span class="n">max_val_name</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;min_val&quot;</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;max_val&quot;</span>
            <span class="k">if</span> <span class="n">min_val_name</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">min_val_name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">0</span><span class="p">]):</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">min_val_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">max_val_name</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">max_val_name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">0</span><span class="p">]):</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="n">max_val_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>

        <span class="n">local_state</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;min_val&quot;</span><span class="p">,</span> <span class="s2">&quot;max_val&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">local_state</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                <span class="n">val</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">strict</span><span class="p">:</span>
                <span class="n">missing_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">prefix</span><span class="p">,</span>
            <span class="n">local_metadata</span><span class="p">,</span>
            <span class="n">strict</span><span class="p">,</span>
            <span class="n">missing_keys</span><span class="p">,</span>
            <span class="n">unexpected_keys</span><span class="p">,</span>
            <span class="n">error_msgs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;min_val=</span><span class="si">{}</span><span class="s2">, max_val=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">FixedQParamsObserver</span><span class="p">(</span><span class="n">ObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Observer that simulates quantize and dequantize with fixed</span>
<span class="sd">    quantization parameters in training time. Only per tensor</span>
<span class="sd">    quantization is supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        `scale` (float): fixed scale for the observer</span>
<span class="sd">        `zero_point` (int): fixed zero point for the observer</span>
<span class="sd">        `dtype`, `qscheme`, `quant_min`, `quant_max`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scale</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">zero_point</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">scale</span><span class="p">,</span>
                 <span class="n">zero_point</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
                 <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
                 <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span> <span class="o">=</span> <span class="n">quant_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span> <span class="o">=</span> <span class="n">quant_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">scale</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;zero_point&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">zero_point</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">=</span> <span class="n">qscheme</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">X</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span>


<div class="viewcode-block" id="PlaceholderObserver"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.PlaceholderObserver.html#torch.ao.quantization.observer.PlaceholderObserver">[docs]</a><span class="k">class</span> <span class="nc">PlaceholderObserver</span><span class="p">(</span><span class="n">ObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Observer that doesn&#39;t do anything and just passes its configuration to the</span>
<span class="sd">    quantized module&#39;s ``.from_float()``.</span>

<span class="sd">    Can be used for quantization to float16 which doesn&#39;t require determining</span>
<span class="sd">    ranges.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype: dtype argument to the `quantize` node needed to implement the</span>
<span class="sd">               reference model spec.</span>
<span class="sd">        quant_min: minimum value in quantized domain (TODO: align behavior with other observers)</span>
<span class="sd">        quant_max: maximum value in quantized domain</span>
<span class="sd">        custom_op_name: (temporary) specify this observer for an operator that doesn&#39;t require any observation</span>
<span class="sd">                        (Can be used in Graph Mode Passes for special case ops).</span>
<span class="sd">        compute_dtype (deprecated): if set, marks the future quantize function to use</span>
<span class="sd">                       dynamic quantization instead of static quantization.</span>
<span class="sd">                       This field is deprecated, use `is_dynamic=True` instead.</span>
<span class="sd">        is_dynamic: if True, the `quantize` function in the reference model</span>
<span class="sd">                    representation taking stats from this observer instance will</span>
<span class="sd">                    use dynamic quantization.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">custom_op_name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">compute_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">quant_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">qscheme</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">qscheme</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">qscheme</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span>
        <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

        <span class="c1"># dtype of input of the target operator, e.g. for dynamic quantization</span>
        <span class="c1"># ops, the dtype will be float32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span> <span class="o">=</span> <span class="n">qscheme</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_min</span> <span class="o">=</span> <span class="n">quant_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_max</span> <span class="o">=</span> <span class="n">quant_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_op</span> <span class="o">=</span> <span class="n">custom_op_name</span>
        <span class="c1"># used for configuration of computation type for dynamic quantization</span>
        <span class="k">if</span> <span class="n">compute_dtype</span><span class="p">:</span>
            <span class="n">is_dynamic</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Please use `is_dynamic` instead of `compute_dtype`. </span><span class="se">\</span>
<span class="s2">                    `compute_dtype` will be deprecated in a future release </span><span class="se">\</span>
<span class="s2">                    of PyTorch.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_dynamic</span> <span class="o">=</span> <span class="n">is_dynamic</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;dtype=</span><span class="si">{}</span><span class="s2">, is_dynamic=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_dynamic</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
            <span class="s2">&quot;calculate_qparams should not be called for PlaceholderObserver&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="RecordingObserver"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.RecordingObserver.html#torch.ao.quantization.observer.RecordingObserver">[docs]</a><span class="k">class</span> <span class="nc">RecordingObserver</span><span class="p">(</span><span class="n">ObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The module is mainly for debug and records the tensor values during runtime.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype: Quantized data type</span>
<span class="sd">        qscheme: Quantization scheme to be used</span>
<span class="sd">        reduce_range: Reduces the range of the quantized data type by 1 bit</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="vm">__annotations__</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;tensor_val&quot;</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[call-arg]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_val</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;calculate_qparams should not be called for RecordingObserver&quot;</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">get_tensor_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_val</span></div>


<div class="viewcode-block" id="NoopObserver"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.NoopObserver.html#torch.ao.quantization.observer.NoopObserver">[docs]</a><span class="k">class</span> <span class="nc">NoopObserver</span><span class="p">(</span><span class="n">ObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Observer that doesn&#39;t do anything and just passes its configuration to the</span>
<span class="sd">    quantized module&#39;s ``.from_float()``.</span>

<span class="sd">    Primarily used for quantization to float16 which doesn&#39;t require determining</span>
<span class="sd">    ranges.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype: Quantized data type</span>
<span class="sd">        custom_op_name: (temporary) specify this observer for an operator that doesn&#39;t require any observation</span>
<span class="sd">                        (Can be used in Graph Mode Passes for special case ops).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">custom_op_name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_op</span> <span class="o">=</span> <span class="n">custom_op_name</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;calculate_qparams should not be called for NoopObserver&quot;</span><span class="p">)</span></div>

<span class="k">class</span> <span class="nc">ReuseInputObserver</span><span class="p">(</span><span class="n">ObserverBase</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; This observer is used when we want to reuse the observer from the operator</span>
<span class="sd">    that produces the input Tensor, typically used for operators like reshape, e.g.</span>
<span class="sd">    ```</span>
<span class="sd">    x0 = ...</span>
<span class="sd">    x1 = x0.reshape()</span>
<span class="sd">    ```</span>
<span class="sd">    if we configure x0 to be observed by some observer, let&#39;s say MinMaxObserver,</span>
<span class="sd">    and reshape is configured with ReuseInputObserver, we&#39;ll reuse the observer instance</span>
<span class="sd">    for x0 for x1 (output of reshape). If x0 is not observed, we also won&#39;t observe x1.</span>

<span class="sd">    Note: this is only enabled in FX Graph Mode Quantization</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">calculate_qparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;calculate_qparams should not be called for ReuseInputObserver&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_is_observer_script_module</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">obs_type_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns true if given mod is an instance of Observer script module.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">RecursiveScriptModule</span><span class="p">):</span>
        <span class="c1"># qualified name looks like &#39;__torch__.torch.ao.quantization.observer.___torch_mangle_2.MinMaxObserver&#39;</span>
        <span class="n">suffix</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">_c</span><span class="o">.</span><span class="n">qualified_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\.___torch_mangle_\d+&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">suffix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">obs_type_name</span> <span class="ow">in</span> <span class="n">name</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_is_activation_post_process</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">ObserverBase</span><span class="p">,</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">FakeQuantizeBase</span><span class="p">))</span> <span class="ow">or</span> <span class="n">_is_observer_script_module</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;quantization.observer&quot;</span><span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_is_per_channel_script_obs_instance</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">RecursiveScriptModule</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_is_observer_script_module</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span> <span class="s2">&quot;quantization.observer.PerChannelMinMaxObserver&quot;</span>
        <span class="p">)</span> <span class="ow">or</span> <span class="n">_is_observer_script_module</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span> <span class="s2">&quot;quantization.observer.MovingAveragePerChannelMinMaxObserver&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>


<div class="viewcode-block" id="get_observer_state_dict"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.get_observer_state_dict.html#torch.ao.quantization.observer.get_observer_state_dict">[docs]</a><span class="k">def</span> <span class="nf">get_observer_state_dict</span><span class="p">(</span><span class="n">mod</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the state dict corresponding to the observer stats.</span>
<span class="sd">    Traverse the model state_dict and extract out the stats.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">od</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">RecursiveScriptModule</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;observer&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
                <span class="n">od</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># path for GraphModule and nn.Module (eager mode)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;activation_post_process&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
                <span class="n">od</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="n">od</span><span class="o">.</span><span class="n">_metadata</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">_metadata</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">return</span> <span class="n">od</span></div>


<div class="viewcode-block" id="load_observer_state_dict"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.observer.load_observer_state_dict.html#torch.ao.quantization.observer.load_observer_state_dict">[docs]</a><span class="k">def</span> <span class="nf">load_observer_state_dict</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">obs_dict</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given input model and a state_dict containing model observer stats,</span>
<span class="sd">    load the stats back into the model. The observer state_dict can be saved</span>
<span class="sd">    using torch.ao.quantization.get_observer_state_dict</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">missing_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unexpected_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
        <span class="k">if</span> <span class="n">_is_activation_post_process</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">_is_per_channel_script_obs_instance</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="c1"># For per-channel observers we need to call a custom load_from_state_dict to resize the tensor.</span>
                <span class="c1"># However this is not called when the module is scripted and we end up calling the default one in module.py</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict_script</span><span class="p">(</span>
                    <span class="n">obs_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="p">{},</span> <span class="kc">True</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="p">[]</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
                    <span class="n">obs_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="p">{},</span> <span class="kc">False</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="p">[]</span>
                <span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">missing_keys</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;observer&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="ow">or</span> <span class="s2">&quot;activation_post_process&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Missing keys for observer </span><span class="si">{}</span><span class="s2"> in state_dict&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;observer&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="ow">or</span> <span class="s2">&quot;activation_post_process&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Unexpected keys for observer </span><span class="si">{}</span><span class="s2"> in state_dict&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span></div>


<span class="c1"># Restrict activations to be in the range (0,127)</span>
<span class="n">default_observer</span> <span class="o">=</span> <span class="n">MinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default observer for static quantization, usually used for debugging.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_placeholder_observer</span> <span class="o">=</span> <span class="n">PlaceholderObserver</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default placeholder observer, usually used for quantization to torch.float16.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_debug_observer</span> <span class="o">=</span> <span class="n">RecordingObserver</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default debug-only observer.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_weight_observer</span> <span class="o">=</span> <span class="n">MinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span>
<span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default weight observer.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">weight_observer_range_neg_127_to_127</span> <span class="o">=</span> <span class="n">MinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="o">=-</span><span class="mi">127</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Symmetric weight observer with the 8-bit values restricted to [-127, +127], excluding -128.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_histogram_observer</span> <span class="o">=</span> <span class="n">HistogramObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default histogram observer, usually used for PTQ.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_per_channel_weight_observer</span> <span class="o">=</span> <span class="n">PerChannelMinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_symmetric</span>
<span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default per-channel weight observer, usually used on backends where per-channel</span>
<span class="sd">weight quantization is supported, such as `fbgemm`.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">per_channel_weight_observer_range_neg_127_to_127</span> <span class="o">=</span> <span class="n">PerChannelMinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_symmetric</span><span class="p">,</span>
    <span class="n">quant_min</span><span class="o">=-</span><span class="mi">127</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="mi">127</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="o">-</span><span class="mi">12</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Per-channel, symmetric weight observer with the 8-bit values restricted to [-127, +127], excluding -128.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_dynamic_quant_observer</span> <span class="o">=</span> <span class="n">PlaceholderObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span> <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default observer for dynamic quantization.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_float_qparams_observer</span> <span class="o">=</span> <span class="n">PerChannelMinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span> <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span> <span class="n">ch_axis</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default observer for a floating point zero-point.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_float_qparams_observer_4bit</span> <span class="o">=</span> <span class="n">PerChannelMinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span> <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">,</span> <span class="n">ch_axis</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default observer for a floating point zero-point and 4 bit activations.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># TODO(future PR): remove these defaults and enforce activation functions</span>
<span class="c1"># to explicitly specify their output range</span>
<span class="n">default_fixed_qparams_range_neg1to1_observer</span> <span class="o">=</span> <span class="n">FixedQParamsObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span> <span class="o">/</span> <span class="mf">256.0</span><span class="p">,</span> <span class="n">zero_point</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span> <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">)</span>
<span class="n">default_fixed_qparams_range_0to1_observer</span> <span class="o">=</span> <span class="n">FixedQParamsObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
    <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">256.0</span><span class="p">,</span> <span class="n">zero_point</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span> <span class="n">quant_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">quant_max</span><span class="o">=</span><span class="mi">255</span><span class="p">)</span>
<span class="c1"># TODO: the following 2 variables are kept for backwards compatibility; remove after a few releases</span>
<span class="n">default_symmetric_fixed_qparams_observer</span> <span class="o">=</span> <span class="n">default_fixed_qparams_range_neg1to1_observer</span>
<span class="n">default_affine_fixed_qparams_observer</span> <span class="o">=</span> <span class="n">default_fixed_qparams_range_0to1_observer</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default observers for fixed qparams operations.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">default_reuse_input_observer</span> <span class="o">=</span> <span class="n">ReuseInputObserver</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Default observer for operators like reshape that reuses the observer of input to</span>
<span class="sd">the operator</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
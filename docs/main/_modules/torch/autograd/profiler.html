


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.autograd.profiler &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/autograd/profiler.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/autograd/profiler.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../autograd.html">torch.autograd</a> &gt;</li>
        
      <li>torch.autograd.profiler</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.autograd.profiler</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">warn</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">torch.cuda</span>
<span class="kn">from</span> <span class="nn">torch._C._profiler</span> <span class="kn">import</span> <span class="n">_ExperimentalConfig</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_get_privateuse1_backend_name</span>

<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_disable_profiler</span><span class="p">,</span>
    <span class="n">_enable_profiler</span><span class="p">,</span>
    <span class="n">_kineto_step</span><span class="p">,</span>
    <span class="n">_prepare_profiler</span><span class="p">,</span>
    <span class="n">_ProfilerResult</span><span class="p">,</span>
    <span class="n">_supported_activities</span><span class="p">,</span>
    <span class="n">DeviceType</span><span class="p">,</span>
    <span class="n">kineto_available</span><span class="p">,</span>
    <span class="n">ProfilerActivity</span><span class="p">,</span>
    <span class="n">ProfilerConfig</span><span class="p">,</span>
    <span class="n">ProfilerState</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.autograd.profiler_util</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_filter_name</span><span class="p">,</span>
    <span class="n">_filter_stack_entry</span><span class="p">,</span>
    <span class="n">_rewrite_name</span><span class="p">,</span>
    <span class="n">EventList</span><span class="p">,</span>
    <span class="n">FunctionEvent</span><span class="p">,</span>
    <span class="n">MEMORY_EVENT_NAME</span><span class="p">,</span>
    <span class="n">MemRecordsAcc</span><span class="p">,</span>
    <span class="n">OUT_OF_MEMORY_EVENT_NAME</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.futures</span> <span class="kn">import</span> <span class="n">Future</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;profile&quot;</span><span class="p">,</span> <span class="s2">&quot;record_function&quot;</span><span class="p">,</span> <span class="s2">&quot;emit_itt&quot;</span><span class="p">,</span> <span class="s2">&quot;emit_nvtx&quot;</span><span class="p">,</span> <span class="s2">&quot;load_nvprof&quot;</span><span class="p">,</span> <span class="s2">&quot;EnforceUnique&quot;</span><span class="p">,</span>
           <span class="s2">&quot;parse_nvprof_trace&quot;</span><span class="p">,</span> <span class="s2">&quot;KinetoStepTracker&quot;</span><span class="p">,</span> <span class="s2">&quot;EventList&quot;</span><span class="p">,</span> <span class="s2">&quot;FunctionEvent&quot;</span><span class="p">,</span> <span class="s2">&quot;MemRecordsAcc&quot;</span><span class="p">]</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Available in Python &gt;= 3.2</span>
    <span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">ContextDecorator</span> <span class="k">as</span> <span class="n">_ContextDecorator</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">functools</span>

    <span class="k">class</span> <span class="nc">_ContextDecorator</span><span class="p">:</span>  <span class="c1"># type: ignore[no-redef]</span>

        <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
            <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
            <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="k">with</span> <span class="bp">self</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">wrapped</span>

<span class="k">def</span> <span class="nf">_enable_dynamo_cache_lookup_profiler</span><span class="p">(</span><span class="n">enable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">torch._dynamo.eval_frame</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="n">clear_profiler_hooks</span><span class="p">,</span>
        <span class="n">set_profiler_hooks</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Registers a hook within dynamo eval_frame.c called before and after</span>
<span class="sd">    the lookup process, which runs guards associated with each cached frame.</span>

<span class="sd">    Clear deregisters the hooks, saving overhead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">enable</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">_profiler_start</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_record_function_enter_new</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_profiler_end</span><span class="p">(</span><span class="n">record</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_record_function_exit</span><span class="o">.</span><span class="n">_RecordFunction</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
        <span class="n">set_profiler_hooks</span><span class="p">(</span><span class="n">_profiler_start</span><span class="p">,</span> <span class="n">_profiler_end</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">clear_profiler_hooks</span><span class="p">()</span>


<div class="viewcode-block" id="profile"><a class="viewcode-back" href="../../../autograd.html#torch.autograd.profiler.profile">[docs]</a><span class="k">class</span> <span class="nc">profile</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Context manager that manages autograd profiler state and holds a summary of results.</span>
<span class="sd">    Under the hood it just records events of functions being executed in C++ and</span>
<span class="sd">    exposes those events to Python. You can wrap any code into it and it will</span>
<span class="sd">    only report runtime of PyTorch functions.</span>
<span class="sd">    Note: profiler is thread local and is automatically propagated into the async tasks</span>

<span class="sd">    Args:</span>
<span class="sd">        enabled (bool, optional): Setting this to False makes this context manager a no-op.</span>

<span class="sd">        use_cuda (bool, optional): Enables timing of CUDA events as well using the cudaEvent API.</span>
<span class="sd">            Adds approximately 4us of overhead to each tensor operation.</span>

<span class="sd">        record_shapes (bool, optional): If shapes recording is set, information</span>
<span class="sd">            about input dimensions will be collected. This allows one to see which</span>
<span class="sd">            dimensions have been used under the hood and further group by them</span>
<span class="sd">            using prof.key_averages(group_by_input_shape=True). Please note that</span>
<span class="sd">            shape recording might skew your profiling data. It is recommended to</span>
<span class="sd">            use separate runs with and without shape recording to validate the timing.</span>
<span class="sd">            Most likely the skew will be negligible for bottom most events (in a case</span>
<span class="sd">            of nested function calls). But for higher level functions the total</span>
<span class="sd">            self cpu time might be artificially increased because of the shape</span>
<span class="sd">            collection.</span>

<span class="sd">        with_flops (bool, optional): If with_flops is set, the profiler will estimate</span>
<span class="sd">            the FLOPs (floating point operations) value using the operator&#39;s input shape.</span>
<span class="sd">            This allows one to estimate the hardware performance. Currently,</span>
<span class="sd">            this option only works for the matrix multiplication and 2D convolution operators.</span>

<span class="sd">        profile_memory (bool, optional): track tensor memory allocation/deallocation.</span>

<span class="sd">        with_stack (bool, optional): record source information (file and line number) for the ops.</span>

<span class="sd">        with_modules (bool): record module hierarchy (including function names)</span>
<span class="sd">            corresponding to the callstack of the op. e.g. If module A&#39;s forward call&#39;s</span>
<span class="sd">            module B&#39;s forward which contains an aten::add op,</span>
<span class="sd">            then aten::add&#39;s module hierarchy is A.B</span>
<span class="sd">            Note that this support exist, at the moment, only for TorchScript models</span>
<span class="sd">            and not eager mode models.</span>

<span class="sd">        use_kineto (bool, optional): experimental, enable profiling with Kineto profiler.</span>

<span class="sd">        use_cpu (bool, optional): profile CPU events; setting to ``False`` requires</span>
<span class="sd">            ``use_kineto=True`` and can be used to lower the overhead for GPU-only profiling.</span>

<span class="sd">        experimental_config (_ExperimentalConfig) : A set of experimental options</span>
<span class="sd">            used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.</span>


<span class="sd">    .. warning:</span>
<span class="sd">        Enabling memory profiling or source attribution incurs additional profiler</span>
<span class="sd">        overhead</span>

<span class="sd">    .. warning:</span>
<span class="sd">        This context managers should not be called recursively, i.e. no nested</span>
<span class="sd">        instances are allowed</span>

<span class="sd">    .. warning:</span>
<span class="sd">        Due to some CUDA multiprocessing limitations (multiprocessing-cuda-note_),</span>
<span class="sd">        one cannot use the profiler with ``use_cuda = True`` to benchmark</span>
<span class="sd">        DataLoaders with ``num_workers &gt; 0``. If you wish to benchmark data loading,</span>
<span class="sd">        please use ``use_cuda = False`` or ``num_workers = 0``.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD_PROFILER)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn((1, 1), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; with torch.autograd.profiler.profile() as prof:</span>
<span class="sd">        &gt;&gt;&gt;     for _ in range(100):  # any normal python code, really!</span>
<span class="sd">        &gt;&gt;&gt;         y = x ** 2</span>
<span class="sd">        &gt;&gt;&gt;         y.backward()</span>
<span class="sd">        &gt;&gt;&gt; # NOTE: some columns were removed for brevity</span>
<span class="sd">        &gt;&gt;&gt; print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;))</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        Name                                 Self CPU total   CPU time avg     Number of Calls</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        mul                                  32.048ms         32.048ms         200</span>
<span class="sd">        pow                                  27.041ms         27.041ms         200</span>
<span class="sd">        PowBackward0                         9.727ms          55.483ms         100</span>
<span class="sd">        torch::autograd::AccumulateGrad      9.148ms          9.148ms          100</span>
<span class="sd">        torch::autograd::GraphRoot           691.816us        691.816us        100</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="o">*</span><span class="p">,</span>
            <span class="n">use_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">use_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">with_flops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">profile_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">with_stack</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">with_modules</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">use_kineto</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">use_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">use_mtia</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">experimental_config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="n">use_cuda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">=</span> <span class="n">use_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EventList</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span> <span class="o">=</span> <span class="n">record_shapes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_flops</span> <span class="o">=</span> <span class="n">with_flops</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span> <span class="o">|=</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_flops</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">profile_memory</span> <span class="o">=</span> <span class="n">profile_memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_stack</span> <span class="o">=</span> <span class="n">with_stack</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_modules</span> <span class="o">=</span> <span class="n">with_modules</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span> <span class="o">=</span> <span class="n">use_cpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_mtia</span> <span class="o">=</span> <span class="n">use_mtia</span>
        <span class="k">if</span> <span class="n">experimental_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">experimental_config</span> <span class="o">=</span> <span class="n">_ExperimentalConfig</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experimental_config</span> <span class="o">=</span> <span class="n">experimental_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ProfilerResult</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">use_kineto</span><span class="p">,</span> \
                <span class="s2">&quot;Device-only events supported only with Kineto (use_kineto=True)&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;CUDA is not available, disabling CUDA profiling&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_mtia</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">MTIA</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">profiler_kind</span> <span class="o">=</span> <span class="n">ProfilerState</span><span class="o">.</span><span class="n">KINETO</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">use_kineto</span> <span class="ow">or</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span> <span class="ow">not</span> <span class="ow">in</span>
                    <span class="n">_supported_activities</span><span class="p">()):</span>
                <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">,</span> <span class="s2">&quot;Legacy CUDA profiling requires use_cpu=True&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">profiler_kind</span> <span class="o">=</span> <span class="n">ProfilerState</span><span class="o">.</span><span class="n">KINETO_GPU_FALLBACK</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
                <span class="c1"># TODO:using &#39;use_device&#39; instead of &#39;use_cuda&#39; facilitates access by other devices</span>
                <span class="c1"># and integrate it in subsequent pr.</span>
                <span class="k">pass</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_device</span> <span class="o">==</span> <span class="n">_get_privateuse1_backend_name</span><span class="p">():</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">use_kineto</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">,</span> <span class="s2">&quot;Legacy custombackend profiling requires use_cpu=True&quot;</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">profiler_kind</span> <span class="o">=</span> <span class="n">ProfilerState</span><span class="o">.</span><span class="n">KINETO_PRIVATEUSE1_FALLBACK</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                        <span class="s2">&quot;Now, custombackend events does not support Kineto (use_kineto=False)&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">use_device</span><span class="si">}</span><span class="s2"> doesn&#39;t support profile.&quot;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> \
            <span class="s2">&quot;No activities specified for the profiler&quot;</span>


    <span class="k">def</span> <span class="nf">config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ProfilerConfig</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">profiler_kind</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">profile_memory</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">with_stack</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">with_flops</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">with_modules</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">experimental_config</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entered</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Profiler context manager is not reentrant&quot;</span><span class="p">)</span>
        <span class="n">_enable_dynamo_cache_lookup_profiler</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_trace</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_trace</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_prepare_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">_prepare_profiler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_start_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">_enable_profiler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">kineto_activities</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">_enable_dynamo_cache_lookup_profiler</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span> <span class="o">=</span> <span class="n">_disable_profiler</span><span class="p">()</span>
        <span class="n">parsed_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_kineto_results</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="o">=</span> <span class="n">EventList</span><span class="p">(</span>
            <span class="n">parsed_results</span><span class="p">,</span>
            <span class="n">use_cuda</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">,</span>
            <span class="n">profile_memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">profile_memory</span><span class="p">,</span>
            <span class="n">with_flops</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">with_flops</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="o">.</span><span class="n">_build_tree</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;&lt;unfinished torch.autograd.profile&gt;&#39;</span>
        <span class="k">return</span> <span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;&lt;unfinished torch.autograd.profile&gt;&#39;</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_finish</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Profiler didn&#39;t finish running&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">table</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">sort_by</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">row_limit</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">max_src_column_width</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span>
            <span class="n">max_name_column_width</span><span class="o">=</span><span class="mi">55</span><span class="p">,</span>
            <span class="n">max_shapes_column_width</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span>
            <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">top_level_events_only</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_finish</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="o">.</span><span class="n">table</span><span class="p">(</span>
            <span class="n">sort_by</span><span class="o">=</span><span class="n">sort_by</span><span class="p">,</span>
            <span class="n">row_limit</span><span class="o">=</span><span class="n">row_limit</span><span class="p">,</span>
            <span class="n">max_src_column_width</span><span class="o">=</span><span class="n">max_src_column_width</span><span class="p">,</span>
            <span class="n">max_name_column_width</span><span class="o">=</span><span class="n">max_name_column_width</span><span class="p">,</span>
            <span class="n">max_shapes_column_width</span><span class="o">=</span><span class="n">max_shapes_column_width</span><span class="p">,</span>
            <span class="n">header</span><span class="o">=</span><span class="n">header</span><span class="p">,</span>
            <span class="n">top_level_events_only</span><span class="o">=</span><span class="n">top_level_events_only</span>
        <span class="p">)</span>
    <span class="n">table</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">EventList</span><span class="o">.</span><span class="n">table</span><span class="o">.</span><span class="vm">__doc__</span>

<div class="viewcode-block" id="profile.export_chrome_trace"><a class="viewcode-back" href="../../../generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace">[docs]</a>    <span class="k">def</span> <span class="nf">export_chrome_trace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_finish</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">kineto_available</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kineto_results</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>  <span class="c1"># type: ignore[union-attr]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>  <span class="c1"># type: ignore[union-attr]</span></div>
    <span class="n">export_chrome_trace</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">EventList</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="o">.</span><span class="vm">__doc__</span>

    <span class="k">def</span> <span class="nf">export_stacks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">metric</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;self_cpu_time_total&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_finish</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected profiling results&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_stack</span><span class="p">,</span> <span class="s2">&quot;export_stacks() requires with_stack=True&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="o">.</span><span class="n">export_stacks</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span>

<div class="viewcode-block" id="profile.key_averages"><a class="viewcode-back" href="../../../generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages">[docs]</a>    <span class="k">def</span> <span class="nf">key_averages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group_by_input_shape</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_finish</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected profiling results&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_input_shape</span><span class="p">,</span> <span class="n">group_by_stack_n</span><span class="p">)</span></div>
    <span class="n">key_averages</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">EventList</span><span class="o">.</span><span class="n">key_averages</span><span class="o">.</span><span class="vm">__doc__</span>

<div class="viewcode-block" id="profile.total_average"><a class="viewcode-back" href="../../../generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average">[docs]</a>    <span class="k">def</span> <span class="nf">total_average</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_finish</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Expected profiling results&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="o">.</span><span class="n">total_average</span><span class="p">()</span></div>
    <span class="n">total_average</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">EventList</span><span class="o">.</span><span class="n">total_average</span><span class="o">.</span><span class="vm">__doc__</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">self_cpu_time_total</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Returns total time spent on CPU obtained as a sum of</span>
<span class="sd">        all self times across all the events.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_finish</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function_events</span><span class="o">.</span><span class="n">self_cpu_time_total</span>

    <span class="k">def</span> <span class="nf">_parse_kineto_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">_ProfilerResult</span><span class="p">):</span>
        <span class="c1"># result.events() has most of the events - PyTorch op-level and device-level events</span>

        <span class="n">trace_start_us</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">trace_start_us</span><span class="p">()</span>
        <span class="n">mem_records</span> <span class="o">=</span> <span class="p">[[</span><span class="n">evt</span><span class="p">,</span> <span class="kc">False</span><span class="p">]</span> <span class="k">for</span> <span class="n">evt</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">events</span><span class="p">()</span> <span class="k">if</span> <span class="n">evt</span><span class="o">.</span><span class="n">name</span><span class="p">()</span> <span class="o">==</span> <span class="n">MEMORY_EVENT_NAME</span><span class="p">]</span>
        <span class="n">oom_records</span> <span class="o">=</span> <span class="p">[</span><span class="n">evt</span> <span class="k">for</span> <span class="n">evt</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">events</span><span class="p">()</span> <span class="k">if</span> <span class="n">evt</span><span class="o">.</span><span class="n">name</span><span class="p">()</span> <span class="o">==</span> <span class="n">OUT_OF_MEMORY_EVENT_NAME</span><span class="p">]</span>
        <span class="n">mem_records_acc</span> <span class="o">=</span> <span class="n">MemRecordsAcc</span><span class="p">(</span><span class="n">mem_records</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_cpu_memory_usage</span><span class="p">(</span><span class="n">mem_record</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">mem_record</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span> <span class="k">if</span> \
                <span class="n">mem_record</span><span class="o">.</span><span class="n">device_type</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">MKLDNN</span><span class="p">,</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">IDEEP</span><span class="p">]</span> \
                <span class="k">else</span> <span class="mi">0</span>

        <span class="k">def</span> <span class="nf">_cuda_memory_usage</span><span class="p">(</span><span class="n">mem_record</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">mem_record</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span> <span class="k">if</span> \
                <span class="n">mem_record</span><span class="o">.</span><span class="n">device_type</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">HIP</span><span class="p">]</span> \
                <span class="k">else</span> <span class="mi">0</span>

        <span class="c1"># Create and return FunctionEvent list</span>
        <span class="n">function_events</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">cuda_corr_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">FunctionEvent</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">max_evt_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">kineto_event</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">events</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">_filter_name</span><span class="p">(</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">name</span><span class="p">()):</span>
                <span class="k">continue</span>
            <span class="n">rel_start_us</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">start_us</span><span class="p">()</span> <span class="o">-</span> <span class="n">trace_start_us</span>
            <span class="n">rel_end_us</span> <span class="o">=</span> <span class="n">rel_start_us</span> <span class="o">+</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">duration_us</span><span class="p">()</span>
            <span class="n">abs_end_us</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">start_us</span><span class="p">()</span> <span class="o">+</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">duration_us</span><span class="p">()</span>

            <span class="n">cpu_memory_usage</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cuda_memory_usage</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">device_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">:</span>
                <span class="c1"># find the corresponding memory allocation events</span>
                <span class="k">for</span> <span class="n">mem_record</span> <span class="ow">in</span> <span class="n">mem_records_acc</span><span class="o">.</span><span class="n">in_interval</span><span class="p">(</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">start_us</span><span class="p">(),</span> <span class="n">abs_end_us</span><span class="p">):</span>
                    <span class="n">cpu_memory_usage</span> <span class="o">+=</span> <span class="n">_cpu_memory_usage</span><span class="p">(</span><span class="n">mem_record</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">cuda_memory_usage</span> <span class="o">+=</span> <span class="n">_cuda_memory_usage</span><span class="p">(</span><span class="n">mem_record</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">mem_record</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">is_async</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">is_async</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="n">kineto_event</span><span class="o">.</span><span class="n">start_thread_id</span><span class="p">()</span> <span class="o">!=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">end_thread_id</span><span class="p">()</span>
            <span class="p">)</span>

            <span class="n">fe</span> <span class="o">=</span> <span class="n">FunctionEvent</span><span class="p">(</span>
                <span class="nb">id</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">correlation_id</span><span class="p">(),</span>
                <span class="n">name</span><span class="o">=</span><span class="n">_rewrite_name</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">name</span><span class="p">(),</span> <span class="n">with_wildcard</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                <span class="n">trace_name</span><span class="o">=</span><span class="n">_rewrite_name</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">name</span><span class="p">(),</span> <span class="n">with_wildcard</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">thread</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">start_thread_id</span><span class="p">(),</span>
                <span class="n">start_us</span><span class="o">=</span><span class="n">rel_start_us</span><span class="p">,</span>
                <span class="n">end_us</span><span class="o">=</span><span class="n">rel_end_us</span><span class="p">,</span>
                <span class="n">fwd_thread</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">fwd_thread_id</span><span class="p">(),</span>
                <span class="n">input_shapes</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">shapes</span><span class="p">(),</span>
                <span class="n">concrete_inputs</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">concrete_inputs</span><span class="p">(),</span>
                <span class="n">stack</span><span class="o">=</span><span class="p">[</span><span class="n">entry</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span> <span class="k">if</span> <span class="n">_filter_stack_entry</span><span class="p">(</span><span class="n">entry</span><span class="p">)],</span>
                <span class="n">scope</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">scope</span><span class="p">(),</span>
                <span class="n">cpu_memory_usage</span><span class="o">=</span><span class="n">cpu_memory_usage</span><span class="p">,</span>
                <span class="n">cuda_memory_usage</span><span class="o">=</span><span class="n">cuda_memory_usage</span><span class="p">,</span>
                <span class="n">is_async</span><span class="o">=</span><span class="n">is_async</span><span class="p">,</span>
                <span class="n">sequence_nr</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">sequence_nr</span><span class="p">(),</span>
                <span class="n">device_type</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">device_type</span><span class="p">(),</span>
                <span class="n">device_index</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">device_index</span><span class="p">(),</span>
                <span class="n">flops</span><span class="o">=</span><span class="n">kineto_event</span><span class="o">.</span><span class="n">flops</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="n">max_evt_id</span> <span class="o">=</span> <span class="n">fe</span><span class="o">.</span><span class="n">id</span> <span class="k">if</span> <span class="n">fe</span><span class="o">.</span><span class="n">id</span> <span class="o">&gt;</span> <span class="n">max_evt_id</span> <span class="k">else</span> <span class="n">max_evt_id</span>
            <span class="k">if</span> <span class="n">fe</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">fe</span><span class="o">.</span><span class="n">is_async</span><span class="p">:</span>
                <span class="c1"># Check if we have CUDA time as a fallback</span>
                <span class="n">cuda_time</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">cuda_elapsed_us</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">cuda_time</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">fe</span><span class="o">.</span><span class="n">append_kernel</span><span class="p">(</span>
                        <span class="n">fe</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                        <span class="n">fe</span><span class="o">.</span><span class="n">device_index</span><span class="p">,</span>
                        <span class="n">cuda_time</span><span class="p">)</span>
                    <span class="n">fe</span><span class="o">.</span><span class="n">is_legacy</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">function_events</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>
            <span class="n">corr_id</span> <span class="o">=</span> <span class="n">kineto_event</span><span class="o">.</span><span class="n">linked_correlation_id</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">corr_id</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">corr_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">cuda_corr_map</span><span class="p">:</span>
                    <span class="n">cuda_corr_map</span><span class="p">[</span><span class="n">corr_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">cuda_corr_map</span><span class="p">[</span><span class="n">corr_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>

        <span class="c1"># associate CUDA kernels and CUDA runtime (CPU) with CPU events</span>
        <span class="k">for</span> <span class="n">fe</span> <span class="ow">in</span> <span class="n">function_events</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">fe</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">fe</span><span class="o">.</span><span class="n">is_async</span> <span class="ow">and</span>
                    <span class="n">fe</span><span class="o">.</span><span class="n">id</span> <span class="ow">in</span> <span class="n">cuda_corr_map</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">f_evt</span> <span class="ow">in</span> <span class="n">cuda_corr_map</span><span class="p">[</span><span class="n">fe</span><span class="o">.</span><span class="n">id</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">f_evt</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span><span class="p">:</span>
                        <span class="n">fe</span><span class="o">.</span><span class="n">append_kernel</span><span class="p">(</span>
                            <span class="n">f_evt</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                            <span class="n">f_evt</span><span class="o">.</span><span class="n">device_index</span><span class="p">,</span>
                            <span class="n">f_evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">end</span> <span class="o">-</span> <span class="n">f_evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">start</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">f_evt</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">:</span>
                        <span class="c1"># make sure that &#39;thread&#39; of a CPU Kineto (e.g. CUDA Runtime) event is associated</span>
                        <span class="c1"># with the &#39;thread&#39; of the corresponding linked PyTorch event to properly track</span>
                        <span class="c1"># parents and children</span>
                        <span class="n">f_evt</span><span class="o">.</span><span class="n">thread</span> <span class="o">=</span> <span class="n">fe</span><span class="o">.</span><span class="n">thread</span>


        <span class="k">def</span> <span class="nf">createFunctionEventForMemoryEvents</span><span class="p">(</span><span class="n">evt</span><span class="p">):</span>
            <span class="n">rel_start_us</span> <span class="o">=</span> <span class="n">evt</span><span class="o">.</span><span class="n">start_us</span><span class="p">()</span> <span class="o">-</span> <span class="n">trace_start_us</span>
            <span class="n">fe</span> <span class="o">=</span> <span class="n">FunctionEvent</span><span class="p">(</span>
                <span class="nb">id</span><span class="o">=</span><span class="n">max_evt_id</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">evt</span><span class="o">.</span><span class="n">name</span><span class="p">(),</span>
                <span class="n">trace_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># not outputting in the trace</span>
                <span class="n">thread</span><span class="o">=</span><span class="n">evt</span><span class="o">.</span><span class="n">start_thread_id</span><span class="p">(),</span>
                <span class="n">start_us</span><span class="o">=</span><span class="n">rel_start_us</span><span class="p">,</span>
                <span class="n">end_us</span><span class="o">=</span><span class="n">rel_start_us</span><span class="p">,</span>  <span class="c1"># no duration</span>
                <span class="n">fwd_thread</span><span class="o">=</span><span class="n">evt</span><span class="o">.</span><span class="n">start_thread_id</span><span class="p">(),</span>
                <span class="n">input_shapes</span><span class="o">=</span><span class="p">[],</span>
                <span class="n">stack</span><span class="o">=</span><span class="p">[],</span>
                <span class="n">scope</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># RecordScope::FUNCTION</span>
                <span class="n">cpu_memory_usage</span><span class="o">=</span><span class="n">_cpu_memory_usage</span><span class="p">(</span><span class="n">evt</span><span class="p">),</span>
                <span class="n">cuda_memory_usage</span><span class="o">=</span><span class="n">_cuda_memory_usage</span><span class="p">(</span><span class="n">evt</span><span class="p">),</span>
                <span class="n">is_async</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">sequence_nr</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">device_type</span><span class="o">=</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
                <span class="n">device_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">fe</span>

        <span class="c1"># output top-level memory events</span>
        <span class="k">for</span> <span class="n">mem_record</span> <span class="ow">in</span> <span class="n">mem_records</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">mem_record</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">max_evt_id</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">fe</span> <span class="o">=</span> <span class="n">createFunctionEventForMemoryEvents</span><span class="p">(</span><span class="n">mem_record</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">function_events</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">oom_record</span> <span class="ow">in</span> <span class="n">oom_records</span><span class="p">:</span>
            <span class="n">max_evt_id</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">fe</span> <span class="o">=</span> <span class="n">createFunctionEventForMemoryEvents</span><span class="p">(</span><span class="n">oom_record</span><span class="p">)</span>
            <span class="n">function_events</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span>

        <span class="n">function_events</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">evt</span><span class="p">:</span> <span class="p">[</span><span class="n">evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="o">-</span><span class="n">evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">end</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">function_events</span></div>


<span class="k">class</span> <span class="nc">record_function</span><span class="p">(</span><span class="n">_ContextDecorator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Context manager/function decorator that adds a label to a block of</span>
<span class="sd">    Python code (or function) when running autograd profiler. It is</span>
<span class="sd">    useful when tracing the code profile.</span>

<span class="sd">    Args:</span>
<span class="sd">        name (str): Label assigned to the block of code.</span>
<span class="sd">        node_id (int): ID of node, for distributed profiling. Unset in</span>
<span class="sd">        non-distributed cases.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD_PROFILER)</span>
<span class="sd">        &gt;&gt;&gt; x = torch.randn((1, 1), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; with torch.autograd.profiler.profile() as prof:</span>
<span class="sd">        ...     y = x ** 2</span>
<span class="sd">        ...     with torch.autograd.profiler.record_function(&quot;label-z&quot;): # label the block</span>
<span class="sd">        ...         z = y ** 3</span>
<span class="sd">        ...     y.backward()</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT</span>
<span class="sd">        &gt;&gt;&gt; # NOTE: some columns were removed for brevity</span>
<span class="sd">        &gt;&gt;&gt; print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;))</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        Name                                 Self CPU total %  CPU time avg     Number of Calls</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        pow                                  60.77%           47.470us         3</span>
<span class="sd">        mul                                  21.73%           25.465us         2</span>
<span class="sd">        PowBackward0                         12.03%           121.891us        1</span>
<span class="sd">        torch::autograd::AccumulateGrad      2.70%            6.324us          1</span>
<span class="sd">        label-z                              2.13%            12.421us         1</span>
<span class="sd">        torch::autograd::GraphRoot           0.64%            1.503us          1</span>
<span class="sd">        -----------------------------------  ---------------  ---------------  ---------------</span>
<span class="sd">        Self CPU time total: 234.344us</span>
<span class="sd">        CUDA time total: 0.000us</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span>
        <span class="c1"># Whether or not we should run record function&#39;s end callbacks when exiting.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_callbacks_on_exit</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># TODO: TorchScript ignores standard type annotation here</span>
        <span class="c1"># self.record: Optional[&quot;torch.classes.profiler._RecordFunction&quot;] = None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;torch.classes.profiler._RecordFunction&quot;</span><span class="p">],</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_record_function_enter_new</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">traceback</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_callbacks_on_exit</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Local variable is needed by TorchScript to refine Optional[T] to T</span>
        <span class="n">record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">record</span>
        <span class="k">assert</span> <span class="n">record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># TODO: Too slow with __torch_function__ handling enabled</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/issues/76410</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_record_function_exit</span><span class="o">.</span><span class="n">_RecordFunction</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_record_function_exit</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_call_end_callbacks_on_future</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fut</span><span class="p">:</span> <span class="n">Future</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Future</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        _call_end_callbacks_on_future is meant to be used for profiling async</span>
<span class="sd">        calls that return a future. Calling this function will extend recording</span>
<span class="sd">        beyond this scope, until the future is satisfied. It is useful for profiling</span>
<span class="sd">        the end to end time of asynchronous calls. This function should only be called</span>
<span class="sd">        once to attach the callback onto the future, and will throw if called multiple</span>
<span class="sd">        times.</span>

<span class="sd">        Args:</span>
<span class="sd">            fut: (torch._C.Future): future for which to schedule</span>
<span class="sd">            callback for.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A future that completes with the value of the passed in future when</span>
<span class="sd">            the profiling callbacks have ran.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Throw if we have already attached a callback onto the future.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">run_callbacks_on_exit</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;_call_end_callbacks_on_future can only be called once.&quot;</span><span class="p">)</span>

        <span class="c1"># We are scheduling to run this RecordFunction&#39;s end callbacks when the</span>
        <span class="c1"># passed in future completes, so don&#39;t run end callbacks on exit.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run_callbacks_on_exit</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Local variable is needed by TorchScript to refine Optional[T] to T</span>
        <span class="n">record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">record</span>
        <span class="k">assert</span> <span class="n">record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># TODO: Too slow with __torch_function__ handling enabled</span>
        <span class="c1"># See https://github.com/pytorch/pytorch/issues/76410</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunctionSubclass</span><span class="p">():</span>
                <span class="n">profiled_future</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_call_end_callbacks_on_jit_fut</span><span class="o">.</span><span class="n">_RecordFunction</span><span class="p">(</span>
                    <span class="n">record</span><span class="p">,</span> <span class="n">fut</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">profiled_future</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_call_end_callbacks_on_jit_fut</span><span class="p">(</span><span class="n">record</span><span class="p">,</span> <span class="n">fut</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">profiled_future</span>


<div class="viewcode-block" id="emit_itt"><a class="viewcode-back" href="../../../autograd.html#torch.autograd.profiler.emit_itt">[docs]</a><span class="k">class</span> <span class="nc">emit_itt</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Context manager that makes every autograd operation emit an ITT range.</span>

<span class="sd">    It is useful when running the program under Intel(R) VTune Profiler::</span>

<span class="sd">        vtune &lt;--vtune-flags&gt; &lt;regular command here&gt;</span>

<span class="sd">    The Instrumentation and Tracing Technology (ITT) API enables your application to generate and</span>
<span class="sd">    control the collection of trace data during its execution across different Intel tools.</span>
<span class="sd">    This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager,</span>
<span class="sd">    you will be able to see labled ranges in Intel(R) VTune Profiler GUI.</span>

<span class="sd">    .. warning:</span>
<span class="sd">        This context manager should not be called recursively, i.e. at most one</span>
<span class="sd">        instance should be enabled at any given time.</span>

<span class="sd">    Args:</span>
<span class="sd">        enabled (bool, optional): Setting ``enabled=False`` makes this context manager a no-op.</span>
<span class="sd">            Default: ``True``.</span>
<span class="sd">        record_shapes (bool, optional): If ``record_shapes=True``, the itt range wrapping</span>
<span class="sd">            each autograd op will append information about the sizes of Tensor arguments received</span>
<span class="sd">            by that op, in the following format:</span>
<span class="sd">            ``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``</span>
<span class="sd">            Non-tensor arguments will be represented by ``[]``.</span>
<span class="sd">            Arguments will be listed in the order they are received by the backend op.</span>
<span class="sd">            Please note that this order may not match the order in which those arguments were passed</span>
<span class="sd">            on the Python side.  Also note that shape recording may increase the overhead of itt range creation.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD_PROFILER)</span>
<span class="sd">        &gt;&gt;&gt; with torch.autograd.profiler.emit_itt():</span>
<span class="sd">        ...     model(x)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span> <span class="o">=</span> <span class="n">record_shapes</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entered</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ITT annotation context manager is not reentrant&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">_enable_profiler</span><span class="p">(</span>
            <span class="n">ProfilerConfig</span><span class="p">(</span>
                <span class="n">ProfilerState</span><span class="o">.</span><span class="n">ITT</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="n">_ExperimentalConfig</span><span class="p">()),</span>
            <span class="nb">set</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">_disable_profiler</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">False</span></div>


<div class="viewcode-block" id="emit_nvtx"><a class="viewcode-back" href="../../../autograd.html#torch.autograd.profiler.emit_nvtx">[docs]</a><span class="k">class</span> <span class="nc">emit_nvtx</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Context manager that makes every autograd operation emit an NVTX range.</span>

<span class="sd">    It is useful when running the program under nvprof::</span>

<span class="sd">        nvprof --profile-from-start off -o trace_name.prof -- &lt;regular command here&gt;</span>

<span class="sd">    Unfortunately, there&#39;s no way to force nvprof to flush the data it collected</span>
<span class="sd">    to disk, so for CUDA profiling one has to use this context manager to annotate</span>
<span class="sd">    nvprof traces and wait for the process to exit before inspecting them.</span>
<span class="sd">    Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or</span>
<span class="sd">    :func:`torch.autograd.profiler.load_nvprof` can load the results for inspection</span>
<span class="sd">    e.g. in Python REPL.</span>

<span class="sd">    .. warning:</span>
<span class="sd">        This context manager should not be called recursively, i.e. at most one</span>
<span class="sd">        instance should be enabled at any given time.</span>

<span class="sd">    Args:</span>
<span class="sd">        enabled (bool, optional): Setting ``enabled=False`` makes this context manager a no-op.</span>
<span class="sd">            Default: ``True``.</span>
<span class="sd">        record_shapes (bool, optional): If ``record_shapes=True``, the nvtx range wrapping</span>
<span class="sd">            each autograd op will append information about the sizes of Tensor arguments received</span>
<span class="sd">            by that op, in the following format:</span>
<span class="sd">            ``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``</span>
<span class="sd">            Non-tensor arguments will be represented by ``[]``.</span>
<span class="sd">            Arguments will be listed in the order they are received by the backend op.</span>
<span class="sd">            Please note that this order may not match the order in which those arguments were passed</span>
<span class="sd">            on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD_PROFILER)</span>
<span class="sd">        &gt;&gt;&gt; with torch.cuda.profiler.profile():</span>
<span class="sd">        ...     model(x)  # Warmup CUDA memory allocator and profiler</span>
<span class="sd">        ...     with torch.autograd.profiler.emit_nvtx():</span>
<span class="sd">        ...         model(x)</span>

<span class="sd">    **Forward-backward correlation**</span>

<span class="sd">    When viewing a profile created using :class:`emit_nvtx` in the Nvidia Visual Profiler,</span>
<span class="sd">    correlating each backward-pass op with the corresponding forward-pass op can be difficult.</span>
<span class="sd">    To ease this task, :class:`emit_nvtx` appends sequence number information to the ranges it</span>
<span class="sd">    generates.</span>

<span class="sd">    During the forward pass, each function range is decorated with ``seq=&lt;N&gt;``.  ``seq`` is a running</span>
<span class="sd">    counter, incremented each time a new backward Function object is created and stashed for backward.</span>
<span class="sd">    Thus, the ``seq=&lt;N&gt;`` annotation associated with each forward function range tells you that</span>
<span class="sd">    if a backward Function object is created by this forward function,</span>
<span class="sd">    the backward object will receive sequence number N.</span>
<span class="sd">    During the backward pass, the top-level range wrapping each C++ backward Function&#39;s</span>
<span class="sd">    ``apply()`` call is decorated with ``stashed seq=&lt;M&gt;``.  ``M`` is the sequence number that</span>
<span class="sd">    the backward object was created with.  By comparing ``stashed seq`` numbers in backward with ``seq``</span>
<span class="sd">    numbers in forward, you can track down which forward op created each backward Function.</span>

<span class="sd">    Any functions executed during the backward pass are also decorated with ``seq=&lt;N&gt;``.  During</span>
<span class="sd">    default backward (with ``create_graph=False``) this information is irrelevant, and in fact,</span>
<span class="sd">    ``N`` may simply be 0 for all such functions.  Only the top-level ranges associated with</span>
<span class="sd">    backward Function objects&#39; ``apply()`` methods are useful, as a way to correlate these Function</span>
<span class="sd">    objects with the earlier forward pass.</span>

<span class="sd">    **Double-backward**</span>

<span class="sd">    If, on the other hand, a backward pass with ``create_graph=True`` is underway (in other words,</span>
<span class="sd">    if you are setting up for a double-backward), each function&#39;s execution during backward</span>
<span class="sd">    is given a nonzero, useful ``seq=&lt;N&gt;``.  Those functions may themselves create Function objects</span>
<span class="sd">    to be executed later during double-backward, just as the original functions in the forward pass did.</span>
<span class="sd">    The relationship between backward and double-backward is conceptually the same as the relationship</span>
<span class="sd">    between forward and backward: The functions still emit current-sequence-number-tagged ranges,</span>
<span class="sd">    the Function objects they create still stash those sequence numbers, and during the eventual</span>
<span class="sd">    double-backward, the Function objects&#39; ``apply()`` ranges are still tagged with ``stashed seq``</span>
<span class="sd">    numbers, which can be compared to `seq` numbers from the backward pass.</span>

<span class="sd">    .. warning:</span>
<span class="sd">        The sequence number is thread-local, and some forward functions don&#39;t create an associated</span>
<span class="sd">        backward Function object (instead delegating that to sub-functions further down the call chain).</span>
<span class="sd">        For these reasons, the correspondence of stashed sequence numbers in</span>
<span class="sd">        backward Function ``apply()`` ranges with `seq` numbers in forward-pass ranges is</span>
<span class="sd">        not guaranteed to be 1 to 1.  The sequence numbers alone may not be enough to fully</span>
<span class="sd">        disambiguate which forward function created which</span>
<span class="sd">        backward Function object.  You may need to make a judgment based on analytic knowledge of what</span>
<span class="sd">        the expected correspondence should be.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="n">enabled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span> <span class="o">=</span> <span class="n">record_shapes</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entered</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;NVTX annotation context manager is not reentrant&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entered</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">_enable_profiler</span><span class="p">(</span>
            <span class="n">ProfilerConfig</span><span class="p">(</span>
                <span class="n">ProfilerState</span><span class="o">.</span><span class="n">NVTX</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record_shapes</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="n">_ExperimentalConfig</span><span class="p">()),</span>
            <span class="nb">set</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">_disable_profiler</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">False</span></div>


<div class="viewcode-block" id="load_nvprof"><a class="viewcode-back" href="../../../generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof">[docs]</a><span class="k">def</span> <span class="nf">load_nvprof</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Opens an nvprof trace file and parses autograd annotations.</span>

<span class="sd">    Args:</span>
<span class="sd">        path (str): path to nvprof trace</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">EventList</span><span class="p">(</span><span class="n">parse_nvprof_trace</span><span class="p">(</span><span class="n">path</span><span class="p">))</span></div>


<span class="k">class</span> <span class="nc">EnforceUnique</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Raises an error if a key is seen more than once.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">see</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">key</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">seen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;duplicate key: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">parse_nvprof_trace</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">sqlite3</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">conn</span><span class="o">.</span><span class="n">row_factory</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">Row</span>

    <span class="c1"># Parse strings table</span>
    <span class="n">strings</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">conn</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s2">&quot;SELECT _id_ as id, value FROM StringTable&quot;</span><span class="p">):</span>
        <span class="n">strings</span><span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_demangle</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">])</span>

    <span class="c1"># First, find all functions and create FunctionEvents for them</span>
    <span class="n">marker_query</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    SELECT</span>
<span class="s2">        start.id AS marker_id, start.name, start.timestamp AS start_time, end.timestamp AS end_time</span>
<span class="s2">    FROM</span>
<span class="s2">        CUPTI_ACTIVITY_KIND_MARKER AS start INNER JOIN CUPTI_ACTIVITY_KIND_MARKER AS end</span>
<span class="s2">        ON start.id = end.id</span>
<span class="s2">    WHERE</span>
<span class="s2">        start.name != 0 AND end.name = 0</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">functions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">functions_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">unique</span> <span class="o">=</span> <span class="n">EnforceUnique</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">conn</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">marker_query</span><span class="p">):</span>
        <span class="n">unique</span><span class="o">.</span><span class="n">see</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;marker_id&#39;</span><span class="p">])</span>
        <span class="n">evt</span> <span class="o">=</span> <span class="n">FunctionEvent</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;marker_id&#39;</span><span class="p">],</span>
                            <span class="n">node_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># missing a node_id when calling FunctionEvent. This is just to ensure</span>
                                        <span class="c1"># that pytorch doesn&#39;t crash when creating a FunctionEvent() object</span>
                            <span class="n">name</span><span class="o">=</span><span class="n">strings</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]],</span>
                            <span class="n">start_us</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;start_time&#39;</span><span class="p">],</span>
                            <span class="n">end_us</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;end_time&#39;</span><span class="p">],</span>
                            <span class="n">thread</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># TODO: find in sqlite database</span>
        <span class="n">functions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evt</span><span class="p">)</span>
        <span class="n">functions_map</span><span class="p">[</span><span class="n">evt</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">evt</span>

    <span class="c1"># Now, correlate all kernels with FunctionEvents</span>
    <span class="n">kernel_query</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    SELECT</span>
<span class="s2">        start.id AS marker_id, start.name, start.timestamp, end.timestamp,</span>
<span class="s2">        runtime._id_ AS runtime_id, runtime.cbid, runtime.start AS runtime_start, runtime.end AS runtime_end,</span>
<span class="s2">        kernel.start AS kernel_start, kernel.end AS kernel_end, kernel.name AS kernel_name</span>
<span class="s2">    FROM</span>
<span class="s2">        CUPTI_ACTIVITY_KIND_MARKER AS start</span>
<span class="s2">        INNER JOIN CUPTI_ACTIVITY_KIND_MARKER AS end</span>
<span class="s2">            ON start.id = end.id</span>
<span class="s2">        INNER JOIN CUPTI_ACTIVITY_KIND_RUNTIME as runtime</span>
<span class="s2">            ON (start.timestamp &lt; runtime.start AND runtime.end &lt; end.timestamp)</span>
<span class="s2">        INNER JOIN CUPTI_ACTIVITY_KIND_CONCURRENT_KERNEL AS kernel</span>
<span class="s2">            ON kernel.correlationId = runtime.correlationId</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">unique</span> <span class="o">=</span> <span class="n">EnforceUnique</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">conn</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">kernel_query</span><span class="p">):</span>
        <span class="n">unique</span><span class="o">.</span><span class="n">see</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;marker_id&#39;</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;runtime_id&#39;</span><span class="p">])</span>
        <span class="c1"># 211 is cudaKernelLaunch for cuda &gt;= 9.2</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;cbid&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">211</span><span class="p">)</span>
        <span class="n">evt</span> <span class="o">=</span> <span class="n">functions_map</span><span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;marker_id&#39;</span><span class="p">]]</span>
        <span class="n">evt</span><span class="o">.</span><span class="n">append_kernel</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;kernel_name&#39;</span><span class="p">],</span>
                          <span class="mi">0</span><span class="p">,</span>
                          <span class="n">row</span><span class="p">[</span><span class="s1">&#39;kernel_end&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;kernel_start&#39;</span><span class="p">])</span>

    <span class="n">functions</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">evt</span><span class="p">:</span> <span class="n">evt</span><span class="o">.</span><span class="n">time_range</span><span class="o">.</span><span class="n">start</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">functions</span>


<span class="k">class</span> <span class="nc">KinetoStepTracker</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Provides an abstraction for incrementing the step count globally.</span>
<span class="sd">    Previously, we only had one place to mark that a step() has occurred</span>
<span class="sd">    in the program via pytorch profiler step(). We will now add step hooks</span>
<span class="sd">    in the Optimizer class https://github.com/pytorch/pytorch/issues/88446</span>

<span class="sd">    - This could mean programs that already call profiler.step() every</span>
<span class="sd">      iteration can end up double incrementing step count.</span>
<span class="sd">    - If a model uses multiple optimizers we can also have double or more</span>
<span class="sd">      counting of the step.</span>

<span class="sd">    We fix this by adding a layer of abstraction before calling step()</span>
<span class="sd">    to the kineto library. The idea is to maintain steps per requester in a dict:</span>
<span class="sd">    ```</span>
<span class="sd">    {</span>
<span class="sd">       &quot;ProfilerStep&quot;: 100,  # triggered by profiler step() call</span>
<span class="sd">       &quot;Optimizer1Step&quot;: 100,   # Optimizer 1 or 2 are just examples, could be SGD, Adam etc</span>
<span class="sd">       &quot;Optimizer2Step&quot;: 100,</span>
<span class="sd">    }</span>
<span class="sd">    ```</span>
<span class="sd">    To figure out the global step count just take the max of dict values (100).</span>

<span class="sd">    If one of the count increments the max will go up.</span>
<span class="sd">    ```</span>
<span class="sd">    {</span>
<span class="sd">       &quot;ProfilerStep&quot;: 100,</span>
<span class="sd">       &quot;Optimizer1Step&quot;: 101,   # Optimizer1 got incremented first say</span>
<span class="sd">       &quot;Optimizer2Step&quot;: 100,</span>
<span class="sd">    }</span>
<span class="sd">    ```</span>
<span class="sd">    Then global step count is 101</span>
<span class="sd">    We only call the kineto step() function when global count increments.</span>

<span class="sd">    NOTE: Please do not use the KinetoStepTracker in modules beside the Optimizer</span>
<span class="sd">    for now. The result could be incorrect increments of the step count.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_current_step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">_step_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">init_step_count</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">requester</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="p">[</span><span class="n">requester</span><span class="p">]</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">erase_step_count</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">requester</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">requester</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">increment_step</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">requester</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Increments the step count for the requester.</span>
<span class="sd">        Additionally if the max over all step counts has incremented then</span>
<span class="sd">        trigger the _kineto_step()</span>
<span class="sd">        returns global step count</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">requester</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">init_step_count</span><span class="p">(</span><span class="n">requester</span><span class="p">)</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="p">[</span><span class="n">requester</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">new_step</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">new_step</span> <span class="o">&gt;</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span><span class="p">:</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">new_step</span> <span class="o">-</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span>
            <span class="k">if</span> <span class="n">delta</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Profiler step count has increased more than 1 - &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;current_step = </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span><span class="si">}</span><span class="s2"> step dict =  </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_step_dict</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
                <span class="n">_kineto_step</span><span class="p">()</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">=</span> <span class="n">new_step</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">current_step</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_current_step</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
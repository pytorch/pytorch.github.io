


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.autograd.functional &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/autograd/functional.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/autograd/functional.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../autograd.html">torch.autograd</a> &gt;</li>
        
      <li>torch.autograd.functional</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.autograd.functional</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">forward_ad</span> <span class="k">as</span> <span class="n">fwAD</span>
<span class="kn">from</span> <span class="nn">torch._vmap_internals</span> <span class="kn">import</span> <span class="n">_vmap</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;vjp&quot;</span><span class="p">,</span> <span class="s2">&quot;jvp&quot;</span><span class="p">,</span> <span class="s2">&quot;jacobian&quot;</span><span class="p">,</span> <span class="s2">&quot;hessian&quot;</span><span class="p">,</span> <span class="s2">&quot;hvp&quot;</span><span class="p">,</span> <span class="s2">&quot;vhp&quot;</span><span class="p">]</span>

<span class="c1"># Utility functions</span>


<span class="k">def</span> <span class="nf">_as_tuple_nocheck</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span>


<span class="k">def</span> <span class="nf">_as_tuple</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fn_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Ensures that inp is a tuple of Tensors</span>
    <span class="c1"># Returns whether or not the original inp was a tuple and the tupled version of the input</span>
    <span class="k">if</span> <span class="n">arg_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">fn_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_as_tuple_nocheck</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

    <span class="n">is_inp_tuple</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">inp</span> <span class="o">=</span> <span class="p">(</span><span class="n">inp</span><span class="p">,)</span>
        <span class="n">is_inp_tuple</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">el</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inp</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">el</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_inp_tuple</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The </span><span class="si">{}</span><span class="s2"> given to </span><span class="si">{}</span><span class="s2"> must be either a Tensor or a tuple of Tensors but the&quot;</span>
                                <span class="s2">&quot; value at index </span><span class="si">{}</span><span class="s2"> has type </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">el</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The </span><span class="si">{}</span><span class="s2"> given to </span><span class="si">{}</span><span class="s2"> must be either a Tensor or a tuple of Tensors but the&quot;</span>
                                <span class="s2">&quot; given </span><span class="si">{}</span><span class="s2"> has type </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">el</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">is_inp_tuple</span><span class="p">,</span> <span class="n">inp</span>


<span class="k">def</span> <span class="nf">_tuple_postprocess</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">to_unpack</span><span class="p">):</span>
    <span class="c1"># Unpacks a potentially nested tuple of Tensors</span>
    <span class="c1"># to_unpack should be a single boolean or a tuple of two booleans.</span>
    <span class="c1"># It is used to:</span>
    <span class="c1"># - invert _as_tuple when res should match the inp given to _as_tuple</span>
    <span class="c1"># - optionally remove nesting of two tuples created by multiple calls to _as_tuple</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">to_unpack</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">to_unpack</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">to_unpack</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">el</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">res</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">to_unpack</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">to_unpack</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">_grad_preprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="p">):</span>
    <span class="c1"># Preprocess the inputs to make sure they require gradient</span>
    <span class="c1"># inputs is a tuple of Tensors to preprocess</span>
    <span class="c1"># create_graph specifies if the user wants gradients to flow back to the Tensors in inputs</span>
    <span class="c1"># need_graph specifies if we internally want gradients to flow back to the Tensors in res</span>
    <span class="c1"># Note that we *always* create a new Tensor object to be able to see the difference between</span>
    <span class="c1"># inputs given as arguments and the same Tensors automatically captured by the user function.</span>
    <span class="c1"># Check this issue for more details on how that can happen: https://github.com/pytorch/pytorch/issues/32576</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">create_graph</span> <span class="ow">and</span> <span class="n">inp</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="c1"># Create at least a new Tensor object in a differentiable way</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">inp</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                <span class="c1"># Use .view_as() to get a shallow copy</span>
                <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">inp</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># We cannot use view for sparse Tensors so we clone</span>
                <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">need_graph</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_grad_postprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">):</span>
    <span class="c1"># Postprocess the generated Tensors to avoid returning Tensors with history when the user did not</span>
    <span class="c1"># request it.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">create_graph</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inputs</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_validate_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">is_other_tuple</span><span class="p">):</span>
    <span class="c1"># This assumes that other is the correct shape, and v should match</span>
    <span class="c1"># Both are assumed to be tuples of Tensors</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">other</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_other_tuple</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;v is a tuple of invalid length: should be </span><span class="si">{}</span><span class="s2"> but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">other</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The given v should contain a single Tensor.&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">el_v</span><span class="p">,</span> <span class="n">el_other</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">other</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">el_v</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">el_other</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
            <span class="n">prepend</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="k">if</span> <span class="n">is_other_tuple</span><span class="p">:</span>
                <span class="n">prepend</span> <span class="o">=</span> <span class="s2">&quot;Entry </span><span class="si">{}</span><span class="s2"> in &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">v has invalid size: should be </span><span class="si">{}</span><span class="s2"> but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                               <span class="n">prepend</span><span class="p">,</span> <span class="n">el_other</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">el_v</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>


<span class="k">def</span> <span class="nf">_check_requires_grad</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">input_type</span><span class="p">,</span> <span class="n">strict</span><span class="p">):</span>
    <span class="c1"># Used to make all the necessary checks to raise nice errors in strict mode.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">input_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;grad_inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;jacobian&quot;</span><span class="p">,</span> <span class="s2">&quot;hessian&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid input_type to _check_requires_grad&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">inp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">inp</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># This can only be reached for grad_inputs.</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The output of the user-provided function is independent of input </span><span class="si">{}</span><span class="s2">.&quot;</span>
                               <span class="s2">&quot; This is not allowed in strict mode.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">inp</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_type</span> <span class="o">==</span> <span class="s2">&quot;hessian&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The hessian of the user-provided function with respect to input </span><span class="si">{}</span><span class="s2">&quot;</span>
                                   <span class="s2">&quot; is independent of the input. This is not allowed in strict mode.&quot;</span>
                                   <span class="s2">&quot; You should ensure that your function is thrice differentiable and that&quot;</span>
                                   <span class="s2">&quot; the hessian depends on the inputs.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">input_type</span> <span class="o">==</span> <span class="s2">&quot;jacobian&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;While computing the hessian, found that the jacobian of the user-provided&quot;</span>
                                   <span class="s2">&quot; function with respect to input </span><span class="si">{}</span><span class="s2"> is independent of the input. This is not&quot;</span>
                                   <span class="s2">&quot; allowed in strict mode. You should ensure that your function is twice&quot;</span>
                                   <span class="s2">&quot; differentiable and that the jacobian depends on the inputs (this would be&quot;</span>
                                   <span class="s2">&quot; violated by a linear function for example).&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">input_type</span> <span class="o">==</span> <span class="s2">&quot;grad_inputs&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The gradient with respect to input </span><span class="si">{}</span><span class="s2"> is independent of the inputs of the&quot;</span>
                                   <span class="s2">&quot; user-provided function. This is not allowed in strict mode.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Output </span><span class="si">{}</span><span class="s2"> of the user-provided function does not require gradients.&quot;</span>
                                   <span class="s2">&quot; The outputs must be computed in a differentiable manner from the input&quot;</span>
                                   <span class="s2">&quot; when running in strict mode.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_autograd_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_grads_batched</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># Version of autograd.grad that accepts `None` in outputs and do not compute gradients for them.</span>
    <span class="c1"># This has the extra constraint that inputs has to be a tuple</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">grad_outputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">grad_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">)</span>

    <span class="n">new_outputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>
    <span class="n">new_grad_outputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">grad_out</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">new_outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">out</span><span class="p">,)</span>
            <span class="n">new_grad_outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">grad_out</span><span class="p">,)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># No differentiable output, we don&#39;t need to call the autograd engine</span>
        <span class="k">return</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">new_outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">new_grad_outputs</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
                                   <span class="n">is_grads_batched</span><span class="o">=</span><span class="n">is_grads_batched</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_fill_in_zeros</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">refs</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
    <span class="c1"># Used to detect None in the grads and depending on the flags, either replace them</span>
    <span class="c1"># with Tensors full of 0s of the appropriate size based on the refs or raise an error.</span>
    <span class="c1"># strict and create graph allow us to detect when it is appropriate to raise an error</span>
    <span class="c1"># stage gives us information of which backward call we consider to give good error message</span>
    <span class="k">if</span> <span class="n">stage</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;back&quot;</span><span class="p">,</span> <span class="s2">&quot;back_trick&quot;</span><span class="p">,</span> <span class="s2">&quot;double_back&quot;</span><span class="p">,</span> <span class="s2">&quot;double_back_trick&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid stage argument &#39;</span><span class="si">{}</span><span class="s2">&#39; to _fill_in_zeros&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stage</span><span class="p">))</span>

    <span class="n">res</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">grads_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grads</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">grads_i</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;back&quot;</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The output of the user-provided function is independent of &quot;</span>
                                       <span class="s2">&quot;input </span><span class="si">{}</span><span class="s2">. This is not allowed in strict mode.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;back_trick&quot;</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The gradient with respect to the input is independent of entry </span><span class="si">{}</span><span class="s2">&quot;</span>
                                       <span class="s2">&quot; in the grad_outputs when using the double backward trick to compute&quot;</span>
                                       <span class="s2">&quot; forward mode gradients. This is not allowed in strict mode.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;double_back&quot;</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The jacobian of the user-provided function is independent of &quot;</span>
                                       <span class="s2">&quot;input </span><span class="si">{}</span><span class="s2">. This is not allowed in strict mode.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The hessian of the user-provided function is independent of &quot;</span>
                                       <span class="s2">&quot;entry </span><span class="si">{}</span><span class="s2"> in the grad_jacobian. This is not allowed in strict &quot;</span>
                                       <span class="s2">&quot;mode as it prevents from using the double backward trick to &quot;</span>
                                       <span class="s2">&quot;replace forward mode AD.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

            <span class="n">grads_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">refs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">strict</span> <span class="ow">and</span> <span class="n">create_graph</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">grads_i</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;double&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stage</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The jacobian of the user-provided function is independent of &quot;</span>
                                       <span class="s2">&quot;input </span><span class="si">{}</span><span class="s2">. This is not allowed in strict mode when create_graph=True.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The hessian of the user-provided function is independent of &quot;</span>
                                       <span class="s2">&quot;input </span><span class="si">{}</span><span class="s2">. This is not allowed in strict mode when create_graph=True.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

        <span class="n">res</span> <span class="o">+=</span> <span class="p">(</span><span class="n">grads_i</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">res</span>


<span class="c1"># Public API</span>

<div class="viewcode-block" id="vjp"><a class="viewcode-back" href="../../../generated/torch.autograd.functional.vjp.html#torch.autograd.functional.vjp">[docs]</a><span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that computes the dot product between a vector ``v`` and the</span>
<span class="sd">    Jacobian of the given function at the point given by the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a tuple of Tensors or a Tensor.</span>
<span class="sd">        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.</span>
<span class="sd">        v (tuple of Tensors or Tensor): The vector for which the vector</span>
<span class="sd">            Jacobian product is computed.  Must be the same size as the output</span>
<span class="sd">            of ``func``. This argument is optional when the output of ``func``</span>
<span class="sd">            contains a single element and (if it is not provided) will be set</span>
<span class="sd">            as a Tensor containing a single ``1``.</span>
<span class="sd">        create_graph (bool, optional): If ``True``, both the output and result</span>
<span class="sd">            will be computed in a differentiable way. Note that when ``strict``</span>
<span class="sd">            is ``False``, the result can not require gradients or be</span>
<span class="sd">            disconnected from the inputs.  Defaults to ``False``.</span>
<span class="sd">        strict (bool, optional): If ``True``, an error will be raised when we</span>
<span class="sd">            detect that there exists an input such that all the outputs are</span>
<span class="sd">            independent of it. If ``False``, we return a Tensor of zeros as the</span>
<span class="sd">            vjp for said inputs, which is the expected mathematical value.</span>
<span class="sd">            Defaults to ``False``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (tuple): tuple with:</span>
<span class="sd">            func_output (tuple of Tensors or Tensor): output of ``func(inputs)``</span>

<span class="sd">            vjp (tuple of Tensors or Tensor): result of the dot product with</span>
<span class="sd">            the same shape as the inputs.</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="sd">        &gt;&gt;&gt; def exp_reducer(x):</span>
<span class="sd">        ...     return x.exp().sum(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; inputs = torch.rand(4, 4)</span>
<span class="sd">        &gt;&gt;&gt; v = torch.ones(4)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="sd">        &gt;&gt;&gt; vjp(exp_reducer, inputs, v)</span>
<span class="sd">        (tensor([5.7817, 7.2458, 5.7830, 6.7782]),</span>
<span class="sd">         tensor([[1.4458, 1.3962, 1.3042, 1.6354],</span>
<span class="sd">                [2.1288, 1.0652, 1.5483, 2.5035],</span>
<span class="sd">                [2.2046, 1.1292, 1.1432, 1.3059],</span>
<span class="sd">                [1.3225, 1.6652, 1.7753, 2.0152]]))</span>

<span class="sd">        &gt;&gt;&gt; vjp(exp_reducer, inputs, v, create_graph=True)</span>
<span class="sd">        (tensor([5.7817, 7.2458, 5.7830, 6.7782], grad_fn=&lt;SumBackward1&gt;),</span>
<span class="sd">         tensor([[1.4458, 1.3962, 1.3042, 1.6354],</span>
<span class="sd">                [2.1288, 1.0652, 1.5483, 2.5035],</span>
<span class="sd">                [2.2046, 1.1292, 1.1432, 1.3059],</span>
<span class="sd">                [1.3225, 1.6652, 1.7753, 2.0152]], grad_fn=&lt;MulBackward0&gt;))</span>

<span class="sd">        &gt;&gt;&gt; def adder(x, y):</span>
<span class="sd">        ...     return 2 * x + 3 * y</span>
<span class="sd">        &gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))</span>
<span class="sd">        &gt;&gt;&gt; v = torch.ones(2)</span>
<span class="sd">        &gt;&gt;&gt; vjp(adder, inputs, v)</span>
<span class="sd">        (tensor([2.4225, 2.3340]),</span>
<span class="sd">         (tensor([2., 2.]), tensor([3., 3.])))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
        <span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;vjp&quot;</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs of the user-provided function&quot;</span><span class="p">,</span> <span class="s2">&quot;vjp&quot;</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="s2">&quot;vjp&quot;</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">_validate_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">is_outputs_tuple</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The vector v can only be None if the &quot;</span>
                                   <span class="s2">&quot;user-provided function returns &quot;</span>
                                   <span class="s2">&quot;a single Tensor with a single element.&quot;</span><span class="p">)</span>

    <span class="n">enable_grad</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">create_graph</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">enable_grad</span><span class="p">):</span>
        <span class="n">grad_res</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>
        <span class="n">vjp</span> <span class="o">=</span> <span class="n">_fill_in_zeros</span><span class="p">(</span><span class="n">grad_res</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="s2">&quot;back&quot;</span><span class="p">)</span>

    <span class="c1"># Cleanup objects and return them to the user</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>
    <span class="n">vjp</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">vjp</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">is_outputs_tuple</span><span class="p">),</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">vjp</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">)</span></div>


<div class="viewcode-block" id="jvp"><a class="viewcode-back" href="../../../generated/torch.autograd.functional.jvp.html#torch.autograd.functional.jvp">[docs]</a><span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that computes the dot product between  the Jacobian of</span>
<span class="sd">    the given function at the point given by the inputs and a vector ``v``.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a tuple of Tensors or a Tensor.</span>
<span class="sd">        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.</span>
<span class="sd">        v (tuple of Tensors or Tensor): The vector for which the Jacobian</span>
<span class="sd">            vector product is computed. Must be the same size as the input of</span>
<span class="sd">            ``func``. This argument is optional when the input to ``func``</span>
<span class="sd">            contains a single element and (if it is not provided) will be set</span>
<span class="sd">            as a Tensor containing a single ``1``.</span>
<span class="sd">        create_graph (bool, optional): If ``True``, both the output and result</span>
<span class="sd">            will be computed in a differentiable way. Note that when ``strict``</span>
<span class="sd">            is ``False``, the result can not require gradients or be</span>
<span class="sd">            disconnected from the inputs.  Defaults to ``False``.</span>
<span class="sd">        strict (bool, optional): If ``True``, an error will be raised when we</span>
<span class="sd">            detect that there exists an input such that all the outputs are</span>
<span class="sd">            independent of it. If ``False``, we return a Tensor of zeros as the</span>
<span class="sd">            jvp for said inputs, which is the expected mathematical value.</span>
<span class="sd">            Defaults to ``False``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (tuple): tuple with:</span>
<span class="sd">            func_output (tuple of Tensors or Tensor): output of ``func(inputs)``</span>

<span class="sd">            jvp (tuple of Tensors or Tensor): result of the dot product with</span>
<span class="sd">            the same shape as the output.</span>

<span class="sd">    Note:</span>
<span class="sd">        ``autograd.functional.jvp`` computes the jvp by using the backward of</span>
<span class="sd">        the backward (sometimes called the double backwards trick). This is not</span>
<span class="sd">        the most performant way of computing the jvp. Please consider using</span>
<span class="sd">        :func:`torch.func.jvp` or the</span>
<span class="sd">        :ref:`low-level forward-mode AD API &lt;forward-mode-ad&gt;` instead.</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="sd">        &gt;&gt;&gt; def exp_reducer(x):</span>
<span class="sd">        ...     return x.exp().sum(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; inputs = torch.rand(4, 4)</span>
<span class="sd">        &gt;&gt;&gt; v = torch.ones(4, 4)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="sd">        &gt;&gt;&gt; jvp(exp_reducer, inputs, v)</span>
<span class="sd">        (tensor([6.3090, 4.6742, 7.9114, 8.2106]),</span>
<span class="sd">         tensor([6.3090, 4.6742, 7.9114, 8.2106]))</span>

<span class="sd">        &gt;&gt;&gt; jvp(exp_reducer, inputs, v, create_graph=True)</span>
<span class="sd">        (tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=&lt;SumBackward1&gt;),</span>
<span class="sd">         tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=&lt;SqueezeBackward1&gt;))</span>

<span class="sd">        &gt;&gt;&gt; def adder(x, y):</span>
<span class="sd">        ...     return 2 * x + 3 * y</span>
<span class="sd">        &gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))</span>
<span class="sd">        &gt;&gt;&gt; v = (torch.ones(2), torch.ones(2))</span>
<span class="sd">        &gt;&gt;&gt; jvp(adder, inputs, v)</span>
<span class="sd">        (tensor([2.2399, 2.5005]),</span>
<span class="sd">         tensor([5., 5.]))</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
        <span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;jvp&quot;</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="s2">&quot;jvp&quot;</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">_validate_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The vector v can only be None if the input to &quot;</span>
                                   <span class="s2">&quot;the user-provided function is a single Tensor &quot;</span>
                                   <span class="s2">&quot;with a single element.&quot;</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs of the user-provided function&quot;</span><span class="p">,</span> <span class="s2">&quot;jvp&quot;</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>
        <span class="c1"># The backward is linear so the value of grad_outputs is not important as</span>
        <span class="c1"># it won&#39;t appear in the double backward graph. We only need to ensure that</span>
        <span class="c1"># it does not contain inf or nan.</span>
        <span class="n">grad_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">)</span>

        <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">,</span> <span class="s2">&quot;grad_inputs&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">create_graph</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
            <span class="n">grad_res</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>
            <span class="n">jvp</span> <span class="o">=</span> <span class="n">_fill_in_zeros</span><span class="p">(</span><span class="n">grad_res</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="s2">&quot;back_trick&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">grad_res</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>
        <span class="n">jvp</span> <span class="o">=</span> <span class="n">_fill_in_zeros</span><span class="p">(</span><span class="n">grad_res</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="s2">&quot;back_trick&quot;</span><span class="p">)</span>

    <span class="c1"># Cleanup objects and return them to the user</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>
    <span class="n">jvp</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">jvp</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">is_outputs_tuple</span><span class="p">),</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">jvp</span><span class="p">,</span> <span class="n">is_outputs_tuple</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_construct_standard_basis_for</span><span class="p">(</span><span class="n">tensors</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">tensor_numels</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
    <span class="c1"># This function:</span>
    <span class="c1"># - constructs a N=sum(tensor_numels) standard basis. i.e. an NxN identity matrix.</span>
    <span class="c1"># - Splits the identity matrix into chunks with each chunk size determined by `tensor_numels`.</span>
    <span class="c1"># - Each chunk corresponds to one tensor. The chunk has the same dtype and</span>
    <span class="c1">#   device as the tensor</span>
    <span class="c1">#</span>
    <span class="c1"># For example, with tensor_numels = [1, 2, 1], this function returns:</span>
    <span class="c1"># ( tensor([[1],     tensor([[0, 0],      tensor([[0],</span>
    <span class="c1">#           [0],             [1, 0],              [0],</span>
    <span class="c1">#           [0],             [0, 1],              [0],</span>
    <span class="c1">#           [0]])  ,         [0, 0]])  ,          [1]])  )</span>
    <span class="c1">#</span>
    <span class="c1"># Precondition: tensor_numels == tuple(tensor.numel() for tensor in tensors)</span>
    <span class="c1"># Precondition: tensors always has at least one element.</span>
    <span class="c1">#</span>
    <span class="c1"># See NOTE: [Computing jacobian with vmap and grad for multiple tensors]</span>
    <span class="c1"># for context behind this function. All the pre-conditions are guarded for</span>
    <span class="c1"># in torch.autograd.functional.jacobian.</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor_numels</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="n">total_numel</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">tensor_numels</span><span class="p">)</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">total_numel</span><span class="p">,</span> <span class="n">tensor_numel</span><span class="p">)</span>
                   <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">tensor_numel</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">tensor_numels</span><span class="p">))</span>
    <span class="n">diag_start_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">numel</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="n">tensor_numels</span><span class="p">):</span>
        <span class="n">chunk</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">diag_start_idx</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">diag_start_idx</span> <span class="o">-=</span> <span class="n">numel</span>
    <span class="k">return</span> <span class="n">chunks</span>


<span class="k">def</span> <span class="nf">_jacfwd</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vectorize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;torch.autograd.functional.jacobian: `strict=True` &#39;</span>
                           <span class="s1">&#39;and `strategy=&quot;forward-mode&quot;` are not supported together (yet). &#39;</span>
                           <span class="s1">&#39;Please either set `strict=False` or &#39;</span>
                           <span class="s1">&#39;`strategy=&quot;reverse-mode&quot;`.&#39;</span><span class="p">)</span>
    <span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;jacobian&quot;</span><span class="p">)</span>
    <span class="n">output_info</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">vectorize</span><span class="p">:</span>
        <span class="c1"># See NOTE: [Computing jacobian with vmap and grad for multiple outputs]</span>
        <span class="n">input_numels</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># Step 1: Prepare tangents</span>
        <span class="n">tangents</span> <span class="o">=</span> <span class="n">_construct_standard_basis_for</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">input_numels</span><span class="p">)</span>

        <span class="c1"># Step 2: Compute vmap over computation with dual tensors</span>
        <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">tangents</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
                <span class="n">dual_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
                    <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tangent</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span> <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">tangent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tangents</span><span class="p">))</span>
                <span class="n">_is_outputs_tuple</span><span class="p">,</span> <span class="n">dual_outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">dual_inputs</span><span class="p">),</span> <span class="s2">&quot;outputs&quot;</span><span class="p">)</span>
                <span class="n">output_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_is_outputs_tuple</span><span class="p">)</span>
                <span class="n">jv</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">primal_outs</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">dual_out</span> <span class="ow">in</span> <span class="n">dual_outputs</span><span class="p">:</span>
                    <span class="n">primal</span><span class="p">,</span> <span class="n">tangent</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">unpack_dual</span><span class="p">(</span><span class="n">dual_out</span><span class="p">)</span>
                    <span class="n">primal_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">primal</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">tangent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">jv</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tangent</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">jv</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">primal</span><span class="p">))</span>
                <span class="n">output_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">primal_outs</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">jv</span><span class="p">)</span>

        <span class="n">outputs_before_split</span> <span class="o">=</span> <span class="n">_vmap</span><span class="p">(</span><span class="n">jvp</span><span class="p">)(</span><span class="n">tangents</span><span class="p">)</span>
        <span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">output_info</span>
        <span class="c1"># Step 3: for each of the output tangents, split along dim 0</span>
        <span class="n">jacobian_input_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">jac</span><span class="p">,</span> <span class="n">output_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">outputs_before_split</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
            <span class="n">jacobian_output_i_output</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">jac</span><span class="p">,</span> <span class="n">input_j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">jac</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">input_numels</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">inputs</span><span class="p">):</span>
                <span class="c1"># We need to transpose the Jacobian because in forward AD, the</span>
                <span class="c1"># batch dimension represents that of the inputs</span>
                <span class="n">jacobian_input_i_output_j</span> <span class="o">=</span> <span class="n">jac</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">jac</span><span class="o">.</span><span class="n">ndim</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> \
                    <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">tuple</span><span class="p">([</span><span class="o">*</span><span class="n">output_i</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="o">*</span><span class="n">input_j</span><span class="o">.</span><span class="n">shape</span><span class="p">]))</span>  <span class="c1"># noqa: C409</span>

                <span class="n">jacobian_output_i_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jacobian_input_i_output_j</span><span class="p">)</span>
            <span class="n">jacobian_input_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jacobian_output_i_output</span><span class="p">)</span>

        <span class="c1"># Omit [Step 4] because everything is already transposed w/ forward AD</span>
        <span class="k">return</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">jacobian_input_output</span><span class="p">,</span> <span class="p">(</span><span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Computing Jacobian using forward-AD or forward-over-reverse Hessian is&quot;</span>
                                  <span class="s2">&quot;only implemented for `vectorize=True`.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="jacobian"><a class="viewcode-back" href="../../../generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian">[docs]</a><span class="k">def</span> <span class="nf">jacobian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vectorize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;reverse-mode&quot;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that computes the Jacobian of a given function.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a tuple of Tensors or a Tensor.</span>
<span class="sd">        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.</span>
<span class="sd">        create_graph (bool, optional): If ``True``, the Jacobian will be</span>
<span class="sd">            computed in a differentiable manner. Note that when ``strict`` is</span>
<span class="sd">            ``False``, the result can not require gradients or be disconnected</span>
<span class="sd">            from the inputs.  Defaults to ``False``.</span>
<span class="sd">        strict (bool, optional): If ``True``, an error will be raised when we</span>
<span class="sd">            detect that there exists an input such that all the outputs are</span>
<span class="sd">            independent of it. If ``False``, we return a Tensor of zeros as the</span>
<span class="sd">            jacobian for said inputs, which is the expected mathematical value.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        vectorize (bool, optional): This feature is experimental.</span>
<span class="sd">            Please consider using :func:`torch.func.jacrev` or</span>
<span class="sd">            :func:`torch.func.jacfwd` instead if you are looking for something</span>
<span class="sd">            less experimental and more performant.</span>
<span class="sd">            When computing the jacobian, usually we invoke</span>
<span class="sd">            ``autograd.grad`` once per row of the jacobian. If this flag is</span>
<span class="sd">            ``True``, we perform only a single ``autograd.grad`` call with</span>
<span class="sd">            ``batched_grad=True`` which uses the vmap prototype feature.</span>
<span class="sd">            Though this should lead to performance improvements in many cases,</span>
<span class="sd">            because this feature is still experimental, there may be performance</span>
<span class="sd">            cliffs. See :func:`torch.autograd.grad`&#39;s ``batched_grad`` parameter for</span>
<span class="sd">            more information.</span>
<span class="sd">        strategy (str, optional): Set to ``&quot;forward-mode&quot;`` or ``&quot;reverse-mode&quot;`` to</span>
<span class="sd">            determine whether the Jacobian will be computed with forward or reverse</span>
<span class="sd">            mode AD. Currently, ``&quot;forward-mode&quot;`` requires ``vectorized=True``.</span>
<span class="sd">            Defaults to ``&quot;reverse-mode&quot;``. If ``func`` has more outputs than</span>
<span class="sd">            inputs, ``&quot;forward-mode&quot;`` tends to be more performant. Otherwise,</span>
<span class="sd">            prefer to use ``&quot;reverse-mode&quot;``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Jacobian (Tensor or nested tuple of Tensors): if there is a single</span>
<span class="sd">        input and output, this will be a single Tensor containing the</span>
<span class="sd">        Jacobian for the linearized inputs and output. If one of the two is</span>
<span class="sd">        a tuple, then the Jacobian will be a tuple of Tensors. If both of</span>
<span class="sd">        them are tuples, then the Jacobian will be a tuple of tuple of</span>
<span class="sd">        Tensors where ``Jacobian[i][j]`` will contain the Jacobian of the</span>
<span class="sd">        ``i``\th output and ``j``\th input and will have as size the</span>
<span class="sd">        concatenation of the sizes of the corresponding output and the</span>
<span class="sd">        corresponding input and will have same dtype and device as the</span>
<span class="sd">        corresponding input. If strategy is ``forward-mode``, the dtype will be</span>
<span class="sd">        that of the output; otherwise, the input.</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="sd">        &gt;&gt;&gt; def exp_reducer(x):</span>
<span class="sd">        ...     return x.exp().sum(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; inputs = torch.rand(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="sd">        &gt;&gt;&gt; jacobian(exp_reducer, inputs)</span>
<span class="sd">        tensor([[[1.4917, 2.4352],</span>
<span class="sd">                 [0.0000, 0.0000]],</span>
<span class="sd">                [[0.0000, 0.0000],</span>
<span class="sd">                 [2.4369, 2.3799]]])</span>

<span class="sd">        &gt;&gt;&gt; jacobian(exp_reducer, inputs, create_graph=True)</span>
<span class="sd">        tensor([[[1.4917, 2.4352],</span>
<span class="sd">                 [0.0000, 0.0000]],</span>
<span class="sd">                [[0.0000, 0.0000],</span>
<span class="sd">                 [2.4369, 2.3799]]], grad_fn=&lt;ViewBackward&gt;)</span>

<span class="sd">        &gt;&gt;&gt; def exp_adder(x, y):</span>
<span class="sd">        ...     return 2 * x.exp() + 3 * y</span>
<span class="sd">        &gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))</span>
<span class="sd">        &gt;&gt;&gt; jacobian(exp_adder, inputs)</span>
<span class="sd">        (tensor([[2.8052, 0.0000],</span>
<span class="sd">                [0.0000, 3.3963]]),</span>
<span class="sd">         tensor([[3., 0.],</span>
<span class="sd">                 [0., 3.]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;forward-mode&quot;</span><span class="p">,</span> <span class="s2">&quot;reverse-mode&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s1">&#39;Expected strategy to be either &quot;forward-mode&quot; or &quot;reverse-mode&quot;. Hint: If your &#39;</span>
        <span class="s1">&#39;function has more outputs than inputs, &quot;forward-mode&quot; tends to be more performant. &#39;</span>
        <span class="s1">&#39;Otherwise, prefer to use &quot;reverse-mode&quot;.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;forward-mode&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">create_graph</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;torch.autograd.functional.jacobian: `create_graph=True` &#39;</span>
                                      <span class="s1">&#39;and `strategy=&quot;forward-mode&quot;` are not supported together (yet). &#39;</span>
                                      <span class="s1">&#39;Please either set `create_graph=False` or &#39;</span>
                                      <span class="s1">&#39;`strategy=&quot;reverse-mode&quot;`.&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_jacfwd</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">vectorize</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
        <span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;jacobian&quot;</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span>
                                              <span class="s2">&quot;outputs of the user-provided function&quot;</span><span class="p">,</span>
                                              <span class="s2">&quot;jacobian&quot;</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">vectorize</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;torch.autograd.functional.jacobian: `strict=True` &#39;</span>
                                   <span class="s1">&#39;and `vectorized=True` are not supported together. &#39;</span>
                                   <span class="s1">&#39;Please either set `strict=False` or &#39;</span>
                                   <span class="s1">&#39;`vectorize=False`.&#39;</span><span class="p">)</span>
            <span class="c1"># NOTE: [Computing jacobian with vmap and grad for multiple outputs]</span>
            <span class="c1">#</span>
            <span class="c1"># Let&#39;s consider f(x) = (x**2, x.sum()) and let x = torch.randn(3).</span>
            <span class="c1"># It turns out we can compute the jacobian of this function with a single</span>
            <span class="c1"># call to autograd.grad by using vmap over the correct grad_outputs.</span>
            <span class="c1">#</span>
            <span class="c1"># Firstly, one way to compute the jacobian is to stack x**2 and x.sum()</span>
            <span class="c1"># into a 4D vector. E.g., use g(x) = torch.stack([x**2, x.sum()])</span>
            <span class="c1">#</span>
            <span class="c1"># To get the first row of the jacobian, we call</span>
            <span class="c1"># &gt;&gt;&gt; autograd.grad(g(x), x, grad_outputs=torch.tensor([1, 0, 0, 0]))</span>
            <span class="c1"># To get the 2nd row of the jacobian, we call</span>
            <span class="c1"># &gt;&gt;&gt; autograd.grad(g(x), x, grad_outputs=torch.tensor([0, 1, 0, 0]))</span>
            <span class="c1"># and so on.</span>
            <span class="c1">#</span>
            <span class="c1"># Using vmap, we can vectorize all 4 of these computations into one by</span>
            <span class="c1"># passing the standard basis for R^4 as the grad_output.</span>
            <span class="c1"># vmap(partial(autograd.grad, g(x), x))(torch.eye(4)).</span>
            <span class="c1">#</span>
            <span class="c1"># Now, how do we compute the jacobian *without stacking the output*?</span>
            <span class="c1"># We can just split the standard basis across the outputs. So to</span>
            <span class="c1"># compute the jacobian of f(x), we&#39;d use</span>
            <span class="c1"># &gt;&gt;&gt; autograd.grad(f(x), x, grad_outputs=_construct_standard_basis_for(...))</span>
            <span class="c1"># The grad_outputs looks like the following:</span>
            <span class="c1"># ( torch.tensor([[1, 0, 0],</span>
            <span class="c1">#                 [0, 1, 0],</span>
            <span class="c1">#                 [0, 0, 1],</span>
            <span class="c1">#                 [0, 0, 0]]),</span>
            <span class="c1">#   torch.tensor([[0],</span>
            <span class="c1">#                 [0],</span>
            <span class="c1">#                 [0],</span>
            <span class="c1">#                 [1]]) )</span>
            <span class="c1">#</span>
            <span class="c1"># But we&#39;re not done yet!</span>
            <span class="c1"># &gt;&gt;&gt; vmap(partial(autograd.grad(f(x), x, grad_outputs=...)))</span>
            <span class="c1"># returns a Tensor of shape [4, 3]. We have to remember to split the</span>
            <span class="c1"># jacobian of shape [4, 3] into two:</span>
            <span class="c1"># - one of shape [3, 3] for the first output</span>
            <span class="c1"># - one of shape [   3] for the second output</span>

            <span class="c1"># Step 1: Construct grad_outputs by splitting the standard basis</span>
            <span class="n">output_numels</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">)</span>
            <span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">_construct_standard_basis_for</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_numels</span><span class="p">)</span>
            <span class="n">flat_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">)</span>

            <span class="c1"># Step 2: Call vmap + autograd.grad</span>
            <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">grad_output</span><span class="p">):</span>
                <span class="n">vj</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_autograd_grad</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">is_grads_batched</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">el_idx</span><span class="p">,</span> <span class="n">vj_el</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vj</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">vj_el</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">vj</span><span class="p">[</span><span class="n">el_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">el_idx</span><span class="p">])</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="nb">sum</span><span class="p">(</span><span class="n">output_numels</span><span class="p">),)</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">[</span><span class="n">el_idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">vj</span><span class="p">)</span>

            <span class="n">jacobians_of_flat_output</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">)</span>

            <span class="c1"># Step 3: The returned jacobian is one big tensor per input. In this step,</span>
            <span class="c1"># we split each Tensor by output.</span>
            <span class="n">jacobian_input_output</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">jac</span><span class="p">,</span> <span class="n">input_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">jacobians_of_flat_output</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
                <span class="n">jacobian_input_i_output</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">jac</span><span class="p">,</span> <span class="n">output_j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">jac</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">output_numels</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">outputs</span><span class="p">):</span>
                    <span class="n">jacobian_input_i_output_j</span> <span class="o">=</span> <span class="n">jac</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output_j</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="n">input_i</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                    <span class="n">jacobian_input_i_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jacobian_input_i_output_j</span><span class="p">)</span>
                <span class="n">jacobian_input_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">jacobian_input_i_output</span><span class="p">)</span>

            <span class="c1"># Step 4: Right now, `jacobian` is a List[List[Tensor]].</span>
            <span class="c1"># The outer List corresponds to the number of inputs,</span>
            <span class="c1"># the inner List corresponds to the number of outputs.</span>
            <span class="c1"># We need to exchange the order of these and convert to tuples</span>
            <span class="c1"># before returning.</span>
            <span class="n">jacobian_output_input</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">jacobian_input_output</span><span class="p">))</span>

            <span class="n">jacobian_output_input</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">jacobian_output_input</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">jacobian_output_input</span><span class="p">,</span> <span class="p">(</span><span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">))</span>

        <span class="n">jacobian</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">out</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>

            <span class="c1"># mypy complains that expression and variable have different types due to the empty list</span>
            <span class="n">jac_i</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>  <span class="c1"># type: ignore[assignment]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">nelement</span><span class="p">()):</span>
                <span class="n">vj</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">((</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="n">j</span><span class="p">],),</span> <span class="n">inputs</span><span class="p">,</span>
                                    <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>

                <span class="k">for</span> <span class="n">el_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">jac_i_el</span><span class="p">,</span> <span class="n">vj_el</span><span class="p">,</span> <span class="n">inp_el</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">jac_i</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)):</span>
                    <span class="k">if</span> <span class="n">vj_el</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">strict</span> <span class="ow">and</span> <span class="n">create_graph</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">vj_el</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The jacobian of the user-provided function is &quot;</span>
                                   <span class="s2">&quot;independent of input </span><span class="si">{}</span><span class="s2">. This is not allowed in &quot;</span>
                                   <span class="s2">&quot;strict mode when create_graph=True.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
                            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                        <span class="n">jac_i_el</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vj_el</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
                            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Output </span><span class="si">{}</span><span class="s2"> of the user-provided function is &quot;</span>
                                   <span class="s2">&quot;independent of input </span><span class="si">{}</span><span class="s2">. This is not allowed in &quot;</span>
                                   <span class="s2">&quot;strict mode.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">el_idx</span><span class="p">))</span>
                            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
                        <span class="n">jac_i_el</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inp_el</span><span class="p">))</span>

            <span class="n">jacobian</span> <span class="o">+=</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">jac_i_el</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># type: ignore[operator]</span>
                         <span class="o">+</span> <span class="n">inputs</span><span class="p">[</span><span class="n">el_idx</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="k">for</span> <span class="p">(</span><span class="n">el_idx</span><span class="p">,</span> <span class="n">jac_i_el</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">jac_i</span><span class="p">)),</span> <span class="p">)</span>

        <span class="n">jacobian</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">jacobian</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">jacobian</span><span class="p">,</span> <span class="p">(</span><span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">))</span></div>


<div class="viewcode-block" id="hessian"><a class="viewcode-back" href="../../../generated/torch.autograd.functional.hessian.html#torch.autograd.functional.hessian">[docs]</a><span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vectorize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">outer_jacobian_strategy</span><span class="o">=</span><span class="s2">&quot;reverse-mode&quot;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that computes the Hessian of a given scalar function.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a Tensor with a single element.</span>
<span class="sd">        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.</span>
<span class="sd">        create_graph (bool, optional): If ``True``, the Hessian will be computed in</span>
<span class="sd">            a differentiable manner. Note that when ``strict`` is ``False``, the result can not</span>
<span class="sd">            require gradients or be disconnected from the inputs.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        strict (bool, optional): If ``True``, an error will be raised when we detect that there exists an input</span>
<span class="sd">            such that all the outputs are independent of it. If ``False``, we return a Tensor of zeros as the</span>
<span class="sd">            hessian for said inputs, which is the expected mathematical value.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        vectorize (bool, optional): This feature is experimental.</span>
<span class="sd">            Please consider using :func:`torch.func.hessian`</span>
<span class="sd">            instead if you are looking for something less experimental and more performant.</span>
<span class="sd">            When computing the hessian, usually we invoke</span>
<span class="sd">            ``autograd.grad`` once per row of the hessian. If this flag is</span>
<span class="sd">            ``True``, we use the vmap prototype feature as the backend to</span>
<span class="sd">            vectorize calls to ``autograd.grad`` so we only invoke it once</span>
<span class="sd">            instead of once per row. This should lead to performance</span>
<span class="sd">            improvements in many use cases, however, due to this feature</span>
<span class="sd">            being incomplete, there may be performance cliffs. Please</span>
<span class="sd">            use `torch._C._debug_only_display_vmap_fallback_warnings(True)`</span>
<span class="sd">            to show any performance warnings and file us issues if</span>
<span class="sd">            warnings exist for your use case. Defaults to ``False``.</span>
<span class="sd">        outer_jacobian_strategy (str, optional): The Hessian is computed by</span>
<span class="sd">            computing the Jacobian of a Jacobian. The inner Jacobian is always</span>
<span class="sd">            computed in reverse-mode AD. Setting strategy to ``&quot;forward-mode&quot;``</span>
<span class="sd">            or ``&quot;reverse-mode&quot;`` determines whether the outer Jacobian will be</span>
<span class="sd">            computed with forward or reverse mode AD. Currently, computing the outer</span>
<span class="sd">            Jacobian in ``&quot;forward-mode&quot;`` requires ``vectorized=True``. Defaults</span>
<span class="sd">            to ``&quot;reverse-mode&quot;``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Hessian (Tensor or a tuple of tuple of Tensors): if there is a single input,</span>
<span class="sd">        this will be a single Tensor containing the Hessian for the input.</span>
<span class="sd">        If it is a tuple, then the Hessian will be a tuple of tuples where</span>
<span class="sd">        ``Hessian[i][j]`` will contain the Hessian of the ``i``\th input</span>
<span class="sd">        and ``j``\th input with size the sum of the size of the ``i``\th input plus</span>
<span class="sd">        the size of the ``j``\th input. ``Hessian[i][j]`` will have the same</span>
<span class="sd">        dtype and device as the corresponding ``i``\th input.</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="sd">        &gt;&gt;&gt; def pow_reducer(x):</span>
<span class="sd">        ...     return x.pow(3).sum()</span>
<span class="sd">        &gt;&gt;&gt; inputs = torch.rand(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="sd">        &gt;&gt;&gt; hessian(pow_reducer, inputs)</span>
<span class="sd">        tensor([[[[5.2265, 0.0000],</span>
<span class="sd">                  [0.0000, 0.0000]],</span>
<span class="sd">                 [[0.0000, 4.8221],</span>
<span class="sd">                  [0.0000, 0.0000]]],</span>
<span class="sd">                [[[0.0000, 0.0000],</span>
<span class="sd">                  [1.9456, 0.0000]],</span>
<span class="sd">                 [[0.0000, 0.0000],</span>
<span class="sd">                  [0.0000, 3.2550]]]])</span>

<span class="sd">        &gt;&gt;&gt; hessian(pow_reducer, inputs, create_graph=True)</span>
<span class="sd">        tensor([[[[5.2265, 0.0000],</span>
<span class="sd">                  [0.0000, 0.0000]],</span>
<span class="sd">                 [[0.0000, 4.8221],</span>
<span class="sd">                  [0.0000, 0.0000]]],</span>
<span class="sd">                [[[0.0000, 0.0000],</span>
<span class="sd">                  [1.9456, 0.0000]],</span>
<span class="sd">                 [[0.0000, 0.0000],</span>
<span class="sd">                  [0.0000, 3.2550]]]], grad_fn=&lt;ViewBackward&gt;)</span>


<span class="sd">        &gt;&gt;&gt; def pow_adder_reducer(x, y):</span>
<span class="sd">        ...     return (2 * x.pow(2) + 3 * y.pow(2)).sum()</span>
<span class="sd">        &gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))</span>
<span class="sd">        &gt;&gt;&gt; hessian(pow_adder_reducer, inputs)</span>
<span class="sd">        ((tensor([[4., 0.],</span>
<span class="sd">                  [0., 4.]]),</span>
<span class="sd">          tensor([[0., 0.],</span>
<span class="sd">                  [0., 0.]])),</span>
<span class="sd">         (tensor([[0., 0.],</span>
<span class="sd">                  [0., 0.]]),</span>
<span class="sd">          tensor([[6., 0.],</span>
<span class="sd">                  [0., 6.]])))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;hessian&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">outer_jacobian_strategy</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;forward-mode&quot;</span><span class="p">,</span> <span class="s2">&quot;reverse-mode&quot;</span><span class="p">),</span> <span class="p">(</span>
        <span class="s1">&#39;Expected strategy to be either &quot;forward-mode&quot; or &quot;reverse-mode&quot;.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ensure_single_output_function</span><span class="p">(</span><span class="o">*</span><span class="n">inp</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inp</span><span class="p">)</span>
        <span class="n">is_out_tuple</span><span class="p">,</span> <span class="n">t_out</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="s2">&quot;outputs of the user-provided function&quot;</span><span class="p">,</span> <span class="s2">&quot;hessian&quot;</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">t_out</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_out_tuple</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The function given to hessian should return a single Tensor&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The Tensor returned by the function given to hessian should contain a single element&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">jac_func</span><span class="p">(</span><span class="o">*</span><span class="n">inp</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">outer_jacobian_strategy</span> <span class="o">==</span> <span class="s2">&quot;forward-mode&quot;</span><span class="p">:</span>
            <span class="c1"># _grad_preprocess requires create_graph=True and input to require_grad</span>
            <span class="c1"># or else the input will be detached</span>
            <span class="n">inp</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">inp</span><span class="p">)</span>
        <span class="n">jac</span> <span class="o">=</span> <span class="n">jacobian</span><span class="p">(</span><span class="n">ensure_single_output_function</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">jac</span><span class="p">,</span> <span class="s2">&quot;jacobian&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">jac</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">jacobian</span><span class="p">(</span><span class="n">jac_func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span> <span class="n">vectorize</span><span class="o">=</span><span class="n">vectorize</span><span class="p">,</span>
                   <span class="n">strategy</span><span class="o">=</span><span class="n">outer_jacobian_strategy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="p">(</span><span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">))</span></div>


<div class="viewcode-block" id="vhp"><a class="viewcode-back" href="../../../generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp">[docs]</a><span class="k">def</span> <span class="nf">vhp</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that computes the dot product between a vector ``v`` and the</span>
<span class="sd">    Hessian of a given scalar function at the point given by the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a Tensor with a single element.</span>
<span class="sd">        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.</span>
<span class="sd">        v (tuple of Tensors or Tensor): The vector for which the vector Hessian</span>
<span class="sd">            product is computed. Must be the same size as the input of</span>
<span class="sd">            ``func``. This argument is optional when ``func``&#39;s input contains</span>
<span class="sd">            a single element and (if it is not provided) will be set as a</span>
<span class="sd">            Tensor containing a single ``1``.</span>
<span class="sd">        create_graph (bool, optional): If ``True``, both the output and result</span>
<span class="sd">            will be computed in a differentiable way. Note that when ``strict``</span>
<span class="sd">            is ``False``, the result can not require gradients or be</span>
<span class="sd">            disconnected from the inputs.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        strict (bool, optional): If ``True``, an error will be raised when we</span>
<span class="sd">            detect that there exists an input such that all the outputs are</span>
<span class="sd">            independent of it. If ``False``, we return a Tensor of zeros as the</span>
<span class="sd">            vhp for said inputs, which is the expected mathematical value.</span>
<span class="sd">            Defaults to ``False``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (tuple): tuple with:</span>
<span class="sd">            func_output (tuple of Tensors or Tensor): output of ``func(inputs)``</span>

<span class="sd">            vhp (tuple of Tensors or Tensor): result of the dot product with the</span>
<span class="sd">            same shape as the inputs.</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="sd">        &gt;&gt;&gt; def pow_reducer(x):</span>
<span class="sd">        ...     return x.pow(3).sum()</span>
<span class="sd">        &gt;&gt;&gt; inputs = torch.rand(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; v = torch.ones(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="sd">        &gt;&gt;&gt; vhp(pow_reducer, inputs, v)</span>
<span class="sd">        (tensor(0.5591),</span>
<span class="sd">         tensor([[1.0689, 1.2431],</span>
<span class="sd">                 [3.0989, 4.4456]]))</span>
<span class="sd">        &gt;&gt;&gt; vhp(pow_reducer, inputs, v, create_graph=True)</span>
<span class="sd">        (tensor(0.5591, grad_fn=&lt;SumBackward0&gt;),</span>
<span class="sd">         tensor([[1.0689, 1.2431],</span>
<span class="sd">                 [3.0989, 4.4456]], grad_fn=&lt;MulBackward0&gt;))</span>
<span class="sd">        &gt;&gt;&gt; def pow_adder_reducer(x, y):</span>
<span class="sd">        ...     return (2 * x.pow(2) + 3 * y.pow(2)).sum()</span>
<span class="sd">        &gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))</span>
<span class="sd">        &gt;&gt;&gt; v = (torch.zeros(2), torch.ones(2))</span>
<span class="sd">        &gt;&gt;&gt; vhp(pow_adder_reducer, inputs, v)</span>
<span class="sd">        (tensor(4.8053),</span>
<span class="sd">         (tensor([0., 0.]),</span>
<span class="sd">          tensor([6., 6.])))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
        <span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;vhp&quot;</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="s2">&quot;vhp&quot;</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">_validate_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The vector v can only be None if the input to the user-provided function &quot;</span>
                                   <span class="s2">&quot;is a single Tensor with a single element.&quot;</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs of the user-provided function&quot;</span><span class="p">,</span> <span class="s2">&quot;vhp&quot;</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_outputs_tuple</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The function given to vhp should return a single Tensor&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The Tensor returned by the function given to vhp should contain a single element&quot;</span><span class="p">)</span>

        <span class="n">jac</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">jac</span><span class="p">,</span> <span class="s2">&quot;jacobian&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

    <span class="n">enable_grad</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">create_graph</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">enable_grad</span><span class="p">):</span>
        <span class="n">grad_res</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">jac</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>
        <span class="n">vhp</span> <span class="o">=</span> <span class="n">_fill_in_zeros</span><span class="p">(</span><span class="n">grad_res</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="s2">&quot;double_back&quot;</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>
    <span class="n">vhp</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">vhp</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">is_outputs_tuple</span><span class="p">),</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">vhp</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">)</span></div>


<div class="viewcode-block" id="hvp"><a class="viewcode-back" href="../../../generated/torch.autograd.functional.hvp.html#torch.autograd.functional.hvp">[docs]</a><span class="k">def</span> <span class="nf">hvp</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that computes the dot product between the Hessian of a given scalar</span>
<span class="sd">    function and a vector ``v`` at the point given by the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (function): a Python function that takes Tensor inputs and returns</span>
<span class="sd">            a Tensor with a single element.</span>
<span class="sd">        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.</span>
<span class="sd">        v (tuple of Tensors or Tensor): The vector for which the Hessian vector</span>
<span class="sd">            product is computed. Must be the same size as the input of</span>
<span class="sd">            ``func``. This argument is optional when ``func``&#39;s input contains</span>
<span class="sd">            a single element and (if it is not provided) will be set as a</span>
<span class="sd">            Tensor containing a single ``1``.</span>
<span class="sd">        create_graph (bool, optional): If ``True``, both the output and result will be</span>
<span class="sd">            computed in a differentiable way. Note that when ``strict`` is</span>
<span class="sd">            ``False``, the result can not require gradients or be disconnected</span>
<span class="sd">            from the inputs.  Defaults to ``False``.</span>
<span class="sd">        strict (bool, optional): If ``True``, an error will be raised when we</span>
<span class="sd">            detect that there exists an input such that all the outputs are</span>
<span class="sd">            independent of it. If ``False``, we return a Tensor of zeros as the</span>
<span class="sd">            hvp for said inputs, which is the expected mathematical value.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">    Returns:</span>
<span class="sd">        output (tuple): tuple with:</span>
<span class="sd">            func_output (tuple of Tensors or Tensor): output of ``func(inputs)``</span>

<span class="sd">            hvp (tuple of Tensors or Tensor): result of the dot product with</span>
<span class="sd">            the same shape as the inputs.</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="sd">        &gt;&gt;&gt; def pow_reducer(x):</span>
<span class="sd">        ...     return x.pow(3).sum()</span>
<span class="sd">        &gt;&gt;&gt; inputs = torch.rand(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; v = torch.ones(2, 2)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span>
<span class="sd">        &gt;&gt;&gt; hvp(pow_reducer, inputs, v)</span>
<span class="sd">        (tensor(0.1448),</span>
<span class="sd">         tensor([[2.0239, 1.6456],</span>
<span class="sd">                 [2.4988, 1.4310]]))</span>

<span class="sd">        &gt;&gt;&gt; hvp(pow_reducer, inputs, v, create_graph=True)</span>
<span class="sd">        (tensor(0.1448, grad_fn=&lt;SumBackward0&gt;),</span>
<span class="sd">         tensor([[2.0239, 1.6456],</span>
<span class="sd">                 [2.4988, 1.4310]], grad_fn=&lt;MulBackward0&gt;))</span>


<span class="sd">        &gt;&gt;&gt; def pow_adder_reducer(x, y):</span>
<span class="sd">        ...     return (2 * x.pow(2) + 3 * y.pow(2)).sum()</span>
<span class="sd">        &gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))</span>
<span class="sd">        &gt;&gt;&gt; v = (torch.zeros(2), torch.ones(2))</span>
<span class="sd">        &gt;&gt;&gt; hvp(pow_adder_reducer, inputs, v)</span>
<span class="sd">        (tensor(2.3030),</span>
<span class="sd">         (tensor([0., 0.]),</span>
<span class="sd">          tensor([6., 6.])))</span>

<span class="sd">    Note:</span>

<span class="sd">        This function is significantly slower than `vhp` due to backward mode AD constraints.</span>
<span class="sd">        If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you</span>
<span class="sd">        know that your function satisfies this condition, you should use vhp instead that is</span>
<span class="sd">        much faster with the current implementation.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
        <span class="n">is_inputs_tuple</span><span class="p">,</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;hvp&quot;</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="s2">&quot;hvp&quot;</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">_grad_preprocess</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span> <span class="n">need_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">_validate_v</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The vector v can only be None if the input to the user-provided function &quot;</span>
                                   <span class="s2">&quot;is a single Tensor with a single element.&quot;</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">is_outputs_tuple</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">_as_tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs of the user-provided function&quot;</span><span class="p">,</span> <span class="s2">&quot;hvp&quot;</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="s2">&quot;outputs&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_outputs_tuple</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The function given to hvp should return a single Tensor&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The Tensor returned by the function given to hvp should contain a single element&quot;</span><span class="p">)</span>

        <span class="n">jac</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">jac</span><span class="p">,</span> <span class="s2">&quot;jacobian&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

        <span class="n">grad_jac</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">)</span>

        <span class="n">double_back</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">jac</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_jac</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">_check_requires_grad</span><span class="p">(</span><span class="n">jac</span><span class="p">,</span> <span class="s2">&quot;hessian&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span>

    <span class="n">enable_grad</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">create_graph</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">enable_grad</span><span class="p">):</span>
        <span class="n">grad_res</span> <span class="o">=</span> <span class="n">_autograd_grad</span><span class="p">(</span><span class="n">double_back</span><span class="p">,</span> <span class="n">grad_jac</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>
        <span class="n">hvp</span> <span class="o">=</span> <span class="n">_fill_in_zeros</span><span class="p">(</span><span class="n">grad_res</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="s2">&quot;double_back_trick&quot;</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>
    <span class="n">hvp</span> <span class="o">=</span> <span class="n">_grad_postprocess</span><span class="p">(</span><span class="n">hvp</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">is_outputs_tuple</span><span class="p">),</span> <span class="n">_tuple_postprocess</span><span class="p">(</span><span class="n">hvp</span><span class="p">,</span> <span class="n">is_inputs_tuple</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.parallel.distributed &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.parallel.distributed</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.parallel.distributed</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">weakref</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">fields</span><span class="p">,</span> <span class="n">is_dataclass</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">auto</span><span class="p">,</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span><span class="p">,</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.join</span> <span class="kn">import</span> <span class="n">Join</span><span class="p">,</span> <span class="n">Joinable</span><span class="p">,</span> <span class="n">JoinHook</span>

<span class="kn">from</span> <span class="nn">torch.utils._pytree</span> <span class="kn">import</span> <span class="n">tree_flatten</span><span class="p">,</span> <span class="n">tree_unflatten</span>

<span class="n">RPC_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">torch.distributed.distributed_c10d</span> <span class="kn">import</span> <span class="n">_get_default_group</span><span class="p">,</span> <span class="n">ReduceOp</span>
    <span class="kn">from</span> <span class="nn">torch.distributed.utils</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">_alloc_storage</span><span class="p">,</span>
        <span class="n">_cast_forward_inputs</span><span class="p">,</span>
        <span class="n">_free_storage</span><span class="p">,</span>
        <span class="n">_sync_module_states</span><span class="p">,</span>
        <span class="n">_to_kwargs</span><span class="p">,</span>
        <span class="n">_verify_param_shape_across_processes</span><span class="p">,</span>
    <span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">rpc</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">RPC_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="kn">from</span> <span class="nn">torch.distributed.rpc</span> <span class="kn">import</span> <span class="n">RRef</span>

<span class="kn">from</span> <span class="nn">torch._utils</span> <span class="kn">import</span> <span class="n">_get_device_index</span>

<span class="kn">from</span> <span class="nn">..modules</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">.scatter_gather</span> <span class="kn">import</span> <span class="n">gather</span><span class="p">,</span> <span class="n">scatter_kwargs</span>  <span class="c1"># noqa: F401</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;DistributedDataParallel&quot;</span><span class="p">]</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">_MixedPrecision</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This configures DDP-native mixed precision training.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        param_dtype (torch.dtype): This specifies the dtype for model</span>
<span class="sd">            parameters, inputs (when ``cast_forward_inputs`` is set to</span>
<span class="sd">            ``True``), and therefore the dtype for computation.</span>
<span class="sd">            However, outside the forward and backward passes, parameters are in</span>
<span class="sd">            full precision. Model checkpointing always happens in full</span>
<span class="sd">            precision.</span>
<span class="sd">        reduce_dtype (torch.dtype): This specifies the dtype for gradient</span>
<span class="sd">            reduction, which is permitted to differ from ``param_dtype``.</span>
<span class="sd">        buffer_dtype (torch.dtype): This specifies the dtype for buffers.</span>

<span class="sd">    .. note:: This API is experimental and subject to change.</span>

<span class="sd">    .. note:: Only floating point tensors are cast to their specified dtypes.</span>

<span class="sd">    .. note:: ``state_dict`` checkpoints parameters and buffers in full</span>
<span class="sd">        precision.</span>

<span class="sd">    .. note:: Each low precision dtype must be specified explicitly. For</span>
<span class="sd">        example, ``_MixedPrecision(reduce_dtype=torch.float16)`` only specifies</span>
<span class="sd">        the reduction dtype to be low precision, and DDP will not cast</span>
<span class="sd">        parameters or buffers.</span>

<span class="sd">    .. note:: If a ``reduce_dtype`` is not specified, then gradient reduction</span>
<span class="sd">        happens in ``param_dtype`` if specified or the original parameter dtype</span>
<span class="sd">        otherwise. For example, ``_MixedPrecision(param_dtype=torch.float16)``</span>
<span class="sd">        would result in communication occurring in fp16.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">reduce_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">buffer_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># TODO (rohan-varma): keep_low_precision_grads: bool = False</span>
    <span class="c1"># TODO (rohan-varma): APIs to allow users to run batchnorm and layernorm</span>
    <span class="c1"># in full precision. For DDP, this can be implemented by not performing the</span>
    <span class="c1"># parameter cast for BN and LN units.</span>


<span class="k">def</span> <span class="nf">_cast_buffers</span><span class="p">(</span><span class="n">mixed_precision_config</span><span class="p">,</span> <span class="n">root_module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Casts buffers to the given ``buffer_dtype``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">root_module</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="s2">&quot;_ddp_ignored&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">buf</span><span class="o">.</span><span class="n">_ddp_ignored</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="n">buf</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">buf</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">mixed_precision_config</span><span class="o">.</span><span class="n">buffer_dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_setup_mixed_precision_params</span><span class="p">(</span><span class="n">mixed_precision_config</span><span class="p">,</span> <span class="n">root_module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates and frees storage for the mixed precision parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">root_module</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="c1"># Do not setup mixed precision for DDP ignored parameters.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s2">&quot;_ddp_ignored&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">_ddp_ignored</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s2">&quot;_mp_param&quot;</span><span class="p">):</span>
            <span class="n">param</span><span class="o">.</span><span class="n">_mp_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                <span class="n">param</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">mixed_precision_config</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">_free_storage</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">_mp_param</span><span class="p">)</span>
            <span class="c1"># _fp_param will point to the full precision param so it can be switched</span>
            <span class="c1"># back to at the end of forward / backward.</span>
            <span class="n">param</span><span class="o">.</span><span class="n">_fp_param</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span>


<span class="k">def</span> <span class="nf">_tree_flatten_with_rref</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="n">output_is_rref</span> <span class="o">=</span> <span class="n">RPC_AVAILABLE</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">RRef</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_is_rref</span><span class="p">:</span>
        <span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">treespec</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">local_value</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">treespec</span> <span class="o">=</span> <span class="n">tree_flatten</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="c1"># Need to return flattened tensors, spec to re-pack them, as well</span>
    <span class="c1"># as if the return type was actually an RRef to reconstruct.</span>
    <span class="k">return</span> <span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">treespec</span><span class="p">,</span> <span class="n">output_is_rref</span>


<span class="k">def</span> <span class="nf">_tree_unflatten_with_rref</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">treespec</span><span class="p">,</span> <span class="n">output_is_rref</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tree_unflatten</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">treespec</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_is_rref</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">RRef</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>


<span class="k">def</span> <span class="nf">_find_tensors</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recursively find all tensors contained in the specified object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">RPC_AVAILABLE</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">RRef</span><span class="p">):</span>
        <span class="c1"># If the current node is the owner of the RRef, unwrap it and try to</span>
        <span class="c1"># find Tensors.</span>
        <span class="c1"># TODO: Expand to remote RRefs.</span>
        <span class="k">if</span> <span class="n">obj</span><span class="o">.</span><span class="n">is_owner</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">_find_tensors</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">local_value</span><span class="p">())</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">obj</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">_find_tensors</span><span class="p">,</span> <span class="n">obj</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">_find_tensors</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
    <span class="k">if</span> <span class="n">is_dataclass</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">_find_tensors</span><span class="p">,</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fields</span><span class="p">(</span><span class="n">obj</span><span class="p">)))</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="p">[]</span>


<span class="k">def</span> <span class="nf">_dump_DDP_relevant_env_vars</span><span class="p">():</span>
    <span class="n">relevant_env_vars</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;RANK&quot;</span><span class="p">,</span>
        <span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span>
        <span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">,</span>
        <span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">,</span>
        <span class="s2">&quot;GLOO_SOCKET_IFNAME&quot;</span><span class="p">,</span>
        <span class="s2">&quot;GLOO_DEVICE_TRANSPORT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_SOCKET_IFNAME&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_BLOCKING_WAIT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_DEBUG&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_DEBUG_SUBSYS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_DISABLE&quot;</span><span class="p">,</span>
        <span class="c1"># More NCCL env vars:</span>
        <span class="s2">&quot;NCCL_P2P_DISABLE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_P2P_LEVEL&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_SHM_DISABLE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_SOCKET_NTHREADS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_NSOCKS_PERTHREAD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_BUFFSIZE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_NTHREADS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_RINGS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_MAX_NCHANNELS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_MIN_NCHANNELS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_CHECKS_DISABLE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_CHECK_POINTERS&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_LAUNCH_MODE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_HCA&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_TIMEOUT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_RETRY_CNT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_GID_INDEX&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_SL&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_TC&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_AR_THRESHOLD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IB_CUDA_SUPPORT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_NET_GDR_LEVEL&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_NET_GDR_READ&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_SINGLE_RING_THRESHOLD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_LL_THRESHOLD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_TREE_THRESHOLD&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_ALGO&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_PROTO&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_IGNORE_CPU_AFFINITY&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_DEBUG_FILE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_COLLNET_ENABLE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_TOPO_FILE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_TOPO_DUMP_FILE&quot;</span><span class="p">,</span>
        <span class="s2">&quot;NCCL_ASYNC_ERROR_HANDLING&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">formatted_output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">relevant_env_vars</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">var</span><span class="p">]</span> <span class="k">if</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="k">else</span> <span class="s2">&quot;N/A&quot;</span>
        <span class="n">formatted_output</span> <span class="o">+=</span> <span class="s2">&quot;env:</span><span class="si">%s</span><span class="s2">=</span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">formatted_output</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_BufferCommHookLocation</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">PRE_FORWARD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">POST_FORWARD</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">_BufferCommHook</span><span class="p">:</span>
    <span class="n">buffer_comm_hook</span><span class="p">:</span> <span class="n">Callable</span>
    <span class="n">buffer_comm_hook_state</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">buffer_comm_hook_location</span><span class="p">:</span> <span class="n">_BufferCommHookLocation</span>


<span class="c1"># Add a DDPSink to run various functions when backwards starts, such as</span>
<span class="c1"># queueing call back of out-most backward/graph task,</span>
<span class="c1"># this helps call back is fired after all gradients&#39; calculation</span>
<span class="c1"># is completed.</span>
<span class="k">class</span> <span class="nc">_DDPSink</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">ddp_weakref</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># set_materialize_grads(False) will ensure that None gradients stay as</span>
        <span class="c1"># None and are not filled with zeros.</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">ddp_weakref</span> <span class="o">=</span> <span class="n">ddp_weakref</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="n">inp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">inp</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">):</span>
        <span class="c1"># Enqueue delay allreduce for static graph training on the first</span>
        <span class="c1"># iteration.</span>
        <span class="n">ddp_weakref</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">ddp_weakref</span><span class="p">()</span>
        <span class="n">reducer</span> <span class="o">=</span> <span class="n">ddp_weakref</span><span class="o">.</span><span class="n">reducer</span>
        <span class="n">static_graph</span> <span class="o">=</span> <span class="n">ddp_weakref</span><span class="o">.</span><span class="n">static_graph</span>
        <span class="n">delay_ar_enqueued</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">static_graph</span> <span class="ow">and</span> <span class="n">ddp_weakref</span><span class="o">.</span><span class="n">_static_graph_delay_allreduce_enqueued</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">static_graph</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">delay_ar_enqueued</span><span class="p">:</span>
            <span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">queue_callback</span><span class="p">(</span>  <span class="c1"># type: ignore[call-arg,misc]</span>
                <span class="n">reducer</span><span class="o">.</span><span class="n">_delay_all_reduce</span>
            <span class="p">)</span>
            <span class="n">ddp_weakref</span><span class="o">.</span><span class="n">_static_graph_delay_allreduce_enqueued</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">grad_outputs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_DDPJoinHook</span><span class="p">(</span><span class="n">JoinHook</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ddp</span><span class="p">,</span> <span class="n">divide_by_initial_world_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets config variables for internal usage.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ddp</span><span class="p">,</span> <span class="n">DistributedDataParallel</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;DDP join hook requires passing in a DistributedDataParallel &quot;</span>
            <span class="s2">&quot;instance as the state&quot;</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">ddp</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">ddp</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_uneven_input_join</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ddp</span> <span class="o">=</span> <span class="n">ddp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ddp</span><span class="o">.</span><span class="n">_divide_by_initial_world_size</span> <span class="o">=</span> <span class="n">divide_by_initial_world_size</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">main_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Shadows the DDP collective communication operations in the forward and</span>
<span class="sd">        backward passes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ddp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ddp</span>
        <span class="c1"># Buckets are rebuilt only once during a training period</span>
        <span class="n">ddp</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_rebuild_buckets</span><span class="p">()</span>

        <span class="c1"># Schedule a broadcast if we are syncing module buffers in the</span>
        <span class="c1"># forward pass</span>
        <span class="c1"># TODO: make DDP uneven inputs context manager support buffer</span>
        <span class="c1"># comm hook (https://github.com/pytorch/pytorch/issues/65436)</span>
        <span class="n">ddp</span><span class="o">.</span><span class="n">_check_and_sync_module_buffers</span><span class="p">()</span>

        <span class="c1"># Check if need to sync in the backward pass</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">ddp</span><span class="o">.</span><span class="n">_check_global_requires_backward_grad_sync</span><span class="p">(</span><span class="n">is_joined_rank</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="n">should_sync_backwards</span> <span class="o">=</span> <span class="n">work</span><span class="o">.</span><span class="n">result</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span>
        <span class="c1"># Forward parameter sync is disabled in the next iteration if we</span>
        <span class="c1"># are skipping gradient sync this iteration, so set</span>
        <span class="c1"># `require_forward_param_sync` accordingly</span>
        <span class="n">ddp</span><span class="o">.</span><span class="n">require_forward_param_sync</span> <span class="o">=</span> <span class="n">should_sync_backwards</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">should_sync_backwards</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Schedule one allreduce per gradient bucket to match the backward</span>
        <span class="c1"># pass allreduce</span>
        <span class="n">ddp</span><span class="o">.</span><span class="n">_match_all_reduce_for_bwd_pass</span><span class="p">()</span>

        <span class="c1"># Check if we need to allreduce locally unused parameters</span>
        <span class="k">if</span> <span class="n">ddp</span><span class="o">.</span><span class="n">find_unused_parameters</span><span class="p">:</span>
            <span class="n">ddp</span><span class="o">.</span><span class="n">_match_unused_params_allreduce</span><span class="p">()</span>

        <span class="c1"># Rebuilt parameters are pushed only once during a training period</span>
        <span class="n">ddp</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_push_all_rebuilt_params</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">post_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_last_joiner</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Syncs the final model to ensure that the model is the same across all</span>
<span class="sd">        processes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ddp</span><span class="o">.</span><span class="n">_sync_final_model</span><span class="p">(</span><span class="n">is_last_joiner</span><span class="p">)</span>


<div class="viewcode-block" id="DistributedDataParallel"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">[docs]</a><span class="k">class</span> <span class="nc">DistributedDataParallel</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="n">Joinable</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements distributed data parallelism that is based on</span>
<span class="sd">    ``torch.distributed`` package at the module level.</span>

<span class="sd">    This container provides data parallelism by synchronizing gradients</span>
<span class="sd">    across each model replica. The devices to synchronize across are</span>
<span class="sd">    specified by the input ``process_group``, which is the entire world</span>
<span class="sd">    by default. Note that ``DistributedDataParallel`` does not chunk or</span>
<span class="sd">    otherwise shard the input across participating GPUs; the user is</span>
<span class="sd">    responsible for defining how to do so, for example through the use</span>
<span class="sd">    of a :class:`DistributedSampler`.</span>

<span class="sd">    See also: :ref:`distributed-basics` and :ref:`cuda-nn-ddp-instead`.</span>
<span class="sd">    The same constraints on input as in :class:`torch.nn.DataParallel` apply.</span>

<span class="sd">    Creation of this class requires that ``torch.distributed`` to be already</span>
<span class="sd">    initialized, by calling :func:`torch.distributed.init_process_group`.</span>

<span class="sd">    ``DistributedDataParallel`` is proven to be significantly faster than</span>
<span class="sd">    :class:`torch.nn.DataParallel` for single-node multi-GPU data</span>
<span class="sd">    parallel training.</span>

<span class="sd">    To use ``DistributedDataParallel`` on a host with N GPUs, you should spawn</span>
<span class="sd">    up ``N`` processes, ensuring that each process exclusively works on a single</span>
<span class="sd">    GPU from 0 to N-1. This can be done by either setting</span>
<span class="sd">    ``CUDA_VISIBLE_DEVICES`` for every process or by calling:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(i)</span>

<span class="sd">    where i is from 0 to N-1. In each process, you should refer the following</span>
<span class="sd">    to construct this module:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.distributed.init_process_group(</span>
<span class="sd">        &gt;&gt;&gt;     backend=&#39;nccl&#39;, world_size=N, init_method=&#39;...&#39;</span>
<span class="sd">        &gt;&gt;&gt; )</span>
<span class="sd">        &gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)</span>

<span class="sd">    In order to spawn up multiple processes per node, you can use either</span>
<span class="sd">    ``torch.distributed.launch`` or ``torch.multiprocessing.spawn``.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Please refer to `PyTorch Distributed Overview &lt;https://pytorch.org/tutorials/beginner/dist_overview.html&gt;`__</span>
<span class="sd">        for a brief introduction to all features related to distributed training.</span>

<span class="sd">    .. note::</span>
<span class="sd">        ``DistributedDataParallel`` can be used in conjunction with</span>
<span class="sd">        :class:`torch.distributed.optim.ZeroRedundancyOptimizer` to reduce</span>
<span class="sd">        per-rank optimizer states memory footprint. Please refer to</span>
<span class="sd">        `ZeroRedundancyOptimizer recipe &lt;https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html&gt;`__</span>
<span class="sd">        for more details.</span>

<span class="sd">    .. note:: ``nccl`` backend is currently the fastest and highly recommended</span>
<span class="sd">        backend when using GPUs. This applies to both single-node and</span>
<span class="sd">        multi-node distributed training.</span>

<span class="sd">    .. note:: This module also supports mixed-precision distributed training.</span>
<span class="sd">        This means that your model can have different types of parameters such</span>
<span class="sd">        as mixed types of ``fp16`` and ``fp32``, the gradient reduction on these</span>
<span class="sd">        mixed types of parameters will just work fine.</span>

<span class="sd">    .. note:: If you use ``torch.save`` on one process to checkpoint the module,</span>
<span class="sd">        and ``torch.load`` on some other processes to recover it, make sure that</span>
<span class="sd">        ``map_location`` is configured properly for every process. Without</span>
<span class="sd">        ``map_location``, ``torch.load`` would recover the module to devices</span>
<span class="sd">        where the module was saved from.</span>

<span class="sd">    .. note:: When a model is trained on ``M`` nodes with ``batch=N``, the</span>
<span class="sd">        gradient will be ``M`` times smaller when compared to the same model</span>
<span class="sd">        trained on a single node with ``batch=M*N`` if the loss is summed (NOT</span>
<span class="sd">        averaged as usual) across instances in a batch (because the gradients</span>
<span class="sd">        between different nodes are averaged). You should take this into</span>
<span class="sd">        consideration when you want to obtain a mathematically equivalent</span>
<span class="sd">        training process compared to the local training counterpart. But in most</span>
<span class="sd">        cases, you can just treat a DistributedDataParallel wrapped model, a</span>
<span class="sd">        DataParallel wrapped model and an ordinary model on a single GPU as the</span>
<span class="sd">        same (E.g. using the same learning rate for equivalent batch size).</span>

<span class="sd">    .. note::</span>
<span class="sd">        Parameters are never broadcast between processes. The module performs</span>
<span class="sd">        an all-reduce step on gradients and assumes that they will be modified</span>
<span class="sd">        by the optimizer in all processes in the same way. Buffers</span>
<span class="sd">        (e.g. BatchNorm stats) are broadcast from the module in process of rank</span>
<span class="sd">        0, to all other replicas in the system in every iteration.</span>

<span class="sd">    .. note::</span>
<span class="sd">        If you are using DistributedDataParallel in conjunction with the</span>
<span class="sd">        :ref:`distributed-rpc-framework`, you should always use</span>
<span class="sd">        :meth:`torch.distributed.autograd.backward` to compute gradients and</span>
<span class="sd">        :class:`torch.distributed.optim.DistributedOptimizer` for optimizing</span>
<span class="sd">        parameters.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.autograd as dist_autograd</span>
<span class="sd">            &gt;&gt;&gt; from torch.nn.parallel import DistributedDataParallel as DDP</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from torch import optim</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.optim import DistributedOptimizer</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.rpc import RRef</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; t1 = torch.rand((3, 3), requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; t2 = torch.rand((3, 3), requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; rref = rpc.remote(&quot;worker1&quot;, torch.add, args=(t1, t2))</span>
<span class="sd">            &gt;&gt;&gt; ddp_model = DDP(my_model)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Setup optimizer</span>
<span class="sd">            &gt;&gt;&gt; optimizer_params = [rref]</span>
<span class="sd">            &gt;&gt;&gt; for param in ddp_model.parameters():</span>
<span class="sd">            &gt;&gt;&gt;     optimizer_params.append(RRef(param))</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; dist_optim = DistributedOptimizer(</span>
<span class="sd">            &gt;&gt;&gt;     optim.SGD,</span>
<span class="sd">            &gt;&gt;&gt;     optimizer_params,</span>
<span class="sd">            &gt;&gt;&gt;     lr=0.05,</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; with dist_autograd.context() as context_id:</span>
<span class="sd">            &gt;&gt;&gt;     pred = ddp_model(rref.to_here())</span>
<span class="sd">            &gt;&gt;&gt;     loss = loss_func(pred, target)</span>
<span class="sd">            &gt;&gt;&gt;     dist_autograd.backward(context_id, [loss])</span>
<span class="sd">            &gt;&gt;&gt;     dist_optim.step(context_id)</span>

<span class="sd">    .. note::</span>
<span class="sd">        DistributedDataParallel currently offers limited support for gradient</span>
<span class="sd">        checkpointing with :meth:`torch.utils.checkpoint`. DDP will work as</span>
<span class="sd">        expected when there are no unused parameters in the model and each layer</span>
<span class="sd">        is checkpointed at most once (make sure you are not passing</span>
<span class="sd">        `find_unused_parameters=True` to DDP). We currently do not support the</span>
<span class="sd">        case where a layer is checkpointed multiple times, or when there unused</span>
<span class="sd">        parameters in the checkpointed model.</span>

<span class="sd">    .. note::</span>
<span class="sd">        To let a non-DDP model load a state dict from a DDP model,</span>
<span class="sd">        :meth:`~torch.nn.modules.utils.consume_prefix_in_state_dict_if_present`</span>
<span class="sd">        needs to be applied to strip the prefix &quot;module.&quot; in the DDP state dict before loading.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Constructor, forward method, and differentiation of the output (or a</span>
<span class="sd">        function of the output of this module) are distributed synchronization</span>
<span class="sd">        points. Take that into account in case different processes might be</span>
<span class="sd">        executing different code.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module assumes all parameters are registered in the model by the</span>
<span class="sd">        time it is created. No parameters should be added nor removed later.</span>
<span class="sd">        Same applies to buffers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module assumes all parameters are registered in the model of each</span>
<span class="sd">        distributed processes are in the same order. The module itself will</span>
<span class="sd">        conduct gradient ``allreduce`` following the reverse order of the</span>
<span class="sd">        registered parameters of the model. In other words, it is users&#39;</span>
<span class="sd">        responsibility to ensure that each distributed process has the exact</span>
<span class="sd">        same model and thus the exact same parameter registration order.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module allows parameters with non-rowmajor-contiguous strides.</span>
<span class="sd">        For example, your model may contain some parameters whose</span>
<span class="sd">        :class:`torch.memory_format` is ``torch.contiguous_format``</span>
<span class="sd">        and others whose format is ``torch.channels_last``.  However,</span>
<span class="sd">        corresponding parameters in different processes must have the</span>
<span class="sd">        same strides.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module doesn&#39;t work with :func:`torch.autograd.grad` (i.e. it will</span>
<span class="sd">        only work if gradients are to be accumulated in ``.grad`` attributes of</span>
<span class="sd">        parameters).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If you plan on using this module with a ``nccl`` backend or a ``gloo``</span>
<span class="sd">        backend (that uses Infiniband), together with a DataLoader that uses</span>
<span class="sd">        multiple workers, please change the multiprocessing start method to</span>
<span class="sd">        ``forkserver`` (Python 3 only) or ``spawn``. Unfortunately</span>
<span class="sd">        Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will</span>
<span class="sd">        likely experience deadlocks if you don&#39;t change this setting.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        You should never try to change your model&#39;s parameters after wrapping</span>
<span class="sd">        up your model with ``DistributedDataParallel``. Because, when</span>
<span class="sd">        wrapping up your model with ``DistributedDataParallel``, the constructor</span>
<span class="sd">        of ``DistributedDataParallel`` will register the additional gradient</span>
<span class="sd">        reduction functions on all the parameters of the model itself at the</span>
<span class="sd">        time of construction. If you change the model&#39;s parameters afterwards,</span>
<span class="sd">        gradient reduction functions no longer match the correct set of</span>
<span class="sd">        parameters.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Using ``DistributedDataParallel`` in conjunction with the</span>
<span class="sd">        :ref:`distributed-rpc-framework` is experimental and subject to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (Module): module to be parallelized</span>
<span class="sd">        device_ids (list of int or torch.device): CUDA devices.</span>
<span class="sd">                   1) For single-device modules, ``device_ids`` can</span>
<span class="sd">                   contain exactly one device id, which represents the only</span>
<span class="sd">                   CUDA device where the input module corresponding to this process resides.</span>
<span class="sd">                   Alternatively, ``device_ids`` can also be ``None``.</span>
<span class="sd">                   2) For multi-device modules and CPU modules,</span>
<span class="sd">                   ``device_ids`` must be ``None``.</span>

<span class="sd">                   When ``device_ids`` is ``None`` for both cases,</span>
<span class="sd">                   both the input data for the forward pass and the actual module</span>
<span class="sd">                   must be placed on the correct device.</span>
<span class="sd">                   (default: ``None``)</span>
<span class="sd">        output_device (int or torch.device): Device location of output for</span>
<span class="sd">                      single-device CUDA modules. For multi-device modules and</span>
<span class="sd">                      CPU modules, it must be ``None``, and the module itself</span>
<span class="sd">                      dictates the output location. (default: ``device_ids[0]``</span>
<span class="sd">                      for single-device modules)</span>
<span class="sd">        broadcast_buffers (bool): Flag that enables syncing (broadcasting)</span>
<span class="sd">                          buffers of the module at beginning of the ``forward``</span>
<span class="sd">                          function. (default: ``True``)</span>
<span class="sd">        process_group: The process group to be used for distributed data</span>
<span class="sd">                       all-reduction. If ``None``, the default process group, which</span>
<span class="sd">                       is created by :func:`torch.distributed.init_process_group`,</span>
<span class="sd">                       will be used. (default: ``None``)</span>
<span class="sd">        bucket_cap_mb: ``DistributedDataParallel`` will bucket parameters into</span>
<span class="sd">                       multiple buckets so that gradient reduction of each</span>
<span class="sd">                       bucket can potentially overlap with backward computation.</span>
<span class="sd">                       :attr:`bucket_cap_mb` controls the bucket size in</span>
<span class="sd">                       MegaBytes (MB). (default: 25)</span>
<span class="sd">        find_unused_parameters (bool): Traverse the autograd graph from all</span>
<span class="sd">                               tensors contained in the return value of the</span>
<span class="sd">                               wrapped module&#39;s ``forward`` function. Parameters</span>
<span class="sd">                               that don&#39;t receive gradients as part of this</span>
<span class="sd">                               graph are preemptively marked as being ready to</span>
<span class="sd">                               be reduced. In addition, parameters that may have</span>
<span class="sd">                               been used in the wrapped module&#39;s ``forward``</span>
<span class="sd">                               function but were not part of loss computation and</span>
<span class="sd">                               thus would also not receive gradients are</span>
<span class="sd">                               preemptively marked as ready to be reduced.</span>
<span class="sd">                               (default: ``False``)</span>
<span class="sd">        check_reduction: This argument is deprecated.</span>
<span class="sd">        gradient_as_bucket_view (bool): When set to ``True``, gradients will be views</span>
<span class="sd">                      pointing to different offsets of ``allreduce`` communication</span>
<span class="sd">                      buckets. This can reduce peak memory usage, where the</span>
<span class="sd">                      saved memory size will be equal to the total gradients</span>
<span class="sd">                      size. Moreover, it avoids the overhead of copying between</span>
<span class="sd">                      gradients and ``allreduce`` communication buckets. When</span>
<span class="sd">                      gradients are views, ``detach_()`` cannot be called on the</span>
<span class="sd">                      gradients. If hitting such errors, please fix it by</span>
<span class="sd">                      referring to the :meth:`~torch.optim.Optimizer.zero_grad`</span>
<span class="sd">                      function in ``torch/optim/optimizer.py`` as a solution.</span>
<span class="sd">                      Note that gradients will be views after first iteration, so</span>
<span class="sd">                      the peak memory saving should be checked after first iteration.</span>
<span class="sd">        static_graph (bool): When set to ``True``, DDP knows the trained graph is</span>
<span class="sd">                     static. Static graph means 1) The set of used and unused</span>
<span class="sd">                     parameters will not change during the whole training loop; in</span>
<span class="sd">                     this case, it does not matter whether users set</span>
<span class="sd">                     ``find_unused_parameters = True`` or not. 2) How the graph is trained</span>
<span class="sd">                     will not change during the whole training loop (meaning there is</span>
<span class="sd">                     no control flow depending on iterations).</span>
<span class="sd">                     When static_graph is set to be ``True``, DDP will support cases that</span>
<span class="sd">                     can not be supported in the past:</span>
<span class="sd">                     1) Reentrant backwards.</span>
<span class="sd">                     2) Activation checkpointing multiple times.</span>
<span class="sd">                     3) Activation checkpointing when model has unused parameters.</span>
<span class="sd">                     4) There are model parameters that are outside of forward function.</span>
<span class="sd">                     5) Potentially improve performance when there are unused parameters,</span>
<span class="sd">                     as DDP will not search graph in each iteration to detect unused</span>
<span class="sd">                     parameters when static_graph is set to be ``True``.</span>
<span class="sd">                     To check whether you can set static_graph to be ``True``, one way is to</span>
<span class="sd">                     check ddp logging data at the end of your previous model training,</span>
<span class="sd">                     if ``ddp_logging_data.get(&quot;can_set_static_graph&quot;) == True``, mostly you</span>
<span class="sd">                     can set ``static_graph = True`` as well.</span>

<span class="sd">                     Example::</span>
<span class="sd">                         &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">                         &gt;&gt;&gt; model_DDP = torch.nn.parallel.DistributedDataParallel(model)</span>
<span class="sd">                         &gt;&gt;&gt; # Training loop</span>
<span class="sd">                         &gt;&gt;&gt; ...</span>
<span class="sd">                         &gt;&gt;&gt; ddp_logging_data = model_DDP._get_ddp_logging_data()</span>
<span class="sd">                         &gt;&gt;&gt; static_graph = ddp_logging_data.get(&quot;can_set_static_graph&quot;)</span>
<span class="sd">        delay_all_reduce_named_params (list of tuple of str and torch.nn.Parameter): a list</span>
<span class="sd">                    of named parameters whose all reduce will be delayed when the gradient of</span>
<span class="sd">                    the parameter specified in ``param_to_hook_all_reduce`` is ready. Other</span>
<span class="sd">                    arguments of DDP do not apply to named params specified in this argument</span>
<span class="sd">                    as these named params will be ignored by DDP reducer.</span>
<span class="sd">        param_to_hook_all_reduce (torch.nn.Parameter): a parameter to hook delayed all reduce</span>
<span class="sd">                    of parameters specified in ``delay_all_reduce_named_params``.</span>


<span class="sd">    Attributes:</span>
<span class="sd">        module (Module): the module to be parallelized.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=4, init_method=&#39;...&#39;)</span>
<span class="sd">        &gt;&gt;&gt; net = torch.nn.parallel.DistributedDataParallel(model)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># used to track whether the given thread is inside ddp forward for torchdynamo purposes</span>
    <span class="n">_active_ddp_module</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">broadcast_buffers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">process_group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">bucket_cap_mb</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
        <span class="n">find_unused_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">check_reduction</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">static_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">delay_all_reduce_named_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">param_to_hook_all_reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_MixedPrecision</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">Joinable</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">bool</span><span class="p">(</span><span class="n">delay_all_reduce_named_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">bool</span><span class="p">(</span>
            <span class="n">param_to_hook_all_reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">,</span>
                <span class="s2">&quot;delay_all_reduce_named_params and param_to_hook_all_reduce &quot;</span>
                <span class="s2">&quot;need to be set at the same time.&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;_ddp_params_and_buffers_to_ignore&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">_ddp_params_and_buffers_to_ignore</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">delay_all_reduce_named_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">delay_all_reduce_named_params</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">p</span>
            <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">((</span><span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Delay the AllReduce of all parameters.&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                    <span class="ne">RuntimeError</span><span class="p">,</span>
                    <span class="s2">&quot;DistributedDataParallel is not needed when a module &quot;</span>
                    <span class="s2">&quot;doesn&#39;t have any parameter that requires a gradient.&quot;</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">device_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">,</span>
                <span class="s2">&quot;device_ids can only be None or contain a single element.&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_multi_device_module</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">({</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span><span class="p">})</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">distinct_device_types</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">distinct_device_types</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">,</span>
                <span class="s2">&quot;DistributedDataParallel&#39;s input module must be on &quot;</span>
                <span class="s2">&quot;the same type of devices, but input module parameters locate in </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">distinct_device_types</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device_type</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">distinct_device_types</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">device_ids</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># For backward compatibility.</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_multi_device_module</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">device_ids</span> <span class="ow">or</span> <span class="n">output_device</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                    <span class="ne">ValueError</span><span class="p">,</span>
                    <span class="s2">&quot;DistributedDataParallel device_ids and output_device arguments &quot;</span>
                    <span class="s2">&quot;only work with single-device/multiple-device GPU modules or CPU modules, &quot;</span>
                    <span class="s2">&quot;but got device_ids </span><span class="si">{}</span><span class="s2">, output_device </span><span class="si">{}</span><span class="s2">, and module parameters </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">device_ids</span><span class="p">,</span>
                        <span class="n">output_device</span><span class="p">,</span>
                        <span class="p">{</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span><span class="p">},</span>
                    <span class="p">),</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">_get_device_index</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">device_ids</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">output_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">output_device</span> <span class="o">=</span> <span class="n">device_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">output_device</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">process_group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">process_group</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span> <span class="o">=</span> <span class="n">broadcast_buffers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span> <span class="o">=</span> <span class="n">find_unused_parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_as_bucket_view</span> <span class="o">=</span> <span class="n">gradient_as_bucket_view</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="o">=</span> <span class="n">mixed_precision</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Received mixed precision config </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">check_reduction</span><span class="p">:</span>
            <span class="c1"># This argument is no longer used since the reducer</span>
            <span class="c1"># will ensure reduction completes even if some parameters</span>
            <span class="c1"># do not receive gradients.</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `check_reduction` argument in `DistributedDataParallel` &quot;</span>
                <span class="s2">&quot;module is deprecated. Please avoid using it.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check that a module does not have Uninitialized parameters</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">UninitializedParameter</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                    <span class="ne">RuntimeError</span><span class="p">,</span>
                    <span class="s2">&quot;Modules with uninitialized parameters can&#39;t be used with `DistributedDataParallel`. &quot;</span>
                    <span class="s2">&quot;Run a dummy forward pass to correctly initialize the modules&quot;</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="c1"># used for intra-node param sync and inter-node sync as well</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">250</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>

        <span class="c1"># reduction bucket size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_bytes_cap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">bucket_cap_mb</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="c1"># Whether to perform input tensor CPU to GPU copies on a side-stream</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_side_stream_for_tensor_copies</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;PYTORCH_DDP_USE_SIDE_STREAM&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;1&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Initialize gradient buffers and register all reduce hook</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_buffer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_views</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_all_params</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_register_delay_all_reduce_hook</span><span class="p">(</span>
                <span class="n">bucket_cap_mb</span><span class="o">=</span><span class="n">bucket_cap_mb</span><span class="p">,</span>
                <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
                <span class="n">param_to_hook_all_reduce</span><span class="o">=</span><span class="n">param_to_hook_all_reduce</span><span class="p">,</span>
                <span class="n">device_ids</span><span class="o">=</span><span class="n">device_ids</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_all_params</span><span class="p">:</span>
                <span class="k">return</span>

        <span class="c1"># Build parameters for reducer.</span>
        <span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_params_for_reducer</span><span class="p">()</span>
        <span class="c1"># Verify model equivalence.</span>
        <span class="n">_verify_param_shape_across_processes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="c1"># Sync params and buffers. Ensures all DDP models start off at the same value.</span>
        <span class="n">_sync_module_states</span><span class="p">(</span>
            <span class="n">module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span>
            <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
            <span class="n">broadcast_bucket_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">,</span>
            <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">params_and_buffers_to_ignore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span><span class="p">,</span>
            <span class="n">broadcast_buffers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># In debug mode, build a mapping of parameter index -&gt; parameter.</span>
        <span class="n">param_to_name_mapping</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_debug_param_to_name_mapping</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>

        <span class="c1"># Builds reducer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ddp_init_helper</span><span class="p">(</span>
            <span class="n">parameters</span><span class="p">,</span>
            <span class="n">expect_sparse_gradient</span><span class="p">,</span>
            <span class="n">param_to_name_mapping</span><span class="p">,</span>
            <span class="n">static_graph</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_setup_mixed_precision_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>
            <span class="n">_cast_buffers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>
            <span class="c1"># Stream used for async low precision copies.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_mp_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_submodule_to_event</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">deque</span><span class="p">)</span>  <span class="c1"># type: ignore[var-annotated]</span>
            <span class="c1"># Add forward pre-hook to root module to kick off copies to lower</span>
            <span class="c1"># precision.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_root_copy_hook</span><span class="p">,</span> <span class="n">prepend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">with_kwargs</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="c1"># Add forward pre hook to all submodules to wait for copy events</span>
            <span class="c1"># before running computation.</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
                <span class="n">module</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_module_wait_for_copy_hook</span><span class="p">,</span>
                    <span class="n">prepend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">with_kwargs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="c1"># Set up callbacks in backward to upcast and use full precision</span>
            <span class="c1"># params. TODO (rohan-varma): Make this compose with general</span>
            <span class="c1"># comm hooks and apply_optimizer_in_backward. Importing inline to</span>
            <span class="c1"># avoid circular import issue.</span>
            <span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks</span> <span class="kn">import</span> <span class="p">(</span>
                <span class="n">_AllreduceUpcastHookState</span><span class="p">,</span>
                <span class="n">_reducer_allreduce_and_upcast_hook</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">upcast_hook_state</span> <span class="o">=</span> <span class="n">_AllreduceUpcastHookState</span><span class="p">(</span>
                <span class="n">ddp_weakref</span><span class="o">=</span><span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="n">upcast_stream</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_comm_hook</span><span class="p">(</span>
                <span class="n">upcast_hook_state</span><span class="p">,</span>
                <span class="n">_reducer_allreduce_and_upcast_hook</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Inform reducer of reduced precision param dtype for correctness</span>
            <span class="c1"># of type checks between gradient and bucket.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_mixed_precision_param_dtype</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_has_rebuilt_buckets</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">static_graph</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_static_graph</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_in_backward_optimizers</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_register_delay_all_reduce_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">bucket_cap_mb</span><span class="p">,</span>
        <span class="n">process_group</span><span class="p">,</span>
        <span class="n">param_to_hook_all_reduce</span><span class="p">,</span>
        <span class="n">device_ids</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># 1. Create gradient buffer</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">device_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">device_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="nb">sum</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span><span class="p">]),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 2. Broadcast the parameters</span>
        <span class="n">detached_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span><span class="p">]</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_broadcast_coalesced</span><span class="p">(</span><span class="n">process_group</span><span class="p">,</span> <span class="n">detached_params</span><span class="p">,</span> <span class="n">bucket_cap_mb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 3. Hook all reduce to the specified parameter</span>
        <span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="n">process_group</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_delayed_all_reduce</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_buffer</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">world_size</span><span class="p">)</span>  <span class="c1"># type: ignore[union-attr]</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_buffer</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">process_group</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">grad</span>

        <span class="n">param_to_hook_all_reduce</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">_delayed_all_reduce</span><span class="p">)</span>

        <span class="c1"># 4. Build tensor views for gradients</span>
        <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span><span class="p">:</span>
            <span class="n">grad_view</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_buffer</span><span class="p">[</span><span class="n">offset</span> <span class="p">:</span> <span class="p">(</span><span class="n">offset</span> <span class="o">+</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">())]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">param</span><span class="o">.</span><span class="n">shape</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_views</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_view</span><span class="p">)</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="c1"># 5. Check whether the all reduce of all params requiring grad is delayed.</span>
        <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="n">full_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="k">if</span> <span class="n">full_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span><span class="p">:</span>
                        <span class="c1"># There is at least a param whose all reduce will not be delayed.</span>
                        <span class="c1"># In this case, we should not set self._delay_all_reduce_all_params</span>
                        <span class="c1"># to True.</span>
                        <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_all_params</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_setup_in_backward_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Check if user has used apply_optim_in_backward to overlap optimizer</span>
        <span class="c1"># step + DDP backward. Current constraints:</span>
        <span class="c1"># 1. Only allreduce is supported at the moment, no custom communication.</span>
        <span class="c1"># 2. For DDP-managed parameters that have their optimizer run in</span>
        <span class="c1"># backward, their gradients are set to ``None``. If your use case</span>
        <span class="c1"># requires DDP parameters grad not to be set to ``None`` after their</span>
        <span class="c1"># in-backward optimizer runs, please ping</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/90052.</span>
        <span class="c1"># NOTE: we use self._module_parameters instead of .parameters() since</span>
        <span class="c1"># the former excludes ignored (non-DDP managed) parameters.</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_in_backward_optimizers&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span><span class="p">):</span>
            <span class="c1"># Remove hooks that apply_optim_in_backward had registered because</span>
            <span class="c1"># DDP customizes how optimizer is overlapped with backward due to</span>
            <span class="c1"># the allreduce.</span>
            <span class="n">param_to_handle_map</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">apply_optimizer_in_backward</span><span class="o">.</span><span class="n">param_to_optim_hook_handle_map</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_parameters</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="n">param_to_handle_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="p">[]):</span>
                    <span class="n">handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

            <span class="c1"># Need a weakref to DDP instance to run all_reduce (from reducer)</span>
            <span class="c1"># and get managed DDP parameters.</span>
            <span class="n">ddp_weakref</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="c1"># Note: importing in function, otherwise this will cause a circular</span>
            <span class="c1"># import.</span>
            <span class="kn">from</span> <span class="nn">torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks</span> <span class="kn">import</span> <span class="p">(</span>
                <span class="n">_apply_optim_in_backward_hook</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">register_comm_hook</span><span class="p">(</span>
                <span class="n">ddp_weakref</span><span class="p">,</span>
                <span class="n">_apply_optim_in_backward_hook</span><span class="p">(</span>
                    <span class="n">gradient_is_bucket_view</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_as_bucket_view</span>
                <span class="p">),</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_optimizer_in_backward</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="nf">_fire_reducer_autograd_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="o">*</span><span class="n">unused</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fires the reducer&#39;s autograd hook to allreduce params in a Reducer bucket.</span>
<span class="sd">        Note that this is only used during mixed precision training as the</span>
<span class="sd">        Reducer&#39;s hooks installed during construction time would not be called</span>
<span class="sd">        as we&#39;re working in the low precision parameter setting.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_autograd_hook</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="nf">_root_copy_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        When training with DDP mixed precision, this root pre-forward hook kicks</span>
<span class="sd">        off low precision copies on a separate stream and creates respective</span>
<span class="sd">        events to wait for them.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Clear out previous iteration submodule to event. This is because we</span>
        <span class="c1"># may have populated some events for modules that didn&#39;t end up being</span>
        <span class="c1"># used.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_submodule_to_event</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">deque</span><span class="p">)</span>  <span class="c1"># type: ignore[var-annotated]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mp_stream</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">submodule</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                    <span class="c1"># Do not cast DDP ignored parameters.</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s2">&quot;_ddp_ignored&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">_ddp_ignored</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">_alloc_storage</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">_mp_param</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
                    <span class="c1"># copy() implicitly casts to low precision</span>
                    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">_mp_param</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                        <span class="c1"># TODO: when zero_grad(set_to_none=False) or in grad</span>
                        <span class="c1"># accumulation case, accumulated grads can be in fp32</span>
                        <span class="c1"># which can cause errors when running DDP backwards due</span>
                        <span class="c1"># to mismatched incoming and accumulated gradient types.</span>
                        <span class="c1"># So we manually cast the accumulated grad down for now,</span>
                        <span class="c1"># in the future we may shift to FSDP style gradient</span>
                        <span class="c1"># accumulation management where the accumulated gradient</span>
                        <span class="c1"># is saved and .grad field is set to None, bypassing</span>
                        <span class="c1"># this issue.</span>
                        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span>  <span class="c1"># type: ignore[union-attr]</span>
                            <span class="p">)</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">_mp_param</span>
                <span class="n">copy_event</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>
                <span class="n">copy_event</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_submodule_to_event</span><span class="p">[</span><span class="n">submodule</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy_event</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_module_wait_for_copy_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Before carrying out computation, wait on the appropriate event to ensure</span>
<span class="sd">        low precision copies have finished.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">event</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_submodule_to_event</span><span class="p">[</span><span class="n">module</span><span class="p">]</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="c1"># copy event has already been waited on</span>
            <span class="k">return</span>

        <span class="n">event</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="c1"># Don&#39;t register hooks if param does not require grad</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s2">&quot;_ddp_ignored&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">_ddp_ignored</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="c1"># We need to register autograd hook here instead of DDP&#39;s ctor</span>
            <span class="c1"># since we&#39;re working with the low precision param. Register them</span>
            <span class="c1"># via obtaining the gradient accumulator.</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">grad_acc</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">hook</span> <span class="o">=</span> <span class="n">grad_acc</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span>
                <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fire_reducer_autograd_hook</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">_idx</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_ddp_mp_hook_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_acc</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_log_and_throw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">err_type</span><span class="p">,</span> <span class="n">err_msg</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">set_error_and_log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">err_type</span><span class="p">)</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">err_msg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">err_type</span><span class="p">(</span><span class="n">err_msg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_ddp_init_helper</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parameters</span><span class="p">,</span>
        <span class="n">expect_sparse_gradient</span><span class="p">,</span>
        <span class="n">param_to_name_mapping</span><span class="p">,</span>
        <span class="n">static_graph</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialization helper function that does the following:</span>
<span class="sd">        (1) bucketing the parameters for reductions</span>
<span class="sd">        (2) resetting the bucketing states</span>
<span class="sd">        (3) registering the grad hooks</span>
<span class="sd">        (4) Logging construction-time DDP logging data</span>
<span class="sd">        (5) passing a handle of DDP to SyncBatchNorm Layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Notice, the parameters order is not in the order in which they are used,</span>
        <span class="c1"># especially in models with control flow.</span>
        <span class="c1">#</span>
        <span class="c1"># Alongside parameters are not presented in the real execution order,</span>
        <span class="c1"># if a certain model happens to also</span>
        <span class="c1">#   1) have other collectives comm ops in its backward graph.</span>
        <span class="c1">#   2) have unused parameter in subset ranks of the whole world.</span>
        <span class="c1"># bucketing could insert ALL-REDUCE comm op too early on the rank with unused parameter,</span>
        <span class="c1"># matching up with other collectives comm ops on other ranks unexpectedly.</span>
        <span class="c1">#</span>
        <span class="c1"># In order to handle this corner case, when the parameters are not in the real execution order,</span>
        <span class="c1"># we don&#39;t do bucketing, thus only one ALL-REDUCE is inserted after all the gradients</span>
        <span class="c1"># of the whole graph are computed.</span>
        <span class="c1">#</span>
        <span class="c1"># Notice, here we only disable bucketing for the first iteration.</span>
        <span class="c1"># After the first iteration, it&#39;s OK to rebuild buckets,</span>
        <span class="c1"># because &quot;bucket rebuild&quot; bucketizes parameters based on its real execution order in backward graph.</span>

        <span class="c1"># Can remove this branching once #73732 is landed.</span>
        <span class="k">if</span> <span class="n">static_graph</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">bucket_size_limits</span> <span class="o">=</span> <span class="p">[</span><span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bucket_size_limits</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">_DEFAULT_FIRST_BUCKET_BYTES</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bucket_bytes_cap</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">(</span>
            <span class="n">bucket_indices</span><span class="p">,</span>
            <span class="n">per_bucket_size_limits</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">_compute_bucket_assignment_by_size</span><span class="p">(</span>
            <span class="n">parameters</span><span class="p">,</span>
            <span class="n">bucket_size_limits</span><span class="p">,</span>
            <span class="n">expect_sparse_gradient</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Remember index for parameters if we are in mixed precision, as we</span>
        <span class="c1"># need to pass in index to Reducer&#39;s autograd hook via python.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
                <span class="n">p</span><span class="o">.</span><span class="n">_idx</span> <span class="o">=</span> <span class="n">i</span>

        <span class="c1"># Note: reverse list of buckets because we want to approximate the</span>
        <span class="c1"># order in which their gradients are produced, and assume they</span>
        <span class="c1"># are used in the forward pass in the order they are defined.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Reducer</span><span class="p">(</span>
            <span class="n">parameters</span><span class="p">,</span>
            <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">bucket_indices</span><span class="p">)),</span>
            <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">per_bucket_size_limits</span><span class="p">)),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
            <span class="n">expect_sparse_gradient</span><span class="p">,</span>
            <span class="c1"># The bucket size limit is specified in the constructor.</span>
            <span class="c1"># Additionally, we allow for a single small bucket for parameters</span>
            <span class="c1"># that are defined first, such that their gradients don&#39;t spill into</span>
            <span class="c1"># a much larger bucket, adding unnecessary latency after gradient</span>
            <span class="c1"># computation finishes. Experiments showed 1MB is a reasonable value.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bucket_bytes_cap</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gradient_as_bucket_view</span><span class="p">,</span>
            <span class="n">param_to_name_mapping</span><span class="p">,</span>
            <span class="c1"># User can set dist._DEFAULT_FIRST_BUCKET_BYTES to tune DDP first</span>
            <span class="c1"># bucket.</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">_DEFAULT_FIRST_BUCKET_BYTES</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">Logger</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="p">)</span>
        <span class="c1"># Set as a weak reference to avoid reference cycle between</span>
        <span class="c1"># logger and reducer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">set_logger</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">)</span>

        <span class="n">has_sync_bn</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="p">):</span>
                <span class="n">has_sync_bn</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span>

        <span class="c1"># Set logging data that can be got during construction time.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">set_construction_data_and_log</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
            <span class="p">[]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">,</span>
            <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span><span class="p">,</span>
            <span class="n">has_sync_bn</span><span class="p">,</span>
            <span class="n">static_graph</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># passing a handle to torch.nn.SyncBatchNorm layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_passing_sync_batchnorm_handle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_default_group</span><span class="p">()</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s2">&quot;process_group&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s2">&quot;reducer&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s2">&quot;logger&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">attrs</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># If serializable, then the process group should be the default one</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;require_forward_param_sync&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;require_backward_grad_sync&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_params_for_reducer</span><span class="p">()</span>
        <span class="c1"># In debug mode, build a mapping of parameter index -&gt; parameter.</span>
        <span class="n">param_to_name_mapping</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_debug_param_to_name_mapping</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
        <span class="c1"># Builds reducer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ddp_init_helper</span><span class="p">(</span>
            <span class="n">parameters</span><span class="p">,</span>
            <span class="n">expect_sparse_gradient</span><span class="p">,</span>
            <span class="n">param_to_name_mapping</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_static_graph</span><span class="p">()</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_static_graph</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_build_params_for_reducer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Build tuple of (module, parameter) for all parameters that require grads.</span>
        <span class="n">modules_and_parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">parameter</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="n">param</span>
                <span class="c1"># Note that we access module.named_parameters instead of</span>
                <span class="c1"># parameters(module). parameters(module) is only needed in the</span>
                <span class="c1"># single-process multi device case, where it accesses replicated</span>
                <span class="c1"># parameters through _former_parameters.</span>
                <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span>
                <span class="ow">and</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span>
            <span class="p">]</span>
        <span class="p">]</span>

        <span class="c1"># Deduplicate any parameters that might be shared across child modules.</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">modules_and_parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># &quot;p not in memo&quot; is the deduplication check.</span>
            <span class="c1"># &quot;not memo.add(p)&quot; is always True, and it&#39;s only there to cause &quot;add(p)&quot; if needed.</span>
            <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">modules_and_parameters</span>
            <span class="k">if</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">memo</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">memo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># type: ignore[func-returns-value]</span>
        <span class="p">]</span>

        <span class="c1"># Build list of parameters.</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">parameter</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">modules_and_parameters</span><span class="p">]</span>

        <span class="c1"># Checks if a module will produce a sparse gradient.</span>
        <span class="k">def</span> <span class="nf">produces_sparse_gradient</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span><span class="p">)):</span>
                <span class="k">return</span> <span class="n">module</span><span class="o">.</span><span class="n">sparse</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Build list of booleans indicating whether or not to expect sparse</span>
        <span class="c1"># gradients for the corresponding parameters.</span>
        <span class="n">expect_sparse_gradient</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">produces_sparse_gradient</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">module</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">modules_and_parameters</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_assign_modules_buffers</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">expect_sparse_gradient</span>

    <span class="k">def</span> <span class="nf">_assign_modules_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Assigns module buffers to self.modules_buffers which are then used to</span>
<span class="sd">        broadcast across ranks when broadcast_buffers=True. Note that this</span>
<span class="sd">        must be called every time buffers need to be synced because buffers can</span>
<span class="sd">        be reassigned by user module,</span>
<span class="sd">        see https://github.com/pytorch/pytorch/issues/63916.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Collect buffers for modules, filtering out buffers that should be ignored.</span>
        <span class="n">named_module_buffers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">buffer_name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">buffer_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">buffer</span> <span class="k">for</span> <span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span> <span class="ow">in</span> <span class="n">named_module_buffers</span>
        <span class="p">]</span>
        <span class="c1"># Dict[str, tensor] representing module buffers not ignored by DDP.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">named_module_buffers</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">buffer_name</span><span class="p">:</span> <span class="n">buffer</span> <span class="k">for</span> <span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">buffer_name</span><span class="p">)</span> <span class="ow">in</span> <span class="n">named_module_buffers</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">_build_debug_param_to_name_mapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
        <span class="n">param_to_param_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">parameters</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">))}</span>
        <span class="n">param_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
        <span class="n">param_index_to_param_fqn</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
                <span class="n">fqn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="c1"># Bypass ignored parameters since those are not reduced by DDP</span>
                <span class="c1"># to begin with.</span>
                <span class="k">if</span> <span class="n">fqn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_set</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                            <span class="ne">ValueError</span><span class="p">,</span>
                            <span class="sa">f</span><span class="s2">&quot;Param with name </span><span class="si">{</span><span class="n">fqn</span><span class="si">}</span><span class="s2"> found in module parameters, but not DDP parameters.&quot;</span>
                            <span class="s2">&quot; This indicates a bug in DDP, please report an issue to PyTorch.&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="n">param_index</span> <span class="o">=</span> <span class="n">param_to_param_index</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
                    <span class="n">param_index_to_param_fqn</span><span class="p">[</span><span class="n">param_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">fqn</span>

        <span class="c1"># Ensure we covered all parameters</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_set</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_index_to_param_fqn</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="s2">&quot;Expected param to name mapping to cover all parameters, but&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; got conflicting lengths: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param_set</span><span class="p">)</span><span class="si">}</span><span class="s2"> vs &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param_index_to_param_fqn</span><span class="p">)</span><span class="si">}</span><span class="s2">. This indicates a bug in DDP&quot;</span>
                    <span class="s2">&quot;, please report an issue to PyTorch.&quot;</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">param_index_to_param_fqn</span>

    <span class="k">def</span> <span class="nf">_get_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a generator of module parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">model_parameters</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">ps</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_former_parameters</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="s2">&quot;_former_parameters&quot;</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">yield from</span> <span class="n">ps</span>

        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">modules</span><span class="p">()</span> <span class="k">if</span> <span class="n">recurse</span> <span class="k">else</span> <span class="p">[</span><span class="n">m</span><span class="p">]:</span>
            <span class="k">yield from</span> <span class="n">model_parameters</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_default_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">!=</span> <span class="n">_get_default_group</span><span class="p">():</span>
                <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
            <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">pickle_not_supported</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">RuntimeError</span><span class="p">,</span>
                <span class="s2">&quot;DDP Pickling/Unpickling are only supported &quot;</span>
                <span class="s2">&quot;when using DDP with the default process &quot;</span>
                <span class="s2">&quot;group. That is, when you have called &quot;</span>
                <span class="s2">&quot;init_process_group and have not passed &quot;</span>
                <span class="s2">&quot;process_group argument to DDP constructor&quot;</span><span class="p">,</span>
            <span class="p">)</span>

<div class="viewcode-block" id="DistributedDataParallel.no_sync"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync">[docs]</a>    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">no_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to disable gradient synchronizations across DDP</span>
<span class="sd">        processes. Within this context, gradients will be accumulated on module</span>
<span class="sd">        variables, which will later be synchronized in the first</span>
<span class="sd">        forward-backward pass exiting the context.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; ddp = torch.nn.parallel.DistributedDataParallel(model, pg)</span>
<span class="sd">            &gt;&gt;&gt; with ddp.no_sync():</span>
<span class="sd">            &gt;&gt;&gt;     for input in inputs:</span>
<span class="sd">            &gt;&gt;&gt;         ddp(input).backward()  # no synchronization, accumulate grads</span>
<span class="sd">            &gt;&gt;&gt; ddp(another_input).backward()  # synchronize grads</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The forward pass should be included inside the context manager, or</span>
<span class="sd">            else gradients will still be synchronized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">old_require_backward_grad_sync</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span> <span class="o">=</span> <span class="n">old_require_backward_grad_sync</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_get_active_ddp_module</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TorchDynamo needs to know whether DDP is currently active, and access the DDP module in order to cooperatively optimize it.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_active_ddp_module</span>

    <span class="c1"># note, this ctxmgr function is marked &#39;skip&#39; in torchdynamo, so dynamo only kicks in</span>
    <span class="c1"># for the &#39;module_to_run&#39; underneath</span>
    <span class="c1"># see torch._dynamo/eval_frame.py TorchPatcher.patch for more details</span>
    <span class="nd">@contextmanager</span>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span><span class="p">(</span><span class="n">recursive</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_inside_ddp_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">DistributedDataParallel</span><span class="o">.</span><span class="n">_active_ddp_module</span> <span class="o">=</span> <span class="bp">self</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">DistributedDataParallel</span><span class="o">.</span><span class="n">_active_ddp_module</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_run_ddp_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inside_ddp_forward</span><span class="p">():</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[index]</span>

    <span class="k">def</span> <span class="nf">_clear_grad_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Making param.grad points to the grad buffers before backward is based on the</span>
        <span class="c1"># assumption that the grad accumulation is done in place in autograd engine,</span>
        <span class="c1"># for some edge cases, if the grad accumulation in autograd engine is not in</span>
        <span class="c1"># place, then the param.grad and grad buffers are detached.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># We batch zero_grad for all params by resetting the whole grad</span>
            <span class="c1"># buffer when the grad of all params is set to None.</span>
            <span class="n">all_param_grad_none</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span>
            <span class="p">)</span>

            <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_params</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_views</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">all_param_grad_none</span><span class="p">:</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">all_param_grad_none</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_delay_grad_buffer</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_pre_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_all_params</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">set_runtime_stats_and_log</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">prepare_for_forward</span><span class="p">()</span>

        <span class="c1"># Notify the join context that this process has not joined, if</span>
        <span class="c1"># needed</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">Join</span><span class="o">.</span><span class="n">notify_join_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">work</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_forward_pass_work_handle</span><span class="p">(</span>
                <span class="n">work</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_divide_by_initial_world_size</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="p">)</span>

        <span class="c1"># Calling _rebuild_buckets before forward computation,</span>
        <span class="c1"># It may allocate new buckets before deallocating old buckets</span>
        <span class="c1"># inside _rebuild_buckets. To save peak memory usage,</span>
        <span class="c1"># call _rebuild_buckets before the peak memory usage increases</span>
        <span class="c1"># during forward computation.</span>
        <span class="c1"># This should be called only once during whole training period.</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_rebuild_buckets</span><span class="p">():</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Reducer buckets have been rebuilt in this iteration.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_has_rebuilt_buckets</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># sync params according to location (before/after forward) user</span>
        <span class="c1"># specified as part of hook, if hook was specified.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_sync_bufs_pre_fwd</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_buffers</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_join_config</span><span class="o">.</span><span class="n">enable</span><span class="p">:</span>
            <span class="c1"># Notify joined ranks whether they should sync in backwards pass or not.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_global_requires_backward_grad_sync</span><span class="p">(</span><span class="n">is_joined_rank</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">:</span>
            <span class="n">moved_inputs</span><span class="p">,</span> <span class="n">moved_kwargs</span> <span class="o">=</span> <span class="n">_to_kwargs</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">,</span>
                <span class="n">kwargs</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">use_side_stream_for_tensor_copies</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">moved_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">moved_kwargs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># Cast inputs to reduced precision if needed.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">_cast_forward_inputs</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
                    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Cast inputs to reduced precision if needed.</span>
            <span class="c1"># TODO (rohan-varma) test this codepath.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">_cast_forward_inputs</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
                    <span class="o">*</span><span class="n">inputs</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">_post_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_all_params</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_grad_buffer</span><span class="p">()</span>
            <span class="k">return</span>

        <span class="c1"># sync params according to location (before/after forward) user</span>
        <span class="c1"># specified as part of hook, if hook was specified.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_sync_bufs_post_fwd</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_buffers</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># We&#39;ll return the output object verbatim since it is a freeform</span>
            <span class="c1"># object. We need to find any tensors in this object, though,</span>
            <span class="c1"># because we need to figure out which parameters were used during</span>
            <span class="c1"># this forward pass, to ensure we short circuit reduction for any</span>
            <span class="c1"># unused parameters. Only if `find_unused_parameters` is set.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span><span class="p">:</span>
                <span class="c1"># Do not need to populate this for static graph.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">prepare_for_backward</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">_find_tensors</span><span class="p">(</span><span class="n">output</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">prepare_for_backward</span><span class="p">([])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># TODO: DDPSink is currently enabled for unused parameter detection and</span>
        <span class="c1"># static graph training for first iteration.</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_static_graph_delay_allreduce_enqueued</span>
        <span class="p">):</span>
            <span class="p">(</span>
                <span class="n">output_tensor_list</span><span class="p">,</span>
                <span class="n">treespec</span><span class="p">,</span>
                <span class="n">output_is_rref</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">_tree_flatten_with_rref</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="n">output_placeholders</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">))]</span>
            <span class="c1"># Do not touch tensors that have no grad_fn, which can cause issues</span>
            <span class="c1"># such as https://github.com/pytorch/pytorch/issues/60733</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">output_placeholders</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span>

            <span class="c1"># When find_unused_parameters=True, makes tensors which require grad</span>
            <span class="c1"># run through the DDPSink backward pass. When not all outputs are</span>
            <span class="c1"># used in loss, this makes those corresponding tensors receive</span>
            <span class="c1"># undefined gradient which the reducer then handles to ensure</span>
            <span class="c1"># param.grad field is not touched and we don&#39;t error out.</span>
            <span class="n">passthrough_tensor_list</span> <span class="o">=</span> <span class="n">_DDPSink</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
                <span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="o">*</span><span class="n">output_tensor_list</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_placeholders</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">output_placeholders</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">output_placeholders</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">passthrough_tensor_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

            <span class="c1"># Reconstruct output data structure.</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">_tree_unflatten_with_rref</span><span class="p">(</span>
                <span class="n">output_placeholders</span><span class="p">,</span> <span class="n">treespec</span><span class="p">,</span> <span class="n">output_is_rref</span>
            <span class="p">)</span>

        <span class="c1"># At the end of the forward pass, reset the grad buffer and grad views</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_clear_grad_buffer</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;DistributedDataParallel.forward&quot;</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_forward</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delay_all_reduce_all_params</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_ddp_forward</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_forward</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scatter_kwargs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
        <span class="c1"># Kept for BC</span>
        <span class="k">return</span> <span class="n">_to_kwargs</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">kwargs</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_type</span><span class="p">,</span> <span class="n">device_id</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_side_stream_for_tensor_copies</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">output_device</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">gather</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_device</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="c1"># When running in join mode, schedules an allreduce to notify joined ranks</span>
    <span class="c1"># of whether backwards pass synchronization will run this iteration or not.</span>
    <span class="k">def</span> <span class="nf">_check_global_requires_backward_grad_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_joined_rank</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_joined_rank</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">require_backward_grad_sync</span><span class="p">:</span>
            <span class="n">requires_sync_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">requires_sync_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">work</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
            <span class="n">requires_sync_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">work</span>

    <span class="c1"># When running in join mode, checks and performs sync of module buffers if</span>
    <span class="c1"># the models have buffers that should be synchronized in the forward pass.</span>
    <span class="k">def</span> <span class="nf">_check_and_sync_module_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_sync_bufs_pre_fwd</span><span class="p">():</span>
            <span class="n">authoritative_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_common_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_distributed_rank</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_module_buffers</span><span class="p">(</span><span class="n">authoritative_rank</span><span class="p">)</span>

    <span class="c1"># When running in join model, agrees upon a common rank and broadcast model</span>
    <span class="c1"># parameters to all other ranks.</span>
    <span class="k">def</span> <span class="nf">_sync_final_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_last_joiner</span><span class="p">):</span>
        <span class="c1"># Agree upon the process that will be the authoritative model copy.</span>
        <span class="c1"># The current rank is a candidate for being the authoritative copy if</span>
        <span class="c1"># is_last_joiner=True. We break ties via picking the larger rank.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_authoritative_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_common_rank</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_rank</span><span class="p">,</span> <span class="n">is_last_joiner</span>
        <span class="p">)</span>
        <span class="n">_sync_module_states</span><span class="p">(</span>
            <span class="n">module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span>
            <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
            <span class="n">broadcast_bucket_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">,</span>
            <span class="n">src</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_authoritative_rank</span><span class="p">,</span>
            <span class="n">params_and_buffers_to_ignore</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters_to_ignore</span><span class="p">,</span>
            <span class="n">broadcast_buffers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Schedule comm ops to match those scheduled in the reducer&#39;s backward</span>
    <span class="c1"># pass.</span>
    <span class="k">def</span> <span class="nf">_match_all_reduce_for_bwd_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">comm_work</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Schedule comm in the same order as Reducer schedules them, i.e.</span>
        <span class="c1"># the order of the buckets. Retrieving the bucket order from the reducer</span>
        <span class="c1"># ensures that we keep the same order in join mode, such as when bucket</span>
        <span class="c1"># order is rebuilt dynamically.</span>

        <span class="c1"># Returns grad_buckets in order, but real tensors are substituted with</span>
        <span class="c1"># zero tensors of the same shape.</span>
        <span class="n">grad_buckets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_get_zeros_like_grad_buckets</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">grad_bucket</span> <span class="ow">in</span> <span class="n">grad_buckets</span><span class="p">:</span>
            <span class="c1"># Joined processes contribute zero gradient. In the case that</span>
            <span class="c1"># divide_by_initial_world_size=True, we divide grads by the static</span>
            <span class="c1"># world size, if not, the dividing factor is reduced by the number</span>
            <span class="c1"># of joined processes.</span>
            <span class="n">work</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_run_comm_hook</span><span class="p">(</span><span class="n">grad_bucket</span><span class="p">)</span>
            <span class="n">comm_work</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">work</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">work</span> <span class="ow">in</span> <span class="n">comm_work</span><span class="p">:</span>
            <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

    <span class="c1"># Allreduces the used parameter mapping across ranks.</span>
    <span class="k">def</span> <span class="nf">_match_unused_params_allreduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">locally_used_param_map</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_get_local_used_map</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">locally_used_param_map</span><span class="p">)</span>

<div class="viewcode-block" id="DistributedDataParallel.join"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join">[docs]</a>    <span class="k">def</span> <span class="nf">join</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">divide_by_initial_world_size</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">enable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">throw_on_early_termination</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to be used in conjunction with an instance of</span>
<span class="sd">        :class:`torch.nn.parallel.DistributedDataParallel` to be</span>
<span class="sd">        able to train with uneven inputs across participating processes.</span>

<span class="sd">        This context manager will keep track of already-joined DDP processes,</span>
<span class="sd">        and &quot;shadow&quot; the forward and backward passes by inserting collective</span>
<span class="sd">        communication operations to match with the ones created by non-joined</span>
<span class="sd">        DDP processes. This will ensure each collective call has a corresponding</span>
<span class="sd">        call by already-joined DDP processes, preventing hangs or errors that</span>
<span class="sd">        would otherwise happen when training with uneven inputs across</span>
<span class="sd">        processes. Alternatively, if the flag ``throw_on_early_termination`` is</span>
<span class="sd">        specified to be ``True``, all trainers will throw an error once one rank</span>
<span class="sd">        runs out of inputs, allowing these errors to be caught and handled</span>
<span class="sd">        according to application logic.</span>

<span class="sd">        Once all DDP processes have joined, the context manager will broadcast</span>
<span class="sd">        the model corresponding to the last joined process to all processes to</span>
<span class="sd">        ensure the model is the same across all processes</span>
<span class="sd">        (which is guaranteed by DDP).</span>

<span class="sd">        To use this to enable training with uneven inputs across processes,</span>
<span class="sd">        simply wrap this context manager around your training loop. No further</span>
<span class="sd">        modifications to the model or data loading is required.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            If the model or training loop this context manager is wrapped around</span>
<span class="sd">            has additional distributed collective operations, such as</span>
<span class="sd">            ``SyncBatchNorm`` in the model&#39;s forward pass, then the flag</span>
<span class="sd">            ``throw_on_early_termination`` must be enabled. This is because this</span>
<span class="sd">            context manager is not aware of non-DDP collective communication.</span>
<span class="sd">            This flag will cause all ranks to throw when any one rank</span>
<span class="sd">            exhausts inputs, allowing these errors to be caught and recovered</span>
<span class="sd">            from across all ranks.</span>

<span class="sd">        Args:</span>
<span class="sd">            divide_by_initial_world_size (bool): If ``True``, will divide</span>
<span class="sd">                gradients by the initial ``world_size`` DDP training was launched</span>
<span class="sd">                with. If ``False``, will compute the effective world size</span>
<span class="sd">                (number of ranks that have not depleted their inputs yet) and</span>
<span class="sd">                divide gradients by that during allreduce. Set</span>
<span class="sd">                ``divide_by_initial_world_size=True`` to ensure every input</span>
<span class="sd">                sample including the uneven inputs have equal weight in terms of</span>
<span class="sd">                how much they contribute to the global gradient. This is</span>
<span class="sd">                achieved by always dividing the gradient by the initial</span>
<span class="sd">                ``world_size`` even when we encounter uneven inputs. If you set</span>
<span class="sd">                this to ``False``, we divide the gradient by the remaining</span>
<span class="sd">                number of nodes. This ensures parity with training on a smaller</span>
<span class="sd">                ``world_size`` although it also means the uneven inputs would</span>
<span class="sd">                contribute more towards the global gradient. Typically, you</span>
<span class="sd">                would want to set this to ``True`` for cases where the last few</span>
<span class="sd">                inputs of your training job are uneven. In extreme cases, where</span>
<span class="sd">                there is a large discrepancy in the number of inputs, setting</span>
<span class="sd">                this to ``False`` might provide better results.</span>
<span class="sd">            enable (bool): Whether to enable uneven input detection or not. Pass</span>
<span class="sd">                in ``enable=False`` to disable in cases where you know that</span>
<span class="sd">                inputs are even across participating processes. Default is</span>
<span class="sd">                ``True``.</span>
<span class="sd">            throw_on_early_termination (bool): Whether to throw an error</span>
<span class="sd">                or continue training when at least one rank has exhausted</span>
<span class="sd">                inputs. If ``True``, will throw upon the first rank reaching end</span>
<span class="sd">                of data. If ``False``, will continue training with a smaller</span>
<span class="sd">                effective world size until all ranks are joined. Note that if</span>
<span class="sd">                this flag is specified, then the flag</span>
<span class="sd">                ``divide_by_initial_world_size`` would be ignored. Default</span>
<span class="sd">                is ``False``.</span>


<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Distributed&quot;)</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">            &gt;&gt;&gt; import os</span>
<span class="sd">            &gt;&gt;&gt; import torch.multiprocessing as mp</span>
<span class="sd">            &gt;&gt;&gt; import torch.nn as nn</span>
<span class="sd">            &gt;&gt;&gt; # On each spawned worker</span>
<span class="sd">            &gt;&gt;&gt; def worker(rank):</span>
<span class="sd">            &gt;&gt;&gt;     dist.init_process_group(&quot;nccl&quot;, rank=rank, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt;     torch.cuda.set_device(rank)</span>
<span class="sd">            &gt;&gt;&gt;     model = nn.Linear(1, 1, bias=False).to(rank)</span>
<span class="sd">            &gt;&gt;&gt;     model = torch.nn.parallel.DistributedDataParallel(</span>
<span class="sd">            &gt;&gt;&gt;         model, device_ids=[rank], output_device=rank</span>
<span class="sd">            &gt;&gt;&gt;     )</span>
<span class="sd">            &gt;&gt;&gt;     # Rank 1 gets one more input than rank 0.</span>
<span class="sd">            &gt;&gt;&gt;     inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]</span>
<span class="sd">            &gt;&gt;&gt;     with model.join():</span>
<span class="sd">            &gt;&gt;&gt;         for _ in range(5):</span>
<span class="sd">            &gt;&gt;&gt;             for inp in inputs:</span>
<span class="sd">            &gt;&gt;&gt;                 loss = model(inp).sum()</span>
<span class="sd">            &gt;&gt;&gt;                 loss.backward()</span>
<span class="sd">            &gt;&gt;&gt;     # Without the join() API, the below synchronization will hang</span>
<span class="sd">            &gt;&gt;&gt;     # blocking for rank 1&#39;s allreduce to complete.</span>
<span class="sd">            &gt;&gt;&gt;     torch.cuda.synchronize(device=rank)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Join</span><span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="p">],</span>
            <span class="n">enable</span><span class="p">,</span>
            <span class="n">throw_on_early_termination</span><span class="p">,</span>
            <span class="n">divide_by_initial_world_size</span><span class="o">=</span><span class="n">divide_by_initial_world_size</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="DistributedDataParallel.join_hook"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join_hook">[docs]</a>    <span class="k">def</span> <span class="nf">join_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the DDP join hook, which enables training on uneven inputs by</span>
<span class="sd">        shadowing the collective communications in the forward and backward</span>
<span class="sd">        passes.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            kwargs (dict): a :class:`dict` containing any keyword arguments</span>
<span class="sd">                to modify the behavior of the join hook at run time; all</span>
<span class="sd">                :class:`Joinable` instances sharing the same join context</span>
<span class="sd">                manager are forwarded the same value for ``kwargs``.</span>

<span class="sd">        The hook supports the following keyword arguments:</span>
<span class="sd">            divide_by_initial_world_size (bool, optional):</span>
<span class="sd">                If ``True``, then gradients are divided by the initial world</span>
<span class="sd">                size that DDP was launched with.</span>
<span class="sd">                If ``False``, then gradients are divided by the effective world</span>
<span class="sd">                size (i.e. the number of non-joined processes), meaning that</span>
<span class="sd">                the uneven inputs contribute more toward the global gradient.</span>
<span class="sd">                Typically, this should be set to ``True`` if the degree of</span>
<span class="sd">                unevenness is small but can be set to ``False`` in extreme</span>
<span class="sd">                cases for possibly better results.</span>
<span class="sd">                Default is ``True``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">divide_by_initial_world_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;divide_by_initial_world_size&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_DDPJoinHook</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">divide_by_initial_world_size</span><span class="o">=</span><span class="n">divide_by_initial_world_size</span>
        <span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">join_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">join_process_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>

    <span class="k">def</span> <span class="nf">_register_buffer_comm_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state</span><span class="p">,</span>
        <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">comm_hook_location</span><span class="o">=</span><span class="n">_BufferCommHookLocation</span><span class="o">.</span><span class="n">POST_FORWARD</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Allows custom registration of hooks that define how buffer are</span>
<span class="sd">        synchronized across ranks. The hook takes in an optional state</span>
<span class="sd">        and is passed in a Dict[str, Tensor] corresponding to buffer names</span>
<span class="sd">        and the buffers, and can run arbitrary reductions on buffers as</span>
<span class="sd">        opposed to DDP&#39;s default broadcast from rank 0. This is useful for</span>
<span class="sd">        example if a counter needs to be summed or averaged across ranks</span>
<span class="sd">        every iteration.</span>

<span class="sd">        Args:</span>
<span class="sd">            state (Any): Optional state that is passed to the hook.</span>
<span class="sd">            hook (Callable): Callable with the following signature:</span>
<span class="sd">                         ``hook(state: object, bucket: dist.GradBucket) -&gt; torch.futures.Future[torch.Tensor]``</span>
<span class="sd">            comm_hook_location (_BufferCommHookLocation): Enum value indicating</span>
<span class="sd">                            where to run the hook.</span>
<span class="sd">                            _BufferCommHookLocation.PRE_FORWARD means that the</span>
<span class="sd">                            hook will run _before_ the forward pass, and</span>
<span class="sd">                            _BufferCommHookLocation.POST_FORWARD means that the</span>
<span class="sd">                            hook will run _after_ the forward pass.</span>

<span class="sd">            NOTE: To maximize performance, users can return a</span>
<span class="sd">                List[torch.futures.Future] from their hook, and DDP will</span>
<span class="sd">                install and await these hooks appropriately at the end of</span>
<span class="sd">                the backward pass. This will ensure all buffers are</span>
<span class="sd">                synchronized by the end of the backward pass. If this</span>
<span class="sd">                setting is used, it is recommended to pass</span>
<span class="sd">                comm_hook_location=_BufferCommHookLocation.POST_FORWARD,</span>
<span class="sd">                which will trigger the hook after the forward pass.</span>
<span class="sd">                If _BufferCommHookLocation.PRE_FORWARD is used, users must</span>
<span class="sd">                ensure appropriate synchronization when manipulating GPU</span>
<span class="sd">                buffers in the forward pass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">callable</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_hook</span> <span class="o">=</span> <span class="n">_BufferCommHook</span><span class="p">(</span>
            <span class="n">buffer_comm_hook</span><span class="o">=</span><span class="n">hook</span><span class="p">,</span>
            <span class="n">buffer_comm_hook_state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
            <span class="n">buffer_comm_hook_location</span><span class="o">=</span><span class="n">comm_hook_location</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DistributedDataParallel.register_comm_hook"><a class="viewcode-back" href="../../../../generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.register_comm_hook">[docs]</a>    <span class="k">def</span> <span class="nf">register_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers a communication hook which is an enhancement that provides a</span>
<span class="sd">        flexible hook to users where they can specify how DDP aggregates gradients</span>
<span class="sd">        across multiple workers.</span>

<span class="sd">        This hook would be very useful for researchers to try out new ideas. For</span>
<span class="sd">        example, this hook can be used to implement several algorithms like GossipGrad</span>
<span class="sd">        and gradient compression which involve different communication strategies for</span>
<span class="sd">        parameter syncs while running Distributed DataParallel training.</span>

<span class="sd">        Args:</span>
<span class="sd">            state (object): Passed to the hook to maintain any state information during the training process.</span>
<span class="sd">                            Examples include error feedback in gradient compression,</span>
<span class="sd">                            peers to communicate with next in GossipGrad, etc.</span>

<span class="sd">                            It is locally stored by each worker</span>
<span class="sd">                            and shared by all the gradient tensors on the worker.</span>
<span class="sd">            hook (Callable): Callable with the following signature:</span>
<span class="sd">                             ``hook(state: object, bucket: dist.GradBucket) -&gt; torch.futures.Future[torch.Tensor]``:</span>

<span class="sd">                             This function is called once the bucket is ready. The</span>
<span class="sd">                             hook can perform whatever processing is needed and return</span>
<span class="sd">                             a Future indicating completion of any async work (ex: allreduce).</span>
<span class="sd">                             If the hook doesn&#39;t perform any communication, it still</span>
<span class="sd">                             must return a completed Future. The Future should hold the</span>
<span class="sd">                             new value of grad bucket&#39;s tensors. Once a bucket is ready,</span>
<span class="sd">                             c10d reducer would call this hook and use the tensors returned</span>
<span class="sd">                             by the Future and copy grads to individual parameters.</span>
<span class="sd">                             Note that the future&#39;s return type must be a single tensor.</span>

<span class="sd">                             We also provide an API called ``get_future`` to retrieve a</span>
<span class="sd">                             Future associated with the completion of ``c10d.ProcessGroup.Work``.</span>
<span class="sd">                             ``get_future`` is currently supported for NCCL and also supported for most</span>
<span class="sd">                             operations on GLOO and MPI, except for peer to peer operations (send/recv).</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            Grad bucket&#39;s tensors will not be predivided by world_size. User is responsible</span>
<span class="sd">            to divide by the world_size in case of operations like allreduce.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            DDP communication hook can only be registered once and should be registered</span>
<span class="sd">            before calling backward.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            The Future object that hook returns should contain a single tensor</span>
<span class="sd">            that has the same shape with the tensors inside grad bucket.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            ``get_future`` API supports NCCL, and partially GLOO and MPI backends (no support</span>
<span class="sd">            for peer-to-peer operations like send/recv) and will return a ``torch.futures.Future``.</span>

<span class="sd">        Example::</span>
<span class="sd">            Below is an example of a noop hook that returns the same tensor.</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&#39;undefined name&#39;)</span>
<span class="sd">            &gt;&gt;&gt; def noop(state: object, bucket: dist.GradBucket) -&gt; torch.futures.Future[torch.Tensor]:</span>
<span class="sd">            &gt;&gt;&gt;     fut = torch.futures.Future()</span>
<span class="sd">            &gt;&gt;&gt;     fut.set_result(bucket.buffer())</span>
<span class="sd">            &gt;&gt;&gt;     return fut</span>
<span class="sd">            &gt;&gt;&gt; ddp.register_comm_hook(state=None, hook=noop)</span>

<span class="sd">        Example::</span>
<span class="sd">            Below is an example of a Parallel SGD algorithm where gradients are encoded before</span>
<span class="sd">            allreduce, and then decoded after allreduce.</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&#39;undefined name&#39;)</span>
<span class="sd">            &gt;&gt;&gt; def encode_and_decode(state: object, bucket: dist.GradBucket) -&gt; torch.futures.Future[torch.Tensor]:</span>
<span class="sd">            &gt;&gt;&gt;     encoded_tensor = encode(bucket.buffer())  # encode gradients</span>
<span class="sd">            &gt;&gt;&gt;     fut = torch.distributed.all_reduce(encoded_tensor).get_future()</span>
<span class="sd">            &gt;&gt;&gt;     # Define the then callback to decode.</span>
<span class="sd">            &gt;&gt;&gt;     def decode(fut):</span>
<span class="sd">            &gt;&gt;&gt;         decoded_tensor = decode(fut.value()[0])  # decode gradients</span>
<span class="sd">            &gt;&gt;&gt;         return decoded_tensor</span>
<span class="sd">            &gt;&gt;&gt;     return fut.then(decode)</span>
<span class="sd">            &gt;&gt;&gt; ddp.register_comm_hook(state=None, hook=encode_and_decode)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_comm_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_comm_hook_name</span><span class="p">(</span><span class="n">hook</span><span class="o">.</span><span class="vm">__qualname__</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_register_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_register_builtin_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">comm_hook_type</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers a built-in communication hook that specifies how DDP</span>
<span class="sd">        aggregates gradients across multiple workers.</span>
<span class="sd">        The built-in hooks aim to provide efficient C++ implementations for certain hooks,</span>
<span class="sd">        which might not be as efficient if implemented in Python using a Python communication hook.</span>

<span class="sd">        Args:</span>
<span class="sd">            comm_hook_type (dist.BuiltinCommHookType): type of communication hook, such as ALLREDUCE, FP16_COMPRESS, etc.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            DDP communication hook can only be registered once and should be registered</span>
<span class="sd">            before calling backward.</span>

<span class="sd">        Example::</span>
<span class="sd">            Below is an example of a FP16 compression where gradients are</span>
<span class="sd">            compressed into 16-bit floating-point numbers before allreduce, and</span>
<span class="sd">            then decompressed after allreduce.</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&#39;undefined name&#39;)</span>
<span class="sd">            &gt;&gt;&gt; ddp._register_builtin_comm_hook(dist.BuiltinCommHookType.FP16_COMPRESS)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_comm_hook_name</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">comm_hook_type</span><span class="p">))</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_register_builtin_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="p">,</span> <span class="n">comm_hook_type</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_register_fused_optim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optim</span><span class="p">:</span> <span class="n">Type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">optim_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers an optimizer with DDP such that the optimization for a</span>
<span class="sd">        parameter will run immediately when that parameter&#39;s gradient is</span>
<span class="sd">        finished with reduction, instead of waiting for all parameters&#39;</span>
<span class="sd">        gradients to finish reduction. This can result in a training speedup</span>
<span class="sd">        depending on your workload since the optimizer can run while gradient</span>
<span class="sd">        reduction for other parameters are still ongoing. In addition, this has</span>
<span class="sd">        the potential to reduce peak memory consumption during training, as it</span>
<span class="sd">        only needs to load the per-parameter optimizer states of a single</span>
<span class="sd">        parameter at a time, instead of loading all per-parameter optimizer</span>
<span class="sd">        states at once.</span>

<span class="sd">        Args:</span>
<span class="sd">            optim (Type): a ``torch.optim.Optimizer`` class to be registered</span>
<span class="sd">            as a fused optimizer.</span>
<span class="sd">            *args (Sequence[Any]): Arguments to forward to `optim`.</span>
<span class="sd">            optim_params (Optional[Iterable[torch.Tensor]]): Set of parameters</span>
<span class="sd">            to optimize, similar to `params` argument of traditional `torch.optim`</span>
<span class="sd">            Optimizers. If this is omitted, all DDP model parameters will be</span>
<span class="sd">            optimized.</span>
<span class="sd">            **kwargs: (Dict[str, Any]): Keyword arguments to forward to `optim`.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            _register_fused_optim should only be called once on a DDP instance,</span>
<span class="sd">            and registering multiple fused optimizers for the same DDP model</span>
<span class="sd">            is not currently supported. Please ping</span>
<span class="sd">            https://github.com/pytorch/pytorch/issues/71595 if this is necessary</span>
<span class="sd">            for your use case.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            _register_fused_optim and register_comm_hook currently do not</span>
<span class="sd">            compose together, meaning that custom DDP communication hooks are</span>
<span class="sd">            not supported with overlapped optimizers. Please ping</span>
<span class="sd">            https://github.com/pytorch/pytorch/issues/71595 if this is necessary</span>
<span class="sd">            for your use case.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            Gradient accumulation and DDP `no_sync` are currently not supported</span>
<span class="sd">            with overlapped optimizer. Please ping</span>
<span class="sd">            https://github.com/pytorch/pytorch/issues/71595 if this is necessary</span>
<span class="sd">            for your use case.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;No rendezvous handler&quot;)</span>
<span class="sd">            &gt;&gt;&gt; torch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=4, init_method=&#39;...&#39;)</span>
<span class="sd">            &gt;&gt;&gt; net = torch.nn.parallel.DistributedDataParallel(model, pg)</span>
<span class="sd">            &gt;&gt;&gt; lr = 1e-2</span>
<span class="sd">            &gt;&gt;&gt; betas = (0.9, 0.99)</span>
<span class="sd">            &gt;&gt;&gt; eps = 1e-6</span>
<span class="sd">            &gt;&gt;&gt; net._register_fused_optim(torch.optim.Adam, lr, betas=betas, eps=eps)</span>
<span class="sd">            &gt;&gt;&gt; # Example with subset of parameters</span>
<span class="sd">            &gt;&gt;&gt; params_to_opt = [list(net.parameters())[0]]</span>
<span class="sd">            &gt;&gt;&gt; net._register_fused_optim(</span>
<span class="sd">            ...   torch.optim.Adam, lr, optim_params=params_to_opt,  betas=betas, eps=eps</span>
<span class="sd">            ... )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Note: importing in function, otherwise this will cause a circular</span>
        <span class="c1"># import as optimizer_overlap module needs to import DistributedDataParallel.</span>
        <span class="kn">from</span> <span class="nn">torch.distributed.algorithms._optimizer_overlap</span> <span class="kn">import</span> <span class="n">_as_overlapped_optim</span>

        <span class="n">overlapped_optim</span> <span class="o">=</span> <span class="n">_as_overlapped_optim</span><span class="p">(</span><span class="n">optim</span><span class="p">,</span> <span class="n">optim_params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">overlapped_optim</span><span class="o">.</span><span class="n">register_ddp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">NotImplementedError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">optim</span><span class="si">}</span><span class="s2"> does not support overlapped DDP. Please file an issue to PyTorch or the respective owner of </span><span class="si">{</span><span class="n">optim</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

    <span class="k">def</span> <span class="nf">_distributed_broadcast_coalesced</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">authoritative_rank</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">):</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_broadcast_coalesced</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">authoritative_rank</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_sync_bufs_post_fwd</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">will_sync_module_buffers</span><span class="p">()</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;buffer_hook&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_hook</span><span class="o">.</span><span class="n">buffer_comm_hook_location</span>
            <span class="o">==</span> <span class="n">_BufferCommHookLocation</span><span class="o">.</span><span class="n">POST_FORWARD</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_sync_bufs_pre_fwd</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">will_sync_module_buffers</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;buffer_hook&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_hook</span><span class="o">.</span><span class="n">buffer_comm_hook_location</span>
            <span class="o">==</span> <span class="n">_BufferCommHookLocation</span><span class="o">.</span><span class="n">PRE_FORWARD</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">will_sync_module_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">require_forward_param_sync</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_find_common_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_rank</span><span class="p">,</span> <span class="n">rank_cond</span><span class="p">):</span>
        <span class="c1"># -1 indicates that this rank is not under consideration to be the</span>
        <span class="c1"># common_rank</span>
        <span class="n">rank_to_use</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">input_rank</span> <span class="k">if</span> <span class="n">rank_cond</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">rank_to_use</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rank_to_use</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">,</span>
                <span class="s2">&quot;BUG! Expected rank_cond to be true for at least one process.&quot;</span>
                <span class="s2">&quot; This indicates a bug in PyTorch, please report an issue.&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">rank_to_use</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_sync_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># module buffer sync</span>
            <span class="c1"># Synchronize buffers across processes.</span>
            <span class="c1"># If we are running DDP with the join manager, we have to agree</span>
            <span class="c1"># upon a rank to sync module buffers from, since rank 0 may</span>
            <span class="c1"># already have been joined and have stale module buffers.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_join_config</span><span class="o">.</span><span class="n">enable</span><span class="p">:</span>
                <span class="n">authoritative_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_common_rank</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_rank</span><span class="p">,</span> <span class="kc">True</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># The process with rank 0 is considered the authoritative copy.</span>
                <span class="n">authoritative_rank</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Update self.modules_buffers incase any buffers were</span>
            <span class="c1"># reassigned.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assign_modules_buffers</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_module_buffers</span><span class="p">(</span><span class="n">authoritative_rank</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sync_module_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">authoritative_rank</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;buffer_hook&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_default_broadcast_coalesced</span><span class="p">(</span><span class="n">authoritative_rank</span><span class="o">=</span><span class="n">authoritative_rank</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hook</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_hook</span><span class="o">.</span><span class="n">buffer_comm_hook</span>
            <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_hook</span><span class="o">.</span><span class="n">buffer_comm_hook_state</span>
            <span class="n">futs</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_module_buffers</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">futs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_install_post_backward_futures</span><span class="p">(</span><span class="n">futs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_default_broadcast_coalesced</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">bufs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bucket_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">authoritative_rank</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Broadcasts buffers from rank 0 to rest of workers. If bufs, bucket_size</span>
<span class="sd">        are None, default values self.modules_buffers and</span>
<span class="sd">        self.broadcast_bucket_size are used instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">bufs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">bufs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers</span>
        <span class="k">if</span> <span class="n">bucket_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">bucket_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_distributed_broadcast_coalesced</span><span class="p">(</span><span class="n">bufs</span><span class="p">,</span> <span class="n">bucket_size</span><span class="p">,</span> <span class="n">authoritative_rank</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_passing_sync_batchnorm_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                        <span class="ne">ValueError</span><span class="p">,</span>
                        <span class="s2">&quot;SyncBatchNorm layers only work with GPU modules&quot;</span><span class="p">,</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">hook</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="s2">&quot;Communication hook must be callable.&quot;</span><span class="p">)</span>

        <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;bucket&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">annotation</span> <span class="o">!=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">_empty</span>
            <span class="ow">and</span> <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;bucket&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">annotation</span> <span class="o">!=</span> <span class="n">dist</span><span class="o">.</span><span class="n">GradBucket</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">,</span>
                <span class="s2">&quot;Communication hook: bucket annotation should be dist.GradBucket.&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">sig</span><span class="o">.</span><span class="n">return_annotation</span> <span class="o">!=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">_empty</span>
            <span class="ow">and</span> <span class="n">sig</span><span class="o">.</span><span class="n">return_annotation</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">Future</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">,</span>
                <span class="s2">&quot;Communication hook: return annotation should be torch.futures.Future[torch.Tensor].&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">hook</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;bf16_compress_hook&quot;</span><span class="p">,</span>
            <span class="s2">&quot;bf16_compress_wrapper_hook&quot;</span><span class="p">,</span>
        <span class="p">]</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mi">11</span>
            <span class="p">)</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_nccl_available</span><span class="p">()</span>
            <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">nccl</span><span class="o">.</span><span class="n">version</span><span class="p">()</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">TypeError</span><span class="p">,</span>
                <span class="s2">&quot;BF16 all reduce communication hook required CUDA 11+ and NCCL 2.10+.&quot;</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_distributed_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_data_parallel_params</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">named_params</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a generator of parameters managed by a given DDP unit.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">named_params</span> <span class="k">else</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s2">&quot;_ddp_ignored&quot;</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">param</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_params_and_buffers_to_ignore_for_model</span><span class="p">(</span>
        <span class="n">module</span><span class="p">,</span> <span class="n">params_and_buffers_to_ignore</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets parameters and buffers to be ignored by DDP. Expected format for</span>
<span class="sd">        parameters is the fully qualified name: {module_name}.{param_name}, and</span>
<span class="sd">        similarly, {module_name}.{buffer_name} for buffers. For example:</span>
<span class="sd">        params_to_ignore = []</span>
<span class="sd">        # NB: model here is vanilla PyTorch module, not yet wrapped with DDP.</span>
<span class="sd">        for module_name, module in model.named_modules():</span>
<span class="sd">            for param_name, param in module.named_parameters(recurse=False):</span>
<span class="sd">                if should_ignore(param):</span>
<span class="sd">                    # Create expected format</span>
<span class="sd">                    fqn = f&quot;{module_name}.{param_name}&quot;</span>
<span class="sd">                    params_to_ignore.append(fqn)</span>
<span class="sd">        torch.nn.parallel.DistributedDataParallel._set_params_and_buffers_to_ignore_for_model(</span>
<span class="sd">            model,</span>
<span class="sd">            params_to_ignore</span>
<span class="sd">        )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># This is a workaround to set parameters and buffers DDP should ignore</span>
        <span class="c1"># during synchronization. It will be removed when the API is finalized</span>
        <span class="c1"># as part of addressing https://github.com/pytorch/pytorch/issues/43690.</span>
        <span class="n">module</span><span class="o">.</span><span class="n">_ddp_params_and_buffers_to_ignore</span> <span class="o">=</span> <span class="n">params_and_buffers_to_ignore</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">params_and_buffers_to_ignore</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">_ddp_ignored</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">params_and_buffers_to_ignore</span><span class="p">:</span>
                <span class="n">buffer</span><span class="o">.</span><span class="n">_ddp_ignored</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_get_ddp_logging_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This interface can be called after DistributedDataParallel() is</span>
<span class="sd">        constructed. It returns a dictionary of logging data. It could help</span>
<span class="sd">        for debugging and analysis. The logging data includes DistributedDataParallel</span>
<span class="sd">        constructor input parameters, some internal states of DistributedDataParallel</span>
<span class="sd">        and performance metrics. Simply print the dictionary and see what</span>
<span class="sd">        these metrics are.</span>
<span class="sd">        This is a prototype interface and subject to change in the future.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">ddp_logging_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_get_ddp_logging_data</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">ddp_logging_data</span><span class="o">.</span><span class="n">strs_map</span><span class="p">,</span> <span class="o">**</span><span class="n">ddp_logging_data</span><span class="o">.</span><span class="n">ints_map</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">_set_ddp_runtime_logging_sample_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This interface allows users to set sample_rate of collecting</span>
<span class="sd">        runtime stats. The runtime stats will be recorded for the</span>
<span class="sd">        first 10 iterations, after 10 iterations runtime stats will be</span>
<span class="sd">        recorded once every &quot;sample_rate&quot; training iterations. In</span>
<span class="sd">        default, runtime stats are recorded for the first 10 iterations,</span>
<span class="sd">        after 10 iterations runtime stats are recorded once every</span>
<span class="sd">        &quot;kDDPRuntimeLoggingSampleRate=100&quot; training iterations.</span>
<span class="sd">        This is a prototype interface and subject to change in the future.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample_rate</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_and_throw</span><span class="p">(</span>
                <span class="ne">ValueError</span><span class="p">,</span>
                <span class="s2">&quot;DDP runtime logging sample rate should be equal or greater than 1&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_ddp_runtime_logging_sample_rate</span><span class="p">(</span><span class="n">sample_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_static_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        It is recommended to set static graph in the DDP constructor, which will</span>
<span class="sd">        call this private API internally.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If self.static_graph has been set, no need to set it again</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You&#39;ve set static_graph to be True, no need to set it again.&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">static_graph</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_static_graph_delay_allreduce_enqueued</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_static_graph</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">_set_static_graph</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;You passed find_unused_parameters=true to DistributedDataParallel, &quot;</span>
                <span class="s2">&quot;`_set_static_graph` will detect unused parameters automatically, so &quot;</span>
                <span class="s2">&quot;you do not need to set find_unused_parameters=true, just be sure these &quot;</span>
                <span class="s2">&quot;unused parameters will not change during training loop while calling &quot;</span>
                <span class="s2">&quot;`_set_static_graph`.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_remove_autograd_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Removes autograd hooks registered by the reducer on the model parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_remove_autograd_hooks</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_check_reducer_finalized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks if the reducer has processed all buckets and finalized the backward</span>
<span class="sd">        appropriately.</span>

<span class="sd">        It is useful to call this method after calling .backward() in your training loop</span>
<span class="sd">        in order to avoid subsequent hard to debug errors down the road due to the</span>
<span class="sd">        reducer not finalizing backward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_check_reducer_finalized</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_set_sparse_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_unique_ids</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reducer</span><span class="o">.</span><span class="n">_set_sparse_metadata</span><span class="p">(</span><span class="n">global_unique_ids</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
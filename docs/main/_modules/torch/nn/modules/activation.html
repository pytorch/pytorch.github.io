


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.modules.activation &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.modules.activation</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.modules.activation</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">.linear</span> <span class="kn">import</span> <span class="n">NonDynamicallyQuantizableLinear</span>
<span class="kn">from</span> <span class="nn">torch.nn.init</span> <span class="kn">import</span> <span class="n">constant_</span><span class="p">,</span> <span class="n">xavier_normal_</span><span class="p">,</span> <span class="n">xavier_uniform_</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">.module</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Threshold&#39;</span><span class="p">,</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="s1">&#39;RReLU&#39;</span><span class="p">,</span> <span class="s1">&#39;Hardtanh&#39;</span><span class="p">,</span> <span class="s1">&#39;ReLU6&#39;</span><span class="p">,</span> <span class="s1">&#39;Sigmoid&#39;</span><span class="p">,</span> <span class="s1">&#39;Hardsigmoid&#39;</span><span class="p">,</span> <span class="s1">&#39;Tanh&#39;</span><span class="p">,</span>
           <span class="s1">&#39;SiLU&#39;</span><span class="p">,</span> <span class="s1">&#39;Mish&#39;</span><span class="p">,</span> <span class="s1">&#39;Hardswish&#39;</span><span class="p">,</span> <span class="s1">&#39;ELU&#39;</span><span class="p">,</span> <span class="s1">&#39;CELU&#39;</span><span class="p">,</span> <span class="s1">&#39;SELU&#39;</span><span class="p">,</span> <span class="s1">&#39;GLU&#39;</span><span class="p">,</span> <span class="s1">&#39;GELU&#39;</span><span class="p">,</span> <span class="s1">&#39;Hardshrink&#39;</span><span class="p">,</span> <span class="s1">&#39;LeakyReLU&#39;</span><span class="p">,</span>
           <span class="s1">&#39;LogSigmoid&#39;</span><span class="p">,</span> <span class="s1">&#39;Softplus&#39;</span><span class="p">,</span> <span class="s1">&#39;Softshrink&#39;</span><span class="p">,</span> <span class="s1">&#39;MultiheadAttention&#39;</span><span class="p">,</span> <span class="s1">&#39;PReLU&#39;</span><span class="p">,</span> <span class="s1">&#39;Softsign&#39;</span><span class="p">,</span> <span class="s1">&#39;Tanhshrink&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Softmin&#39;</span><span class="p">,</span> <span class="s1">&#39;Softmax&#39;</span><span class="p">,</span> <span class="s1">&#39;Softmax2d&#39;</span><span class="p">,</span> <span class="s1">&#39;LogSoftmax&#39;</span><span class="p">]</span>


<div class="viewcode-block" id="Threshold"><a class="viewcode-back" href="../../../../generated/torch.nn.Threshold.html#torch.nn.Threshold">[docs]</a><span class="k">class</span> <span class="nc">Threshold</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Thresholds each element of the input Tensor.</span>

<span class="sd">    Threshold is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x, &amp;\text{ if } x &gt; \text{threshold} \\</span>
<span class="sd">        \text{value}, &amp;\text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        threshold: The value to threshold at</span>
<span class="sd">        value: The value to replace with</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Threshold(0.1, 20)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;threshold&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="s1">&#39;inplace&#39;</span><span class="p">]</span>

    <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">value</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>
        <span class="c1"># TODO: check in THNN (if inplace == True, then assert value &lt;= threshold)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;, inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="s1">&#39;threshold=</span><span class="si">{}</span><span class="s1">, value=</span><span class="si">{}{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">inplace_str</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../../generated/torch.nn.ReLU.html#torch.nn.ReLU">[docs]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the rectified linear unit function element-wise:</span>

<span class="sd">    :math:`\text{ReLU}(x) = (x)^+ = \max(0, x)`</span>

<span class="sd">    Args:</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/ReLU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ReLU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>


<span class="sd">      An implementation of CReLU - https://arxiv.org/abs/1603.05201</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ReLU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2).unsqueeze(0)</span>
<span class="sd">        &gt;&gt;&gt; output = torch.cat((m(input), m(-input)))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;inplace&#39;</span><span class="p">]</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="n">inplace_str</span></div>


<div class="viewcode-block" id="RReLU"><a class="viewcode-back" href="../../../../generated/torch.nn.RReLU.html#torch.nn.RReLU">[docs]</a><span class="k">class</span> <span class="nc">RReLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the randomized leaky rectified liner unit function, element-wise,</span>
<span class="sd">    as described in the paper:</span>

<span class="sd">    `Empirical Evaluation of Rectified Activations in Convolutional Network`_.</span>

<span class="sd">    The function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{RReLU}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            x &amp; \text{if } x \geq 0 \\</span>
<span class="sd">            ax &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`a` is randomly sampled from uniform distribution</span>
<span class="sd">    :math:`\mathcal{U}(\text{lower}, \text{upper})` during training while during</span>
<span class="sd">    evaluation :math:`a` is fixed with :math:`a = \frac{\text{lower} + \text{upper}}{2}`.</span>

<span class="sd">     See: https://arxiv.org/pdf/1505.00853.pdf</span>

<span class="sd">    Args:</span>
<span class="sd">        lower: lower bound of the uniform distribution. Default: :math:`\frac{1}{8}`</span>
<span class="sd">        upper: upper bound of the uniform distribution. Default: :math:`\frac{1}{3}`</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/RReLU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.RReLU(0.1, 0.3)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _`Empirical Evaluation of Rectified Activations in Convolutional Network`:</span>
<span class="sd">        https://arxiv.org/abs/1505.00853</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span> <span class="s1">&#39;upper&#39;</span><span class="p">,</span> <span class="s1">&#39;inplace&#39;</span><span class="p">]</span>

    <span class="n">lower</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">upper</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lower</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">upper</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">lower</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upper</span> <span class="o">=</span> <span class="n">upper</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lower</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">upper</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;, inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="s1">&#39;lower=</span><span class="si">{}</span><span class="s1">, upper=</span><span class="si">{}{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lower</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">upper</span><span class="p">,</span> <span class="n">inplace_str</span><span class="p">)</span></div>


<div class="viewcode-block" id="Hardtanh"><a class="viewcode-back" href="../../../../generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh">[docs]</a><span class="k">class</span> <span class="nc">Hardtanh</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the HardTanh function element-wise.</span>

<span class="sd">    HardTanh is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{HardTanh}(x) = \begin{cases}</span>
<span class="sd">            \text{max\_val} &amp; \text{ if } x &gt; \text{ max\_val } \\</span>
<span class="sd">            \text{min\_val} &amp; \text{ if } x &lt; \text{ min\_val } \\</span>
<span class="sd">            x &amp; \text{ otherwise } \\</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        min_val: minimum value of the linear region range. Default: -1</span>
<span class="sd">        max_val: maximum value of the linear region range. Default: 1</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Keyword arguments :attr:`min_value` and :attr:`max_value`</span>
<span class="sd">    have been deprecated in favor of :attr:`min_val` and :attr:`max_val`.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Hardtanh.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Hardtanh(-2, 2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;min_val&#39;</span><span class="p">,</span> <span class="s1">&#39;max_val&#39;</span><span class="p">,</span> <span class="s1">&#39;inplace&#39;</span><span class="p">]</span>

    <span class="n">min_val</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">max_val</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">min_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span>
        <span class="n">max_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span>
        <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">min_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">min_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;keyword argument min_value is deprecated and rename to min_val&quot;</span><span class="p">)</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="n">min_value</span>
        <span class="k">if</span> <span class="n">max_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;keyword argument max_value is deprecated and rename to max_val&quot;</span><span class="p">)</span>
            <span class="n">max_val</span> <span class="o">=</span> <span class="n">max_value</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span> <span class="o">=</span> <span class="n">min_val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">=</span> <span class="n">max_val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;, inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="s1">&#39;min_val=</span><span class="si">{}</span><span class="s1">, max_val=</span><span class="si">{}{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">,</span> <span class="n">inplace_str</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="ReLU6"><a class="viewcode-back" href="../../../../generated/torch.nn.ReLU6.html#torch.nn.ReLU6">[docs]</a><span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">Hardtanh</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the element-wise function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{ReLU6}(x) = \min(\max(0,x), 6)</span>

<span class="sd">    Args:</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/ReLU6.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ReLU6()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="n">inplace_str</span></div>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../../generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the element-wise function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}</span>


<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Sigmoid.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Sigmoid()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="Hardsigmoid"><a class="viewcode-back" href="../../../../generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid">[docs]</a><span class="k">class</span> <span class="nc">Hardsigmoid</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Hardsigmoid function element-wise.</span>

<span class="sd">    Hardsigmoid is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Hardsigmoid}(x) = \begin{cases}</span>
<span class="sd">            0 &amp; \text{if~} x \le -3, \\</span>
<span class="sd">            1 &amp; \text{if~} x \ge +3, \\</span>
<span class="sd">            x / 6 + 1 / 2 &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Hardsigmoid.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Hardsigmoid()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;inplace&#39;</span><span class="p">]</span>

    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">hardsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../../../../generated/torch.nn.Tanh.html#torch.nn.Tanh">[docs]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Hyperbolic Tangent (Tanh) function element-wise.</span>

<span class="sd">    Tanh is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Tanh.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Tanh()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>

<div class="viewcode-block" id="SiLU"><a class="viewcode-back" href="../../../../generated/torch.nn.SiLU.html#torch.nn.SiLU">[docs]</a><span class="k">class</span> <span class="nc">SiLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Sigmoid Linear Unit (SiLU) function, element-wise.</span>
<span class="sd">    The SiLU function is also known as the swish function.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}</span>

<span class="sd">    .. note::</span>
<span class="sd">        See `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_</span>
<span class="sd">        where the SiLU (Sigmoid Linear Unit) was originally coined, and see</span>
<span class="sd">        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation</span>
<span class="sd">        in Reinforcement Learning &lt;https://arxiv.org/abs/1702.03118&gt;`_ and `Swish:</span>
<span class="sd">        a Self-Gated Activation Function &lt;https://arxiv.org/abs/1710.05941v1&gt;`_</span>
<span class="sd">        where the SiLU was experimented with later.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/SiLU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.SiLU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;inplace&#39;</span><span class="p">]</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="n">inplace_str</span></div>

<div class="viewcode-block" id="Mish"><a class="viewcode-back" href="../../../../generated/torch.nn.Mish.html#torch.nn.Mish">[docs]</a><span class="k">class</span> <span class="nc">Mish</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Mish function, element-wise.</span>
<span class="sd">    Mish: A Self Regularized Non-Monotonic Neural Activation Function.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Mish}(x) = x * \text{Tanh}(\text{Softplus}(x))</span>

<span class="sd">    .. note::</span>
<span class="sd">        See `Mish: A Self Regularized Non-Monotonic Neural Activation Function &lt;https://arxiv.org/abs/1908.08681&gt;`_</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Mish.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Mish()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;inplace&#39;</span><span class="p">]</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mish</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="n">inplace_str</span></div>

<div class="viewcode-block" id="Hardswish"><a class="viewcode-back" href="../../../../generated/torch.nn.Hardswish.html#torch.nn.Hardswish">[docs]</a><span class="k">class</span> <span class="nc">Hardswish</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Hardswish function, element-wise, as described in the paper:</span>
<span class="sd">    `Searching for MobileNetV3 &lt;https://arxiv.org/abs/1905.02244&gt;`_.</span>

<span class="sd">    Hardswish is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Hardswish}(x) = \begin{cases}</span>
<span class="sd">            0 &amp; \text{if~} x \le -3, \\</span>
<span class="sd">            x &amp; \text{if~} x \ge +3, \\</span>
<span class="sd">            x \cdot (x + 3) /6 &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Hardswish.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Hardswish()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;inplace&#39;</span><span class="p">]</span>

    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">hardswish</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="ELU"><a class="viewcode-back" href="../../../../generated/torch.nn.ELU.html#torch.nn.ELU">[docs]</a><span class="k">class</span> <span class="nc">ELU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Exponential Linear Unit (ELU) function, element-wise, as described</span>
<span class="sd">    in the paper: `Fast and Accurate Deep Network Learning by Exponential Linear</span>
<span class="sd">    Units (ELUs) &lt;https://arxiv.org/abs/1511.07289&gt;`__.</span>

<span class="sd">    ELU is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{ELU}(x) = \begin{cases}</span>
<span class="sd">        x, &amp; \text{ if } x &gt; 0\\</span>
<span class="sd">        \alpha * (\exp(x) - 1), &amp; \text{ if } x \leq 0</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha: the :math:`\alpha` value for the ELU formulation. Default: 1.0</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/ELU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.ELU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;inplace&#39;</span><span class="p">]</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;, inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="s1">&#39;alpha=</span><span class="si">{}{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">inplace_str</span><span class="p">)</span></div>


<div class="viewcode-block" id="CELU"><a class="viewcode-back" href="../../../../generated/torch.nn.CELU.html#torch.nn.CELU">[docs]</a><span class="k">class</span> <span class="nc">CELU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the element-wise function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</span>

<span class="sd">    More details can be found in the paper `Continuously Differentiable Exponential Linear Units`_ .</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha: the :math:`\alpha` value for the CELU formulation. Default: 1.0</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/CELU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.CELU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _`Continuously Differentiable Exponential Linear Units`:</span>
<span class="sd">        https://arxiv.org/abs/1704.07483</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;inplace&#39;</span><span class="p">]</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">celu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;, inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="s1">&#39;alpha=</span><span class="si">{}{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">inplace_str</span><span class="p">)</span></div>


<div class="viewcode-block" id="SELU"><a class="viewcode-back" href="../../../../generated/torch.nn.SELU.html#torch.nn.SELU">[docs]</a><span class="k">class</span> <span class="nc">SELU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applied element-wise, as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</span>

<span class="sd">    with :math:`\alpha = 1.6732632423543772848170429916717` and</span>
<span class="sd">    :math:`\text{scale} = 1.0507009873554804934193349852946`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        When using ``kaiming_normal`` or ``kaiming_normal_`` for initialisation,</span>
<span class="sd">        ``nonlinearity=&#39;linear&#39;`` should be used instead of ``nonlinearity=&#39;selu&#39;``</span>
<span class="sd">        in order to get `Self-Normalizing Neural Networks`_.</span>
<span class="sd">        See :func:`torch.nn.init.calculate_gain` for more information.</span>

<span class="sd">    More details can be found in the paper `Self-Normalizing Neural Networks`_ .</span>

<span class="sd">    Args:</span>
<span class="sd">        inplace (bool, optional): can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/SELU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.SELU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;inplace&#39;</span><span class="p">]</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="n">inplace_str</span></div>


<div class="viewcode-block" id="GLU"><a class="viewcode-back" href="../../../../generated/torch.nn.GLU.html#torch.nn.GLU">[docs]</a><span class="k">class</span> <span class="nc">GLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the gated linear unit function</span>
<span class="sd">    :math:`{GLU}(a, b)= a \otimes \sigma(b)` where :math:`a` is the first half</span>
<span class="sd">    of the input matrices and :math:`b` is the second half.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int): the dimension on which to split the input. Default: -1</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(\ast_1, N, \ast_2)` where `*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Output: :math:`(\ast_1, M, \ast_2)` where :math:`M=N/2`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.GLU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(4, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dim&#39;</span><span class="p">]</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;dim=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="GELU"><a class="viewcode-back" href="../../../../generated/torch.nn.GELU.html#torch.nn.GELU">[docs]</a><span class="k">class</span> <span class="nc">GELU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Gaussian Error Linear Units function:</span>

<span class="sd">    .. math:: \text{GELU}(x) = x * \Phi(x)</span>

<span class="sd">    where :math:`\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.</span>

<span class="sd">    When the approximate argument is &#39;tanh&#39;, Gelu is estimated with:</span>

<span class="sd">    .. math:: \text{GELU}(x) = 0.5 * x * (1 + \text{Tanh}(\sqrt{2 / \pi} * (x + 0.044715 * x^3)))</span>

<span class="sd">    Args:</span>
<span class="sd">        approximate (str, optional): the gelu approximation algorithm to use:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;tanh&#39;``. Default: ``&#39;none&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/GELU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.GELU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;approximate&#39;</span><span class="p">]</span>
    <span class="n">approximate</span><span class="p">:</span> <span class="nb">str</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">approximate</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">approximate</span> <span class="o">=</span> <span class="n">approximate</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">approximate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;approximate=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">approximate</span><span class="p">))</span></div>


<div class="viewcode-block" id="Hardshrink"><a class="viewcode-back" href="../../../../generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink">[docs]</a><span class="k">class</span> <span class="nc">Hardshrink</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Hard Shrinkage (Hardshrink) function element-wise.</span>

<span class="sd">    Hardshrink is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{HardShrink}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">        x, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">        0, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        lambd: the :math:`\lambda` value for the Hardshrink formulation. Default: 0.5</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Hardshrink.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Hardshrink()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;lambd&#39;</span><span class="p">]</span>
    <span class="n">lambd</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">)</span></div>


<div class="viewcode-block" id="LeakyReLU"><a class="viewcode-back" href="../../../../generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU">[docs]</a><span class="k">class</span> <span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the element-wise function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</span>


<span class="sd">    or</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{LeakyReLU}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x, &amp; \text{ if } x \geq 0 \\</span>
<span class="sd">        \text{negative\_slope} \times x, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        negative_slope: Controls the angle of the negative slope (which is used for</span>
<span class="sd">          negative input values). Default: 1e-2</span>
<span class="sd">        inplace: can optionally do the operation in-place. Default: ``False``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where `*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input</span>

<span class="sd">    .. image:: ../scripts/activation_images/LeakyReLU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.LeakyReLU(0.1)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;inplace&#39;</span><span class="p">,</span> <span class="s1">&#39;negative_slope&#39;</span><span class="p">]</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">negative_slope</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">negative_slope</span> <span class="o">=</span> <span class="n">negative_slope</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative_slope</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">inplace_str</span> <span class="o">=</span> <span class="s1">&#39;, inplace=True&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">inplace</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="k">return</span> <span class="s1">&#39;negative_slope=</span><span class="si">{}{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">negative_slope</span><span class="p">,</span> <span class="n">inplace_str</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogSigmoid"><a class="viewcode-back" href="../../../../generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid">[docs]</a><span class="k">class</span> <span class="nc">LogSigmoid</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the element-wise function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/LogSigmoid.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.LogSigmoid()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">logsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="Softplus"><a class="viewcode-back" href="../../../../generated/torch.nn.Softplus.html#torch.nn.Softplus">[docs]</a><span class="k">class</span> <span class="nc">Softplus</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Softplus function :math:`\text{Softplus}(x) = \frac{1}{\beta} *</span>
<span class="sd">    \log(1 + \exp(\beta * x))` element-wise.</span>

<span class="sd">    SoftPlus is a smooth approximation to the ReLU function and can be used</span>
<span class="sd">    to constrain the output of a machine to always be positive.</span>

<span class="sd">    For numerical stability the implementation reverts to the linear function</span>
<span class="sd">    when :math:`input \times \beta &gt; threshold`.</span>

<span class="sd">    Args:</span>
<span class="sd">        beta: the :math:`\beta` value for the Softplus formulation. Default: 1</span>
<span class="sd">        threshold: values above this revert to a linear function. Default: 20</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Softplus.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Softplus()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;threshold&#39;</span><span class="p">]</span>
    <span class="n">beta</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">threshold</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;beta=</span><span class="si">{}</span><span class="s1">, threshold=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">)</span></div>


<div class="viewcode-block" id="Softshrink"><a class="viewcode-back" href="../../../../generated/torch.nn.Softshrink.html#torch.nn.Softshrink">[docs]</a><span class="k">class</span> <span class="nc">Softshrink</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the soft shrinkage function elementwise:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SoftShrinkage}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x - \lambda, &amp; \text{ if } x &gt; \lambda \\</span>
<span class="sd">        x + \lambda, &amp; \text{ if } x &lt; -\lambda \\</span>
<span class="sd">        0, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        lambd: the :math:`\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Softshrink.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Softshrink()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;lambd&#39;</span><span class="p">]</span>
    <span class="n">lambd</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_arg_device</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">backend_registration</span><span class="o">.</span><span class="n">_privateuse1_backend_name</span><span class="p">]</span>

    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_arg_requires_grad</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>

    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="MultiheadAttention"><a class="viewcode-back" href="../../../../generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Allows the model to jointly attend to information</span>
<span class="sd">    from different representation subspaces as described in the paper:</span>
<span class="sd">    `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_.</span>

<span class="sd">    Multi-Head Attention is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O</span>

<span class="sd">    where :math:`head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.</span>

<span class="sd">    ``nn.MultiHeadAttention`` will use the optimized implementations of</span>
<span class="sd">    ``scaled_dot_product_attention()`` when possible.</span>

<span class="sd">    In addition to support for the new ``scaled_dot_product_attention()``</span>
<span class="sd">    function, for speeding up Inference, MHA will use</span>
<span class="sd">    fastpath inference with support for Nested Tensors, iff:</span>

<span class="sd">    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).</span>
<span class="sd">    - inputs are batched (3D) with ``batch_first==True``</span>
<span class="sd">    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``</span>
<span class="sd">    - training is disabled (using ``.eval()``)</span>
<span class="sd">    - ``add_bias_kv`` is ``False``</span>
<span class="sd">    - ``add_zero_attn`` is ``False``</span>
<span class="sd">    - ``batch_first`` is ``True`` and the input is batched</span>
<span class="sd">    - ``kdim`` and ``vdim`` are equal to ``embed_dim``</span>
<span class="sd">    - if a `NestedTensor &lt;https://pytorch.org/docs/stable/nested.html&gt;`_ is passed, neither ``key_padding_mask``</span>
<span class="sd">      nor ``attn_mask`` is passed</span>
<span class="sd">    - autocast is disabled</span>

<span class="sd">    If the optimized inference fastpath implementation is in use, a</span>
<span class="sd">    `NestedTensor &lt;https://pytorch.org/docs/stable/nested.html&gt;`_ can be passed for</span>
<span class="sd">    ``query``/``key``/``value`` to represent padding more efficiently than using a</span>
<span class="sd">    padding mask. In this case, a `NestedTensor &lt;https://pytorch.org/docs/stable/nested.html&gt;`_</span>
<span class="sd">    will be returned, and an additional speedup proportional to the fraction of the input</span>
<span class="sd">    that is padding can be expected.</span>

<span class="sd">    Args:</span>
<span class="sd">        embed_dim: Total dimension of the model.</span>
<span class="sd">        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split</span>
<span class="sd">            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).</span>
<span class="sd">        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).</span>
<span class="sd">        bias: If specified, adds bias to input / output projection layers. Default: ``True``.</span>
<span class="sd">        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.</span>
<span class="sd">        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.</span>
<span class="sd">            Default: ``False``.</span>
<span class="sd">        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).</span>
<span class="sd">        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).</span>
<span class="sd">        batch_first: If ``True``, then the input and output tensors are provided</span>
<span class="sd">            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)</span>
<span class="sd">        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</span>

<span class="sd">    .. _`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness`:</span>
<span class="sd">         https://arxiv.org/abs/2205.14135</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;batch_first&#39;</span><span class="p">]</span>
    <span class="n">bias_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">bias_v</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_bias_kv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_zero_attn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">kdim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vdim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kdim</span> <span class="o">=</span> <span class="n">kdim</span> <span class="k">if</span> <span class="n">kdim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vdim</span> <span class="o">=</span> <span class="n">vdim</span> <span class="k">if</span> <span class="n">vdim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kdim</span> <span class="o">==</span> <span class="n">embed_dim</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">vdim</span> <span class="o">==</span> <span class="n">embed_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="s2">&quot;embed_dim must be divisible by num_heads&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_proj_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k_proj_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kdim</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v_proj_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vdim</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;in_proj_weight&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">3</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;q_proj_weight&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;k_proj_weight&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;v_proj_weight&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;in_proj_bias&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">NonDynamicallyQuantizableLinear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">add_bias_kv</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_k</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_v</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_v</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_zero_attn</span> <span class="o">=</span> <span class="n">add_zero_attn</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span><span class="p">:</span>
            <span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_proj_weight</span><span class="p">)</span>
            <span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj_weight</span><span class="p">)</span>
            <span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj_weight</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
            <span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">0.</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias_k</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias_v</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># Support loading old MultiheadAttention checkpoints generated by v1.1.0</span>
        <span class="k">if</span> <span class="s1">&#39;_qkv_same_embed_dim&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s1">&#39;_qkv_same_embed_dim&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="MultiheadAttention.forward"><a class="viewcode-back" href="../../../../generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">query</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">need_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">attn_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">average_attn_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">is_causal</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        query: Query embeddings of shape :math:`(L, E_q)` for unbatched input, :math:`(L, N, E_q)` when ``batch_first=False``</span>
<span class="sd">            or :math:`(N, L, E_q)` when ``batch_first=True``, where :math:`L` is the target sequence length,</span>
<span class="sd">            :math:`N` is the batch size, and :math:`E_q` is the query embedding dimension ``embed_dim``.</span>
<span class="sd">            Queries are compared against key-value pairs to produce the output.</span>
<span class="sd">            See &quot;Attention Is All You Need&quot; for more details.</span>
<span class="sd">        key: Key embeddings of shape :math:`(S, E_k)` for unbatched input, :math:`(S, N, E_k)` when ``batch_first=False``</span>
<span class="sd">            or :math:`(N, S, E_k)` when ``batch_first=True``, where :math:`S` is the source sequence length,</span>
<span class="sd">            :math:`N` is the batch size, and :math:`E_k` is the key embedding dimension ``kdim``.</span>
<span class="sd">            See &quot;Attention Is All You Need&quot; for more details.</span>
<span class="sd">        value: Value embeddings of shape :math:`(S, E_v)` for unbatched input, :math:`(S, N, E_v)` when</span>
<span class="sd">            ``batch_first=False`` or :math:`(N, S, E_v)` when ``batch_first=True``, where :math:`S` is the source</span>
<span class="sd">            sequence length, :math:`N` is the batch size, and :math:`E_v` is the value embedding dimension ``vdim``.</span>
<span class="sd">            See &quot;Attention Is All You Need&quot; for more details.</span>
<span class="sd">        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``</span>
<span class="sd">            to ignore for the purpose of attention (i.e. treat as &quot;padding&quot;). For unbatched `query`, shape should be :math:`(S)`.</span>
<span class="sd">            Binary and float masks are supported.</span>
<span class="sd">            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for</span>
<span class="sd">            the purpose of attention. For a float mask, it will be directly added to the corresponding ``key`` value.</span>
<span class="sd">        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.</span>
<span class="sd">            Set ``need_weights=False`` to use the optimized ``scaled_dot_product_attention``</span>
<span class="sd">            and achieve the best performance for MHA.</span>
<span class="sd">            Default: ``True``.</span>
<span class="sd">        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape</span>
<span class="sd">            :math:`(L, S)` or :math:`(N\cdot\text{num\_heads}, L, S)`, where :math:`N` is the batch size,</span>
<span class="sd">            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be</span>
<span class="sd">            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.</span>
<span class="sd">            Binary and float masks are supported. For a binary mask, a ``True`` value indicates that the</span>
<span class="sd">            corresponding position is not allowed to attend. For a float mask, the mask values will be added to</span>
<span class="sd">            the attention weight.</span>
<span class="sd">            If both attn_mask and key_padding_mask are supplied, their types should match.</span>
<span class="sd">        average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across</span>
<span class="sd">            heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an</span>
<span class="sd">            effect when ``need_weights=True``. Default: ``True`` (i.e. average weights across heads)</span>
<span class="sd">        is_causal: If specified, applies a causal mask as attention mask.</span>
<span class="sd">            Default: ``False``.</span>
<span class="sd">            Warning:</span>
<span class="sd">            ``is_causal`` provides a hint that ``attn_mask`` is the</span>
<span class="sd">            causal mask. Providing incorrect hints can result in</span>
<span class="sd">            incorrect execution, including forward and backward</span>
<span class="sd">            compatibility.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **attn_output** - Attention outputs of shape :math:`(L, E)` when input is unbatched,</span>
<span class="sd">          :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` when ``batch_first=True``,</span>
<span class="sd">          where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` is the</span>
<span class="sd">          embedding dimension ``embed_dim``.</span>
<span class="sd">        - **attn_output_weights** - Only returned when ``need_weights=True``. If ``average_attn_weights=True``,</span>
<span class="sd">          returns attention weights averaged across heads of shape :math:`(L, S)` when input is unbatched or</span>
<span class="sd">          :math:`(N, L, S)`, where :math:`N` is the batch size, :math:`L` is the target sequence length, and</span>
<span class="sd">          :math:`S` is the source sequence length. If ``average_attn_weights=False``, returns attention weights per</span>
<span class="sd">          head of shape :math:`(\text{num\_heads}, L, S)` when input is unbatched or :math:`(N, \text{num\_heads}, L, S)`.</span>

<span class="sd">        .. note::</span>
<span class="sd">            `batch_first` argument is ignored for unbatched inputs.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">is_batched</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span>

        <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">_canonical_mask</span><span class="p">(</span>
            <span class="n">mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
            <span class="n">mask_name</span><span class="o">=</span><span class="s2">&quot;key_padding_mask&quot;</span><span class="p">,</span>
            <span class="n">other_type</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">_none_or_dtype</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">),</span>
            <span class="n">other_name</span><span class="o">=</span><span class="s2">&quot;attn_mask&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

        <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">_canonical_mask</span><span class="p">(</span>
            <span class="n">mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
            <span class="n">mask_name</span><span class="o">=</span><span class="s2">&quot;attn_mask&quot;</span><span class="p">,</span>
            <span class="n">other_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">other_name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">target_type</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">check_other</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>


        <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_batched</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;input not batched; expected query.dim() of 3 but got </span><span class="si">{</span><span class="n">query</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">query</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">key</span> <span class="ow">or</span> <span class="n">key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">value</span><span class="p">:</span>
            <span class="c1"># When lifting this restriction, don&#39;t forget to either</span>
            <span class="c1"># enforce that the dtypes all match or test cases where</span>
            <span class="c1"># they don&#39;t!</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;non-self attention was used (query, key, and value are not the same Tensor)&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;dtypes of query (</span><span class="si">{</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">) and self.in_proj_bias (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">) don&#39;t match&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;in_proj_weight was None&quot;</span>
        <span class="k">elif</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="c1"># this case will fail anyway, but at least they&#39;ll get a useful error message.</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;dtypes of query (</span><span class="si">{</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">) and self.in_proj_weight (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">) don&#39;t match&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;training is enabled&quot;</span>
        <span class="k">elif</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;self.num_heads is not even&quot;</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;batch_first was not True&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;self.bias_k was not None&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;self.bias_v was not None&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_zero_attn</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;add_zero_attn was enabled&quot;</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span><span class="p">:</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;_qkv_same_embed_dim was not True&quot;</span>
        <span class="k">elif</span> <span class="n">query</span><span class="o">.</span><span class="n">is_nested</span> <span class="ow">and</span> <span class="p">(</span><span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;supplying both src_key_padding_mask and src_mask at the same time </span><span class="se">\</span>
<span class="s2">                                 is not supported with NestedTensor input&quot;</span>
        <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_autocast_enabled</span><span class="p">():</span>
            <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;autocast is enabled&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">why_not_fast_path</span><span class="p">:</span>
            <span class="n">tensor_args</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="n">key</span><span class="p">,</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># We have to use list comprehensions below because TorchScript does not support</span>
            <span class="c1"># generator expressions.</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">overrides</span><span class="o">.</span><span class="n">has_torch_function</span><span class="p">(</span><span class="n">tensor_args</span><span class="p">):</span>
                <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="s2">&quot;some Tensor argument has_torch_function&quot;</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">_check_arg_device</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensor_args</span><span class="p">):</span>
                <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;some Tensor argument&#39;s device is neither one of &quot;</span>
                                     <span class="sa">f</span><span class="s2">&quot;cpu, cuda or </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">backend_registration</span><span class="o">.</span><span class="n">_privateuse1_backend_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">_arg_requires_grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensor_args</span><span class="p">):</span>
                <span class="n">why_not_fast_path</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;grad is enabled and at least one of query or the &quot;</span>
                                     <span class="s2">&quot;input/output projection weights or biases requires_grad&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">why_not_fast_path</span><span class="p">:</span>
                <span class="n">merged_mask</span><span class="p">,</span> <span class="n">mask_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">merge_masks</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_native_multi_head_attention</span><span class="p">(</span>
                        <span class="n">query</span><span class="p">,</span>
                        <span class="n">key</span><span class="p">,</span>
                        <span class="n">value</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                        <span class="n">merged_mask</span><span class="p">,</span>
                        <span class="n">need_weights</span><span class="p">,</span>
                        <span class="n">average_attn_weights</span><span class="p">,</span>
                        <span class="n">mask_type</span><span class="p">)</span>

        <span class="n">any_nested</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">is_nested</span> <span class="ow">or</span> <span class="n">key</span><span class="o">.</span><span class="n">is_nested</span> <span class="ow">or</span> <span class="n">value</span><span class="o">.</span><span class="n">is_nested</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">any_nested</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;MultiheadAttention does not support NestedTensor outside of its fast path. &quot;</span> <span class="o">+</span>
                                <span class="sa">f</span><span class="s2">&quot;The fast path was not hit because </span><span class="si">{</span><span class="n">why_not_fast_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="ow">and</span> <span class="n">is_batched</span><span class="p">:</span>
            <span class="c1"># make sure that the transpose op does not affect the &quot;is&quot; property</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">is</span> <span class="n">value</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">query</span> <span class="ow">is</span> <span class="n">key</span><span class="p">:</span>
                    <span class="n">query</span> <span class="o">=</span> <span class="n">key</span> <span class="o">=</span> <span class="n">value</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">query</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)]</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">key</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span><span class="p">:</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">multi_head_attention_forward</span><span class="p">(</span>
                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_zero_attn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
                <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">need_weights</span><span class="o">=</span><span class="n">need_weights</span><span class="p">,</span>
                <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
                <span class="n">use_separate_proj_weight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">q_proj_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_proj_weight</span><span class="p">,</span> <span class="n">k_proj_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_proj_weight</span><span class="p">,</span>
                <span class="n">v_proj_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">v_proj_weight</span><span class="p">,</span>
                <span class="n">average_attn_weights</span><span class="o">=</span><span class="n">average_attn_weights</span><span class="p">,</span>
                <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">multi_head_attention_forward</span><span class="p">(</span>
                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_zero_attn</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
                <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
                <span class="n">need_weights</span><span class="o">=</span><span class="n">need_weights</span><span class="p">,</span>
                <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
                <span class="n">average_attn_weights</span><span class="o">=</span><span class="n">average_attn_weights</span><span class="p">,</span>
                <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="ow">and</span> <span class="n">is_batched</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">attn_output_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_output_weights</span></div>

<div class="viewcode-block" id="MultiheadAttention.merge_masks"><a class="viewcode-back" href="../../../../generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.merge_masks">[docs]</a>    <span class="k">def</span> <span class="nf">merge_masks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">query</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determine mask type and combine masks if necessary. If only one mask is provided, that mask</span>
<span class="sd">        and the corresponding mask type will be returned. If both masks are provided, they will be both</span>
<span class="sd">        expanded to shape ``(batch_size, num_heads, seq_len, seq_len)``, combined with logical ``or``</span>
<span class="sd">        and mask type 2 will be returned</span>
<span class="sd">        Args:</span>
<span class="sd">            attn_mask: attention mask of shape ``(seq_len, seq_len)``, mask type 0</span>
<span class="sd">            key_padding_mask: padding mask of shape ``(batch_size, seq_len)``, mask type 1</span>
<span class="sd">            query: query embeddings of shape ``(batch_size, seq_len, embed_dim)``</span>
<span class="sd">        Returns:</span>
<span class="sd">            merged_mask: merged mask</span>
<span class="sd">            mask_type: merged mask type (0, 1, or 2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mask_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">merged_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask_type</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">merged_mask</span> <span class="o">=</span> <span class="n">key_padding_mask</span>

        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># In this branch query can&#39;t be a nested tensor, so it has a shape</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">mask_type</span> <span class="o">=</span> <span class="mi">2</span>

            <span class="c1"># Always expands attn_mask to 4D</span>
            <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">attn_mask_expanded</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># attn_mask.dim() == 2:</span>
                <span class="n">attn_mask_expanded</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">merged_mask</span> <span class="o">=</span> <span class="n">attn_mask_expanded</span>

            <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">key_padding_mask_expanded</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">merged_mask</span> <span class="o">=</span> <span class="n">attn_mask_expanded</span> <span class="o">+</span> <span class="n">key_padding_mask_expanded</span>

        <span class="c1"># no attn_mask and no key_padding_mask, returns None, None</span>
        <span class="k">return</span> <span class="n">merged_mask</span><span class="p">,</span> <span class="n">mask_type</span></div></div>


<div class="viewcode-block" id="PReLU"><a class="viewcode-back" href="../../../../generated/torch.nn.PReLU.html#torch.nn.PReLU">[docs]</a><span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the element-wise function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{PReLU}(x) = \max(0,x) + a * \min(0,x)</span>

<span class="sd">    or</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{PReLU}(x) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x, &amp; \text{ if } x \geq 0 \\</span>
<span class="sd">        ax, &amp; \text{ otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single</span>
<span class="sd">    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,</span>
<span class="sd">    a separate :math:`a` is used for each input channel.</span>


<span class="sd">    .. note::</span>
<span class="sd">        weight decay should not be used when learning :math:`a` for good performance.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is</span>
<span class="sd">        no channel dim and the number of channels = 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_parameters (int): number of :math:`a` to learn.</span>
<span class="sd">            Although it takes an int as input, there is only two values are legitimate:</span>
<span class="sd">            1, or the number of channels at input. Default: 1</span>
<span class="sd">        init (float): the initial value of :math:`a`. Default: 0.25</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`( *)` where `*` means, any number of additional</span>
<span class="sd">          dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).</span>

<span class="sd">    .. image:: ../scripts/activation_images/PReLU.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.PReLU()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;num_parameters&#39;</span><span class="p">]</span>
    <span class="n">num_parameters</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_parameters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">init</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
                 <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_parameters</span> <span class="o">=</span> <span class="n">num_parameters</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">init</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;num_parameters=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_parameters</span><span class="p">)</span></div>


<div class="viewcode-block" id="Softsign"><a class="viewcode-back" href="../../../../generated/torch.nn.Softsign.html#torch.nn.Softsign">[docs]</a><span class="k">class</span> <span class="nc">Softsign</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the element-wise function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SoftSign}(x) = \frac{x}{ 1 + |x|}</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Softsign.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Softsign()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tanhshrink"><a class="viewcode-back" href="../../../../generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink">[docs]</a><span class="k">class</span> <span class="nc">Tanhshrink</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the element-wise function:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Tanhshrink}(x) = x - \tanh(x)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input.</span>

<span class="sd">    .. image:: ../scripts/activation_images/Tanhshrink.png</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Tanhshrink()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tanhshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="Softmin"><a class="viewcode-back" href="../../../../generated/torch.nn.Softmin.html#torch.nn.Softmin">[docs]</a><span class="k">class</span> <span class="nc">Softmin</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Softmin function to an n-dimensional input Tensor</span>
<span class="sd">    rescaling them so that the elements of the n-dimensional output Tensor</span>
<span class="sd">    lie in the range `[0, 1]` and sum to 1.</span>

<span class="sd">    Softmin is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where `*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int): A dimension along which Softmin will be computed (so every slice</span>
<span class="sd">            along dim will sum to 1).</span>

<span class="sd">    Returns:</span>
<span class="sd">        a Tensor of the same dimension and shape as the input, with</span>
<span class="sd">        values in the range [0, 1]</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Softmin(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dim&#39;</span><span class="p">]</span>
    <span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;dim&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;dim=</span><span class="si">{dim}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span></div>

<div class="viewcode-block" id="Softmax"><a class="viewcode-back" href="../../../../generated/torch.nn.Softmax.html#torch.nn.Softmax">[docs]</a><span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the Softmax function to an n-dimensional input Tensor</span>
<span class="sd">    rescaling them so that the elements of the n-dimensional output Tensor</span>
<span class="sd">    lie in the range [0,1] and sum to 1.</span>

<span class="sd">    Softmax is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</span>

<span class="sd">    When the input Tensor is a sparse tensor then the unspecified</span>
<span class="sd">    values are treated as ``-inf``.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where `*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input</span>

<span class="sd">    Returns:</span>
<span class="sd">        a Tensor of the same dimension and shape as the input with</span>
<span class="sd">        values in the range [0, 1]</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int): A dimension along which Softmax will be computed (so every slice</span>
<span class="sd">            along dim will sum to 1).</span>

<span class="sd">    .. note::</span>
<span class="sd">        This module doesn&#39;t work directly with NLLLoss,</span>
<span class="sd">        which expects the Log to be computed between the Softmax and itself.</span>
<span class="sd">        Use `LogSoftmax` instead (it&#39;s faster and has better numerical properties).</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Softmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dim&#39;</span><span class="p">]</span>
    <span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;dim&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;dim=</span><span class="si">{dim}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="Softmax2d"><a class="viewcode-back" href="../../../../generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d">[docs]</a><span class="k">class</span> <span class="nc">Softmax2d</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies SoftMax over features to each spatial location.</span>

<span class="sd">    When given an image of ``Channels x Height x Width``, it will</span>
<span class="sd">    apply `Softmax` to each location :math:`(Channels, h_i, w_j)`</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`.</span>
<span class="sd">        - Output: :math:`(N, C, H, W)` or :math:`(C, H, W)` (same shape as input)</span>

<span class="sd">    Returns:</span>
<span class="sd">        a Tensor of the same dimension and shape as the input with</span>
<span class="sd">        values in the range [0, 1]</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Softmax2d()</span>
<span class="sd">        &gt;&gt;&gt; # you softmax over the 2nd dimension</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2, 3, 12, 13)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">or</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;Softmax2d requires a 3D or 4D tensor as input&#39;</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogSoftmax"><a class="viewcode-back" href="../../../../generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax">[docs]</a><span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the :math:`\log(\text{Softmax}(x))` function to an n-dimensional</span>
<span class="sd">    input Tensor. The LogSoftmax formulation can be simplified as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where `*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Output: :math:`(*)`, same shape as the input</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int): A dimension along which LogSoftmax will be computed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        a Tensor of the same dimension and shape as the input with</span>
<span class="sd">        values in the range [-inf, 0)</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.LogSoftmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dim&#39;</span><span class="p">]</span>
    <span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;dim&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;dim=</span><span class="si">{dim}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
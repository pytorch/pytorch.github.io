


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.modules.conv &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.modules.conv</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.modules.conv</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">UninitializedParameter</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">.lazy</span> <span class="kn">import</span> <span class="n">LazyModuleMixin</span>
<span class="kn">from</span> <span class="nn">.module</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span><span class="p">,</span> <span class="n">_reverse_repeat_tuple</span>
<span class="kn">from</span> <span class="nn">torch._torch_docs</span> <span class="kn">import</span> <span class="n">reproducibility_notes</span>

<span class="kn">from</span> <span class="nn">..common_types</span> <span class="kn">import</span> <span class="n">_size_1_t</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">,</span> <span class="n">_size_3_t</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Conv1d&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv2d&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv3d&#39;</span><span class="p">,</span> <span class="s1">&#39;ConvTranspose1d&#39;</span><span class="p">,</span> <span class="s1">&#39;ConvTranspose2d&#39;</span><span class="p">,</span> <span class="s1">&#39;ConvTranspose3d&#39;</span><span class="p">,</span>
           <span class="s1">&#39;LazyConv1d&#39;</span><span class="p">,</span> <span class="s1">&#39;LazyConv2d&#39;</span><span class="p">,</span> <span class="s1">&#39;LazyConv3d&#39;</span><span class="p">,</span> <span class="s1">&#39;LazyConvTranspose1d&#39;</span><span class="p">,</span> <span class="s1">&#39;LazyConvTranspose2d&#39;</span><span class="p">,</span>
           <span class="s1">&#39;LazyConvTranspose3d&#39;</span><span class="p">]</span>

<span class="n">convolution_notes</span> <span class="o">=</span> \
    <span class="p">{</span><span class="s2">&quot;groups_note&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;* :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="s2">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="s2">      :attr:`groups`. For example,</span>

<span class="s2">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="s2">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="s2">          layers side by side, each seeing half the input channels</span>
<span class="s2">          and producing half the output channels, and both subsequently</span>
<span class="s2">          concatenated.</span>
<span class="s2">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="s2">          its own set of filters (of size</span>
<span class="s2">          :math:`\frac{\text{out\_channels}}{\text{in\_channels}}`).&quot;&quot;&quot;</span><span class="p">,</span>

        <span class="s2">&quot;depthwise_separable_note&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="s2">        where `K` is a positive integer, this operation is also known as a &quot;depthwise convolution&quot;.</span>

<span class="s2">        In other words, for an input of size :math:`(N, C_</span><span class="si">{in}</span><span class="s2">, L_</span><span class="si">{in}</span><span class="s2">)`,</span>
<span class="s2">        a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments</span>
<span class="s2">        :math:`(C_\text</span><span class="si">{in}</span><span class="s2">=C_\text</span><span class="si">{in}</span><span class="s2">, C_\text</span><span class="si">{out}</span><span class="s2">=C_\text</span><span class="si">{in}</span><span class="s2"> \times \text</span><span class="si">{K}</span><span class="s2">, ..., \text</span><span class="si">{groups}</span><span class="s2">=C_\text</span><span class="si">{in}</span><span class="s2">)`.&quot;&quot;&quot;</span><span class="p">}</span>  <span class="c1"># noqa: B950</span>





<span class="k">class</span> <span class="nc">_ConvNd</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="s1">&#39;groups&#39;</span><span class="p">,</span>
                     <span class="s1">&#39;padding_mode&#39;</span><span class="p">,</span> <span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="s1">&#39;in_channels&#39;</span><span class="p">,</span>
                     <span class="s1">&#39;out_channels&#39;</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">]</span>
    <span class="vm">__annotations__</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;bias&#39;</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]}</span>

    <span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">_reversed_padding_repeated_twice</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">output_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
                 <span class="n">stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
                 <span class="n">padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
                 <span class="n">dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
                 <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                 <span class="n">output_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
                 <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                 <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                 <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">groups</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;groups must be a positive integer&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;in_channels must be divisible by groups&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;out_channels must be divisible by groups&#39;</span><span class="p">)</span>
        <span class="n">valid_padding_strings</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">}</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">padding</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_padding_strings</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Invalid padding string </span><span class="si">{!r}</span><span class="s2">, should be one of </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">padding</span><span class="p">,</span> <span class="n">valid_padding_strings</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">padding</span> <span class="o">==</span> <span class="s1">&#39;same&#39;</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">s</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">stride</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;padding=&#39;same&#39; is not supported for strided convolutions&quot;</span><span class="p">)</span>

        <span class="n">valid_padding_modes</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="s1">&#39;reflect&#39;</span><span class="p">,</span> <span class="s1">&#39;replicate&#39;</span><span class="p">,</span> <span class="s1">&#39;circular&#39;</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_padding_modes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;padding_mode must be one of </span><span class="si">{}</span><span class="s2">, but got padding_mode=&#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">valid_padding_modes</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span> <span class="o">=</span> <span class="n">transposed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="n">output_padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="c1"># `_reversed_padding_repeated_twice` is the padding to be passed to</span>
        <span class="c1"># `F.pad` if needed (e.g., for non-zero padding types that are</span>
        <span class="c1"># implemented as two ops: padding + conv). `F.pad` accepts paddings in</span>
        <span class="c1"># reverse order than the dimension.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">padding</span> <span class="o">==</span> <span class="s1">&#39;same&#39;</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                                   <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
                    <span class="n">total_padding</span> <span class="o">=</span> <span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">left_pad</span> <span class="o">=</span> <span class="n">total_padding</span> <span class="o">//</span> <span class="mi">2</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">left_pad</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">total_padding</span> <span class="o">-</span> <span class="n">left_pad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span> <span class="o">=</span> <span class="n">_reverse_repeat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">transposed</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">),</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Setting a=sqrt(5) in kaiming_uniform is the same as initializing with</span>
        <span class="c1"># uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)</span>
        <span class="c1"># For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">fan_in</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
                <span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">{in_channels}</span><span class="s1">, </span><span class="si">{out_channels}</span><span class="s1">, kernel_size=</span><span class="si">{kernel_size}</span><span class="s1">&#39;</span>
             <span class="s1">&#39;, stride=</span><span class="si">{stride}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, padding=</span><span class="si">{padding}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, dilation=</span><span class="si">{dilation}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, output_padding=</span><span class="si">{output_padding}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, groups=</span><span class="si">{groups}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, bias=False&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, padding_mode=</span><span class="si">{padding_mode}</span><span class="s1">&#39;</span>
        <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;padding_mode&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>


<div class="viewcode-block" id="Conv1d"><a class="viewcode-back" href="../../../../generated/torch.nn.Conv1d.html#torch.nn.Conv1d">[docs]</a><span class="k">class</span> <span class="nc">Conv1d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="vm">__doc__</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;Applies a 1D convolution over an input signal composed of several input</span>
<span class="s2">    planes.</span>

<span class="s2">    In the simplest case, the output value of the layer with input size</span>
<span class="s2">    :math:`(N, C_{\text</span><span class="si">{in}</span><span class="s2">}, L)` and output :math:`(N, C_{\text</span><span class="si">{out}</span><span class="s2">}, L_{\text</span><span class="si">{out}</span><span class="s2">})` can be</span>
<span class="s2">    precisely described as:</span>

<span class="s2">    .. math::</span>
<span class="s2">        \text</span><span class="si">{out}</span><span class="s2">(N_i, C_{\text</span><span class="si">{out}</span><span class="s2">_j}) = \text</span><span class="si">{bias}</span><span class="s2">(C_{\text</span><span class="si">{out}</span><span class="s2">_j}) +</span>
<span class="s2">        \sum_{k = 0}^{C_</span><span class="si">{in}</span><span class="s2"> - 1} \text</span><span class="si">{weight}</span><span class="s2">(C_{\text</span><span class="si">{out}</span><span class="s2">_j}, k)</span>
<span class="s2">        \star \text</span><span class="si">{input}</span><span class="s2">(N_i, k)</span>

<span class="s2">    where :math:`\star` is the valid `cross-correlation`_ operator,</span>
<span class="s2">    :math:`N` is a batch size, :math:`C` denotes a number of channels,</span>
<span class="s2">    :math:`L` is a length of signal sequence.</span>
<span class="s2">    &quot;&quot;&quot;</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward.</span>

<span class="s2">    * :attr:`stride` controls the stride for the cross-correlation, a single</span>
<span class="s2">      number or a one-element tuple.</span>

<span class="s2">    * :attr:`padding` controls the amount of padding applied to the input. It</span>
<span class="s2">      can be either a string {{&#39;valid&#39;, &#39;same&#39;}} or a tuple of ints giving the</span>
<span class="s2">      amount of implicit padding applied on both sides.</span>

<span class="s2">    * :attr:`dilation` controls the spacing between the kernel points; also</span>
<span class="s2">      known as the Ã  trous algorithm. It is harder to describe, but this `link`_</span>
<span class="s2">      has a nice visualization of what :attr:`dilation` does.</span>

<span class="s2">    </span><span class="si">{groups_note}</span><span class="s2"></span>

<span class="s2">    Note:</span>
<span class="s2">        </span><span class="si">{depthwise_separable_note}</span><span class="s2"></span>
<span class="s2">    Note:</span>
<span class="s2">        </span><span class="si">{cudnn_reproducibility_note}</span><span class="s2"></span>

<span class="s2">    Note:</span>
<span class="s2">        ``padding=&#39;valid&#39;`` is the same as no padding. ``padding=&#39;same&#39;`` pads</span>
<span class="s2">        the input so the output has the shape as the input. However, this mode</span>
<span class="s2">        doesn&#39;t support any stride values other than 1.</span>

<span class="s2">    Note:</span>
<span class="s2">        This module supports complex data types i.e. ``complex32, complex64, complex128``.</span>

<span class="s2">    Args:</span>
<span class="s2">        in_channels (int): Number of channels in the input image</span>
<span class="s2">        out_channels (int): Number of channels produced by the convolution</span>
<span class="s2">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="s2">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="s2">        padding (int, tuple or str, optional): Padding added to both sides of</span>
<span class="s2">            the input. Default: 0</span>
<span class="s2">        padding_mode (str, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,</span>
<span class="s2">            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="s2">        dilation (int or tuple, optional): Spacing between kernel</span>
<span class="s2">            elements. Default: 1</span>
<span class="s2">        groups (int, optional): Number of blocked connections from input</span>
<span class="s2">            channels to output channels. Default: 1</span>
<span class="s2">        bias (bool, optional): If ``True``, adds a learnable bias to the</span>
<span class="s2">            output. Default: ``True``</span>

<span class="s2">    &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">reproducibility_notes</span><span class="p">,</span> <span class="o">**</span><span class="n">convolution_notes</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    Shape:</span>
<span class="s2">        - Input: :math:`(N, C_</span><span class="si">{in}</span><span class="s2">, L_</span><span class="si">{in}</span><span class="s2">)` or :math:`(C_</span><span class="si">{in}</span><span class="s2">, L_</span><span class="si">{in}</span><span class="s2">)`</span>
<span class="s2">        - Output: :math:`(N, C_</span><span class="si">{out}</span><span class="s2">, L_</span><span class="si">{out}</span><span class="s2">)` or :math:`(C_</span><span class="si">{out}</span><span class="s2">, L_</span><span class="si">{out}</span><span class="s2">)`, where</span>

<span class="s2">          .. math::</span>
<span class="s2">              L_</span><span class="si">{out}</span><span class="s2"> = \left\lfloor\frac{L_</span><span class="si">{in}</span><span class="s2"> + 2 \times \text</span><span class="si">{padding}</span><span class="s2"> - \text</span><span class="si">{dilation}</span><span class="s2"></span>
<span class="s2">                        \times (\text{kernel\_size} - 1) - 1}{\text</span><span class="si">{stride}</span><span class="s2">} + 1\right\rfloor</span>

<span class="s2">    Attributes:</span>
<span class="s2">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="s2">            :math:`(\text{out\_channels},</span>
<span class="s2">            \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">}, \text{kernel\_size})`.</span>
<span class="s2">            The values of these weights are sampled from</span>
<span class="s2">            :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">            :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{in}</span><span class="s2"> * \text{kernel\_size}}`</span>
<span class="s2">        bias (Tensor):   the learnable bias of the module of shape</span>
<span class="s2">            (out_channels). If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="s2">            sampled from :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">            :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{in}</span><span class="s2"> * \text{kernel\_size}}`</span>

<span class="s2">    Examples::</span>

<span class="s2">        &gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)</span>
<span class="s2">        &gt;&gt;&gt; input = torch.randn(20, 16, 50)</span>
<span class="s2">        &gt;&gt;&gt; output = m(input)</span>

<span class="s2">    .. _cross-correlation:</span>
<span class="s2">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="s2">    .. _link:</span>
<span class="s2">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_1_t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>  <span class="c1"># TODO: refine this type</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="c1"># we create new variables below to make mypy happy since kernel_size has</span>
        <span class="c1"># type Union[int, Tuple[int]] and kernel_size_ has type Tuple[int]</span>
        <span class="n">kernel_size_</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride_</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding_</span> <span class="o">=</span> <span class="n">padding</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation_</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size_</span><span class="p">,</span> <span class="n">stride_</span><span class="p">,</span> <span class="n">padding_</span><span class="p">,</span> <span class="n">dilation_</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
                            <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                            <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2d"><a class="viewcode-back" href="../../../../generated/torch.nn.Conv2d.html#torch.nn.Conv2d">[docs]</a><span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="vm">__doc__</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;Applies a 2D convolution over an input signal composed of several input</span>
<span class="s2">    planes.</span>

<span class="s2">    In the simplest case, the output value of the layer with input size</span>
<span class="s2">    :math:`(N, C_{\text</span><span class="si">{in}</span><span class="s2">}, H, W)` and output :math:`(N, C_{\text</span><span class="si">{out}</span><span class="s2">}, H_{\text</span><span class="si">{out}</span><span class="s2">}, W_{\text</span><span class="si">{out}</span><span class="s2">})`</span>
<span class="s2">    can be precisely described as:</span>

<span class="s2">    .. math::</span>
<span class="s2">        \text</span><span class="si">{out}</span><span class="s2">(N_i, C_{\text</span><span class="si">{out}</span><span class="s2">_j}) = \text</span><span class="si">{bias}</span><span class="s2">(C_{\text</span><span class="si">{out}</span><span class="s2">_j}) +</span>
<span class="s2">        \sum_{k = 0}^{C_{\text</span><span class="si">{in}</span><span class="s2">} - 1} \text</span><span class="si">{weight}</span><span class="s2">(C_{\text</span><span class="si">{out}</span><span class="s2">_j}, k) \star \text</span><span class="si">{input}</span><span class="s2">(N_i, k)</span>


<span class="s2">    where :math:`\star` is the valid 2D `cross-correlation`_ operator,</span>
<span class="s2">    :math:`N` is a batch size, :math:`C` denotes a number of channels,</span>
<span class="s2">    :math:`H` is a height of input planes in pixels, and :math:`W` is</span>
<span class="s2">    width in pixels.</span>
<span class="s2">    &quot;&quot;&quot;</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward.</span>

<span class="s2">    * :attr:`stride` controls the stride for the cross-correlation, a single</span>
<span class="s2">      number or a tuple.</span>

<span class="s2">    * :attr:`padding` controls the amount of padding applied to the input. It</span>
<span class="s2">      can be either a string {{&#39;valid&#39;, &#39;same&#39;}} or an int / a tuple of ints giving the</span>
<span class="s2">      amount of implicit padding applied on both sides.</span>

<span class="s2">    * :attr:`dilation` controls the spacing between the kernel points; also</span>
<span class="s2">      known as the Ã  trous algorithm. It is harder to describe, but this `link`_</span>
<span class="s2">      has a nice visualization of what :attr:`dilation` does.</span>

<span class="s2">    </span><span class="si">{groups_note}</span><span class="s2"></span>

<span class="s2">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:</span>

<span class="s2">        - a single ``int`` -- in which case the same value is used for the height and width dimension</span>
<span class="s2">        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,</span>
<span class="s2">          and the second `int` for the width dimension</span>

<span class="s2">    Note:</span>
<span class="s2">        </span><span class="si">{depthwise_separable_note}</span><span class="s2"></span>

<span class="s2">    Note:</span>
<span class="s2">        </span><span class="si">{cudnn_reproducibility_note}</span><span class="s2"></span>

<span class="s2">    Note:</span>
<span class="s2">        ``padding=&#39;valid&#39;`` is the same as no padding. ``padding=&#39;same&#39;`` pads</span>
<span class="s2">        the input so the output has the shape as the input. However, this mode</span>
<span class="s2">        doesn&#39;t support any stride values other than 1.</span>

<span class="s2">    Note:</span>
<span class="s2">        This module supports complex data types i.e. ``complex32, complex64, complex128``.</span>

<span class="s2">    Args:</span>
<span class="s2">        in_channels (int): Number of channels in the input image</span>
<span class="s2">        out_channels (int): Number of channels produced by the convolution</span>
<span class="s2">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="s2">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="s2">        padding (int, tuple or str, optional): Padding added to all four sides of</span>
<span class="s2">            the input. Default: 0</span>
<span class="s2">        padding_mode (str, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,</span>
<span class="s2">            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="s2">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="s2">        groups (int, optional): Number of blocked connections from input</span>
<span class="s2">            channels to output channels. Default: 1</span>
<span class="s2">        bias (bool, optional): If ``True``, adds a learnable bias to the</span>
<span class="s2">            output. Default: ``True``</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">reproducibility_notes</span><span class="p">,</span> <span class="o">**</span><span class="n">convolution_notes</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    Shape:</span>
<span class="s2">        - Input: :math:`(N, C_</span><span class="si">{in}</span><span class="s2">, H_</span><span class="si">{in}</span><span class="s2">, W_</span><span class="si">{in}</span><span class="s2">)` or :math:`(C_</span><span class="si">{in}</span><span class="s2">, H_</span><span class="si">{in}</span><span class="s2">, W_</span><span class="si">{in}</span><span class="s2">)`</span>
<span class="s2">        - Output: :math:`(N, C_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)` or :math:`(C_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)`, where</span>

<span class="s2">          .. math::</span>
<span class="s2">              H_</span><span class="si">{out}</span><span class="s2"> = \left\lfloor\frac{H_</span><span class="si">{in}</span><span class="s2">  + 2 \times \text</span><span class="si">{padding}</span><span class="s2">[0] - \text</span><span class="si">{dilation}</span><span class="s2">[0]</span>
<span class="s2">                        \times (\text{kernel\_size}[0] - 1) - 1}{\text</span><span class="si">{stride}</span><span class="s2">[0]} + 1\right\rfloor</span>

<span class="s2">          .. math::</span>
<span class="s2">              W_</span><span class="si">{out}</span><span class="s2"> = \left\lfloor\frac{W_</span><span class="si">{in}</span><span class="s2">  + 2 \times \text</span><span class="si">{padding}</span><span class="s2">[1] - \text</span><span class="si">{dilation}</span><span class="s2">[1]</span>
<span class="s2">                        \times (\text{kernel\_size}[1] - 1) - 1}{\text</span><span class="si">{stride}</span><span class="s2">[1]} + 1\right\rfloor</span>

<span class="s2">    Attributes:</span>
<span class="s2">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="s2">            :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">},`</span>
<span class="s2">            :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.</span>
<span class="s2">            The values of these weights are sampled from</span>
<span class="s2">            :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">            :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{in}</span><span class="s2"> * \prod_{i=0}^</span><span class="si">{1}</span><span class="s2">\text{kernel\_size}[i]}`</span>
<span class="s2">        bias (Tensor):   the learnable bias of the module of shape</span>
<span class="s2">            (out_channels). If :attr:`bias` is ``True``,</span>
<span class="s2">            then the values of these weights are</span>
<span class="s2">            sampled from :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">            :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{in}</span><span class="s2"> * \prod_{i=0}^</span><span class="si">{1}</span><span class="s2">\text{kernel\_size}[i]}`</span>

<span class="s2">    Examples:</span>

<span class="s2">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2)</span>
<span class="s2">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="s2">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="s2">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation</span>
<span class="s2">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))</span>
<span class="s2">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)</span>
<span class="s2">        &gt;&gt;&gt; output = m(input)</span>

<span class="s2">    .. _cross-correlation:</span>
<span class="s2">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="s2">    .. _link:</span>
<span class="s2">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>  <span class="c1"># TODO: refine this type</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="n">kernel_size_</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride_</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding_</span> <span class="o">=</span> <span class="n">padding</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation_</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size_</span><span class="p">,</span> <span class="n">stride_</span><span class="p">,</span> <span class="n">padding_</span><span class="p">,</span> <span class="n">dilation_</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
                            <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                            <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span></div>

<div class="viewcode-block" id="Conv3d"><a class="viewcode-back" href="../../../../generated/torch.nn.Conv3d.html#torch.nn.Conv3d">[docs]</a><span class="k">class</span> <span class="nc">Conv3d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="vm">__doc__</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;Applies a 3D convolution over an input signal composed of several input</span>
<span class="s2">    planes.</span>

<span class="s2">    In the simplest case, the output value of the layer with input size :math:`(N, C_</span><span class="si">{in}</span><span class="s2">, D, H, W)`</span>
<span class="s2">    and output :math:`(N, C_</span><span class="si">{out}</span><span class="s2">, D_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)` can be precisely described as:</span>

<span class="s2">    .. math::</span>
<span class="s2">        out(N_i, C_</span><span class="si">{out_j}</span><span class="s2">) = bias(C_</span><span class="si">{out_j}</span><span class="s2">) +</span>
<span class="s2">                                \sum_{k = 0}^{C_</span><span class="si">{in}</span><span class="s2"> - 1} weight(C_</span><span class="si">{out_j}</span><span class="s2">, k) \star input(N_i, k)</span>

<span class="s2">    where :math:`\star` is the valid 3D `cross-correlation`_ operator</span>
<span class="s2">    &quot;&quot;&quot;</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward.</span>

<span class="s2">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="s2">    * :attr:`padding` controls the amount of padding applied to the input. It</span>
<span class="s2">      can be either a string {{&#39;valid&#39;, &#39;same&#39;}} or a tuple of ints giving the</span>
<span class="s2">      amount of implicit padding applied on both sides.</span>

<span class="s2">    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.</span>
<span class="s2">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="s2">    </span><span class="si">{groups_note}</span><span class="s2"></span>

<span class="s2">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:</span>

<span class="s2">        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension</span>
<span class="s2">        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,</span>
<span class="s2">          the second `int` for the height dimension and the third `int` for the width dimension</span>

<span class="s2">    Note:</span>
<span class="s2">        </span><span class="si">{depthwise_separable_note}</span><span class="s2"></span>

<span class="s2">    Note:</span>
<span class="s2">        </span><span class="si">{cudnn_reproducibility_note}</span><span class="s2"></span>

<span class="s2">    Note:</span>
<span class="s2">        ``padding=&#39;valid&#39;`` is the same as no padding. ``padding=&#39;same&#39;`` pads</span>
<span class="s2">        the input so the output has the shape as the input. However, this mode</span>
<span class="s2">        doesn&#39;t support any stride values other than 1.</span>

<span class="s2">    Note:</span>
<span class="s2">        This module supports complex data types i.e. ``complex32, complex64, complex128``.</span>

<span class="s2">    Args:</span>
<span class="s2">        in_channels (int): Number of channels in the input image</span>
<span class="s2">        out_channels (int): Number of channels produced by the convolution</span>
<span class="s2">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="s2">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="s2">        padding (int, tuple or str, optional): Padding added to all six sides of</span>
<span class="s2">            the input. Default: 0</span>
<span class="s2">        padding_mode (str, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``, ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="s2">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="s2">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="s2">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">reproducibility_notes</span><span class="p">,</span> <span class="o">**</span><span class="n">convolution_notes</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    Shape:</span>
<span class="s2">        - Input: :math:`(N, C_</span><span class="si">{in}</span><span class="s2">, D_</span><span class="si">{in}</span><span class="s2">, H_</span><span class="si">{in}</span><span class="s2">, W_</span><span class="si">{in}</span><span class="s2">)` or :math:`(C_</span><span class="si">{in}</span><span class="s2">, D_</span><span class="si">{in}</span><span class="s2">, H_</span><span class="si">{in}</span><span class="s2">, W_</span><span class="si">{in}</span><span class="s2">)`</span>
<span class="s2">        - Output: :math:`(N, C_</span><span class="si">{out}</span><span class="s2">, D_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)` or :math:`(C_</span><span class="si">{out}</span><span class="s2">, D_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)`,</span>
<span class="s2">          where</span>

<span class="s2">          .. math::</span>
<span class="s2">              D_</span><span class="si">{out}</span><span class="s2"> = \left\lfloor\frac{D_</span><span class="si">{in}</span><span class="s2"> + 2 \times \text</span><span class="si">{padding}</span><span class="s2">[0] - \text</span><span class="si">{dilation}</span><span class="s2">[0]</span>
<span class="s2">                    \times (\text{kernel\_size}[0] - 1) - 1}{\text</span><span class="si">{stride}</span><span class="s2">[0]} + 1\right\rfloor</span>

<span class="s2">          .. math::</span>
<span class="s2">              H_</span><span class="si">{out}</span><span class="s2"> = \left\lfloor\frac{H_</span><span class="si">{in}</span><span class="s2"> + 2 \times \text</span><span class="si">{padding}</span><span class="s2">[1] - \text</span><span class="si">{dilation}</span><span class="s2">[1]</span>
<span class="s2">                    \times (\text{kernel\_size}[1] - 1) - 1}{\text</span><span class="si">{stride}</span><span class="s2">[1]} + 1\right\rfloor</span>

<span class="s2">          .. math::</span>
<span class="s2">              W_</span><span class="si">{out}</span><span class="s2"> = \left\lfloor\frac{W_</span><span class="si">{in}</span><span class="s2"> + 2 \times \text</span><span class="si">{padding}</span><span class="s2">[2] - \text</span><span class="si">{dilation}</span><span class="s2">[2]</span>
<span class="s2">                    \times (\text{kernel\_size}[2] - 1) - 1}{\text</span><span class="si">{stride}</span><span class="s2">[2]} + 1\right\rfloor</span>

<span class="s2">    Attributes:</span>
<span class="s2">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="s2">                         :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">},`</span>
<span class="s2">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.</span>
<span class="s2">                         The values of these weights are sampled from</span>
<span class="s2">                         :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">                         :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{in}</span><span class="s2"> * \prod_{i=0}^</span><span class="si">{2}</span><span class="s2">\text{kernel\_size}[i]}`</span>
<span class="s2">        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,</span>
<span class="s2">                         then the values of these weights are</span>
<span class="s2">                         sampled from :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">                         :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{in}</span><span class="s2"> * \prod_{i=0}^</span><span class="si">{2}</span><span class="s2">\text{kernel\_size}[i]}`</span>

<span class="s2">    Examples::</span>

<span class="s2">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">        &gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)</span>
<span class="s2">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="s2">        &gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))</span>
<span class="s2">        &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)</span>
<span class="s2">        &gt;&gt;&gt; output = m(input)</span>

<span class="s2">    .. _cross-correlation:</span>
<span class="s2">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="s2">    .. _link:</span>
<span class="s2">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_3_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_3_t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="n">kernel_size_</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride_</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding_</span> <span class="o">=</span> <span class="n">padding</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation_</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size_</span><span class="p">,</span> <span class="n">stride_</span><span class="p">,</span> <span class="n">padding_</span><span class="p">,</span> <span class="n">dilation_</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span>
                <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                    <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span>
                <span class="p">),</span>
                <span class="n">weight</span><span class="p">,</span>
                <span class="n">bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span></div>



<span class="k">class</span> <span class="nc">_ConvTransposeNd</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only &quot;zeros&quot; padding mode is supported for </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>

        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="c1"># dilation being an optional parameter is for backwards</span>
    <span class="c1"># compatibility</span>
    <span class="k">def</span> <span class="nf">_output_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                        <span class="n">stride</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">padding</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">num_spatial_dims</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dilation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">)</span>  <span class="c1"># converting to list if was not already</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">has_batch_dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">num_spatial_dims</span> <span class="o">+</span> <span class="mi">2</span>
            <span class="n">num_non_spatial_dims</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">has_batch_dim</span> <span class="k">else</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="n">num_non_spatial_dims</span> <span class="o">+</span> <span class="n">num_spatial_dims</span><span class="p">:</span>
                <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="n">num_non_spatial_dims</span><span class="p">:]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_spatial_dims</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;ConvTranspose</span><span class="si">{}</span><span class="s2">D: for </span><span class="si">{}</span><span class="s2">D input, output_size must have </span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements (got </span><span class="si">{}</span><span class="s2">)&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_spatial_dims</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">num_spatial_dims</span><span class="p">,</span>
                            <span class="n">num_non_spatial_dims</span> <span class="o">+</span> <span class="n">num_spatial_dims</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>

            <span class="n">min_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="n">max_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_spatial_dims</span><span class="p">):</span>
                <span class="n">dim_size</span> <span class="o">=</span> <span class="p">((</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="n">num_non_spatial_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span>
                            <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span>
                            <span class="p">(</span><span class="n">dilation</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">min_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim_size</span><span class="p">)</span>
                <span class="n">max_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">min_sizes</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)):</span>
                <span class="n">size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">min_size</span> <span class="o">=</span> <span class="n">min_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">max_size</span> <span class="o">=</span> <span class="n">max_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;</span> <span class="n">min_size</span> <span class="ow">or</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="n">max_size</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span>
                        <span class="s2">&quot;requested an output size of </span><span class="si">{}</span><span class="s2">, but valid sizes range &quot;</span>
                        <span class="s2">&quot;from </span><span class="si">{}</span><span class="s2"> to </span><span class="si">{}</span><span class="s2"> (for an input of </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">output_size</span><span class="p">,</span> <span class="n">min_sizes</span><span class="p">,</span> <span class="n">max_sizes</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]))</span>

            <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_spatial_dims</span><span class="p">):</span>
                <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">min_sizes</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>

            <span class="n">ret</span> <span class="o">=</span> <span class="n">res</span>
        <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="ConvTranspose1d"><a class="viewcode-back" href="../../../../generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d">[docs]</a><span class="k">class</span> <span class="nc">ConvTranspose1d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="vm">__doc__</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;Applies a 1D transposed convolution operator over an input image</span>
<span class="s2">    composed of several input planes.</span>

<span class="s2">    This module can be seen as the gradient of Conv1d with respect to its input.</span>
<span class="s2">    It is also known as a fractionally-strided convolution or</span>
<span class="s2">    a deconvolution (although it is not an actual deconvolution operation as it does</span>
<span class="s2">    not compute a true inverse of convolution). For more information, see the visualizations</span>
<span class="s2">    `here`_ and the `Deconvolutional Networks`_ paper.</span>

<span class="s2">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward.</span>

<span class="s2">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="s2">    * :attr:`padding` controls the amount of implicit zero padding on both</span>
<span class="s2">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="s2">      below for details.</span>

<span class="s2">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="s2">      of the output shape. See note below for details.</span>

<span class="s2">    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.</span>
<span class="s2">      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="s2">    </span><span class="si">{groups_note}</span><span class="s2"></span>

<span class="s2">    Note:</span>
<span class="s2">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="s2">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="s2">        when a :class:`~torch.nn.Conv1d` and a :class:`~torch.nn.ConvTranspose1d`</span>
<span class="s2">        are initialized with same parameters, they are inverses of each other in</span>
<span class="s2">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="s2">        :class:`~torch.nn.Conv1d` maps multiple input shapes to the same output</span>
<span class="s2">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="s2">        effectively increasing the calculated output shape on one side. Note</span>
<span class="s2">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="s2">        not actually add zero-padding to output.</span>

<span class="s2">    Note:</span>
<span class="s2">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="s2">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="s2">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="s2">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="s2">        True``.</span>
<span class="s2">        Please see the notes on :doc:`/notes/randomness` for background.</span>


<span class="s2">    Args:</span>
<span class="s2">        in_channels (int): Number of channels in the input image</span>
<span class="s2">        out_channels (int): Number of channels produced by the convolution</span>
<span class="s2">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="s2">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="s2">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="s2">            will be added to both sides of the input. Default: 0</span>
<span class="s2">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="s2">            of the output shape. Default: 0</span>
<span class="s2">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="s2">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="s2">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">reproducibility_notes</span><span class="p">,</span> <span class="o">**</span><span class="n">convolution_notes</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    Shape:</span>
<span class="s2">        - Input: :math:`(N, C_</span><span class="si">{in}</span><span class="s2">, L_</span><span class="si">{in}</span><span class="s2">)` or :math:`(C_</span><span class="si">{in}</span><span class="s2">, L_</span><span class="si">{in}</span><span class="s2">)`</span>
<span class="s2">        - Output: :math:`(N, C_</span><span class="si">{out}</span><span class="s2">, L_</span><span class="si">{out}</span><span class="s2">)` or :math:`(C_</span><span class="si">{out}</span><span class="s2">, L_</span><span class="si">{out}</span><span class="s2">)`, where</span>

<span class="s2">          .. math::</span>
<span class="s2">              L_</span><span class="si">{out}</span><span class="s2"> = (L_</span><span class="si">{in}</span><span class="s2"> - 1) \times \text</span><span class="si">{stride}</span><span class="s2"> - 2 \times \text</span><span class="si">{padding}</span><span class="s2"> + \text</span><span class="si">{dilation}</span><span class="s2"></span>
<span class="s2">                        \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1</span>

<span class="s2">    Attributes:</span>
<span class="s2">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="s2">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">},`</span>
<span class="s2">                         :math:`\text{kernel\_size})`.</span>
<span class="s2">                         The values of these weights are sampled from</span>
<span class="s2">                         :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">                         :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{out}</span><span class="s2"> * \text{kernel\_size}}`</span>
<span class="s2">        bias (Tensor):   the learnable bias of the module of shape (out_channels).</span>
<span class="s2">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="s2">                         sampled from :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">                         :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{out}</span><span class="s2"> * \text{kernel\_size}}`</span>

<span class="s2">    .. _`here`:</span>
<span class="s2">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>

<span class="s2">    .. _`Deconvolutional Networks`:</span>
<span class="s2">        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose1d&#39;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
        <span class="c1"># One cannot replace List by Tuple or Sequence in &quot;_output_padding&quot; because</span>
        <span class="c1"># TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.</span>
        <span class="n">num_spatial_dims</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="n">num_spatial_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConvTranspose2d"><a class="viewcode-back" href="../../../../generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d">[docs]</a><span class="k">class</span> <span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="vm">__doc__</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;Applies a 2D transposed convolution operator over an input image</span>
<span class="s2">    composed of several input planes.</span>

<span class="s2">    This module can be seen as the gradient of Conv2d with respect to its input.</span>
<span class="s2">    It is also known as a fractionally-strided convolution or</span>
<span class="s2">    a deconvolution (although it is not an actual deconvolution operation as it does</span>
<span class="s2">    not compute a true inverse of convolution). For more information, see the visualizations</span>
<span class="s2">    `here`_ and the `Deconvolutional Networks`_ paper.</span>

<span class="s2">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward.</span>

<span class="s2">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="s2">    * :attr:`padding` controls the amount of implicit zero padding on both</span>
<span class="s2">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="s2">      below for details.</span>

<span class="s2">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="s2">      of the output shape. See note below for details.</span>

<span class="s2">    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.</span>
<span class="s2">      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="s2">    </span><span class="si">{groups_note}</span><span class="s2"></span>

<span class="s2">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`</span>
<span class="s2">    can either be:</span>

<span class="s2">        - a single ``int`` -- in which case the same value is used for the height and width dimensions</span>
<span class="s2">        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,</span>
<span class="s2">          and the second `int` for the width dimension</span>

<span class="s2">    Note:</span>
<span class="s2">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="s2">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="s2">        when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`</span>
<span class="s2">        are initialized with same parameters, they are inverses of each other in</span>
<span class="s2">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="s2">        :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output</span>
<span class="s2">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="s2">        effectively increasing the calculated output shape on one side. Note</span>
<span class="s2">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="s2">        not actually add zero-padding to output.</span>

<span class="s2">    Note:</span>
<span class="s2">        </span><span class="si">{cudnn_reproducibility_note}</span><span class="s2"></span>

<span class="s2">    Args:</span>
<span class="s2">        in_channels (int): Number of channels in the input image</span>
<span class="s2">        out_channels (int): Number of channels produced by the convolution</span>
<span class="s2">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="s2">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="s2">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="s2">            will be added to both sides of each dimension in the input. Default: 0</span>
<span class="s2">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="s2">            of each dimension in the output shape. Default: 0</span>
<span class="s2">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="s2">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="s2">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">reproducibility_notes</span><span class="p">,</span> <span class="o">**</span><span class="n">convolution_notes</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    Shape:</span>
<span class="s2">        - Input: :math:`(N, C_</span><span class="si">{in}</span><span class="s2">, H_</span><span class="si">{in}</span><span class="s2">, W_</span><span class="si">{in}</span><span class="s2">)` or :math:`(C_</span><span class="si">{in}</span><span class="s2">, H_</span><span class="si">{in}</span><span class="s2">, W_</span><span class="si">{in}</span><span class="s2">)`</span>
<span class="s2">        - Output: :math:`(N, C_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)` or :math:`(C_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)`, where</span>

<span class="s2">        .. math::</span>
<span class="s2">              H_</span><span class="si">{out}</span><span class="s2"> = (H_</span><span class="si">{in}</span><span class="s2"> - 1) \times \text</span><span class="si">{stride}</span><span class="s2">[0] - 2 \times \text</span><span class="si">{padding}</span><span class="s2">[0] + \text</span><span class="si">{dilation}</span><span class="s2">[0]</span>
<span class="s2">                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</span>
<span class="s2">        .. math::</span>
<span class="s2">              W_</span><span class="si">{out}</span><span class="s2"> = (W_</span><span class="si">{in}</span><span class="s2"> - 1) \times \text</span><span class="si">{stride}</span><span class="s2">[1] - 2 \times \text</span><span class="si">{padding}</span><span class="s2">[1] + \text</span><span class="si">{dilation}</span><span class="s2">[1]</span>
<span class="s2">                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</span>

<span class="s2">    Attributes:</span>
<span class="s2">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="s2">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">},`</span>
<span class="s2">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.</span>
<span class="s2">                         The values of these weights are sampled from</span>
<span class="s2">                         :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">                         :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{out}</span><span class="s2"> * \prod_{i=0}^</span><span class="si">{1}</span><span class="s2">\text{kernel\_size}[i]}`</span>
<span class="s2">        bias (Tensor):   the learnable bias of the module of shape (out_channels)</span>
<span class="s2">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="s2">                         sampled from :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">                         :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{out}</span><span class="s2"> * \prod_{i=0}^</span><span class="si">{1}</span><span class="s2">\text{kernel\_size}[i]}`</span>

<span class="s2">    Examples::</span>

<span class="s2">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">        &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)</span>
<span class="s2">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="s2">        &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="s2">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)</span>
<span class="s2">        &gt;&gt;&gt; output = m(input)</span>
<span class="s2">        &gt;&gt;&gt; # exact output size can be also specified as an argument</span>
<span class="s2">        &gt;&gt;&gt; input = torch.randn(1, 16, 12, 12)</span>
<span class="s2">        &gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)</span>
<span class="s2">        &gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)</span>
<span class="s2">        &gt;&gt;&gt; h = downsample(input)</span>
<span class="s2">        &gt;&gt;&gt; h.size()</span>
<span class="s2">        torch.Size([1, 16, 6, 6])</span>
<span class="s2">        &gt;&gt;&gt; output = upsample(h, output_size=input.size())</span>
<span class="s2">        &gt;&gt;&gt; output.size()</span>
<span class="s2">        torch.Size([1, 16, 12, 12])</span>

<span class="s2">    .. _`here`:</span>
<span class="s2">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>

<span class="s2">    .. _`Deconvolutional Networks`:</span>
<span class="s2">        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose2d&#39;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
        <span class="c1"># One cannot replace List by Tuple or Sequence in &quot;_output_padding&quot; because</span>
        <span class="c1"># TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.</span>
        <span class="n">num_spatial_dims</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="n">num_spatial_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span></div>


<div class="viewcode-block" id="ConvTranspose3d"><a class="viewcode-back" href="../../../../generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d">[docs]</a><span class="k">class</span> <span class="nc">ConvTranspose3d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="vm">__doc__</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;Applies a 3D transposed convolution operator over an input image composed of several input</span>
<span class="s2">    planes.</span>
<span class="s2">    The transposed convolution operator multiplies each input value element-wise by a learnable kernel,</span>
<span class="s2">    and sums over the outputs from all input feature planes.</span>

<span class="s2">    This module can be seen as the gradient of Conv3d with respect to its input.</span>
<span class="s2">    It is also known as a fractionally-strided convolution or</span>
<span class="s2">    a deconvolution (although it is not an actual deconvolution operation as it does</span>
<span class="s2">    not compute a true inverse of convolution). For more information, see the visualizations</span>
<span class="s2">    `here`_ and the `Deconvolutional Networks`_ paper.</span>

<span class="s2">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward.</span>

<span class="s2">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="s2">    * :attr:`padding` controls the amount of implicit zero padding on both</span>
<span class="s2">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="s2">      below for details.</span>

<span class="s2">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="s2">      of the output shape. See note below for details.</span>

<span class="s2">    * :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.</span>
<span class="s2">      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="s2">    </span><span class="si">{groups_note}</span><span class="s2"></span>

<span class="s2">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`</span>
<span class="s2">    can either be:</span>

<span class="s2">        - a single ``int`` -- in which case the same value is used for the depth, height and width dimensions</span>
<span class="s2">        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,</span>
<span class="s2">          the second `int` for the height dimension and the third `int` for the width dimension</span>

<span class="s2">    Note:</span>
<span class="s2">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="s2">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="s2">        when a :class:`~torch.nn.Conv3d` and a :class:`~torch.nn.ConvTranspose3d`</span>
<span class="s2">        are initialized with same parameters, they are inverses of each other in</span>
<span class="s2">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="s2">        :class:`~torch.nn.Conv3d` maps multiple input shapes to the same output</span>
<span class="s2">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="s2">        effectively increasing the calculated output shape on one side. Note</span>
<span class="s2">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="s2">        not actually add zero-padding to output.</span>

<span class="s2">    Note:</span>
<span class="s2">        </span><span class="si">{cudnn_reproducibility_note}</span><span class="s2"></span>

<span class="s2">    Args:</span>
<span class="s2">        in_channels (int): Number of channels in the input image</span>
<span class="s2">        out_channels (int): Number of channels produced by the convolution</span>
<span class="s2">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="s2">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="s2">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="s2">            will be added to both sides of each dimension in the input. Default: 0</span>
<span class="s2">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="s2">            of each dimension in the output shape. Default: 0</span>
<span class="s2">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="s2">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="s2">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">reproducibility_notes</span><span class="p">,</span> <span class="o">**</span><span class="n">convolution_notes</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>

<span class="s2">    Shape:</span>
<span class="s2">        - Input: :math:`(N, C_</span><span class="si">{in}</span><span class="s2">, D_</span><span class="si">{in}</span><span class="s2">, H_</span><span class="si">{in}</span><span class="s2">, W_</span><span class="si">{in}</span><span class="s2">)` or :math:`(C_</span><span class="si">{in}</span><span class="s2">, D_</span><span class="si">{in}</span><span class="s2">, H_</span><span class="si">{in}</span><span class="s2">, W_</span><span class="si">{in}</span><span class="s2">)`</span>
<span class="s2">        - Output: :math:`(N, C_</span><span class="si">{out}</span><span class="s2">, D_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)` or</span>
<span class="s2">          :math:`(C_</span><span class="si">{out}</span><span class="s2">, D_</span><span class="si">{out}</span><span class="s2">, H_</span><span class="si">{out}</span><span class="s2">, W_</span><span class="si">{out}</span><span class="s2">)`, where</span>

<span class="s2">        .. math::</span>
<span class="s2">              D_</span><span class="si">{out}</span><span class="s2"> = (D_</span><span class="si">{in}</span><span class="s2"> - 1) \times \text</span><span class="si">{stride}</span><span class="s2">[0] - 2 \times \text</span><span class="si">{padding}</span><span class="s2">[0] + \text</span><span class="si">{dilation}</span><span class="s2">[0]</span>
<span class="s2">                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</span>
<span class="s2">        .. math::</span>
<span class="s2">              H_</span><span class="si">{out}</span><span class="s2"> = (H_</span><span class="si">{in}</span><span class="s2"> - 1) \times \text</span><span class="si">{stride}</span><span class="s2">[1] - 2 \times \text</span><span class="si">{padding}</span><span class="s2">[1] + \text</span><span class="si">{dilation}</span><span class="s2">[1]</span>
<span class="s2">                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</span>
<span class="s2">        .. math::</span>
<span class="s2">              W_</span><span class="si">{out}</span><span class="s2"> = (W_</span><span class="si">{in}</span><span class="s2"> - 1) \times \text</span><span class="si">{stride}</span><span class="s2">[2] - 2 \times \text</span><span class="si">{padding}</span><span class="s2">[2] + \text</span><span class="si">{dilation}</span><span class="s2">[2]</span>
<span class="s2">                        \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1</span>


<span class="s2">    Attributes:</span>
<span class="s2">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="s2">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">},`</span>
<span class="s2">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.</span>
<span class="s2">                         The values of these weights are sampled from</span>
<span class="s2">                         :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">                         :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{out}</span><span class="s2"> * \prod_{i=0}^</span><span class="si">{2}</span><span class="s2">\text{kernel\_size}[i]}`</span>
<span class="s2">        bias (Tensor):   the learnable bias of the module of shape (out_channels)</span>
<span class="s2">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="s2">                         sampled from :math:`\mathcal</span><span class="si">{U}</span><span class="s2">(-\sqrt</span><span class="si">{k}</span><span class="s2">, \sqrt</span><span class="si">{k}</span><span class="s2">)` where</span>
<span class="s2">                         :math:`k = \frac</span><span class="si">{groups}</span><span class="s2">{C_\text</span><span class="si">{out}</span><span class="s2"> * \prod_{i=0}^</span><span class="si">{2}</span><span class="s2">\text{kernel\_size}[i]}`</span>

<span class="s2">    Examples::</span>

<span class="s2">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">        &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)</span>
<span class="s2">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="s2">        &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))</span>
<span class="s2">        &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)</span>
<span class="s2">        &gt;&gt;&gt; output = m(input)</span>

<span class="s2">    .. _`here`:</span>
<span class="s2">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>

<span class="s2">    .. _`Deconvolutional Networks`:</span>
<span class="s2">        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_3_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose3d&#39;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
        <span class="c1"># One cannot replace List by Tuple or Sequence in &quot;_output_padding&quot; because</span>
        <span class="c1"># TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.</span>
        <span class="n">num_spatial_dims</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="n">num_spatial_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span></div>


<span class="c1"># TODO: Deprecate and remove the following alias `_ConvTransposeMixin`.</span>
<span class="c1">#</span>
<span class="c1"># `_ConvTransposeMixin` was a mixin that was removed.  It is meant to be used</span>
<span class="c1"># with `_ConvNd` to construct actual module classes that implements conv</span>
<span class="c1"># transpose ops:</span>
<span class="c1">#</span>
<span class="c1">#   class MyConvTranspose(_ConvNd, _ConvTransposeMixin):</span>
<span class="c1">#       ...</span>
<span class="c1">#</span>
<span class="c1"># In PyTorch, it has been replaced by `_ConvTransposeNd`, which is a proper</span>
<span class="c1"># subclass of `_ConvNd`.  However, some user code in the wild still (incorrectly)</span>
<span class="c1"># use the internal class `_ConvTransposeMixin`.  Hence, we provide this alias</span>
<span class="c1"># for BC, because it is cheap and easy for us to do so, even though that</span>
<span class="c1"># `_ConvTransposeNd` is really not a mixin anymore (but multiple inheritance as</span>
<span class="c1"># above would still work).</span>
<span class="k">class</span> <span class="nc">_ConvTransposeMixin</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;_ConvTransposeMixin is a deprecated internal class. &quot;</span>
            <span class="s2">&quot;Please consider using public APIs.&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="c1"># TODO: Conv2dLocal</span>
<span class="c1"># TODO: Conv2dMap</span>
<span class="c1"># TODO: ConvTranspose2dMap</span>


<span class="k">class</span> <span class="nc">_LazyConvXdMixin</span><span class="p">(</span><span class="n">LazyModuleMixin</span><span class="p">):</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">UninitializedParameter</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">UninitializedParameter</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># has_uninitialized_params is defined in parent class and it is using a protocol on self</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_uninitialized_params</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># type: ignore[misc]</span>
            <span class="c1"># &quot;type:ignore[..]&quot; is required because mypy thinks that &quot;reset_parameters&quot; is undefined</span>
            <span class="c1"># in super class. Turns out that it is defined in _ConvND which is inherited by any class</span>
            <span class="c1"># that also inherits _LazyConvXdMixin</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>  <span class="c1"># type: ignore[misc]</span>

    <span class="c1"># Signature of &quot;initialize_parameters&quot; is incompatible with the definition in supertype LazyModuleMixin</span>
    <span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore[override]</span>
        <span class="c1"># defined by parent class but using a protocol</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_uninitialized_params</span><span class="p">():</span>  <span class="c1"># type: ignore[misc]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_in_channels</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;in_channels must be divisible by groups&#39;</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">UninitializedParameter</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">materialize</span><span class="p">((</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">materialize</span><span class="p">((</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">UninitializedParameter</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">materialize</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="c1"># Function to extract in_channels from first input.</span>
    <span class="k">def</span> <span class="nf">_get_in_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">num_spatial_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_num_spatial_dims</span><span class="p">()</span>
        <span class="n">num_dims_no_batch</span> <span class="o">=</span> <span class="n">num_spatial_dims</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># +1 for channels dim</span>
        <span class="n">num_dims_batch</span> <span class="o">=</span> <span class="n">num_dims_no_batch</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">num_dims_no_batch</span><span class="p">,</span> <span class="n">num_dims_batch</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Expected </span><span class="si">{}</span><span class="s2">D (unbatched) or </span><span class="si">{}</span><span class="s2">D (batched) input to </span><span class="si">{}</span><span class="s2">, but &quot;</span>
                               <span class="s2">&quot;got input of size: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_dims_no_batch</span><span class="p">,</span> <span class="n">num_dims_batch</span><span class="p">,</span>
                                                              <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="n">num_dims_batch</span> <span class="k">else</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Function to return the number of spatial dims expected for inputs to the module.</span>
    <span class="c1"># This is expected to be implemented by subclasses.</span>
    <span class="k">def</span> <span class="nf">_get_num_spatial_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="c1"># LazyConv1d defines weight as a Tensor but derived class defines it as UnitializeParameter</span>
<div class="viewcode-block" id="LazyConv1d"><a class="viewcode-back" href="../../../../generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d">[docs]</a><span class="k">class</span> <span class="nc">LazyConv1d</span><span class="p">(</span><span class="n">_LazyConvXdMixin</span><span class="p">,</span> <span class="n">Conv1d</span><span class="p">):</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A :class:`torch.nn.Conv1d` module with lazy initialization of</span>
<span class="sd">    the ``in_channels`` argument of the :class:`Conv1d` that is inferred from</span>
<span class="sd">    the ``input.size(1)``.</span>
<span class="sd">    The attributes that will be lazily initialized are `weight` and `bias`.</span>

<span class="sd">    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation</span>
<span class="sd">    on lazy modules and their limitations.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to both sides of</span>
<span class="sd">            the input. Default: 0</span>
<span class="sd">        padding_mode (str, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,</span>
<span class="sd">            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel</span>
<span class="sd">            elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input</span>
<span class="sd">            channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the</span>
<span class="sd">            output. Default: ``True``</span>

<span class="sd">    .. seealso:: :class:`torch.nn.Conv1d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># super class define this variable as None. &quot;type: ignore[..] is required</span>
    <span class="c1"># since we are redefining the variable.</span>
    <span class="n">cls_to_become</span> <span class="o">=</span> <span class="n">Conv1d</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="c1"># bias is hardcoded to False to avoid creating tensor</span>
            <span class="c1"># that will soon be overwritten.</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="p">,</span>
            <span class="o">**</span><span class="n">factory_kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_num_spatial_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span></div>


<span class="c1"># LazyConv2d defines weight as a Tensor but derived class defines it as UnitializeParameter</span>
<div class="viewcode-block" id="LazyConv2d"><a class="viewcode-back" href="../../../../generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d">[docs]</a><span class="k">class</span> <span class="nc">LazyConv2d</span><span class="p">(</span><span class="n">_LazyConvXdMixin</span><span class="p">,</span> <span class="n">Conv2d</span><span class="p">):</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A :class:`torch.nn.Conv2d` module with lazy initialization of</span>
<span class="sd">    the ``in_channels`` argument of the :class:`Conv2d` that is inferred from</span>
<span class="sd">    the ``input.size(1)``.</span>
<span class="sd">    The attributes that will be lazily initialized are `weight` and `bias`.</span>

<span class="sd">    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation</span>
<span class="sd">    on lazy modules and their limitations.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to both sides of</span>
<span class="sd">            the input. Default: 0</span>
<span class="sd">        padding_mode (str, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,</span>
<span class="sd">            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel</span>
<span class="sd">            elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input</span>
<span class="sd">            channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the</span>
<span class="sd">            output. Default: ``True``</span>

<span class="sd">    .. seealso:: :class:`torch.nn.Conv2d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># super class define this variable as None. &quot;type: ignore[..] is required</span>
    <span class="c1"># since we are redefining the variable.</span>
    <span class="n">cls_to_become</span> <span class="o">=</span> <span class="n">Conv2d</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>  <span class="c1"># TODO: refine this type</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="c1"># bias is hardcoded to False to avoid creating tensor</span>
            <span class="c1"># that will soon be overwritten.</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="p">,</span>
            <span class="o">**</span><span class="n">factory_kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_num_spatial_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">2</span></div>


<span class="c1"># LazyConv3d defines weight as a Tensor but derived class defines it as UnitializeParameter</span>
<div class="viewcode-block" id="LazyConv3d"><a class="viewcode-back" href="../../../../generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d">[docs]</a><span class="k">class</span> <span class="nc">LazyConv3d</span><span class="p">(</span><span class="n">_LazyConvXdMixin</span><span class="p">,</span> <span class="n">Conv3d</span><span class="p">):</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A :class:`torch.nn.Conv3d` module with lazy initialization of</span>
<span class="sd">    the ``in_channels`` argument of the :class:`Conv3d` that is inferred from</span>
<span class="sd">    the ``input.size(1)``.</span>
<span class="sd">    The attributes that will be lazily initialized are `weight` and `bias`.</span>

<span class="sd">    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation</span>
<span class="sd">    on lazy modules and their limitations.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to both sides of</span>
<span class="sd">            the input. Default: 0</span>
<span class="sd">        padding_mode (str, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,</span>
<span class="sd">            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel</span>
<span class="sd">            elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input</span>
<span class="sd">            channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the</span>
<span class="sd">            output. Default: ``True``</span>

<span class="sd">    .. seealso:: :class:`torch.nn.Conv3d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># super class define this variable as None. &quot;type: ignore[..] is required</span>
    <span class="c1"># since we are redefining the variable.</span>
    <span class="n">cls_to_become</span> <span class="o">=</span> <span class="n">Conv3d</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_3_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="c1"># bias is hardcoded to False to avoid creating tensor</span>
            <span class="c1"># that will soon be overwritten.</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="p">,</span>
            <span class="o">**</span><span class="n">factory_kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_num_spatial_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">3</span></div>


<span class="c1"># LazyConvTranspose1d defines weight as a Tensor but derived class defines it as UnitializeParameter</span>
<div class="viewcode-block" id="LazyConvTranspose1d"><a class="viewcode-back" href="../../../../generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d">[docs]</a><span class="k">class</span> <span class="nc">LazyConvTranspose1d</span><span class="p">(</span><span class="n">_LazyConvXdMixin</span><span class="p">,</span> <span class="n">ConvTranspose1d</span><span class="p">):</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A :class:`torch.nn.ConvTranspose1d` module with lazy initialization of</span>
<span class="sd">    the ``in_channels`` argument of the :class:`ConvTranspose1d` that is inferred from</span>
<span class="sd">    the ``input.size(1)``.</span>
<span class="sd">    The attributes that will be lazily initialized are `weight` and `bias`.</span>

<span class="sd">    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation</span>
<span class="sd">    on lazy modules and their limitations.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    .. seealso:: :class:`torch.nn.ConvTranspose1d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># super class define this variable as None. &quot;type: ignore[..] is required</span>
    <span class="c1"># since we are redefining the variable.</span>
    <span class="n">cls_to_become</span> <span class="o">=</span> <span class="n">ConvTranspose1d</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="c1"># bias is hardcoded to False to avoid creating tensor</span>
            <span class="c1"># that will soon be overwritten.</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="p">,</span>
            <span class="o">**</span><span class="n">factory_kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_num_spatial_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span></div>


<span class="c1"># LazyConvTranspose2d defines weight as a Tensor but derived class defines it as UnitializeParameter</span>
<div class="viewcode-block" id="LazyConvTranspose2d"><a class="viewcode-back" href="../../../../generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d">[docs]</a><span class="k">class</span> <span class="nc">LazyConvTranspose2d</span><span class="p">(</span><span class="n">_LazyConvXdMixin</span><span class="p">,</span> <span class="n">ConvTranspose2d</span><span class="p">):</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A :class:`torch.nn.ConvTranspose2d` module with lazy initialization of</span>
<span class="sd">    the ``in_channels`` argument of the :class:`ConvTranspose2d` that is inferred from</span>
<span class="sd">    the ``input.size(1)``.</span>
<span class="sd">    The attributes that will be lazily initialized are `weight` and `bias`.</span>

<span class="sd">    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation</span>
<span class="sd">    on lazy modules and their limitations.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of each dimension in the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of each dimension in the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    .. seealso:: :class:`torch.nn.ConvTranspose2d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># super class define this variable as None. &quot;type: ignore[..] is required</span>
    <span class="c1"># since we are redefining the variable.</span>
    <span class="n">cls_to_become</span> <span class="o">=</span> <span class="n">ConvTranspose2d</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="c1"># bias is hardcoded to False to avoid creating tensor</span>
            <span class="c1"># that will soon be overwritten.</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="p">,</span>
            <span class="o">**</span><span class="n">factory_kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_num_spatial_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">2</span></div>


<span class="c1"># LazyConvTranspose3d defines weight as a Tensor but derived class defines it as UnitializeParameter</span>
<div class="viewcode-block" id="LazyConvTranspose3d"><a class="viewcode-back" href="../../../../generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d">[docs]</a><span class="k">class</span> <span class="nc">LazyConvTranspose3d</span><span class="p">(</span><span class="n">_LazyConvXdMixin</span><span class="p">,</span> <span class="n">ConvTranspose3d</span><span class="p">):</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A :class:`torch.nn.ConvTranspose3d` module with lazy initialization of</span>
<span class="sd">    the ``in_channels`` argument of the :class:`ConvTranspose3d` that is inferred from</span>
<span class="sd">    the ``input.size(1)``.</span>
<span class="sd">    The attributes that will be lazily initialized are `weight` and `bias`.</span>

<span class="sd">    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation</span>
<span class="sd">    on lazy modules and their limitations.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of each dimension in the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of each dimension in the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    .. seealso:: :class:`torch.nn.ConvTranspose3d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># super class define this variable as None. &quot;type: ignore[..] is required</span>
    <span class="c1"># since we are redefining the variable.</span>
    <span class="n">cls_to_become</span> <span class="o">=</span> <span class="n">ConvTranspose3d</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_3_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="c1"># bias is hardcoded to False to avoid creating tensor</span>
            <span class="c1"># that will soon be overwritten.</span>
            <span class="kc">False</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="p">,</span>
            <span class="o">**</span><span class="n">factory_kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">UninitializedParameter</span><span class="p">(</span><span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_num_spatial_dims</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">3</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
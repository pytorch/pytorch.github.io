


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.utils.parametrize &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/nn/utils/parametrize.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.utils.parametrize</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.utils.parametrize</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.modules.container</span> <span class="kn">import</span> <span class="n">ModuleList</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">,</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">copyreg</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Sequence</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cached&#39;</span><span class="p">,</span> <span class="s1">&#39;ParametrizationList&#39;</span><span class="p">,</span> <span class="s1">&#39;register_parametrization&#39;</span><span class="p">,</span> <span class="s1">&#39;is_parametrized&#39;</span><span class="p">,</span> <span class="s1">&#39;remove_parametrizations&#39;</span><span class="p">,</span>
           <span class="s1">&#39;type_before_parametrizations&#39;</span><span class="p">,</span> <span class="s1">&#39;transfer_parametrizations_and_params&#39;</span><span class="p">]</span>

<span class="n">_cache_enabled</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>


<div class="viewcode-block" id="cached"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached">[docs]</a><span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">cached</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Context manager that enables the caching system within parametrizations</span>
<span class="sd">    registered with :func:`register_parametrization`.</span>

<span class="sd">    The value of the parametrized objects is computed and cached the first time</span>
<span class="sd">    they are required when this context manager is active. The cached values are</span>
<span class="sd">    discarded when leaving the context manager.</span>

<span class="sd">    This is useful when using a parametrized parameter more than once in the forward pass.</span>
<span class="sd">    An example of this is when parametrizing the recurrent kernel of an RNN or when</span>
<span class="sd">    sharing weights.</span>

<span class="sd">    The simplest way to activate the cache is by wrapping the forward pass of the neural network</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        import torch.nn.utils.parametrize as P</span>
<span class="sd">        ...</span>
<span class="sd">        with P.cached():</span>
<span class="sd">            output = model(inputs)</span>

<span class="sd">    in training and evaluation. One may also wrap the parts of the modules that use</span>
<span class="sd">    several times the parametrized tensors. For example, the loop of an RNN with a</span>
<span class="sd">    parametrized recurrent kernel:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        with P.cached():</span>
<span class="sd">            for x in xs:</span>
<span class="sd">                out_rnn = self.rnn_cell(x, out_rnn)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_cache</span>
    <span class="k">global</span> <span class="n">_cache_enabled</span>
    <span class="n">_cache_enabled</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">_cache_enabled</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_cache_enabled</span><span class="p">:</span>
            <span class="n">_cache</span> <span class="o">=</span> <span class="p">{}</span></div>


<span class="k">def</span> <span class="nf">_register_parameter_or_buffer</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">module</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>


<div class="viewcode-block" id="ParametrizationList"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList">[docs]</a><span class="k">class</span> <span class="nc">ParametrizationList</span><span class="p">(</span><span class="n">ModuleList</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A sequential container that holds and manages the ``original`` or ``original0``, ``original1``, ...</span>
<span class="sd">    parameters or buffers of a parametrized :class:`torch.nn.Module`.</span>

<span class="sd">    It is the type of ``module.parametrizations[tensor_name]`` when ``module[tensor_name]``</span>
<span class="sd">    has been parametrized with :func:`register_parametrization`.</span>

<span class="sd">    If the first registered parametrization has a ``right_inverse`` that returns one tensor or</span>
<span class="sd">    does not have a ``right_inverse`` (in which case we assume that ``right_inverse`` is the identity),</span>
<span class="sd">    it will hold the tensor under the name ``original``.</span>
<span class="sd">    If it has a ``right_inverse`` that returns more than one tensor, these will be registered as</span>
<span class="sd">    ``original0``, ``original1``, ...</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This class is used internally by :func:`register_parametrization`. It is documented</span>
<span class="sd">        here for completeness. It shall not be instantiated by the user.</span>

<span class="sd">    Args:</span>
<span class="sd">        modules (sequence): sequence of modules representing the parametrizations</span>
<span class="sd">        original (Parameter or Tensor): parameter or buffer that is parametrized</span>
<span class="sd">        unsafe (bool): a boolean flag that denotes whether the parametrization</span>
<span class="sd">            may change the dtype and shape of the tensor. Default: `False`</span>
<span class="sd">            Warning: the parametrization is not checked for consistency upon registration.</span>
<span class="sd">            Enable this flag at your own risk.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">original</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">unsafe</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">modules</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Module</span><span class="p">],</span> <span class="n">original</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">],</span> <span class="n">unsafe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># We require this because we need to treat differently the first parametrization</span>
        <span class="c1"># This should never throw, unless this class is used from the outside</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ParametrizationList requires one or more modules.&quot;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unsafe</span> <span class="o">=</span> <span class="n">unsafe</span>

        <span class="c1"># In plain words:</span>
        <span class="c1"># module.weight must keep its dtype and shape.</span>
        <span class="c1"># Furthermore, if there is no right_inverse or the right_inverse returns a tensor,</span>
        <span class="c1"># this should be of the same dtype as the original tensor</span>
        <span class="c1">#</span>
        <span class="c1"># We check that the following invariants hold:</span>
        <span class="c1">#    X = module.weight</span>
        <span class="c1">#    Y = param.right_inverse(X)</span>
        <span class="c1">#    assert isinstance(Y, Tensor) or</span>
        <span class="c1">#           (isinstance(Y, collections.abc.Sequence) and all(isinstance(t, Tensor) for t in Y))</span>
        <span class="c1">#    Z = param(Y) if isinstance(Y, Tensor) else param(*Y)</span>
        <span class="c1">#    # Consistency checks</span>
        <span class="c1">#    assert X.dtype == Z.dtype and X.shape == Z.shape</span>
        <span class="c1">#    # If it has one input, this allows to be able to use set_ to be able to</span>
        <span class="c1">#    # move data to/from the original tensor without changing its id (which is what the</span>
        <span class="c1">#    # optimizer uses to track parameters)</span>
        <span class="c1">#    if isinstance(Y, Tensor)</span>
        <span class="c1">#      assert X.dtype == Y.dtype</span>
        <span class="c1"># Below we use original = X, new = Y</span>

        <span class="n">original_shape</span> <span class="o">=</span> <span class="n">original</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">original</span><span class="o">.</span><span class="n">dtype</span>

        <span class="c1"># Compute new</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">new</span> <span class="o">=</span> <span class="n">original</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># type: ignore[call-overload]</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;right_inverse&quot;</span><span class="p">):</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">new</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">right_inverse</span><span class="p">(</span><span class="n">new</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="c1"># else, or if it throws, we assume that right_inverse is the identity</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;right_inverse&#39; must return a Tensor or a Sequence of tensors (list, tuple...). &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">new</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Set the number of original tensors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ntensors</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tensor</span> <span class="k">else</span> <span class="nb">len</span><span class="p">(</span><span class="n">new</span><span class="p">)</span>

        <span class="c1"># Register the tensor(s)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">original</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">new</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;When `right_inverse` outputs one tensor, it may not change the dtype.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;original.dtype: </span><span class="si">{</span><span class="n">original</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;right_inverse(original).dtype: </span><span class="si">{</span><span class="n">new</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Set the original to original so that the user does not need to re-register the parameter</span>
            <span class="c1"># manually in the optimiser</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">original</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">new</span><span class="p">)</span>  <span class="c1"># type: ignore[call-overload]</span>
            <span class="n">_register_parameter_or_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;original&quot;</span><span class="p">,</span> <span class="n">original</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">originali</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">new</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">originali</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;right_inverse&#39; must return a Tensor or a Sequence of tensors &quot;</span>
                                     <span class="s2">&quot;(list, tuple...). &quot;</span>
                                     <span class="sa">f</span><span class="s2">&quot;Got element </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> of the sequence with type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">originali</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

                <span class="c1"># If the original tensor was a Parameter that required grad, we expect the user to</span>
                <span class="c1"># add the new parameters to the optimizer after registering the parametrization</span>
                <span class="c1"># (this is documented)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
                    <span class="n">originali</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">originali</span><span class="p">)</span>
                <span class="n">originali</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">original</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
                <span class="n">_register_parameter_or_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;original</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">originali</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsafe</span><span class="p">:</span>
            <span class="c1"># Consistency checks:</span>
            <span class="c1"># Since f : A -&gt; B, right_inverse : B -&gt; A, Z and original should live in B</span>
            <span class="c1"># Z = forward(right_inverse(original))</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;A parametrization must return a tensor. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">Z</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">original_dtype</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Registering a parametrization may not change the dtype of the tensor, unless `unsafe` flag is enabled.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;unparametrized dtype: </span><span class="si">{</span><span class="n">original_dtype</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;parametrized dtype: </span><span class="si">{</span><span class="n">Z</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">Z</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">original_shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Registering a parametrization may not change the shape of the tensor, unless `unsafe` flag is enabled.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;unparametrized shape: </span><span class="si">{</span><span class="n">original_shape</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;parametrized shape: </span><span class="si">{</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

<div class="viewcode-block" id="ParametrizationList.right_inverse"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList.right_inverse">[docs]</a>    <span class="k">def</span> <span class="nf">right_inverse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calls the methods ``right_inverse`` (see :func:`register_parametrization`)</span>
<span class="sd">        of the parametrizations in the inverse order they were registered in.</span>
<span class="sd">        Then, it stores the result in ``self.original`` if ``right_inverse`` outputs one tensor</span>
<span class="sd">        or in ``self.original0``, ``self.original1``, ... if it outputs several.</span>

<span class="sd">        Args:</span>
<span class="sd">            value (Tensor): Value to which initialize the module</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># All the exceptions in this function should almost never throw.</span>
        <span class="c1"># They could throw if, for example, right_inverse function returns a different</span>
        <span class="c1"># dtype when given a different input, which should most likely be caused by a</span>
        <span class="c1"># bug in the user&#39;s code</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># See https://github.com/pytorch/pytorch/issues/53103</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># type: ignore[call-overload]</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;right_inverse&quot;</span><span class="p">):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">right_inverse</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;parametrization </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not implement &quot;</span>
                                       <span class="s2">&quot;right_inverse.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">:</span>
                <span class="c1"># These exceptions should only throw when a right_inverse function does not</span>
                <span class="c1"># return the same dtype for every input, which should most likely be caused by a bug</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;`right_inverse` should return a tensor. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The tensor returned by `right_inverse` has dtype </span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;while `original` has dtype </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">original</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># We know that the result is going to have the same dtype</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">original</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>  <span class="c1"># type: ignore[call-overload]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;&#39;right_inverse&#39; must return a sequence of tensors. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntensors</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;&#39;right_inverse&#39; must return a sequence of tensors of length &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ntensors</span><span class="si">}</span><span class="s2">. Got a sequence of length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
                    <span class="n">original_i</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;original</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;`right_inverse` must return a sequence of tensors. &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;Got element </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">original_i</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Tensor </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> returned by `right_inverse` has dtype </span><span class="si">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;while `original</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">` has dtype </span><span class="si">{</span><span class="n">original_i</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="n">original_i</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Parametrization is not working with scripting.&#39;</span><span class="p">)</span>
        <span class="c1"># Unpack the originals for the first parametrization</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">original</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">originals</span> <span class="o">=</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;original</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ntensors</span><span class="p">))</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="o">*</span><span class="n">originals</span><span class="p">)</span>
        <span class="c1"># It&#39;s not possible to call self[1:] here, so we have to be a bit more cryptic</span>
        <span class="c1"># Also we want to skip all non-integer keys</span>
        <span class="n">curr_idx</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">curr_idx</span><span class="p">)):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">curr_idx</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">curr_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span></div>


<span class="k">def</span> <span class="nf">_inject_new_class</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets up a module to be parametrized.</span>

<span class="sd">    This works by substituting the class of the module by a class</span>
<span class="sd">    that extends it to be able to inject a property</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module into which to inject the property</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span>

    <span class="k">def</span> <span class="nf">default_deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="c1"># Just emulate a standard deepcopy procedure when __deepcopy__ doesn&#39;t exist in the current class.</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">memo</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">obj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">obj</span>
        <span class="n">replica</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
        <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n">replica</span>
        <span class="n">replica</span><span class="o">.</span><span class="vm">__dict__</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span> <span class="n">memo</span><span class="p">)</span>
        <span class="c1"># Also save all slots if they exist.</span>
        <span class="n">slots_to_save</span> <span class="o">=</span> <span class="n">copyreg</span><span class="o">.</span><span class="n">_slotnames</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">for</span> <span class="n">slot</span> <span class="ow">in</span> <span class="n">slots_to_save</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">replica</span><span class="p">,</span> <span class="n">slot</span><span class="p">,</span> <span class="n">deepcopy</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">),</span> <span class="n">memo</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">replica</span>

    <span class="k">def</span> <span class="nf">getstate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Serialization of parametrized modules is only &quot;</span>
            <span class="s2">&quot;supported through state_dict(). See:</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;https://pytorch.org/tutorials/beginner/saving_loading_models.html&quot;</span>
            <span class="s2">&quot;#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training&quot;</span>
        <span class="p">)</span>

    <span class="n">dct</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;__getstate__&quot;</span><span class="p">:</span> <span class="n">getstate</span><span class="p">}</span>
    <span class="c1"># We don&#39;t allow serialization of parametrized modules but should still allow deepcopying.</span>
    <span class="c1"># Default &#39;deepcopy&#39; function invokes __deepcopy__ method instead of __getstate__ when it exists.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;__deepcopy__&quot;</span><span class="p">):</span>
        <span class="n">dct</span><span class="p">[</span><span class="s2">&quot;__deepcopy__&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_deepcopy</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="n">param_cls</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Parametrized</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">(</span><span class="bp">cls</span><span class="p">,),</span>
        <span class="n">dct</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">=</span> <span class="n">param_cls</span>


<span class="k">def</span> <span class="nf">_inject_property</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Injects a property into module[tensor_name].</span>

<span class="sd">    It assumes that the class in the module has already been modified from its</span>
<span class="sd">    original one using _inject_new_class and that the tensor under :attr:`tensor_name`</span>
<span class="sd">    has already been moved out</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module into which to inject the property</span>
<span class="sd">        tensor_name (str): name of the name of the property to create</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We check the precondition.</span>
    <span class="c1"># This should never fire if register_parametrization is correctly implemented</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">unused</span>
    <span class="k">def</span> <span class="nf">get_cached_parametrization</span><span class="p">(</span><span class="n">parametrization</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">global</span> <span class="n">_cache</span>
        <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">module</span><span class="p">),</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">parametrization</span><span class="p">()</span>
            <span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="k">return</span> <span class="n">tensor</span>

    <span class="k">def</span> <span class="nf">get_parametrized</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Parametrization is not working with scripting.&#39;</span><span class="p">)</span>
        <span class="n">parametrization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">_cache_enabled</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
                <span class="c1"># Scripting</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Caching is not implemented for scripting. &#39;</span>
                                   <span class="s1">&#39;Either disable caching or avoid scripting.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Tracing</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Cannot trace a model while caching parametrizations.&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">get_cached_parametrization</span><span class="p">(</span><span class="n">parametrization</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If caching is not active, this function just evaluates the parametrization</span>
            <span class="k">return</span> <span class="n">parametrization</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">set_original</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Parametrization is not working with scripting.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span><span class="o">.</span><span class="n">right_inverse</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">,</span> <span class="nb">property</span><span class="p">(</span><span class="n">get_parametrized</span><span class="p">,</span> <span class="n">set_original</span><span class="p">))</span>

<div class="viewcode-block" id="register_parametrization"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization">[docs]</a><span class="k">def</span> <span class="nf">register_parametrization</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">parametrization</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">unsafe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Adds a parametrization to a tensor in a module.</span>

<span class="sd">    Assume that ``tensor_name=&quot;weight&quot;`` for simplicity. When accessing ``module.weight``,</span>
<span class="sd">    the module will return the parametrized version ``parametrization(module.weight)``.</span>
<span class="sd">    If the original tensor requires a gradient, the backward pass will differentiate</span>
<span class="sd">    through :attr:`parametrization`, and the optimizer will update the tensor accordingly.</span>

<span class="sd">    The first time that a module registers a parametrization, this function will add an attribute</span>
<span class="sd">    ``parametrizations`` to the module of type :class:`~ParametrizationList`.</span>

<span class="sd">    The list of parametrizations on the tensor ``weight`` will be accessible under</span>
<span class="sd">    ``module.parametrizations.weight``.</span>

<span class="sd">    The original tensor will be accessible under</span>
<span class="sd">    ``module.parametrizations.weight.original``.</span>

<span class="sd">    Parametrizations may be concatenated by registering several parametrizations</span>
<span class="sd">    on the same attribute.</span>

<span class="sd">    The training mode of a registered parametrization is updated on registration</span>
<span class="sd">    to match the training mode of the host module</span>

<span class="sd">    Parametrized parameters and buffers have an inbuilt caching system that can be activated</span>
<span class="sd">    using the context manager :func:`cached`.</span>

<span class="sd">    A :attr:`parametrization` may optionally implement a method with signature</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        def right_inverse(self, X: Tensor) -&gt; Union[Tensor, Sequence[Tensor]]</span>

<span class="sd">    This method is called on the unparametrized tensor when the first parametrization</span>
<span class="sd">    is registered to compute the initial value of the original tensor.</span>
<span class="sd">    If this method is not implemented, the original tensor will be just the unparametrized tensor.</span>

<span class="sd">    If all the parametrizations registered on a tensor implement `right_inverse` it is possible</span>
<span class="sd">    to initialize a parametrized tensor by assigning to it, as shown in the example below.</span>

<span class="sd">    It is possible for the first parametrization to depend on several inputs.</span>
<span class="sd">    This may be implemented returning a tuple of tensors from ``right_inverse``</span>
<span class="sd">    (see the example implementation of a ``RankOne`` parametrization below).</span>

<span class="sd">    In this case, the unconstrained tensors are also located under ``module.parametrizations.weight``</span>
<span class="sd">    with names ``original0``, ``original1``,...</span>

<span class="sd">    .. note::</span>

<span class="sd">        If unsafe=False (default) both the forward and right_inverse methods will be called</span>
<span class="sd">        once to perform a number of consistency checks.</span>
<span class="sd">        If unsafe=True, then right_inverse will be called if the tensor is not parametrized,</span>
<span class="sd">        and nothing will be called otherwise.</span>

<span class="sd">    .. note::</span>

<span class="sd">        In most situations, ``right_inverse`` will be a function such that</span>
<span class="sd">        ``forward(right_inverse(X)) == X`` (see</span>
<span class="sd">        `right inverse &lt;https://en.wikipedia.org/wiki/Inverse_function#Right_inverses&gt;`_).</span>
<span class="sd">        Sometimes, when the parametrization is not surjective, it may be reasonable</span>
<span class="sd">        to relax this.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        If a parametrization depends on several inputs, :func:`~register_parametrization`</span>
<span class="sd">        will register a number of new parameters. If such parametrization is registered</span>
<span class="sd">        after the optimizer is created, these new parameters will need to be added manually</span>
<span class="sd">        to the optimizer. See :meth:`torch.Optimizer.add_param_group`.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module on which to register the parametrization</span>
<span class="sd">        tensor_name (str): name of the parameter or buffer on which to register</span>
<span class="sd">            the parametrization</span>
<span class="sd">        parametrization (nn.Module): the parametrization to register</span>
<span class="sd">    Keyword args:</span>
<span class="sd">        unsafe (bool): a boolean flag that denotes whether the parametrization</span>
<span class="sd">            may change the dtype and shape of the tensor. Default: `False`</span>
<span class="sd">            Warning: the parametrization is not checked for consistency upon registration.</span>
<span class="sd">            Enable this flag at your own risk.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if the module does not have a parameter or a buffer named :attr:`tensor_name`</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK)</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import torch.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import torch.nn.utils.parametrize as P</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; class Symmetric(nn.Module):</span>
<span class="sd">        &gt;&gt;&gt;     def forward(self, X):</span>
<span class="sd">        &gt;&gt;&gt;         return X.triu() + X.triu(1).T  # Return a symmetric matrix</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def right_inverse(self, A):</span>
<span class="sd">        &gt;&gt;&gt;         return A.triu()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Linear(5, 5)</span>
<span class="sd">        &gt;&gt;&gt; P.register_parametrization(m, &quot;weight&quot;, Symmetric())</span>
<span class="sd">        &gt;&gt;&gt; print(torch.allclose(m.weight, m.weight.T))  # m.weight is now symmetric</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; A = torch.rand(5, 5)</span>
<span class="sd">        &gt;&gt;&gt; A = A + A.T   # A is now symmetric</span>
<span class="sd">        &gt;&gt;&gt; m.weight = A  # Initialize the weight to be the symmetric matrix A</span>
<span class="sd">        &gt;&gt;&gt; print(torch.allclose(m.weight, A))</span>
<span class="sd">        True</span>

<span class="sd">        &gt;&gt;&gt; class RankOne(nn.Module):</span>
<span class="sd">        &gt;&gt;&gt;     def forward(self, x, y):</span>
<span class="sd">        &gt;&gt;&gt;         # Form a rank 1 matrix multiplying two vectors</span>
<span class="sd">        &gt;&gt;&gt;         return x.unsqueeze(-1) @ y.unsqueeze(-2)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def right_inverse(self, Z):</span>
<span class="sd">        &gt;&gt;&gt;         # Project Z onto the rank 1 matrices</span>
<span class="sd">        &gt;&gt;&gt;         U, S, Vh = torch.linalg.svd(Z, full_matrices=False)</span>
<span class="sd">        &gt;&gt;&gt;         # Return rescaled singular vectors</span>
<span class="sd">        &gt;&gt;&gt;         s0_sqrt = S[0].sqrt().unsqueeze(-1)</span>
<span class="sd">        &gt;&gt;&gt;         return U[..., :, 0] * s0_sqrt, Vh[..., 0, :] * s0_sqrt</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; linear_rank_one = P.register_parametrization(nn.Linear(4, 4), &quot;weight&quot;, RankOne())</span>
<span class="sd">        &gt;&gt;&gt; print(torch.linalg.matrix_rank(linear_rank_one.weight).item())</span>
<span class="sd">        1</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parametrization</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">):</span>
        <span class="c1"># Correctness checks.</span>
        <span class="c1"># If A is the space of tensors with shape and dtype equal to module.weight</span>
        <span class="c1"># we check that parametrization.forward and parametrization.right_inverse are</span>
        <span class="c1"># functions from A to A</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">unsafe</span><span class="p">:</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">parametrization</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;A parametrization must return a tensor. Got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">Y</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Registering a parametrization may not change the dtype of the tensor, unless the `unsafe` flag is enabled.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;module.</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">.dtype: </span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;parametrization(module.</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">).dtype: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Registering a parametrization may not change the shape of the tensor, unless the `unsafe` flag is enabled.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;module.</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">.shape: </span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;parametrization(module.</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">).shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">parametrization</span><span class="p">,</span> <span class="s2">&quot;right_inverse&quot;</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">Z</span> <span class="o">=</span> <span class="n">parametrization</span><span class="o">.</span><span class="n">right_inverse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>
                <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
                    <span class="k">pass</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;parametrization.right_inverse must return a tensor. Got: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">Z</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">Y</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;The tensor returned by parametrization.right_inverse must have the same dtype &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;as module.</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">, unless the `unsafe` flag is enabled.</span><span class="se">\n</span><span class="s2">&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;module.</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">.dtype: </span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;returned dtype: </span><span class="si">{</span><span class="n">Z</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">Z</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;The tensor returned by parametrization.right_inverse must have the same shape &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;as module.</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">, unless the `unsafe` flag is enabled.</span><span class="se">\n</span><span class="s2">&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;module.</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">.shape: </span><span class="si">{</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;returned shape: </span><span class="si">{</span><span class="n">Z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
            <span class="c1"># else right_inverse is assumed to be the identity</span>

        <span class="c1"># add the new parametrization to the parametrization list</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">)</span>  <span class="c1"># Make mypy happy</span>
        <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parametrization</span><span class="p">)</span>
        <span class="c1"># If unsafe was True in previous parametrization, keep it enabled</span>
        <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span><span class="o">.</span><span class="n">unsafe</span> <span class="o">|=</span> <span class="n">unsafe</span>  <span class="c1"># type: ignore[index, union-attr]</span>
    <span class="k">elif</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_buffers</span> <span class="ow">or</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">_parameters</span><span class="p">:</span>
        <span class="c1"># Set the parametrization mechanism</span>
        <span class="c1"># Fetch the original buffer or parameter</span>
        <span class="n">original</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="c1"># We create this early to check for possible errors</span>
        <span class="n">parametrizations</span> <span class="o">=</span> <span class="n">ParametrizationList</span><span class="p">([</span><span class="n">parametrization</span><span class="p">],</span> <span class="n">original</span><span class="p">,</span> <span class="n">unsafe</span><span class="o">=</span><span class="n">unsafe</span><span class="p">)</span>
        <span class="c1"># Delete the previous parameter or buffer</span>
        <span class="nb">delattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="c1"># If this is the first parametrization registered on the module,</span>
        <span class="c1"># we prepare the module to inject the property</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="c1"># Change the class</span>
            <span class="n">_inject_new_class</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
            <span class="c1"># Inject a ``ModuleDict`` into the instance under module.parametrizations</span>
            <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span> <span class="o">=</span> <span class="n">ModuleDict</span><span class="p">()</span>
        <span class="c1"># Add a property into the class</span>
        <span class="n">_inject_property</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
        <span class="c1"># Add a ParametrizationList</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">)</span>  <span class="c1"># Make mypy happy</span>
        <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">parametrizations</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Module &#39;</span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s2">&#39; does not have a parameter, a buffer, or a &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;parametrized element with name &#39;</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span></div>


<div class="viewcode-block" id="is_parametrized"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized">[docs]</a><span class="k">def</span> <span class="nf">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns ``True`` if module has an active parametrization.</span>

<span class="sd">    If the argument :attr:`tensor_name` is specified, returns ``True`` if</span>
<span class="sd">    ``module[tensor_name]`` is parametrized.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module to query</span>
<span class="sd">        tensor_name (str, optional): attribute in the module to query</span>
<span class="sd">            Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parametrizations</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;parametrizations&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">parametrizations</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parametrizations</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">tensor_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Check that there is at least one parametrized buffer or Parameter</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">parametrizations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="n">parametrizations</span></div>

<div class="viewcode-block" id="remove_parametrizations"><a class="viewcode-back" href="../../../../generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations">[docs]</a><span class="k">def</span> <span class="nf">remove_parametrizations</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">leave_parametrized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Removes the parametrizations on a tensor in a module.</span>

<span class="sd">    - If ``leave_parametrized=True``, ``module[tensor_name]`` will be set to</span>
<span class="sd">      its current output. In this case, the parametrization shall not change the ``dtype``</span>
<span class="sd">      of the tensor.</span>
<span class="sd">    - If ``leave_parametrized=False``, ``module[tensor_name]`` will be set to</span>
<span class="sd">      the unparametrised tensor in ``module.parametrizations[tensor_name].original``.</span>
<span class="sd">      This is only possible when the parametrization depends on just one tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module from which remove the parametrization</span>
<span class="sd">        tensor_name (str): name of the parametrization to be removed</span>
<span class="sd">        leave_parametrized (bool, optional): leave the attribute :attr:`tensor_name` parametrized.</span>
<span class="sd">            Default: ``True``</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: module</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if ``module[tensor_name]`` is not parametrized</span>
<span class="sd">        ValueError: if ``leave_parametrized=False`` and the parametrization depends on several tensors</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Module </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s2"> does not have a parametrization on </span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Fetch the original tensor</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">)</span>  <span class="c1"># Make mypy happy</span>
    <span class="n">parametrizations</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">parametrizations</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">:</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">parametrizations</span><span class="o">.</span><span class="n">original</span>
        <span class="k">if</span> <span class="n">leave_parametrized</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">t</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
            <span class="c1"># We know they have the same dtype because we have checked this when registering the</span>
            <span class="c1"># parametrizations. As such, we can use set_</span>
            <span class="c1"># We do this so that the parameter does not to change the id()</span>
            <span class="c1"># This way the user does not need to update the optimizer</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">original</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
                    <span class="n">original</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">original</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="c1"># TODO: Fix this for tensor subclasses that are parameters:</span>
                        <span class="c1"># RuntimeError: set_storage is not allowed on a Tensor created from .data or .detach().</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Calling remove_parametrizations() with leave_parametrized=True &quot;</span>
                                           <span class="s2">&quot;for a parameter that is an instance of a tensor subclass requires &quot;</span>
                                           <span class="s2">&quot;set_() to be implemented correctly for the tensor subclass. Either &quot;</span>
                                           <span class="s2">&quot;set leave_parametrized=False or provide a working implementation for &quot;</span>
                                           <span class="s2">&quot;set_() in the tensor subclass.&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">leave_parametrized</span><span class="p">:</span>
            <span class="c1"># We cannot use no_grad because we need to know whether one or more</span>
            <span class="c1"># original tensors required grad</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
            <span class="c1"># We&#39;ll have to trust the user to add it to the optimizer</span>
            <span class="n">original</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">else</span> <span class="n">t</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot leave unparametrized (`leave_parametrized=False`) a tensor &quot;</span>
                             <span class="s2">&quot;that is parametrized in terms of a sequence of tensors.&quot;</span><span class="p">)</span>

    <span class="c1"># Delete the property that manages the parametrization</span>
    <span class="nb">delattr</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
    <span class="c1"># Delete the ParametrizationList</span>
    <span class="k">del</span> <span class="n">module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span>

    <span class="c1"># Restore the parameter / buffer into the main class</span>
    <span class="n">_register_parameter_or_buffer</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">,</span> <span class="n">original</span><span class="p">)</span>

    <span class="c1"># Roll back the parametrized class if no other buffer or parameter</span>
    <span class="c1"># is currently parametrized in this class</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
        <span class="nb">delattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;parametrizations&quot;</span><span class="p">)</span>
        <span class="c1"># Restore class</span>
        <span class="n">orig_cls</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__bases__</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span> <span class="o">=</span> <span class="n">orig_cls</span>
    <span class="k">return</span> <span class="n">module</span></div>

<span class="k">def</span> <span class="nf">type_before_parametrizations</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">type</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the module type before parametrizations were applied and if not,</span>
<span class="sd">    then it returns the module type.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module to get type of</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">module</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__bases__</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">transfer_parametrizations_and_params</span><span class="p">(</span>
    <span class="n">from_module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">to_module</span><span class="p">:</span> <span class="n">Module</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Transfers parametrizations and the parameters they parametrize from from_module</span>
<span class="sd">    to to_module. If tensor_name is specified, only transfers the specified parameter, otherwise</span>
<span class="sd">    transfers all parametrized parameters. If those parameters do not exist in to_module, it will create them.</span>
<span class="sd">    Does nothing if from_module is not parametrized.</span>

<span class="sd">    Args:</span>
<span class="sd">        from_module (nn.Module): module to transfer from</span>
<span class="sd">        to_module (nn.Module): module to transfer to</span>
<span class="sd">        tensor_name (str, optional): parameter to transfer</span>

<span class="sd">    Returns:</span>
<span class="sd">        Module: to_module</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">is_parametrized</span><span class="p">(</span><span class="n">from_module</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">from_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">)</span>  <span class="c1"># for mypy</span>

        <span class="c1"># get list of all params or the single param to transfer</span>
        <span class="n">parameters_to_transfer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">from_module</span><span class="o">.</span><span class="n">parametrizations</span> <span class="k">if</span> <span class="n">tensor_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">parameters_to_transfer</span><span class="p">,</span> <span class="s2">&quot;__iter__&quot;</span><span class="p">)</span>  <span class="c1"># for mypy</span>
        <span class="k">for</span> <span class="n">parameter_name</span> <span class="ow">in</span> <span class="n">parameters_to_transfer</span><span class="p">:</span>

            <span class="c1"># initialize the to-be-transferred param in to_module if it doesn&#39;t exist already</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">to_module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span>
                    <span class="n">to_module</span><span class="p">,</span>
                    <span class="n">parameter_name</span><span class="p">,</span>
                    <span class="n">Parameter</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">from_module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">)),</span>
                <span class="p">)</span>

            <span class="c1"># apply the params&#39;s parametrizations to to_module</span>
            <span class="k">for</span> <span class="n">param_func</span> <span class="ow">in</span> <span class="n">from_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">parameter_name</span><span class="p">]:</span>
                <span class="n">register_parametrization</span><span class="p">(</span><span class="n">to_module</span><span class="p">,</span> <span class="n">parameter_name</span><span class="p">,</span> <span class="n">param_func</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">to_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">,</span> <span class="n">ModuleDict</span><span class="p">)</span>  <span class="c1"># for mypy</span>

            <span class="c1"># make values match, original values can be stored in either original or</span>
            <span class="c1"># original0, original1..., need to check both cases</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">from_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">parameter_name</span><span class="p">],</span> <span class="s2">&quot;original&quot;</span><span class="p">):</span>
                <span class="n">to_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">parameter_name</span><span class="p">]</span><span class="o">.</span><span class="n">original</span> <span class="o">=</span> \
                    <span class="n">from_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">parameter_name</span><span class="p">]</span><span class="o">.</span><span class="n">original</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">num</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">orig_num</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
                <span class="c1"># loop through each original# until all values have been set</span>
                <span class="k">while</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">from_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">parameter_name</span><span class="p">],</span> <span class="n">orig_num</span><span class="p">):</span>
                    <span class="nb">setattr</span><span class="p">(</span>
                        <span class="n">to_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">parameter_name</span><span class="p">],</span>
                        <span class="n">orig_num</span><span class="p">,</span>
                        <span class="nb">getattr</span><span class="p">(</span><span class="n">from_module</span><span class="o">.</span><span class="n">parametrizations</span><span class="p">[</span><span class="n">parameter_name</span><span class="p">],</span> <span class="n">orig_num</span><span class="p">),</span>
                    <span class="p">)</span>
                    <span class="n">num</span> <span class="o">=</span> <span class="n">num</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">orig_num</span> <span class="o">=</span> <span class="s2">&quot;original&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">to_module</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
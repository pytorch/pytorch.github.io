


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.nn.api.remote_module &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/nn/api/remote_module.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/distributed/nn/api/remote_module.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.nn.api.remote_module</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.nn.api.remote_module</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/python3</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">types</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">TypeVar</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed.rpc</span> <span class="k">as</span> <span class="nn">rpc</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.distributed.nn.jit</span> <span class="kn">import</span> <span class="n">instantiator</span>
<span class="kn">from</span> <span class="nn">torch.distributed</span> <span class="kn">import</span> <span class="n">_remote_device</span>
<span class="kn">from</span> <span class="nn">torch.distributed.rpc.internal</span> <span class="kn">import</span> <span class="n">_internal_rpc_pickler</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">torch.utils.hooks</span> <span class="kn">import</span> <span class="n">RemovableHandle</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;RemoteModule&quot;</span><span class="p">]</span>

<span class="n">_grad_t</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]</span>
<span class="c1"># See https://mypy.readthedocs.io/en/latest/generics.html#generic-methods-and-generic-self for the use</span>
<span class="c1"># of `T` to annotate `self`. Many methods of `Module` return `self` and we want those return values to be</span>
<span class="c1"># the type of the subclass, not the looser type of `Module`.</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="s2">&quot;Module&quot;</span><span class="p">)</span>

<span class="n">_NON_SCRIPTABLE_REMOTE_MODULE_MODULE</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">instantiator</span><span class="o">.</span><span class="n">instantiate_non_scriptable_remote_module_template</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">_REMOTE_MODULE_PICKLED_ATTRIBUTES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;on&quot;</span><span class="p">,</span>
    <span class="s2">&quot;device&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_device_map_set&quot;</span><span class="p">,</span>
    <span class="s2">&quot;is_scriptable&quot;</span><span class="p">,</span>
    <span class="s2">&quot;generated_methods&quot;</span><span class="p">,</span>
    <span class="s2">&quot;module_rref&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">_SerializedRemoteModule</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;_SerializedRemoteModule&quot;</span><span class="p">,</span> <span class="n">_REMOTE_MODULE_PICKLED_ATTRIBUTES</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>

<span class="c1"># These attributes are mostly from RemoteModule&#39;s parent class and are intentionally not pickled.</span>
<span class="c1"># A new attribute of RemoteModule should be either in _REMOTE_MODULE_PICKLED_ATTRIBUTES</span>
<span class="c1"># or _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING.</span>
<span class="c1"># Otherwise, it will not be pickled.</span>
<span class="n">_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;training&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_parameters&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_buffers&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_non_persistent_buffers_set&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_backward_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_backward_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_is_full_backward_hook&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_hooks_with_kwargs&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_forward_pre_hooks_with_kwargs&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_state_dict_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_state_dict_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_load_state_dict_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_load_state_dict_post_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_state_dict_pre_hooks&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_modules&quot;</span><span class="p">,</span>
    <span class="c1"># The two attributes below are generated methods, not available at pickling time.</span>
    <span class="s2">&quot;forward_async&quot;</span><span class="p">,</span>
    <span class="s2">&quot;forward&quot;</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># RPC handler.</span>
<span class="k">def</span> <span class="nf">_instantiate_template</span><span class="p">(</span><span class="n">module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span><span class="p">):</span>
    <span class="n">instantiator</span><span class="o">.</span><span class="n">instantiate_scriptable_remote_module_template</span><span class="p">(</span>
        <span class="n">module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_create_module</span><span class="p">(</span><span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">module_cls</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Expect `module_cls(*args, **kwargs)` returns an instance of &lt;class nn.Module&gt;, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;but it returns an instance of </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">module</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span> <span class="nf">_create_module_with_interface</span><span class="p">(</span>
    <span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">module_interface_cls</span>
<span class="p">):</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">_create_module</span><span class="p">(</span><span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">module_interface_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">module_interface_cls</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_param_rrefs</span><span class="p">(</span><span class="n">module_rref</span><span class="p">,</span> <span class="n">recurse</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]]:</span>
    <span class="n">ret</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">module_rref</span><span class="o">.</span><span class="n">local_value</span><span class="p">()</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="p">):</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">(</span><span class="n">param</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">_raise_not_supported</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Method ``</span><span class="si">{}</span><span class="s2">`` not supported for RemoteModule&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">_RemoteModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Use __new__ for logging purposes.</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.distributed.nn.api.remote_module&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_RemoteModule</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">remote_device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">module_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">_module_interface_cls</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A RemoteModule instance can only be created after RPC initialization.</span>
<span class="sd">        It creates a user-specified module on a specified remote node.</span>
<span class="sd">        It behaves like a regular ``nn.Module`` except that the ``forward`` method is</span>
<span class="sd">        executed on the remote node.</span>
<span class="sd">        It takes care of autograd recording to ensure the backward pass propagates</span>
<span class="sd">        gradients back to the corresponding remote module.</span>
<span class="sd">        It can be shared across processors using `RPC framework &lt;https://pytorch.org/docs/stable/rpc.html&gt;`__,</span>
<span class="sd">        without incurring any overheads of copying the actual module,</span>
<span class="sd">        which is equivalent to an :class:`~torch.distributed.rpc.RRef`</span>
<span class="sd">        pointing to the remote module.</span>

<span class="sd">        The arguments of ``forward_async`` and ``forward`` are the same as</span>
<span class="sd">        the ``forward`` method of the module returned by the ``module_cls``.</span>

<span class="sd">        Apart from ``forward_async`` and ``forward``, no other methods are supported from nn.Module for now.</span>

<span class="sd">        Particularly, to create a hybrid model, typically the local modules should be</span>
<span class="sd">        created outside of remote modules, rather than as submodules of any remote module (by calling ``add_module``).</span>
<span class="sd">        Hybrid Example:</span>
<span class="sd">                &gt;&gt;&gt; class HybridModel(nn.Module):</span>
<span class="sd">                &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">                &gt;&gt;&gt;         nn.Module.__init__(self)</span>
<span class="sd">                &gt;&gt;&gt;         self.remote_embedding = RemoteModule(...)</span>
<span class="sd">                &gt;&gt;&gt;         self.local_linear = nn.Linear(...)</span>

<span class="sd">        For example, if ``module_cls`` returns an instance of ``nn.Linear``,</span>
<span class="sd">        that has ``forward`` method signature, ``def forward(input: Tensor) -&gt; Tensor:``,</span>
<span class="sd">        the generated ``RemoteModule`` will have 2 methods in signature of</span>
<span class="sd">        ``def forward(input: Tensor) -&gt; Tensor:`` and</span>
<span class="sd">        ``def forward_async(input: Tensor) -&gt; Future[Tensor]:``.</span>

<span class="sd">        .. note::</span>
<span class="sd">            If the remote module is placed on a cuda device,</span>
<span class="sd">            any input CPU tensors will be automatically moved to the same cuda device,</span>
<span class="sd">            and GPU tensors are returned over the wire according to the device map of the remote worker on TensorPipe RPC backend.</span>

<span class="sd">        Args:</span>
<span class="sd">            remote_device (str): Device on the destination worker where we&#39;d like to place this module.</span>
<span class="sd">                The device can be a local device or a remote device specified by one of the following remote</span>
<span class="sd">                formats:</span>

<span class="sd">                    1. &quot;rank:&lt;rank&gt;/&lt;device&gt;&quot; (ex: &quot;rank:0/cuda:0&quot;).</span>
<span class="sd">                    2. &quot;&lt;worker_name&gt;/&lt;device&gt;&quot; (ex: &quot;trainer0/cuda:0&quot;).</span>

<span class="sd">                In addition, the device field can be optional and the default value is &quot;cpu&quot;.</span>
<span class="sd">            module_cls (nn.Module): For example,</span>
<span class="sd">                &gt;&gt;&gt; class MyModule(nn.Module):</span>
<span class="sd">                &gt;&gt;&gt;     def forward(input):</span>
<span class="sd">                &gt;&gt;&gt;         return input + 1</span>
<span class="sd">                &gt;&gt;&gt;</span>
<span class="sd">                &gt;&gt;&gt; module_cls = MyModule</span>
<span class="sd">            args (Sequence, optional): args to be passed to ``module_cls``.</span>
<span class="sd">            kwargs (Dict, optional): kwargs to be passed to ``module_cls``.</span>
<span class="sd">            _module_interface_cls (type, optional): The TorchScript interface type for the module</span>
<span class="sd">                to be created. The type object should be decorated by @torch.jit.interface.</span>
<span class="sd">                If not provided, the generated RemoteModule is not torchscript-able.</span>
<span class="sd">                Warning, this is an experimental API and susceptible to frequent changes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A remote module instance which wraps the :class:`~nn.Module` created by the</span>
<span class="sd">            user-provided ``module_cls``, it has a blocking ``forward`` method and an</span>
<span class="sd">            asynchronous ``forward_async`` method that returns a future of the ``forward`` call</span>
<span class="sd">            on the user-provided module on the remote side.</span>

<span class="sd">        Example::</span>
<span class="sd">            Run the following code in two different processes:</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;distributed&quot;)</span>
<span class="sd">            &gt;&gt;&gt; # On worker 0:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt; from torch import nn, Tensor</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.nn.api.remote_module import RemoteModule</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rpc.init_rpc(&quot;worker0&quot;, rank=0, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt; remote_linear_module = RemoteModule(</span>
<span class="sd">            &gt;&gt;&gt;     &quot;worker1/cpu&quot;, nn.Linear, args=(20, 30),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; input = torch.randn(128, 20)</span>
<span class="sd">            &gt;&gt;&gt; ret_fut = remote_linear_module.forward_async(input)</span>
<span class="sd">            &gt;&gt;&gt; ret = ret_fut.wait()</span>
<span class="sd">            &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">            &gt;&gt;&gt; # On worker 1:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rpc.init_rpc(&quot;worker1&quot;, rank=1, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt; rpc.shutdown()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">enable_moving_cpu_tensors_to_cuda</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_init</span><span class="p">(</span><span class="n">remote_device</span><span class="p">)</span>

        <span class="c1"># Default arguments preparation.</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">()</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">_module_interface_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Users reply on this field to know if this generated RemoteModule is TorchScript-able.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_scriptable</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># Instantiate template on remote side.</span>
            <span class="n">fut</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">rpc_async</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span>
                <span class="n">_instantiate_template</span><span class="p">,</span>
                <span class="p">(</span><span class="n">_module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_init_template</span><span class="p">(</span>
                <span class="n">_module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>
            <span class="p">)</span>

            <span class="c1"># Instantiate template on remote side.</span>
            <span class="n">fut</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">rpc_async</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span>
                <span class="n">_instantiate_template</span><span class="p">,</span>
                <span class="p">(</span><span class="n">_module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span><span class="p">),</span>
            <span class="p">)</span>

            <span class="c1"># Create the module on the remote side.</span>
            <span class="n">fut</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>  <span class="c1"># Ensure remote_module_cls is available on remote side.</span>

            <span class="c1"># TODO: We need to change this to rpc.remote, and make it async (see the else branch below).</span>
            <span class="c1"># For that we need to be able to apply _module_interface_cls to the RRef returned by rpc.remote</span>
            <span class="c1"># See https://github.com/pytorch/pytorch/issues/58098 for more context.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">rpc_sync</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span>
                <span class="n">_create_module_with_interface</span><span class="p">,</span>
                <span class="p">(</span><span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">_module_interface_cls</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_scriptable</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generated_methods</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_NON_SCRIPTABLE_REMOTE_MODULE_MODULE</span><span class="o">.</span><span class="n">_generated_methods</span>
            <span class="p">)</span>
            <span class="c1"># Create the module on the remote side.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span>
                <span class="n">_create_module</span><span class="p">,</span>
                <span class="p">(</span><span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_install_generated_methods</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_attribute_picklability</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">remote_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of :class:`~torch.distributed.rpc.RRef` pointing to the</span>
<span class="sd">        remote module&#39;s parameters. This can typically be used in conjunction</span>
<span class="sd">        with :class:`~torch.distributed.optim.DistributedOptimizer`.</span>

<span class="sd">        Args:</span>
<span class="sd">            recurse (bool): if True, then returns parameters of the remote</span>
<span class="sd">                module and all submodules of the remote module. Otherwise,</span>
<span class="sd">                returns only parameters that are direct members of the</span>
<span class="sd">                remote module.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of :class:`~torch.distributed.rpc.RRef` (``List[RRef[nn.Parameter]]``)</span>
<span class="sd">            to remote module&#39;s parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">rpc</span><span class="o">.</span><span class="n">rpc_sync</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">,</span> <span class="n">_param_rrefs</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span><span class="p">,</span> <span class="n">recurse</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">get_module_rref</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns an :class:`~torch.distributed.rpc.RRef` (``RRef[nn.Module]``)</span>
<span class="sd">        pointing to the remote module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot pickle RemoteModule in python pickler. RemoteModule can only be pickled when using RPC&quot;</span>
        <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot unpickle RemoteModule in python pickler. RemoteModule can only be unpickled when using RPC&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">register_buffer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">register_parameter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Module</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ipu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ipu</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">xpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">float</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">double</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">half</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bfloat16</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">register_backward_hook</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Module</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">],</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">_grad_t</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">register_forward_pre_hook</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hook</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">T</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">T</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]],</span>
        <span class="p">],</span>
        <span class="n">prepend</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_kwargs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hook</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">T</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
            <span class="n">Callable</span><span class="p">[[</span><span class="n">T</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]],</span>
        <span class="p">],</span>
        <span class="n">prepend</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_kwargs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RemovableHandle</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">assign</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Method ``parameters`` not supported for RemoteModule. Please use ``remote_parameters`` instead.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffers</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span>  <span class="c1"># type: ignore[return]</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Module</span><span class="p">]:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">named_children</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Module</span><span class="p">]]:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">modules</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Module</span><span class="p">]:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">named_modules</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">memo</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">remove_duplicate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_modules</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span><span class="o">.</span><span class="n">rpc_sync</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># type: ignore[operator, union-attr]</span>

    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_rref</span><span class="o">.</span><span class="n">rpc_sync</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># type: ignore[operator, union-attr]</span>

    <span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">requires_grad_</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_to_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">share_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">share_memory</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>  <span class="c1"># type: ignore[return]</span>
        <span class="n">_raise_not_supported</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">extra_repr</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prepare_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">remote_device_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares the initialization and returns whether to enable automatically moving CPU tensors to CUDA devices.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Sanity check.</span>
        <span class="k">assert</span> <span class="n">rpc</span><span class="o">.</span><span class="n">_is_current_rpc_agent_set</span><span class="p">(),</span> <span class="s2">&quot;RemoteModule only works in RPC.&quot;</span>

        <span class="n">remote_device</span> <span class="o">=</span> <span class="n">_remote_device</span><span class="p">(</span><span class="n">remote_device_str</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on</span> <span class="o">=</span> <span class="n">remote_device</span><span class="o">.</span><span class="n">worker_name</span><span class="p">()</span> <span class="k">if</span> <span class="n">remote_device</span><span class="o">.</span><span class="n">worker_name</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">remote_device</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">remote_device</span><span class="o">.</span><span class="n">device</span><span class="p">())</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">_get_current_rpc_agent</span><span class="p">()</span>
        <span class="c1"># If the device map of the remote worker is set,</span>
        <span class="c1"># then enable moving any input CPU tensors to the same cuda device.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_device_map_set</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">_get_device_map</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">get_worker_info</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">on</span><span class="p">))</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
        <span class="c1"># ``enable_moving_cpu_tensors_to_cuda`` is less strict than ``is_device_map_set``:</span>
        <span class="c1"># If ``enable_moving_cpu_tensors_to_cuda`` is true, but the device map is not set,</span>
        <span class="c1"># then any CPU tensors can still be moved to a cuda device to run forward,</span>
        <span class="c1"># but the output must be moved back to CPU before being sent over the wire.</span>
        <span class="n">enable_moving_cpu_tensors_to_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span>
        <span class="k">return</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>

    <span class="k">def</span> <span class="nf">_init_template</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiates template on local side.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">generated_module</span> <span class="o">=</span> <span class="n">instantiator</span><span class="o">.</span><span class="n">instantiate_scriptable_remote_module_template</span><span class="p">(</span>
            <span class="n">module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generated_methods</span> <span class="o">=</span> <span class="n">generated_module</span><span class="o">.</span><span class="n">_generated_methods</span>

    <span class="k">def</span> <span class="nf">_check_attribute_picklability</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks if all the attribute has explicitly defined whether to be pickled (i.e., picklability).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_REMOTE_MODULE_PICKLED_ATTRIBUTES</span>
                <span class="ow">and</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                    <span class="s2">&quot;Attribute </span><span class="si">{}</span><span class="s2"> must be either in ``_REMOTE_MODULE_PICKLED_ATTRIBUTES`` or &quot;</span>
                    <span class="s2">&quot;``_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING``.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_install_generated_methods</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generated_methods</span><span class="p">:</span>
            <span class="n">method_name</span> <span class="o">=</span> <span class="n">method</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">init_from_module_rref</span><span class="p">(</span>
        <span class="n">remote_device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">module_rref</span><span class="p">:</span> <span class="n">rpc</span><span class="o">.</span><span class="n">RRef</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">_module_interface_cls</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Besides the constructor, a RemoteModule instance can also be initialized given a module RRef.</span>
<span class="sd">        This alternate initialization method can be particularly useful if we want to create multiple</span>
<span class="sd">        RemoteModule instances that share the same underlying module and reduce memory consumption.</span>

<span class="sd">        Moreover, this also provides a workaround for passing script RemoteModule over RPC,</span>
<span class="sd">        which is not supported. The recommended way is as follows:</span>

<span class="sd">            1. the sender creates a RemoteModule;</span>
<span class="sd">            2. the sender sends its ``module_rref`` over RPC;</span>
<span class="sd">            3. the receiver calls this method to initialize another RemoteModule using the same ``module_rref``.</span>

<span class="sd">        Example::</span>
<span class="sd">            Run the following code in two different processes:</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;distributed&quot;)</span>
<span class="sd">            &gt;&gt;&gt; # On worker 0:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt; from torch import nn, Tensor</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.nn.api.remote_module import RemoteModule</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rpc.init_rpc(&quot;worker0&quot;, rank=0, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt; remote_module = RemoteModule(</span>
<span class="sd">            &gt;&gt;&gt;     &quot;worker1/cpu&quot;, nn.Linear, args=(20, 30),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; remote_module1 = rpc.rpc_sync(</span>
<span class="sd">            &gt;&gt;&gt;     &quot;worker1/cpu&quot;,</span>
<span class="sd">            &gt;&gt;&gt;     RemoteModule.init_from_module_rref,</span>
<span class="sd">            &gt;&gt;&gt;     (&quot;worker1/cpu&quot;, remote_module1.get_module_rref()),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">            &gt;&gt;&gt; # On worker 1:</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; rpc.init_rpc(&quot;worker1&quot;, rank=1, world_size=2)</span>
<span class="sd">            &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">        Args:</span>
<span class="sd">            remote_device (str): Device on the destination worker where we&#39;d like to place this module.</span>
<span class="sd">                The device can be a local device or a remote device specified by one of the following remote</span>
<span class="sd">                formats:</span>

<span class="sd">                    1. &quot;rank:&lt;rank&gt;/&lt;device&gt;&quot; (ex: &quot;rank:0/cuda:0&quot;).</span>
<span class="sd">                    2. &quot;&lt;worker_name&gt;/&lt;device&gt;&quot; (ex: &quot;trainer0/cuda:0&quot;).</span>

<span class="sd">                In addition, the device field can be optional and the default value is &quot;cpu&quot;.</span>
<span class="sd">            module_rref (RRef[nn.Module]): The module reference shared by both the caller and</span>
<span class="sd">                the created remote module.</span>
<span class="sd">            _module_interface_cls (type, optional): The TorchScript interface type for the module</span>
<span class="sd">                to be created. The type object should be decorated by @torch.jit.interface.</span>
<span class="sd">                If not provided, the generated RemoteModule is not torchscript-able.</span>
<span class="sd">                Warning, this is an experimental API and susceptible to frequent changes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A remote module instance which wraps the :class:`~nn.Module` created by the</span>
<span class="sd">            user-provided ``module_rref``, it has a blocking ``forward`` method and an</span>
<span class="sd">            asynchronous ``forward_async`` method that returns a future of the ``forward`` call</span>
<span class="sd">            on the user-provided module on the remote side.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># NOTE: if a new attribute is added to this class, also need to add it</span>
        <span class="c1"># to ``_REMOTE_MODULE_PICKLED_ATTRIBUTES`` for pickling/unpickling.</span>

        <span class="n">remote_module</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">RemoteModule</span><span class="p">)</span>

        <span class="n">enable_moving_cpu_tensors_to_cuda</span> <span class="o">=</span> <span class="n">remote_module</span><span class="o">.</span><span class="n">_prepare_init</span><span class="p">(</span><span class="n">remote_device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">_module_interface_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Users reply on this field to know if this generated RemoteModule is TorchScript-able.</span>
            <span class="n">remote_module</span><span class="o">.</span><span class="n">is_scriptable</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">remote_module</span><span class="o">.</span><span class="n">_init_template</span><span class="p">(</span>
                <span class="n">_module_interface_cls</span><span class="p">,</span> <span class="n">enable_moving_cpu_tensors_to_cuda</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">remote_module</span><span class="o">.</span><span class="n">is_scriptable</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">remote_module</span><span class="o">.</span><span class="n">generated_methods</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_NON_SCRIPTABLE_REMOTE_MODULE_MODULE</span><span class="o">.</span><span class="n">_generated_methods</span>
            <span class="p">)</span>
        <span class="n">remote_module</span><span class="o">.</span><span class="n">module_rref</span> <span class="o">=</span> <span class="n">module_rref</span>

        <span class="n">remote_module</span><span class="o">.</span><span class="n">_install_generated_methods</span><span class="p">()</span>
        <span class="n">remote_module</span><span class="o">.</span><span class="n">_check_attribute_picklability</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">remote_module</span>


<div class="viewcode-block" id="RemoteModule"><a class="viewcode-back" href="../../../../../rpc.html#torch.distributed.nn.api.remote_module.RemoteModule">[docs]</a><span class="k">class</span> <span class="nc">RemoteModule</span><span class="p">(</span><span class="n">_RemoteModule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A RemoteModule instance can only be created after RPC initialization.</span>
<span class="sd">        It creates a user-specified module on a specified remote node.</span>
<span class="sd">        It behaves like a regular ``nn.Module`` except that the ``forward`` method is</span>
<span class="sd">        executed on the remote node.</span>
<span class="sd">        It takes care of autograd recording to ensure the backward pass propagates</span>
<span class="sd">        gradients back to the corresponding remote module.</span>

<span class="sd">        It generates two methods ``forward_async`` and ``forward`` based on the</span>
<span class="sd">        signature of the ``forward`` method of ``module_cls``. ``forward_async``</span>
<span class="sd">        runs asynchronously and returns a Future. The arguments of ``forward_async``</span>
<span class="sd">        and ``forward`` are the same as the ``forward`` method of the module</span>
<span class="sd">        returned by the ``module_cls``.</span>

<span class="sd">        For example, if ``module_cls`` returns an instance of ``nn.Linear``,</span>
<span class="sd">        that has ``forward`` method signature: ``def forward(input: Tensor) -&gt; Tensor:``,</span>
<span class="sd">        the generated ``RemoteModule`` will have 2 methods with the signatures:</span>

<span class="sd">        | ``def forward(input: Tensor) -&gt; Tensor:``</span>
<span class="sd">        | ``def forward_async(input: Tensor) -&gt; Future[Tensor]:``</span>

<span class="sd">    Args:</span>
<span class="sd">        remote_device (str): Device on the destination worker where we&#39;d like to place this module.</span>
<span class="sd">            The format should be &quot;&lt;workername&gt;/&lt;device&gt;&quot;, where the device field can be parsed as torch.device type.</span>
<span class="sd">            E.g., &quot;trainer0/cpu&quot;, &quot;trainer0&quot;, &quot;ps0/cuda:0&quot;.</span>
<span class="sd">            In addition, the device field can be optional and the default value is &quot;cpu&quot;.</span>
<span class="sd">        module_cls (nn.Module): Class for the module to be created remotely. For example,</span>

<span class="sd">            &gt;&gt;&gt; class MyModule(nn.Module):</span>
<span class="sd">            &gt;&gt;&gt;     def forward(input):</span>
<span class="sd">            &gt;&gt;&gt;         return input + 1</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; module_cls = MyModule</span>

<span class="sd">        args (Sequence, optional): args to be passed to ``module_cls``.</span>
<span class="sd">        kwargs (Dict, optional): kwargs to be passed to ``module_cls``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A remote module instance which wraps the :class:`~nn.Module` created by the</span>
<span class="sd">        user-provided ``module_cls``, it has a blocking ``forward`` method and an</span>
<span class="sd">        asynchronous ``forward_async`` method that returns a future of the ``forward`` call</span>
<span class="sd">        on the user-provided module on the remote side.</span>

<span class="sd">    Example::</span>
<span class="sd">        Run the following code in two different processes:</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;distributed&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # On worker 0:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">        &gt;&gt;&gt; from torch import nn, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.nn.api.remote_module import RemoteModule</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rpc.init_rpc(&quot;worker0&quot;, rank=0, world_size=2)</span>
<span class="sd">        &gt;&gt;&gt; remote_linear_module = RemoteModule(</span>
<span class="sd">        &gt;&gt;&gt;     &quot;worker1/cpu&quot;, nn.Linear, args=(20, 30),</span>
<span class="sd">        &gt;&gt;&gt; )</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(128, 20)</span>
<span class="sd">        &gt;&gt;&gt; ret_fut = remote_linear_module.forward_async(input)</span>
<span class="sd">        &gt;&gt;&gt; ret = ret_fut.wait()</span>
<span class="sd">        &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">        &gt;&gt;&gt; # On worker 1:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed.rpc as rpc</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rpc.init_rpc(&quot;worker1&quot;, rank=1, world_size=2)</span>
<span class="sd">        &gt;&gt;&gt; rpc.shutdown()</span>

<span class="sd">        Furthermore, a more practical example that is combined with</span>
<span class="sd">        `DistributedDataParallel &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel&gt;`__ (DDP)</span>
<span class="sd">        can be found in this `tutorial &lt;https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html&gt;`__.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">remote_device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">module_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">remote_device</span><span class="p">,</span> <span class="n">module_cls</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_remote_module_receiver</span><span class="p">(</span>
    <span class="o">*</span><span class="n">remote_module_pickled_attrs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deserializes a RemoteModule.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">serialized_remote_module</span> <span class="o">=</span> <span class="n">_SerializedRemoteModule</span><span class="o">.</span><span class="n">_make</span><span class="p">(</span>
        <span class="n">remote_module_pickled_attrs</span>
    <span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">RemoteModule</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">serialized_remote_module</span><span class="o">.</span><span class="n">_asdict</span><span class="p">())</span>

    <span class="c1"># Unpickling the attribute `module_rref` must invoke RRef&#39;s `_deserialize()` method.</span>
    <span class="n">m</span><span class="o">.</span><span class="n">module_rref</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">PyRRef</span><span class="o">.</span><span class="n">_deserialize</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">module_rref</span><span class="p">)</span>

    <span class="c1"># Install generated methods when unpickled.</span>
    <span class="k">for</span> <span class="n">method</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">generated_methods</span><span class="p">:</span>
        <span class="n">method_name</span> <span class="o">=</span> <span class="n">method</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">method</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">MethodType</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">m</span>


<span class="k">def</span> <span class="nf">_remote_module_reducer</span><span class="p">(</span><span class="n">remote_module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Serializes a RemoteModule.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pickled_attrs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">remote_module</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Pickling the attribute `module_rref` must invoke RRef&#39;s `_serialize()` method.</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;module_rref&quot;</span><span class="p">:</span>
            <span class="n">pickled_attrs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">_serialize</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">_REMOTE_MODULE_PICKLED_ATTRIBUTES</span><span class="p">:</span>
            <span class="n">pickled_attrs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1"># Check if unpickled attributes are all in _REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING.</span>
        <span class="k">elif</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;The new attribute ``</span><span class="si">{}</span><span class="s2">`` of RemoteModule is ignored during RPC pickling. &quot;</span>
                <span class="s2">&quot;To pickle this attribute, please add it to ``_REMOTE_MODULE_PICKLED_ATTRIBUTES``. &quot;</span>
                <span class="s2">&quot;Otherwise, please explicitly add it to ``_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING``.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">k</span>
                <span class="p">),</span>
                <span class="n">file</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">_remote_module_receiver</span><span class="p">,</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="n">pickled_attrs</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_recursive_script_module_receiver</span><span class="p">(</span>
    <span class="n">recursive_script_module_serialized</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deserializes a RecursiveScriptModule that does not contain a script RemoteModule.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">recursive_script_module_serialized</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>


<span class="k">def</span> <span class="nf">_recursive_script_module_reducer</span><span class="p">(</span><span class="n">recursive_script_module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Serializes a RecursiveScriptModule that does not contain a script RemoteModule,</span>
<span class="sd">    and raises an error otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">recursive_script_module</span><span class="o">.</span><span class="n">_c</span><span class="p">,</span> <span class="s2">&quot;module_rref&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Passing a script RemoteModule over RPC is not supported. Please create a RemoteModule in the sender, &quot;</span>
            <span class="s2">&quot;send the `module_rref` to the receiver, and create a new instance on the receiver end by passing this `module_rref`.&quot;</span>
        <span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">recursive_script_module</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_recursive_script_module_receiver</span><span class="p">,</span> <span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">getvalue</span><span class="p">(),))</span>


<span class="n">_internal_rpc_pickler</span><span class="o">.</span><span class="n">_register_reducer</span><span class="p">(</span><span class="n">RemoteModule</span><span class="p">,</span> <span class="n">_remote_module_reducer</span><span class="p">)</span>
<span class="n">_internal_rpc_pickler</span><span class="o">.</span><span class="n">_register_reducer</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">RecursiveScriptModule</span><span class="p">,</span> <span class="n">_recursive_script_module_reducer</span>
<span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
         <script src="../../../../../_static/jquery.js"></script>
         <script src="../../../../../_static/underscore.js"></script>
         <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../../_static/doctools.js"></script>
         <script src="../../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../../_static/clipboard.min.js"></script>
         <script src="../../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
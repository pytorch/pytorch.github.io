


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">default_hooks</span> <span class="k">as</span> <span class="n">default</span>
<span class="kn">from</span> <span class="nn">torch.distributed</span> <span class="kn">import</span> <span class="n">distributed_c10d</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;PowerSGDState&quot;</span><span class="p">,</span> <span class="s2">&quot;powerSGD_hook&quot;</span><span class="p">,</span> <span class="s2">&quot;batched_powerSGD_hook&quot;</span>
<span class="p">]</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_orthogonalize</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decide between Gram-Schmidt or QR factorization to orthogonalize a batch of matrices.</span>
<span class="sd">    QR factorization doesn&#39;t work with half-precision, but it is usually faster with a rank &gt; 2.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">matrices</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">matrices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">matrices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">num_matrices</span> <span class="o">=</span> <span class="n">matrices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">matrices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">matrices</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">]:</span>
        <span class="n">_orthogonalize_gram_schmidt</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span>
            <span class="n">matrices</span><span class="p">,</span>
            <span class="n">out</span><span class="o">=</span><span class="p">(</span>
                <span class="n">matrices</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_matrices</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">matrices</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

<span class="k">def</span> <span class="nf">_orthogonalize_gram_schmidt</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Gram-Schmidt procedure to orthogonalize a batch of matrices.</span>
<span class="sd">    If epsilon is 0, this is equivalent to `torch.qr(matrices, out=(matrices, _))`,</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_cols</span> <span class="o">=</span> <span class="n">matrices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_cols</span><span class="p">):</span>
        <span class="c1"># Normalize the i&#39;th column.</span>
        <span class="n">col</span> <span class="o">=</span> <span class="n">matrices</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># If no epsilon is added here, division by zero may be caused by vanishing gradients.</span>
        <span class="c1"># This epsilon is not needed if the input batch of matrices covers the gradients of at least one entire layer</span>
        <span class="c1"># in the neural network.</span>
        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Note that col ** 2 can underflow/overflow if we use FP16.</span>
            <span class="c1"># May need to consider multiplying a scaling factor and dividing it later, or using bfloat16 instead.</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">col</span> <span class="o">/=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ZeroDivisionError</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="s2">&quot;The matrices to be orthogonalized has at least a column of all 0s. Please set a small value such as 1e-8 &quot;</span>
                    <span class="s2">&quot;as `orthogonalization_epsilon` in PowerSGD state.&quot;</span>
                <span class="p">)</span>
                <span class="c1"># Recover the values from NaNs to 0s.</span>
                <span class="n">col</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">col</span> <span class="o">/=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>
        <span class="c1"># Project it on the rest and remove it.</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">num_cols</span><span class="p">:</span>
            <span class="n">rest</span> <span class="o">=</span> <span class="n">matrices</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
            <span class="n">rest</span> <span class="o">-=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">col</span> <span class="o">*</span> <span class="n">rest</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">col</span>


<span class="k">def</span> <span class="nf">_should_compress</span><span class="p">(</span>
    <span class="n">num_rows</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">,</span> <span class="n">matrix_approximation_rank</span><span class="p">,</span> <span class="n">min_compression_rate</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a recommendation as to whether the 2D tensor described by the arguments is worth compressing,</span>
<span class="sd">    including statistics describing the expected savings from compression.  We consider a tensor worth</span>
<span class="sd">    compressing when ``min_compression_rate`` &lt; uncompressed size / compressed size, where</span>
<span class="sd">    uncompressed size = ``num_rows`` * ``num_cols``,</span>
<span class="sd">    and compressed size = (``num_rows`` + ``num_cols``) * ``matrix_approximation_rank``.</span>

<span class="sd">    The result of this function is a tuple of the form (compression_recommendation, uncompressed_el_count, compressed_el_count), where:</span>

<span class="sd">    compression_recommendation is true if the tensor is worth compressing, and false otherwise (see above);</span>

<span class="sd">    uncompressed_el_count is the uncompressed element count, i.e. ``num_rows`` * ``num_cols``; and,</span>

<span class="sd">    compress_el_count is the element count after compression, i.e. (``num_rows`` + ``num_cols``) * ``matrix_approximation_rank``.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: B950</span>
    <span class="n">uncompressed_size</span> <span class="o">=</span> <span class="n">num_rows</span> <span class="o">*</span> <span class="n">num_cols</span>
    <span class="n">compressed_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_rows</span> <span class="o">+</span> <span class="n">num_cols</span><span class="p">)</span> <span class="o">*</span> <span class="n">matrix_approximation_rank</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">compressed_size</span> <span class="o">*</span> <span class="n">min_compression_rate</span> <span class="o">&lt;</span> <span class="n">uncompressed_size</span><span class="p">,</span>
        <span class="n">uncompressed_size</span><span class="p">,</span>
        <span class="n">compressed_size</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_report_compression_stats</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Report compression stats at the frequency of `compression_stats_logging_frequency` specified in PowerSGD state.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">bucket</span><span class="o">.</span><span class="n">is_last</span><span class="p">()</span>
        <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">iter</span> <span class="o">&gt;=</span> <span class="n">state</span><span class="o">.</span><span class="n">next_stats_report</span>
    <span class="p">):</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">compression_stats</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Compression stats: iter </span><span class="si">%s</span><span class="s2">, total before compression </span><span class="si">%s</span><span class="s2">, total after compression </span><span class="si">%s</span><span class="s2">, &quot;</span>
            <span class="s2">&quot;rate </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span> <span class="n">stats</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">stats</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">state</span><span class="o">.</span><span class="n">next_stats_report</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">iter</span> <span class="o">+</span> <span class="n">state</span><span class="o">.</span><span class="n">compression_stats_logging_frequency</span>


<div class="viewcode-block" id="PowerSGDState"><a class="viewcode-back" href="../../../../../ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState">[docs]</a><span class="k">class</span> <span class="nc">PowerSGDState</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stores both the algorithm&#39;s hyperparameters and the internal state for all the gradients during the training.</span>
<span class="sd">    Particularly, ``matrix_approximation_rank`` and ``start_powerSGD_iter`` are the main hyperparameters that should be tuned by the user.</span>
<span class="sd">    For performance, we suggest to keep binary hyperparameters ``use_error_feedback`` and ``warm_start`` on.</span>

<span class="sd">    1. ``matrix_approximation_rank`` controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression.</span>

<span class="sd">        1.1. If ``matrix_approximation_rank`` is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy.</span>

<span class="sd">        1.2. The increase of ``matrix_approximation_rank`` can substantially increase the computation costs of the compression, and the accuracy may not be further improved beyond a certain ``matrix_approximation_rank`` threshold.</span>

<span class="sd">    To tune ``matrix_approximation_rank``, we suggest to start from 1 and increase by factors of 2 (like an exponential grid search, 1, 2, 4, ...), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.</span>

<span class="sd">    2. ``start_powerSGD_iter`` defers PowerSGD compression until step ``start_powerSGD_iter``, and vanilla allreduce runs prior to step ``start_powerSGD_iter``. This hybrid scheme of **vanilla allreduce + PowerSGD** can effectively improve the accuracy, even a relatively small ``matrix_approximation_rank`` is used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.</span>

<span class="sd">    To tune ``start_powerSGD_iter``, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached. If there is a warm-up stage in the training, ``start_powerSGD_iter`` typically should be no less than the number of warm-up steps.</span>

<span class="sd">    3. ``min_compression_rate`` is the minimum compression rate required when a layer is compressed. Due to the computation overheads incurred by the compression, a tensor is worth compressing only if there can be sufficient saving in bandwidth, where ``(num_rows + num_cols) * matrix_approximation_rank * min_compression_rate &lt; num_rows * num_cols``. If the specified compression rate threshold cannot be satisfied, the tensor will be directly allreduced without compression.</span>

<span class="sd">    Compression statistics are logged every ``compression_stats_logging_frequency`` iterations once PowerSGD compression starts.</span>

<span class="sd">    4. ``orthogonalization_epsilon`` can be a very small value (e.g., 1e-8) added to every normalized matrix column in orthogonalization step, to prevent div-by-zero error if any column has all 0s. If this can already be prevented (e.g., by batch normalization), an epsilon of 0 is recommended for accuracy.</span>

<span class="sd">    5. ``batch_tensors_with_same_shape`` controls whether to compress and decompress tensors with same shape in a batched operation to achieve higher parallelism. Note that you should also increase the bucket size (i.e., ``bucket_cap_mb`` arg in DDP constructor) to make more same-shaped tensors appear in the same bucket, however this may reduce the overlap between computation and communication, and increase the memory footprint due to stacking the tensors of the same shape. Set to ``True`` if the compression / decompression computation is a bottleneck.</span>

<span class="sd">    .. warning ::</span>
<span class="sd">        If error feedback or warm-up is enabled, the minimum value of ``start_powerSGD_iter`` allowed in DDP is 2.</span>
<span class="sd">        This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP,</span>
<span class="sd">        and this can conflict with any tensor memorized before the rebuild process.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: B950</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;process_group&quot;</span><span class="p">,</span>
        <span class="c1"># The fields below are the hyperparameters that often need to be tuned by the user.</span>
        <span class="s2">&quot;matrix_approximation_rank&quot;</span><span class="p">,</span>
        <span class="s2">&quot;start_powerSGD_iter&quot;</span><span class="p">,</span>
        <span class="c1"># The fields below are the hyperparameters that seldom need be tuned by the user.</span>
        <span class="s2">&quot;min_compression_rate&quot;</span><span class="p">,</span>
        <span class="s2">&quot;orthogonalization_epsilon&quot;</span><span class="p">,</span>
        <span class="c1"># The fields below are the binary hyperparameters recommended to be turned on for performance and accuracy.</span>
        <span class="s2">&quot;use_error_feedback&quot;</span><span class="p">,</span>
        <span class="s2">&quot;warm_start&quot;</span><span class="p">,</span>
        <span class="s2">&quot;batch_tensors_with_same_shape&quot;</span><span class="p">,</span>
        <span class="c1"># The fields below are internal state.</span>
        <span class="s2">&quot;rng&quot;</span><span class="p">,</span>
        <span class="s2">&quot;error_dict&quot;</span><span class="p">,</span>
        <span class="s2">&quot;p_memory_dict&quot;</span><span class="p">,</span>
        <span class="s2">&quot;q_memory_dict&quot;</span><span class="p">,</span>
        <span class="s2">&quot;iter&quot;</span><span class="p">,</span>
        <span class="c1"># The fields below are for recording compression stats.</span>
        <span class="s2">&quot;total_numel_before_compression&quot;</span><span class="p">,</span>
        <span class="s2">&quot;total_numel_after_compression&quot;</span><span class="p">,</span>
        <span class="s2">&quot;compression_stats_logging_frequency&quot;</span><span class="p">,</span>
        <span class="s2">&quot;next_stats_report&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">process_group</span><span class="p">,</span>
        <span class="n">matrix_approximation_rank</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">start_powerSGD_iter</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span>
        <span class="n">min_compression_rate</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">use_error_feedback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">orthogonalization_epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">compression_stats_logging_frequency</span><span class="o">=</span><span class="mi">10_000</span><span class="p">,</span>
        <span class="n">batch_tensors_with_same_shape</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;PowerSGD config: matrix_approximation_rank = </span><span class="si">%s</span><span class="s2">; start_powerSGD_iter = </span><span class="si">%s</span><span class="s2">; &quot;</span>
            <span class="s2">&quot;min_compression_rate = </span><span class="si">%s</span><span class="s2">; orthogonalization_epsilon = </span><span class="si">%s</span><span class="s2">; use_error_feedback = </span><span class="si">%s</span><span class="s2">; warm_start = </span><span class="si">%s</span><span class="s2">; &quot;</span>
            <span class="s2">&quot;random_seed = </span><span class="si">%s</span><span class="s2">; compression_stats_logging_frequency = </span><span class="si">%s</span><span class="s2">; batch_tensors_with_same_shape = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">matrix_approximation_rank</span><span class="p">,</span>
            <span class="n">start_powerSGD_iter</span><span class="p">,</span>
            <span class="n">min_compression_rate</span><span class="p">,</span>
            <span class="n">orthogonalization_epsilon</span><span class="p">,</span>
            <span class="n">use_error_feedback</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="p">,</span>
            <span class="n">random_seed</span><span class="p">,</span>
            <span class="n">compression_stats_logging_frequency</span><span class="p">,</span>
            <span class="n">batch_tensors_with_same_shape</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">process_group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matrix_approximation_rank</span> <span class="o">=</span> <span class="n">matrix_approximation_rank</span>
        <span class="c1"># Deferring PowerSGD compression util step &#39;start_powerSGD_iter&#39; can have two advantages:</span>
        <span class="c1"># 1) It turns out that PowerSGD may lead to a non-trivial accuracy loss,</span>
        <span class="c1"># even if the matrix approximation rank is increased to a large value.</span>
        <span class="c1"># To mitigate the accuracy loss, a simple yet effective way is mixing vanilla allreduce</span>
        <span class="c1"># (or a more conservative compression such as FP16 compression) with PowerSGD.</span>
        <span class="c1"># 2) There is an internal optimization of rebuilding buckets process in DDP,</span>
        <span class="c1"># in order to save the memory space.</span>
        <span class="c1"># This step takes place after the first iteration.</span>
        <span class="c1"># However, this means that the shape of input bucketized tensors is subject to change,</span>
        <span class="c1"># which will complicate the implementations of error feedback and warm-up.</span>
        <span class="c1"># Running vanilla allreduce in the first few iterations can avoid this complexity.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">use_error_feedback</span> <span class="ow">or</span> <span class="n">warm_start</span><span class="p">)</span> <span class="ow">and</span> <span class="n">start_powerSGD_iter</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expect `start_powerSGD_iter` &gt; 1 if `use_error_feedback` or `warm_start` is enabled, &quot;</span>
                <span class="s2">&quot;because PowerSGD can only be applied after the first two iterations in DDP.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_powerSGD_iter</span> <span class="o">=</span> <span class="n">start_powerSGD_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_compression_rate</span> <span class="o">=</span> <span class="n">min_compression_rate</span>
        <span class="c1"># Error feedback is usually crucial for both for convergence and generalization,</span>
        <span class="c1"># because PowerSGD is a biased compressor,</span>
        <span class="c1"># i.e., compressing and decompressing a random gradient does not yield the original in expectation.</span>
        <span class="c1"># This mechanism requires a temporary copy of the input gradients,</span>
        <span class="c1"># so it increases the peak memory consumption by the size of the gradient tensor.</span>
        <span class="c1"># However, if the target matrices are known to be exactly low-ranked (instead of just low stable rank),</span>
        <span class="c1"># sometimes it is possible to converge to the optima without error feedback.</span>
        <span class="c1"># See: http://proceedings.mlr.press/v54/yurtsever17a/yurtsever17a.pdf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_error_feedback</span> <span class="o">=</span> <span class="n">use_error_feedback</span>
        <span class="c1"># Warm-start reuses P(s) and Q(s) from the previous iteration.</span>
        <span class="c1"># This can improve the approximation quality and hence improve the accuracy.</span>
        <span class="c1"># Additionally, by avoiding the initialization of these low-rank tensors at every step,</span>
        <span class="c1"># this can also accelerate training.</span>
        <span class="c1"># However, this is at the cost of extra memory.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="c1"># Can use a very small value to prevent div-by-zero error caused by orthogonalization of vanishing gradients.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">orthogonalization_epsilon</span> <span class="o">=</span> <span class="n">orthogonalization_epsilon</span>
        <span class="c1"># The purpose of this RNG is to generate different random seeds for initializing Q across iterations,</span>
        <span class="c1"># but in the same order for all the DDP replicas.</span>
        <span class="c1"># Different random seeds across iterations indicate different &#39;projections&#39; of the gradients at different SGD steps.</span>
        <span class="c1"># If the same random projection is used,</span>
        <span class="c1"># there will be differences between the gradients that are never synchronized.</span>
        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="c1"># Since there is only a single state instance for all the input buckets,</span>
        <span class="c1"># need to maintain a dictionary that maps each bucket index to the local error.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">error_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Iteration/step in the training loop.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Compression stats accumulators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_numel_before_compression</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_numel_after_compression</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># We&#39;ll report compression stats every &#39;compression_stats_logging_frequency&#39; iterations</span>
        <span class="c1"># Note that we always report compression stats at least once.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compression_stats_logging_frequency</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="n">compression_stats_logging_frequency</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">next_stats_report</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Batching tensors with same shape can increase parallelism in compression / decompression computation.</span>
        <span class="c1"># This requires a larger bucket size to make more same-shaped tensor to appear in one bucket, however</span>
        <span class="c1"># this may reduce the overlap between computation and communication, and increase the memory footprint</span>
        <span class="c1"># due to stacking tensors.</span>
        <span class="c1"># Turn on if compression / decompression computation is a bottleneck.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_tensors_with_same_shape</span> <span class="o">=</span> <span class="n">batch_tensors_with_same_shape</span>

<div class="viewcode-block" id="PowerSGDState.__getstate__"><a class="viewcode-back" href="../../../../../ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__">[docs]</a>    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a ``Dict[str, Any]`` which will be pickled and saved.</span>
<span class="sd">        ``process_group`` is not serializable and excluded from</span>
<span class="sd">        a returned state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;NOTE: Process group is not serializable and excluded from a saved state.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">slot</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">slot</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__slots__</span> <span class="k">if</span> <span class="n">slot</span> <span class="o">!=</span> <span class="s2">&quot;process_group&quot;</span>
        <span class="p">}</span></div>

<div class="viewcode-block" id="PowerSGDState.__setstate__"><a class="viewcode-back" href="../../../../../ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__">[docs]</a>    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Takes a provided ``state`` and retrieves ``PowerSGDState``.</span>
<span class="sd">        ``process_group`` is set to default.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;NOTE: Process group will be set to a default group (i.e. the world size).</span><span class="se">\</span>
<span class="s2">                If a different group is desired, please set `self.process_group` after PowerSGD state is loaded.&quot;</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">slot</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slot</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">maybe_increase_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bucket</span><span class="p">):</span>
        <span class="c1"># Since bucket 0 is the last bucket to allreduce in an iteration.</span>
        <span class="c1"># Only increase `iter` when bucket 0 is processed.</span>
        <span class="k">if</span> <span class="n">bucket</span><span class="o">.</span><span class="n">is_last</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iter</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_powerSGD_iter</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Start to apply PowerSGD after </span><span class="si">%s</span><span class="s2"> iterations.&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">compression_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the latest compression statistics as a tuple of the form (compress_rate, numel_before_compression, numel_after_compression), where:</span>

<span class="sd">        compress_rate is the effective compression rate i.e. (number of elements before compression) / (number of elements after compression);</span>

<span class="sd">        numel_before_compression is the total number of elements before compression was applied; and,</span>

<span class="sd">        numel_after_compression is the total number of elements after compression was applied.</span>
<span class="sd">        &quot;&quot;&quot;</span>  <span class="c1"># noqa: B950</span>
        <span class="n">compress_rate</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_numel_before_compression</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_numel_after_compression</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_numel_after_compression</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">compress_rate</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_numel_before_compression</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_numel_after_compression</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="powerSGD_hook"><a class="viewcode-back" href="../../../../../ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook">[docs]</a><span class="k">def</span> <span class="nf">powerSGD_hook</span><span class="p">(</span>
    <span class="n">state</span><span class="p">:</span> <span class="n">PowerSGDState</span><span class="p">,</span> <span class="n">bucket</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">GradBucket</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">Future</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This DDP communication hook implements PowerSGD gradient compression</span>
<span class="sd">    algorithm described in the `paper &lt;https://arxiv.org/abs/1905.13727&gt;`_.</span>
<span class="sd">    Once gradient tensors are aggregated across all workers, this hook applies</span>
<span class="sd">    compression as follows:</span>

<span class="sd">    1. Views the input flattened 1D gradient tensor as a list of per-parameter tensors, and divides all the tensors into two groups:</span>

<span class="sd">        1.1 The tensors that should be compressed before allreduce, because the compression can give enough saving in bandwidth.</span>

<span class="sd">        1.2 Rest of the tensors will be directly allreduced without compression, including all the vector tensors (for biases).</span>

<span class="sd">    2. Handles uncompressed tensors:</span>

<span class="sd">        2.1. Allocate contiguous memory for those uncompressed tensors, and allreduces all the uncompressed tensors as a batch, without compression;</span>

<span class="sd">        2.2. Copies the individual uncompressed tensors from the contiguous memory back to the input tensor.</span>

<span class="sd">    3. Handles the tensors that should be compressed by PowerSGD compression:</span>

<span class="sd">        3.1. For each tensor M, creates two low-rank tensors P and Q for decomposing M,</span>
<span class="sd">        such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;</span>

<span class="sd">        3.2. Computes each P in Ps, which is equal to MQ;</span>

<span class="sd">        3.3. Allreduces Ps as a batch;</span>

<span class="sd">        3.4. Orthogonalizes each P in Ps;</span>

<span class="sd">        3.5. Computes each Q in Qs, which is approximately equal to M^TP;</span>

<span class="sd">        3.6. Allreduces Qs as a batch;</span>

<span class="sd">        3.7. Computes each M among all the compressed tensors, which is approximately equal to PQ^T.</span>

<span class="sd">    Note that this communication hook enforces vanilla allreduce for the first ``state.start_powerSGD_iter`` iterations.</span>
<span class="sd">    This not only gives the user more control over the tradeoff between speedup and accuracy,</span>
<span class="sd">    but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (PowerSGDState): State information to configure the compression rate and support error feedback, warm start, etc.</span>
<span class="sd">            To tune the compression configs, mainly need to tune ``matrix_approximation_rank``, ``start_powerSGD_iter``</span>
<span class="sd">            and ``min_compression_rate``.</span>
<span class="sd">        bucket (dist.GradBucket): Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors.</span>
<span class="sd">            Note that since DDP comm hook only supports single process single device mode,</span>
<span class="sd">            only exactly one tensor is stored in this bucket.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Future handler of the communication, which updates the gradients in place.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1,</span>
<span class="sd">                                  start_powerSGD_iter=10, min_compression_rate=0.5)</span>
<span class="sd">        &gt;&gt;&gt; ddp_model.register_comm_hook(state, powerSGD_hook)</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: B950</span>
    <span class="n">process_group</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">process_group</span>
    <span class="n">group_to_use</span> <span class="o">=</span> <span class="n">process_group</span> <span class="k">if</span> <span class="n">process_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">group_to_use</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="c1"># The input tensor is a flattened 1D tensor.</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">buffer</span><span class="p">()</span>

    <span class="c1"># Run vanilla allreduce in the first `start_powerSGD_iter` iterations.</span>
    <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">iter</span> <span class="o">&lt;</span> <span class="n">state</span><span class="o">.</span><span class="n">start_powerSGD_iter</span><span class="p">:</span>
        <span class="n">state</span><span class="o">.</span><span class="n">maybe_increase_iter</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">default</span><span class="o">.</span><span class="n">_allreduce_fut</span><span class="p">(</span><span class="n">group_to_use</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>

    <span class="c1"># Apply PowerSGD after `start_powerSGD_iter` iterations.</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">device</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span>

    <span class="c1"># Incorporate the error from the previous state into the gradients.</span>
    <span class="n">bucket_index</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">index</span><span class="p">()</span>
    <span class="n">input_tensor_cp</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">total_length</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">use_error_feedback</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">bucket_index</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">error_dict</span><span class="p">:</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">error_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;A zero tensor of length </span><span class="si">%s</span><span class="s2"> that represents local error is created.&quot;</span><span class="p">,</span>
                <span class="n">total_length</span>
            <span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">error_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">total_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
            <span class="p">)</span>

        <span class="c1"># Keep a copy of the input tensor,</span>
        <span class="c1"># so that we can compute the local error caused by compression later,</span>
        <span class="c1"># by comparing this copy and the input tensor updated after decompression.</span>
        <span class="n">input_tensor_cp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="c1"># Unflatten the input tensor into per-parameter tensors, for layer-wise compression.</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">gradients</span><span class="p">()</span>

    <span class="c1"># Step I: Divide all the tensors into two groups,</span>
    <span class="c1"># one will be compressed before allreduce and the other will be directly allreduced without compression.</span>
    <span class="n">tensors_to_compress</span><span class="p">,</span> <span class="n">uncompressed_tensors</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">total_Ps_size</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_Qs_size</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="n">matrix</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">matrix</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">matrix_approximation_rank</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">matrix_approximation_rank</span><span class="p">)</span>
        <span class="n">compress_test</span> <span class="o">=</span> <span class="n">_should_compress</span><span class="p">(</span>
            <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">matrix_approximation_rank</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">min_compression_rate</span>
        <span class="p">)</span>
        <span class="n">state</span><span class="o">.</span><span class="n">total_numel_before_compression</span> <span class="o">+=</span> <span class="n">compress_test</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">compress_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">tensors_to_compress</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
            <span class="n">total_Ps_size</span> <span class="o">+=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">matrix_approximation_rank</span>
            <span class="n">total_Qs_size</span> <span class="o">+=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">matrix_approximation_rank</span>
            <span class="n">state</span><span class="o">.</span><span class="n">total_numel_after_compression</span> <span class="o">+=</span> <span class="n">compress_test</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">uncompressed_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">total_numel_after_compression</span> <span class="o">+=</span> <span class="n">compress_test</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">_report_compression_stats</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

    <span class="c1"># Step II: Handle uncompressed tensors.</span>
    <span class="c1"># Allocate contiguous memory for these tensors to allreduce efficiently.</span>
    <span class="n">uncompressed_tensors_memory</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">uncompressed_tensors</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">uncompressed_tensors</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Step III: Handle the tensors that should be compressed.</span>
    <span class="c1"># Allocate contiguous memory for Ps and Qs to allreduce efficiently.</span>
    <span class="c1"># If warm-start is enabled, reuse Ps and Qs from the previous iteration if possible.</span>
    <span class="c1"># The memory spaces of Ps and Qs need to be allocated in the first iteration when PowerSGD is applied.</span>
    <span class="n">need_randomize_qs</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">state</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">or</span> <span class="n">bucket_index</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">:</span>
        <span class="n">need_randomize_qs</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># If warm-start is disabled, low-rank tensors will be initialized at every step.</span>
        <span class="c1"># Only log this if warm-start to avoid spamming.</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Allocating contiguous memory of length </span><span class="si">%s</span><span class="s2"> for Ps, and of length </span><span class="si">%s</span><span class="s2"> for Qs, respectively.&quot;</span><span class="p">,</span>
                <span class="n">total_Ps_size</span><span class="p">,</span> <span class="n">total_Qs_size</span>
            <span class="p">)</span>
        <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">total_Ps_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">total_Qs_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
        <span class="p">)</span>

    <span class="c1"># Batch tensors to compress by shape.</span>
    <span class="n">shape_to_tensors</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors_to_compress</span><span class="p">:</span>
        <span class="n">shape_to_tensors</span><span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="c1"># This function decides whether to batch tensors with same shape or not according to the argument,</span>
    <span class="c1"># so the following process could share the same code.</span>
    <span class="k">def</span> <span class="nf">maybe_batched_tensors_to_compress</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="n">shape_to_tensors</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">batch_tensors_with_same_shape</span><span class="p">:</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Use the original tensor to avoid copy.</span>
                    <span class="k">yield</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Create Ps and Qs that point to the allocated memory.</span>
    <span class="n">tensors_to_compress</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">qs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">p_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">q_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">maybe_batched_tensors_to_compress</span><span class="p">():</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">matrix_approximation_rank</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">matrix_approximation_rank</span><span class="p">)</span>
        <span class="n">tensors_to_compress</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">ps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">][</span>
                <span class="n">p_idx</span> <span class="p">:</span> <span class="n">p_idx</span> <span class="o">+</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">matrix_approximation_rank</span>
            <span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">matrix_approximation_rank</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">qs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">][</span>
                <span class="n">q_idx</span> <span class="p">:</span> <span class="n">q_idx</span> <span class="o">+</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">matrix_approximation_rank</span>
            <span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">matrix_approximation_rank</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">p_idx</span> <span class="o">+=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">matrix_approximation_rank</span>
        <span class="n">q_idx</span> <span class="o">+=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">matrix_approximation_rank</span>

    <span class="c1"># If warm-start is enabled, reuse Qs from the previous iteration if possible and skip filling random values.</span>
    <span class="c1"># The exception is the first iteration when PowerSGD is applied.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">need_randomize_qs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">qs</span><span class="p">:</span>
            <span class="n">_orthogonalize</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">orthogonalization_epsilon</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">fork_rng</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="p">[]):</span>
            <span class="c1"># Fork this RNG to avoid changing the seed globally and affecting the random sampling anywhere else in the training.</span>
            <span class="c1"># The seed makes sure that the initial random values are the same across all the DDP replicas.</span>
            <span class="c1"># This seed should differ at every step.</span>
            <span class="c1"># Since it is very slow to fork RNG state across all the CUDA devices,</span>
            <span class="c1"># only fork on CPU and then move the generated tensor to the CUDA device (by overwriting q).</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1_000_000_000</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">qs</span><span class="p">:</span>
                <span class="n">q</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                        <span class="o">*</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">_orthogonalize</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">orthogonalization_epsilon</span><span class="p">)</span>

    <span class="c1"># Compute Ps.</span>
    <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tensors_to_compress</span><span class="p">,</span> <span class="n">qs</span><span class="p">,</span> <span class="n">ps</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>

    <span class="c1"># This allreduce is only applied to uncompressed tensors,</span>
    <span class="c1"># so it should have been kicked off before the above computation on the compressed tensors to hide more communication costs.</span>
    <span class="c1"># However, this somehow requires a separate future chain at this time.</span>
    <span class="n">allreduce_contiguous_uncompressed_tensors_fut</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
        <span class="n">uncompressed_tensors_memory</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group_to_use</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span><span class="o">.</span><span class="n">get_future</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">unpack_uncompressed_tensors_and_allreduce_ps</span><span class="p">(</span><span class="n">fut</span><span class="p">):</span>
        <span class="n">uncompressed_tensors_memory</span> <span class="o">=</span> <span class="n">fut</span><span class="o">.</span><span class="n">value</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">world_size</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">uncompressed_tensors</span><span class="p">:</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>
                <span class="n">uncompressed_tensors_memory</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()]</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">idx</span> <span class="o">+=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="c1"># Since these Ps will be orthogonalized later, no need to divide them by world size.</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
                <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="n">group_to_use</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">get_future</span><span class="p">()</span>
            <span class="o">.</span><span class="n">wait</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_qs</span><span class="p">(</span><span class="n">fut</span><span class="p">):</span>
        <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">fut</span><span class="o">.</span><span class="n">value</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ps</span><span class="p">:</span>
            <span class="n">_orthogonalize</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">orthogonalization_epsilon</span><span class="p">)</span>

        <span class="c1"># Compute Qs.</span>
        <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">q</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tensors_to_compress</span><span class="p">,</span> <span class="n">ps</span><span class="p">,</span> <span class="n">qs</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">p</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>

        <span class="c1"># TODO: The above procedure does two matmul+allreduce steps per iteration --</span>
        <span class="c1"># one left multiplication and one right multiplication.</span>
        <span class="c1"># For warm-start, can take one such step at a time, and alternate between them.</span>

        <span class="c1"># Allreduce Qs.</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
                <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="n">group_to_use</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">get_future</span><span class="p">()</span>
            <span class="o">.</span><span class="n">wait</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">decompress</span><span class="p">(</span><span class="n">fut</span><span class="p">):</span>
        <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">fut</span><span class="o">.</span><span class="n">value</span><span class="p">()</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">world_size</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">qs</span><span class="p">,</span> <span class="n">tensors_to_compress</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">out</span><span class="o">=</span><span class="n">tensor</span><span class="p">)</span>

        <span class="c1"># Copy batched tensors back to original buffer.</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">batch_tensors_with_same_shape</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors_to_compress</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Skip tensor with batch_size == 1 since itself is the original tensor.</span>
                    <span class="k">continue</span>
                <span class="n">original_tensors</span> <span class="o">=</span> <span class="n">shape_to_tensors</span><span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">original_tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">original_tensors</span><span class="p">):</span>
                    <span class="n">original_tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">use_error_feedback</span><span class="p">:</span>
            <span class="c1"># Memorize the local errors.</span>
            <span class="n">state</span><span class="o">.</span><span class="n">error_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_tensor_cp</span> <span class="o">-</span> <span class="n">input_tensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">state</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="n">state</span><span class="o">.</span><span class="n">maybe_increase_iter</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_tensor</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="n">allreduce_contiguous_uncompressed_tensors_fut</span><span class="o">.</span><span class="n">then</span><span class="p">(</span>
            <span class="n">unpack_uncompressed_tensors_and_allreduce_ps</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="n">compute_qs</span><span class="p">)</span>
        <span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="n">decompress</span><span class="p">)</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="batched_powerSGD_hook"><a class="viewcode-back" href="../../../../../ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook">[docs]</a><span class="k">def</span> <span class="nf">batched_powerSGD_hook</span><span class="p">(</span>
    <span class="n">state</span><span class="p">:</span> <span class="n">PowerSGDState</span><span class="p">,</span> <span class="n">bucket</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">GradBucket</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">Future</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This DDP communication hook implements a simplified PowerSGD gradient compression</span>
<span class="sd">    algorithm described in the `paper &lt;https://arxiv.org/abs/1905.13727&gt;`_.</span>
<span class="sd">    This variant does not compress the gradients layer by layer,</span>
<span class="sd">    but instead compresses the flattened input tensor that batches all the gradients.</span>
<span class="sd">    Therefore, it is **faster** than :meth:`powerSGD_hook`,</span>
<span class="sd">    but usually results in a **much lower accuracy**, unless ``matrix_approximation_rank`` is 1.</span>

<span class="sd">    .. warning ::</span>
<span class="sd">        Increasing ``matrix_approximation_rank`` here may not necessarily increase the accuracy,</span>
<span class="sd">        because batching per-parameter tensors without column/row alignment can destroy low-rank structure.</span>
<span class="sd">        Therefore, the user should always consider :meth:`powerSGD_hook` first,</span>
<span class="sd">        and only consider this variant when a satisfactory accuracy can be achieved when ``matrix_approximation_rank`` is 1.</span>

<span class="sd">    Once gradient tensors are aggregated across all workers, this hook applies</span>
<span class="sd">    compression as follows:</span>

<span class="sd">    1. Views the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings;</span>

<span class="sd">    2. Creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;</span>

<span class="sd">    3. Computes P, which is equal to MQ;</span>

<span class="sd">    4. Allreduces P;</span>

<span class="sd">    5. Orthogonalizes P;</span>

<span class="sd">    6. Computes Q, which is approximately equal to M^TP;</span>

<span class="sd">    7. Allreduces Q;</span>

<span class="sd">    8. Computes M, which is approximately equal to PQ^T.</span>

<span class="sd">    9. Truncates the input tensor to the original length.</span>

<span class="sd">    Note that this communication hook enforces vanilla allreduce for the first ``state.start_powerSGD_iter`` iterations.</span>
<span class="sd">    This not only gives the user more control over the tradeoff between speedup and accuracy,</span>
<span class="sd">    but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.</span>

<span class="sd">    Args:</span>
<span class="sd">        state (PowerSGDState): State information to configure the compression rate and support error feedback, warm start, etc.</span>
<span class="sd">            To tune the compression configs, mainly need to tune ``matrix_approximation_rank`` and ``start_powerSGD_iter``.</span>
<span class="sd">        bucket (dist.GradBucket): Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors.</span>
<span class="sd">            Note that since DDP comm hook only supports single process single device mode,</span>
<span class="sd">            only exactly one tensor is stored in this bucket.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Future handler of the communication, which updates the gradients in place.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)</span>
<span class="sd">        &gt;&gt;&gt; ddp_model.register_comm_hook(state, batched_powerSGD_hook)</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: B950</span>
    <span class="n">process_group</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">process_group</span>
    <span class="n">group_to_use</span> <span class="o">=</span> <span class="n">process_group</span> <span class="k">if</span> <span class="n">process_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">group_to_use</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="c1"># The input tensor is a flattened 1D tensor.</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">buffer</span><span class="p">()</span>

    <span class="c1"># Run vanilla allreduce in the first `start_powerSGD_iter` iterations.</span>
    <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">iter</span> <span class="o">&lt;</span> <span class="n">state</span><span class="o">.</span><span class="n">start_powerSGD_iter</span><span class="p">:</span>
        <span class="n">state</span><span class="o">.</span><span class="n">maybe_increase_iter</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">default</span><span class="o">.</span><span class="n">_allreduce_fut</span><span class="p">(</span><span class="n">group_to_use</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>

    <span class="c1"># Apply PowerSGD after `start_powerSGD_iter` iterations.</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">device</span>
    <span class="n">total_length</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">state</span><span class="o">.</span><span class="n">total_numel_before_compression</span> <span class="o">+=</span> <span class="n">total_length</span>

    <span class="c1"># View the input tensor as a 2D square-shape tensor, and pad 0s if necessary.</span>
    <span class="n">square_side_length</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">total_length</span><span class="p">))</span>
    <span class="n">state</span><span class="o">.</span><span class="n">total_numel_after_compression</span> <span class="o">+=</span> <span class="p">(</span>
        <span class="n">square_side_length</span> <span class="o">*</span> <span class="n">state</span><span class="o">.</span><span class="n">matrix_approximation_rank</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="n">padded_total_length</span> <span class="o">=</span> <span class="n">square_side_length</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">input_tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">padded_total_length</span><span class="p">)</span>
    <span class="n">input_tensor</span><span class="p">[</span><span class="n">total_length</span><span class="p">:</span><span class="n">padded_total_length</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">_report_compression_stats</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

    <span class="c1"># Incorporate the error from the previous state into the gradients.</span>
    <span class="n">bucket_index</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">index</span><span class="p">()</span>
    <span class="n">input_tensor_cp</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">use_error_feedback</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">bucket_index</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">error_dict</span><span class="p">:</span>
            <span class="n">input_tensor</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">error_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;A zero tensor of length </span><span class="si">%s</span><span class="s2"> that represents local error is created.&quot;</span><span class="p">,</span>
                <span class="n">padded_total_length</span>
            <span class="p">)</span>
            <span class="n">state</span><span class="o">.</span><span class="n">error_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">padded_total_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span>

        <span class="c1"># Keep a copy of the input tensor,</span>
        <span class="c1"># so that we can compute the local error caused by compression later,</span>
        <span class="c1"># by comparing this copy and the input tensor updated after decompression.</span>
        <span class="n">input_tensor_cp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">square_side_length</span><span class="p">,</span> <span class="n">square_side_length</span><span class="p">)</span>

    <span class="c1"># Reuse P and Q from the previous iteration if possible.</span>
    <span class="c1"># The memory spaces of P and Q need to be allocated in the first iteration when PowerSGD is applied.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">state</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">or</span> <span class="n">bucket_index</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">:</span>
        <span class="c1"># If warm-start is disabled, low-rank tensors will be initialized at every step.</span>
        <span class="c1"># Only log this if warm-start to avoid spamming.</span>
        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Initializing low-rank tensors P and Q, each of which has a shape of </span><span class="si">%s</span><span class="s2"> x </span><span class="si">%s</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">square_side_length</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">matrix_approximation_rank</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">create_low_rank_tensor</span><span class="p">(</span><span class="n">fill_random_values</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
            <span class="s2">&quot;Returns a low-rank 2D tensor of square_side_length * matrix_approximation_rank.&quot;</span>
            <span class="k">if</span> <span class="n">fill_random_values</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">fork_rng</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="p">[]):</span>
                    <span class="c1"># Fork this RNG to avoid changing the seed globally and affecting the random sampling</span>
                    <span class="c1"># anywhere else in the training.</span>
                    <span class="c1"># The seed makes sure that the initial random values are the same across all the DDP replicas.</span>
                    <span class="c1"># This seed should differ at every step.</span>
                    <span class="c1"># Since it is very slow to fork RNG state across all the CUDA devices,</span>
                    <span class="c1"># only fork on CPU and then move the generated tensor to the CUDA device.</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1_000_000_000</span><span class="p">))</span>
                    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                        <span class="n">square_side_length</span><span class="p">,</span>
                        <span class="n">state</span><span class="o">.</span><span class="n">matrix_approximation_rank</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                    <span class="n">square_side_length</span><span class="p">,</span>
                    <span class="n">state</span><span class="o">.</span><span class="n">matrix_approximation_rank</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_low_rank_tensor</span><span class="p">(</span>
            <span class="n">fill_random_values</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">rng</span>
        <span class="p">)</span>
        <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_low_rank_tensor</span><span class="p">(</span>
            <span class="n">fill_random_values</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">rng</span>
        <span class="p">)</span>
    <span class="n">_orthogonalize</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">])</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
        <span class="n">matrix</span><span class="p">,</span> <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">],</span> <span class="n">out</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">allreduce_p_fut</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
        <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="n">group_to_use</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span><span class="o">.</span><span class="n">get_future</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">compute_q</span><span class="p">(</span><span class="n">fut</span><span class="p">):</span>
        <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">fut</span><span class="o">.</span><span class="n">value</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">_orthogonalize</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">])</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
            <span class="n">matrix</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span>
            <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">],</span>
            <span class="n">out</span><span class="o">=</span><span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1"># TODO: The above procedure does two matmul+allreduce steps per iteration --</span>
        <span class="c1"># one left multiplication and one right multiplication.</span>
        <span class="c1"># For warm-start, can take one such step at a time, and alternate between them.</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
                <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="n">group_to_use</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">get_future</span><span class="p">()</span>
            <span class="o">.</span><span class="n">wait</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">decompress</span><span class="p">(</span><span class="n">fut</span><span class="p">):</span>
        <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">fut</span><span class="o">.</span><span class="n">value</span><span class="p">()</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">world_size</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
            <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">],</span>
            <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span>
            <span class="n">out</span><span class="o">=</span><span class="n">matrix</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">use_error_feedback</span><span class="p">:</span>
            <span class="c1"># Memorize the local errors.</span>
            <span class="n">state</span><span class="o">.</span><span class="n">error_dict</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_tensor_cp</span> <span class="o">-</span> <span class="n">input_tensor</span>
        <span class="c1"># Removing this seemingly unnecessary sync somehow may cause failures.</span>
        <span class="c1"># See: https://github.com/pytorch/pytorch/pull/54838</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">state</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="n">state</span><span class="o">.</span><span class="n">p_memory_dict</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="n">state</span><span class="o">.</span><span class="n">q_memory_dict</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">total_length</span><span class="p">)</span>

        <span class="n">state</span><span class="o">.</span><span class="n">maybe_increase_iter</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">return</span> <span class="n">allreduce_p_fut</span><span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="n">compute_q</span><span class="p">)</span><span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="n">decompress</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
         <script src="../../../../../_static/jquery.js"></script>
         <script src="../../../../../_static/underscore.js"></script>
         <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../../_static/doctools.js"></script>
         <script src="../../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../../_static/clipboard.min.js"></script>
         <script src="../../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
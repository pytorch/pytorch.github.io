


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.optim.zero_redundancy_optimizer &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/optim/zero_redundancy_optimizer.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/distributed/optim/zero_redundancy_optimizer.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.optim.zero_redundancy_optimizer</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.optim.zero_redundancy_optimizer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">enum</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms.join</span> <span class="kn">import</span> <span class="n">Join</span><span class="p">,</span> <span class="n">Joinable</span><span class="p">,</span> <span class="n">JoinHook</span>
<span class="kn">from</span> <span class="nn">torch.distributed.optim.utils</span> <span class="kn">import</span> <span class="n">functional_optim_map</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ZeroRedundancyOptimizer&quot;</span><span class="p">]</span>


<span class="c1"># Credits:  classy_vision/generic/distributed_util.py</span>
<span class="k">def</span> <span class="nf">_recursive_copy_to_device</span><span class="p">(</span>
    <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recursively searches lists, tuples, dicts and copies tensors to device if</span>
<span class="sd">    possible. Non-tensor values are passed as-is in the result.</span>

<span class="sd">    .. note:  These are all copies, so if there are two objects that reference</span>
<span class="sd">    the same object, then after this call, there will be two different objects</span>
<span class="sd">    referenced on the device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">values</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">_recursive_copy_to_device</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">value</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="n">values</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Mapping</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">_recursive_copy_to_device</span><span class="p">(</span>
                <span class="n">val</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>

    <span class="k">return</span> <span class="n">value</span>


<span class="k">def</span> <span class="nf">_is_trainable</span><span class="p">(</span><span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns if a parameter is trainable, where trainability is equivalent to</span>
<span class="sd">    requiring a gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span>


<span class="k">def</span> <span class="nf">_broadcast_object</span><span class="p">(</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="n">src_rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">group</span><span class="p">:</span> <span class="nb">object</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts an object to the given group, sending the object if called from</span>
<span class="sd">    the source rank and receiving the object otherwise.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        obj: object to broadcast; only used if called on the source rank.</span>
<span class="sd">        src_rank (int): source rank.</span>
<span class="sd">        group (``ProcessGroup``, optional): group used for the broadcast</span>
<span class="sd">            (default: ``dist.group.WORLD``).</span>
<span class="sd">        device (``torch.device``, optional): device to send from or receive</span>
<span class="sd">            to (default: ``torch.device(&quot;cpu&quot;)``).</span>

<span class="sd">    Returns:</span>
<span class="sd">        The broadcasted object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="n">src_rank</span><span class="p">:</span>
        <span class="c1"># Send the object</span>
        <span class="n">buffer</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">buffer</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">bytearray</span><span class="p">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">getbuffer</span><span class="p">())</span>
        <span class="n">length_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">data_send_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">length_tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src_rank</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">data_send_tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src_rank</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive the object</span>
        <span class="n">length_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">length_tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src_rank</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">data_recv_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">length_tensor</span><span class="o">.</span><span class="n">item</span><span class="p">())],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">data_recv_tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src_rank</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">buffer</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">data_recv_tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">obj</span>


<span class="k">class</span> <span class="nc">_ZeROJoinHook</span><span class="p">(</span><span class="n">JoinHook</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">zero</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">zero</span><span class="p">,</span> <span class="n">ZeroRedundancyOptimizer</span><span class="p">),</span> <span class="p">(</span>
            <span class="s2">&quot;ZeRO join hook requires passing in a ZeroRedundancyOptimizer &quot;</span>
            <span class="s2">&quot;instance as the state&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero</span> <span class="o">=</span> <span class="n">zero</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">main_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs an optimizer step, which updates the joined process&#39;s shard of</span>
<span class="sd">        the parameters and broadcasts those parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_DDPBucketAssignment</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This represents a :class:`DistributedDataParallel` bucket assignment,</span>
<span class="sd">    meaning a (possibly non-strict) subset of the parameters corresponding to</span>
<span class="sd">    a DDP bucket assigned to a rank to update.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        bucket_index (int): index of the bucket determined by the DDP gradient</span>
<span class="sd">            bucket all-reduce order.</span>
<span class="sd">        parameters (List[torch.Tensor]): model parameters in the bucket</span>
<span class="sd">            assigned to this rank.</span>
<span class="sd">        offset (int): offset into the :class:`GradBucket` &#39;s :meth:`parameters`</span>
<span class="sd">            giving the index of the first element in the passed-in</span>
<span class="sd">            ``parameters``; this equivalently indexes into the</span>
<span class="sd">            :class:`GradBucket` &#39;s :meth:`gradients`.</span>
<span class="sd">        device (torch.device): device on which the parameters are stored.</span>
<span class="sd">        tensor (torch.Tensor): flattened tensor giving the data of the</span>
<span class="sd">            parameter subset assigned to the rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">bucket_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">offset</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_index</span> <span class="o">=</span> <span class="n">bucket_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span> <span class="o">=</span> <span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offset</span> <span class="o">=</span> <span class="n">offset</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Empty bucket assignment&quot;</span><span class="p">)</span>
        <span class="c1"># DDP guarantees all parameters in the bucket have the same device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">_OverlapStatus</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This defines the three possible statuses that</span>
<span class="sd">    :class:`ZeroRedundancyOptimizer` can be in when overlapping with</span>
<span class="sd">    :class:`DistributedDataParallel`.</span>

<span class="sd">        ``UNINITIALIZED``: The ZeRO instance is effectively uninitialized and</span>
<span class="sd">            is waiting for DDP to finalize its bucketing.</span>
<span class="sd">        ``DDP_HAS_REBUILT_BUCKETS``: DDP has rebuilt its buckets, meaning that</span>
<span class="sd">            its bucketing is finalized. The ZeRO instance can now collect the</span>
<span class="sd">            necessary information about the DDP bucketing.</span>
<span class="sd">        ``INITIALIZED``: The ZeRO instance is fully initialized and can now</span>
<span class="sd">            optimize parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">UNINITIALIZED</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">DDP_HAS_REBUILT_BUCKETS</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">INITIALIZED</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">class</span> <span class="nc">_OverlapInfo</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This contains the information needed by :class:`ZeroRedundancyOptimizer`</span>
<span class="sd">    to overlap with :class:`DistributedDataParallel`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        world_size (int): world size of the process group being used.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        shard_buckets (bool): if ``True``, then the assignment of each</span>
<span class="sd">            :class:`DistributedDataParallel` bucket is partitioned across</span>
<span class="sd">            possibly multiple :class:`ZeroRedundancyOptimizer` instances (i.e.</span>
<span class="sd">            across possibly multiple ranks) to approximate uniformity following</span>
<span class="sd">            a threshold given by the total parameter size divided by the world</span>
<span class="sd">            size; if ``False``, then each bucket is wholly assigned to a single</span>
<span class="sd">            :class:`ZeroRedundancyOptimizer` instance (i.e. to a single rank);</span>
<span class="sd">            this should be set to the value passed into the hook constructor.</span>
<span class="sd">        status (_OverlapStatus): current status; see :class:`_OverlapStatus`</span>
<span class="sd">            for more information.</span>
<span class="sd">        params_per_bucket (List[List[torch.Tensor]]): ``params_per_bucket[i]``</span>
<span class="sd">            gives the model parameters in the ``i``th bucket.</span>
<span class="sd">        params_per_rank (List[List[torch.Tensor]]): ``params_per_rank[i]``</span>
<span class="sd">            gives the model parameters assigned to the ``i``th rank, where the</span>
<span class="sd">            parameters are grouped by increasing bucket indices.</span>
<span class="sd">        offsets (Dict[int, int]): maps from bucket index to the offset in</span>
<span class="sd">            ``self.params_per_rank[rank]`` giving the index of the first</span>
<span class="sd">            parameter in that bucket, where ``rank`` is this process&#39;s own</span>
<span class="sd">            rank; the keys of this :class:`dict` are the bucket indices</span>
<span class="sd">            assigned to this rank.</span>
<span class="sd">        num_bucket_assignments (int): total number of bucket assignments across</span>
<span class="sd">            all ranks; this is equal to the number of</span>
<span class="sd">            :class:`DistributedDataParallel` gradient buckets if</span>
<span class="sd">            ``shard_buckets=False`` and possibly greater otherwise.</span>
<span class="sd">        total_size (int, optional): total size of all buckets (i.e. sum of</span>
<span class="sd">            ``param.numel()`` for all ``param`` across all buckets) if</span>
<span class="sd">            ``shard_buckets=True``; otherwise, ``None``.</span>
<span class="sd">        broadcast_handles (List[Work]): :class:`list` of async work handles for</span>
<span class="sd">            the parameter broadcasts.</span>
<span class="sd">        bucket_index_to_future (Dict[int, torch.futures.Future]):</span>
<span class="sd">            :class:`dict` mapping bucket index to the corresponding all-reduce</span>
<span class="sd">            future.</span>
<span class="sd">        bucket_index_to_bucket (Dict[int, dist.GradBucket]): :class:`dict`</span>
<span class="sd">            mapping bucket index to the corresponding bucket.</span>
<span class="sd">        bucket_indices_seen (List[int]): :class:`list` of the bucket indices</span>
<span class="sd">            seen on this iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">status</span><span class="p">:</span> <span class="n">_OverlapStatus</span> <span class="o">=</span> <span class="n">_OverlapStatus</span><span class="o">.</span><span class="n">UNINITIALIZED</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shard_buckets</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Modified per bucket reconstruction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params_per_bucket</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params_per_rank</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">offsets</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># Group Ranks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assigned_ranks_per_bucket</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bucket_assignments</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Modified per iteration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_handles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_indices_seen</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Used by `hook_with_zero_step()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_index_to_future</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">Future</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_index_to_bucket</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">GradBucket</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">wait_for_broadcasts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Waits for all parameter broadcasts. This should be called once all</span>
<span class="sd">        broadcasts have been scheduled, meaning ``self.broadcast_handles`` is</span>
<span class="sd">        filled. This clears ``self.broadcast_handles`` in preparation for the</span>
<span class="sd">        next iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">broadcast_handles</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bucket_assignments</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Missing at least one broadcast handle on rank </span><span class="si">{</span><span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">_</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_handles</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_handles</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">clear_per_iter_info</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clears the data structures that are modified per-iteration. This should</span>
<span class="sd">        be called at the end of an iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_indices_seen</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_index_to_future</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_index_to_bucket</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>


<div class="viewcode-block" id="ZeroRedundancyOptimizer"><a class="viewcode-back" href="../../../../distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer">[docs]</a><span class="k">class</span> <span class="nc">ZeroRedundancyOptimizer</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">Joinable</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class wraps an arbitrary :class:`optim.Optimizer</span>
<span class="sd">    &lt;torch.optim.Optimizer&gt;` and shards its states across ranks in the group as</span>
<span class="sd">    described by ZeRO_. The local optimizer instance in each rank is only</span>
<span class="sd">    responsible for updating approximately ``1 / world_size`` parameters and</span>
<span class="sd">    hence only needs to keep ``1 / world_size`` optimizer states. After</span>
<span class="sd">    parameters are updated locally, each rank will broadcast its parameters to</span>
<span class="sd">    all other peers to keep all model replicas in the same state.</span>
<span class="sd">    ``ZeroRedundancyOptimizer`` can be used in conjunction with</span>
<span class="sd">    :class:`torch.nn.parallel.DistributedDataParallel` to reduce per-rank peak</span>
<span class="sd">    memory consumption.</span>

<span class="sd">    ``ZeroRedundancyOptimizer`` uses a sorted-greedy algorithm to pack a number</span>
<span class="sd">    of parameters at each rank. Each parameter belongs to a single rank and is</span>
<span class="sd">    not divided among ranks. The partition is arbitrary and might not match the</span>
<span class="sd">    the parameter registration or usage order.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        params (``Iterable``): an ``Iterable`` of :class:`torch.Tensor` s</span>
<span class="sd">            or :class:`dict` s giving all parameters, which will be sharded</span>
<span class="sd">            across ranks.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        optimizer_class (:class:`torch.nn.Optimizer`): the class of the local</span>
<span class="sd">            optimizer.</span>
<span class="sd">        process_group (``ProcessGroup``, optional): ``torch.distributed``</span>
<span class="sd">            ``ProcessGroup`` (default: ``dist.group.WORLD`` initialized by</span>
<span class="sd">            :meth:`torch.distributed.init_process_group`).</span>
<span class="sd">        parameters_as_bucket_view (bool, optional): if ``True``, parameters are</span>
<span class="sd">            packed into buckets to speed up communication, and ``param.data``</span>
<span class="sd">            fields point to bucket views at different offsets; if ``False``,</span>
<span class="sd">            each individual parameter is communicated separately, and each</span>
<span class="sd">            ``params.data`` stays intact (default: ``False``).</span>
<span class="sd">        overlap_with_ddp (bool, optional): if ``True``, :meth:`step` is</span>
<span class="sd">            overlapped with :class:`DistributedDataParallel` &#39;s gradient</span>
<span class="sd">            synchronization; this requires (1) either a functional optimizer</span>
<span class="sd">            for the ``optimizer_class`` argument or one with a functional</span>
<span class="sd">            equivalent and (2) registering a DDP communication hook</span>
<span class="sd">            constructed from one of the functions in ``ddp_zero_hook.py``;</span>
<span class="sd">            parameters are packed into buckets matching those in</span>
<span class="sd">            :class:`DistributedDataParallel`, meaning that the</span>
<span class="sd">            ``parameters_as_bucket_view`` argument is ignored.</span>
<span class="sd">            If ``False``, :meth:`step` runs disjointly after the backward pass</span>
<span class="sd">            (per normal).</span>
<span class="sd">            (default: ``False``)</span>
<span class="sd">        **defaults: any trailing arguments, which are forwarded to the local</span>
<span class="sd">            optimizer.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; import torch.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.optim import ZeroRedundancyOptimizer</span>
<span class="sd">        &gt;&gt;&gt; from torch.nn.parallel import DistributedDataParallel as DDP</span>
<span class="sd">        &gt;&gt;&gt; model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])</span>
<span class="sd">        &gt;&gt;&gt; ddp = DDP(model, device_ids=[rank])</span>
<span class="sd">        &gt;&gt;&gt; opt = ZeroRedundancyOptimizer(</span>
<span class="sd">        &gt;&gt;&gt;     ddp.parameters(),</span>
<span class="sd">        &gt;&gt;&gt;     optimizer_class=torch.optim.Adam,</span>
<span class="sd">        &gt;&gt;&gt;     lr=0.01</span>
<span class="sd">        &gt;&gt;&gt; )</span>
<span class="sd">        &gt;&gt;&gt; ddp(inputs).sum().backward()</span>
<span class="sd">        &gt;&gt;&gt; opt.step()</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, ``ZeroRedundancyOptimizer`` requires that all of the</span>
<span class="sd">        passed-in parameters are the same dense type.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If you pass ``overlap_with_ddp=True``, be wary of the following: Given</span>
<span class="sd">        the way that overlapping :class:`DistributedDataParallel` with</span>
<span class="sd">        :class:`ZeroRedundancyOptimizer` is currently implemented, the first</span>
<span class="sd">        two or three training iterations do not perform parameter updates in</span>
<span class="sd">        the optimizer step, depending on if ``static_graph=False`` or</span>
<span class="sd">        ``static_graph=True``, respectively. This is because it needs</span>
<span class="sd">        information about the gradient bucketing strategy used by</span>
<span class="sd">        :class:`DistributedDataParallel`, which is not finalized until the</span>
<span class="sd">        second forward pass if ``static_graph=False`` or until the third</span>
<span class="sd">        forward pass if ``static_graph=True``. To adjust for this, one option</span>
<span class="sd">        is to prepend dummy inputs.</span>

<span class="sd">    .. warning:: ZeroRedundancyOptimizer is experimental and subject to change.</span>

<span class="sd">    .. _ZeRO: https://arxiv.org/abs/1910.02054</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params</span><span class="p">,</span>
        <span class="n">optimizer_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">],</span>
        <span class="n">process_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parameters_as_bucket_view</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">overlap_with_ddp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">defaults</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Perform type and assumption checks on the input parameters</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_verify_and_init_params</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_same_dense_param_type</span><span class="p">()</span>

        <span class="c1"># NOTE: The parent constructor uses `add_param_group()` which is</span>
        <span class="c1"># partially overloaded in ZeroRedundancyOptimizer, so we use the</span>
        <span class="c1"># `initialized` flag to dissociate the behaviour of `add_param_group()`</span>
        <span class="c1"># between the parent and child.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">Optimizer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
        <span class="n">Joinable</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># Now, all parameters are held in both `self._all_params` and</span>
        <span class="c1"># `self.param_groups`</span>

        <span class="c1"># Internal data structures (`_cache` indicates lazily evaluated)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_rank_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_index_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_index_to_param_cache</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank_cache</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span>
            <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">_DDPBucketAssignment</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_trainable_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_is_trainable_mask</span><span class="p">()</span>

        <span class="c1"># Default device for collective communication and buckets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_default_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">process_group</span> <span class="k">if</span> <span class="n">process_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">overlap_with_ddp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optim_defaults</span> <span class="o">=</span> <span class="n">defaults</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optim_constructor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_optimizer_constructor</span><span class="p">(</span><span class="n">optimizer_class</span><span class="p">)</span>

        <span class="c1"># If `overlap_with_ddp=True`, local optimizer initialization is delayed</span>
        <span class="c1"># to run time after the necessary information has been collected</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">overlap_with_ddp</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_local_optimizer</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span><span class="p">:</span> <span class="n">_OverlapInfo</span> <span class="o">=</span> <span class="n">_OverlapInfo</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">parameters_as_bucket_view</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`parameters_as_bucket_view=True` will be ignored since &quot;</span>
                    <span class="s2">&quot;`overlap_with_ddp=True`; instead, a different bucketing &quot;</span>
                    <span class="s2">&quot;strategy will be used&quot;</span>
                <span class="p">)</span>

        <span class="c1"># `self._buckets` is used if `parameters_as_bucket_view=True`, in</span>
        <span class="c1"># which case parameter data is flattened into contiguous bucket tensors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters_as_bucket_view</span> <span class="o">=</span> <span class="n">parameters_as_bucket_view</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buckets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_param_buckets</span><span class="p">()</span>

        <span class="c1"># Optional consolidated optimizer state, only populated if this rank</span>
        <span class="c1"># is the target in `consolidate_state_dict()`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_all_state_dicts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_clear_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clears the cached data structures giving partition information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_rank_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_index_to_param_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_index_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

<div class="viewcode-block" id="ZeroRedundancyOptimizer.add_param_group"><a class="viewcode-back" href="../../../../distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group">[docs]</a>    <span class="k">def</span> <span class="nf">add_param_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_group</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a parameter group to the :class:`Optimizer` &#39;s ``param_groups``.</span>

<span class="sd">        This can be useful when fine tuning a pre-trained network, as frozen</span>
<span class="sd">        layers can be made trainable and added to the :class:`Optimizer` as</span>
<span class="sd">        training progresses.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            param_group (dict): specifies the parameters to be optimized and</span>
<span class="sd">                group-specific optimization options.</span>

<span class="sd">        .. warning:: This method handles updating the shards on all partitions</span>
<span class="sd">            but needs to be called on all ranks. Calling this on a subset of</span>
<span class="sd">            the ranks will cause the training to hang because communication</span>
<span class="sd">            primitives are called depending on the managed parameters and</span>
<span class="sd">            expect all the ranks to participate on the same set of parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;ZeroRedundancyOptimizer with `overlap_with_ddp=True` only &quot;</span>
                <span class="s2">&quot;supports a single parameter group&quot;</span>
            <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">add_param_group</span><span class="p">(</span><span class="n">param_group</span><span class="p">)</span>
        <span class="c1"># NOTE: The rest of the method assumes that the call to the parent&#39;s</span>
        <span class="c1"># `add_param_group()` appends the new parameter group and preserves</span>
        <span class="c1"># the previous parameter-group ordering</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span><span class="p">:</span>
            <span class="c1"># Force a re-partitioning of the parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_cache</span><span class="p">()</span>
            <span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters</span><span class="p">()[</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">]</span>
            <span class="c1"># NOTE: All parameters in the old parameter groups should be</span>
            <span class="c1"># assigned to the same ranks so that the local optimizers do not</span>
            <span class="c1"># need to be reinitialized</span>

            <span class="c1"># Add the parameters assigned to this rank from the new parameter</span>
            <span class="c1"># group to the local optimizer, if any</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">add_param_group</span><span class="p">(</span><span class="n">param_groups</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

            <span class="c1"># Update the bucketing strategy accordingly</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_as_bucket_view</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_build_param_buckets</span><span class="p">()</span></div>

<div class="viewcode-block" id="ZeroRedundancyOptimizer.consolidate_state_dict"><a class="viewcode-back" href="../../../../distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">consolidate_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">to</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Consolidate a list of ``state_dict`` s (one per rank) on the target</span>
<span class="sd">        rank.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            to (int): the rank that receives the optimizer states (default: 0).</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: if ``overlap_with_ddp=True`` and this method is</span>
<span class="sd">                called before this :class:`ZeroRedundancyOptimizer` instance</span>
<span class="sd">                has been fully initialized, which happens once</span>
<span class="sd">                :class:`DistributedDataParallel` gradient buckets have been</span>
<span class="sd">                rebuilt.</span>

<span class="sd">        .. warning:: This needs to be called on all ranks.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_overlap_initialized</span><span class="p">()</span>

        <span class="c1"># Sync the exposed `param_groups` attributes to the local optimizer in</span>
        <span class="c1"># case they have been updated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_param_groups</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="c1"># Pull the sharded state from all ranks and store them in rank order</span>
        <span class="n">empty_messenger</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_device</span>
        <span class="p">)</span>

        <span class="c1"># NOTE: We wastefully use `broadcast()` (e.g. instead of `gather()`)</span>
        <span class="c1"># due to compatibility issues with NCCL backend; a possible follow-up</span>
        <span class="c1"># is to move all sharded state management to RPC RRef</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_all_state_dicts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">):</span>
            <span class="n">global_rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">rank</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="n">to</span><span class="p">:</span>
                <span class="c1"># Consolidate all local `state_dict`s on this rank, storing on</span>
                <span class="c1"># CPU to save GPU memory</span>
                <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">:</span>
                    <span class="c1"># Directly append own optimizer state</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_all_state_dicts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">_recursive_copy_to_device</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                            <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Receive the optimizer state from the source rank</span>
                    <span class="n">local_state_dict</span> <span class="o">=</span> <span class="n">_broadcast_object</span><span class="p">(</span>
                        <span class="n">empty_messenger</span><span class="p">,</span>
                        <span class="n">src_rank</span><span class="o">=</span><span class="n">global_rank</span><span class="p">,</span>
                        <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_device</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_all_state_dicts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">_recursive_copy_to_device</span><span class="p">(</span>
                            <span class="n">local_state_dict</span><span class="p">,</span>
                            <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">:</span>
                    <span class="c1"># Send the optimizer state to the target rank</span>
                    <span class="n">_</span> <span class="o">=</span> <span class="n">_broadcast_object</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                        <span class="n">src_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="p">,</span>
                        <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_device</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">rank</span> <span class="o">!=</span> <span class="n">to</span><span class="p">:</span>
                    <span class="c1"># Discard the received object; `broadcast()` is used for</span>
                    <span class="c1"># compatibility reasons</span>
                    <span class="n">_</span> <span class="o">=</span> <span class="n">_broadcast_object</span><span class="p">(</span>
                        <span class="n">empty_messenger</span><span class="p">,</span>
                        <span class="n">src_rank</span><span class="o">=</span><span class="n">global_rank</span><span class="p">,</span>
                        <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_default_device</span><span class="p">,</span>
                    <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_verify_params_per_rank</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params_per_rank</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Verifies ``params_per_rank`` for :meth:`_partition_parameters`,</span>
<span class="sd">        checking that ``params_per_rank`` has length equal to the world size</span>
<span class="sd">        and that it does not contain any parameters not passed into the</span>
<span class="sd">        :class:`ZeroRedundancyOptimizer` constructor.</span>

<span class="sd">        The parameters in ``params_per_rank`` being a strict subset of those</span>
<span class="sd">        passed into the constructor is valid since some parameters may be</span>
<span class="sd">        frozen.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: if ``params_per_rank`` does not have length equal to</span>
<span class="sd">                the world size or if it contains a parameter that was not</span>
<span class="sd">                passed into the :class:`ZeroRedundancyOptimizer` constructor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_per_rank</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`params_per_rank` must have length equal to the world size&quot;</span>
            <span class="p">)</span>
        <span class="n">all_params_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">params_per_rank</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_params_set</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Passing a new parameter in `params_per_rank` that &quot;</span>
                        <span class="s2">&quot;was not passed into the ZeroRedundancyOptimizer &quot;</span>
                        <span class="s2">&quot;constructor&quot;</span>
                    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_partition_param_group</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">param_group</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">params_per_rank</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Partitions the parameter group ``param_group`` according to</span>
<span class="sd">        ``params_per_rank`` by modifying ``self._partition_parameters_cache``.</span>

<span class="sd">        This method should only be used as a subroutine for</span>
<span class="sd">        :meth:`_partition_parameters`.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            param_group (dict[str, Any]): a parameter group as normally defined</span>
<span class="sd">                in an optimizer state.</span>
<span class="sd">            params_per_rank (list[list[torch.Tensor]]): a :class:`list` of</span>
<span class="sd">                length world size containing :class:`list` s of parameters to</span>
<span class="sd">                assign to each rank.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params_per_rank</span><span class="p">):</span>
            <span class="n">rank_param_group</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">param_group</span><span class="p">)</span>
            <span class="n">rank_param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rank_param_group</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_partition_parameters</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params_per_rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Partitions parameters across distributed data parallel ranks.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            params_per_rank (list[list[torch.Tensor]], optional): a</span>
<span class="sd">                :class:`list` of length world size containing :class:`list` s</span>
<span class="sd">                of parameters to assign to each rank; this provides a way to</span>
<span class="sd">                specify a partition manually.</span>
<span class="sd">                If ``None``, the parameters are partitioned according to an</span>
<span class="sd">                internal algorithm.</span>
<span class="sd">                (default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            A :class:`list` where each element of the list contains the</span>
<span class="sd">            ``param_groups`` for a rank (which itself is a :class:`list` of</span>
<span class="sd">            :class:`dict`); element 0 corresponds to rank 0, etc.; each rank</span>
<span class="sd">            stores the ``param_groups`` for all ranks for the collective</span>
<span class="sd">            communication in :meth:`step`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: see :meth:`_validate_params_per_rank`.</span>
<span class="sd">            RuntimeError: if ``params_per_rank`` is not ``None`` and this</span>
<span class="sd">                :class:`ZeroRedundancyOptimizer` instance is using more than</span>
<span class="sd">                one parameter group.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">params_per_rank</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Partition the parameters optimizing for uniformity</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)]</span>
                <span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span>
                <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                    <span class="n">param_group_params_per_rank</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="p">[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
                    <span class="p">]</span>
                    <span class="c1"># Sort the parameters by size (largest first)</span>
                    <span class="n">params_sorted</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
                        <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params_sorted</span><span class="p">:</span>
                        <span class="c1"># Greedily add the parameter to rank with smallest size so far</span>
                        <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_min_index</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
                        <span class="n">param_group_params_per_rank</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                        <span class="n">sizes</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                    <span class="c1"># Apply the constructed partition of the parameter group</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_partition_param_group</span><span class="p">(</span>
                        <span class="n">param_group</span><span class="p">,</span> <span class="n">param_group_params_per_rank</span>
                    <span class="p">)</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span>

        <span class="c1"># Partition the parameters according to `params_per_rank`</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;Specifying `params_per_rank` should only be done when the &quot;</span>
            <span class="s2">&quot;parameters have not been partitioned yet&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Specifying `params_per_rank` only supports a single &quot;</span> <span class="s2">&quot;parameter group&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_params_per_rank</span><span class="p">(</span><span class="n">params_per_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)]</span>

        <span class="c1"># Apply the passed-in partition of the parameter group</span>
        <span class="n">param_group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partition_param_group</span><span class="p">(</span><span class="n">param_group</span><span class="p">,</span> <span class="n">params_per_rank</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters_cache</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_param_to_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :class:`dict` mapping parameters to their assigned data parallel rank</span>
<span class="sd">        in the partition.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_param_to_rank_cache</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">param_groups</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters</span><span class="p">()):</span>
                <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">param_groups</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_rank_cache</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_rank_cache</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_param_to_index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :class:`dict` mapping parameters to their indices in the global</span>
<span class="sd">        optimizer state.</span>

<span class="sd">        NOTE: This assumes that the global optimizer state&#39;s indexing (in</span>
<span class="sd">        ``state_dict``) follows a linear ordering over the parameter groups.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_param_to_index_cache</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_index_cache</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">p</span><span class="p">:</span> <span class="n">i</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)))</span>
            <span class="p">}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_index_cache</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_index_to_param</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        List mapping parameter indices in the global optimizer scheme to the</span>
<span class="sd">        actual params.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_index_to_param_cache</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_index_to_param_cache</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
                <span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_index_to_param_cache</span>

    <span class="k">def</span> <span class="nf">_broadcast_params_from_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Broadcasts the shard of parameters from a given rank to all other</span>
<span class="sd">        ranks asynchronously.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            rank (int): the source rank.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A :class:`list` of async work handles for the ``broadcast()`` s</span>
<span class="sd">            performed to synchronize the parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;`_broadcast_params_from_rank()` should not be used if &quot;</span>
            <span class="s2">&quot;`overlap_with_ddp=True`; instead, the broadcasting should &quot;</span>
            <span class="s2">&quot;happen in the DDP communication hook&quot;</span>
        <span class="p">)</span>
        <span class="n">handles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_as_bucket_view</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">dev_i_buckets</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buckets</span><span class="p">:</span>
                <span class="n">bucket</span> <span class="o">=</span> <span class="n">dev_i_buckets</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span>
                <span class="n">global_rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">rank</span>
                <span class="p">)</span>
                <span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span>
                        <span class="n">tensor</span><span class="o">=</span><span class="n">bucket</span><span class="p">,</span>
                        <span class="n">src</span><span class="o">=</span><span class="n">global_rank</span><span class="p">,</span>
                        <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
                        <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters</span><span class="p">()[</span><span class="n">rank</span><span class="p">]</span>
            <span class="n">global_rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">rank</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                    <span class="n">handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span>
                            <span class="n">tensor</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                            <span class="n">src</span><span class="o">=</span><span class="n">global_rank</span><span class="p">,</span>
                            <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
                            <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
        <span class="k">return</span> <span class="n">handles</span>

    <span class="k">def</span> <span class="nf">_sync_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Syncs all parameter shards across the ranks.</span>

<span class="sd">        This rank sends its shard of the parameters to all other ranks and</span>
<span class="sd">        receives a shard from each other rank. This is done using</span>
<span class="sd">        ``broadcast()``. Parameters are sent bucket-by-bucket if</span>
<span class="sd">        ``parameters_as_bucket_view=True``and sent parameter-by-parameter</span>
<span class="sd">        otherwise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">handles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">):</span>
            <span class="n">handles</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_broadcast_params_from_rank</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
        <span class="n">_</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">handles</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_device_to_params_per_rank</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :class:`dict` mapping each device to a :class:`list` of the per-rank parameter</span>
<span class="sd">        lists filtered to only include the parameters stored on that device.</span>
<span class="sd">        Each per-rank parameter list gives the parameters assigned to that rank</span>
<span class="sd">        to update.</span>

<span class="sd">        This is used for constructing the parameter buckets if</span>
<span class="sd">        ``parameters_as_bucket_view=True``.</span>

<span class="sd">        Let ``dev_i`` denote the ``i``th device for this rank. Then:</span>
<span class="sd">        ``dev_0`` maps to a list containing:</span>
<span class="sd">            rank 0&#39;s assigned parameters stored on ``dev_0``,</span>
<span class="sd">            rank 1&#39;s assigned parameters stored on ``dev_0``,</span>
<span class="sd">            ...</span>
<span class="sd">        ``dev_1`` maps to a list containing:</span>
<span class="sd">            rank 0&#39;s assigned parameters stored on ``dev_1``,</span>
<span class="sd">            rank 1&#39;s assigned parameters stored on ``dev_1``,</span>
<span class="sd">            ...</span>
<span class="sd">        ...</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_as_bucket_view</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;`_device_to_params_per_rank` should only be used if &quot;</span>
            <span class="s2">&quot;`parameters_as_bucket_view=True`&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank_cache</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">param_groups</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters</span><span class="p">()):</span>
                <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">param_groups</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                        <span class="n">device</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">device</span>
                        <span class="k">if</span> <span class="n">device</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank_cache</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank_cache</span><span class="p">[</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                                <span class="p">[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
                            <span class="p">]</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank_cache</span><span class="p">[</span><span class="n">device</span><span class="p">][</span><span class="n">rank</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                            <span class="n">param</span>
                        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank_cache</span>

    <span class="k">def</span> <span class="nf">_get_min_index</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">disallowed_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns ``values.index(min(values))``, except only uses one pass. It</span>
<span class="sd">        also excludes any indices in ``disallowed_indices`` if provided.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            values: (List[int]): :class:`list` of values.</span>
<span class="sd">            disallowed_indices (Optional[Set[int]]): indices that are</span>
<span class="sd">                disallowed from being the returned min index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">min_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">min_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">disallowed_indices</span> <span class="ow">and</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">disallowed_indices</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">value</span> <span class="o">&lt;</span> <span class="n">min_value</span><span class="p">:</span>
                <span class="n">min_value</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">min_index</span> <span class="o">=</span> <span class="n">i</span>
        <span class="k">assert</span> <span class="n">min_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;All indices are disallowed&quot;</span>
        <span class="k">return</span> <span class="n">min_index</span>

    <span class="k">def</span> <span class="nf">_assign_bucket_subset_to_rank</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">bucket_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bucket_params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">bucket_offset</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">assigned_rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">assigned_ranks_per_bucket</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Assigns the model parameters given by ``bucket_params``, representing a</span>
<span class="sd">        (possibly non-strict) subset of the parameters corresponding to a</span>
<span class="sd">        :class:`DistributedDataParallel` bucket, to the rank with the least</span>
<span class="sd">        size assigned so far and collects relevant information.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            bucket_index (int): index of the :class:`DistributedDataParallel`</span>
<span class="sd">                gradient bucket.</span>
<span class="sd">            bucket_params (List[torch.Tensor]): subset of the parameters</span>
<span class="sd">                corresponding to the bucket to assign.</span>
<span class="sd">            bucket_offset (int): offset giving the index of the first element</span>
<span class="sd">                in ``bucket_params`` in the bucket&#39;s full parameter list.</span>
<span class="sd">            assigned_rank (int): group rank to assign to.</span>
<span class="sd">            assigned_ranks_per_bucket (List[Set[int]]): :class:`set` of group ranks</span>
<span class="sd">                assigned to each bucket.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">overlap_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">bucket_params</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Empty bucket assignment&quot;</span><span class="p">)</span>
        <span class="n">params_per_rank</span> <span class="o">=</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">params_per_rank</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">offsets</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank_cache</span><span class="p">[</span><span class="n">assigned_rank</span><span class="p">][</span>
            <span class="n">bucket_index</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">_DDPBucketAssignment</span><span class="p">(</span><span class="n">bucket_index</span><span class="p">,</span> <span class="n">bucket_params</span><span class="p">,</span> <span class="n">bucket_offset</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="n">assigned_rank</span><span class="p">:</span>
            <span class="n">offsets</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_per_rank</span><span class="p">[</span><span class="n">assigned_rank</span><span class="p">])</span>
        <span class="n">params_per_rank</span><span class="p">[</span><span class="n">assigned_rank</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">bucket_params</span><span class="p">)</span>
        <span class="n">assigned_ranks_per_bucket</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">assigned_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span><span class="o">.</span><span class="n">num_bucket_assignments</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_bucket_assignments_per_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">_DDPBucketAssignment</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :class:`list` of length world size consisting of :class:`dict` s</span>
<span class="sd">        mapping bucket indices to :class:`_DDPBucketAssignment` s for each</span>
<span class="sd">        rank.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;`_bucket_assignments_per_rank` &quot;</span> <span class="s2">&quot;only be used if `overlap_with_ddp=True`&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank_cache</span>

        <span class="n">overlap_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span>
        <span class="k">assert</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="n">_OverlapStatus</span><span class="o">.</span><span class="n">INITIALIZED</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank_cache</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)]</span>
        <span class="n">params_per_bucket</span> <span class="o">=</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">params_per_bucket</span>

        <span class="k">if</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">shard_buckets</span><span class="p">:</span>
            <span class="c1"># Define the assignment threshold to approximate uniformity</span>
            <span class="k">assert</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">total_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;`total_size` was not computed&quot;</span>
            <span class="n">threshold</span> <span class="o">=</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">total_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span>  <span class="c1"># type: ignore[operator]</span>
            <span class="n">size_per_rank</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)]</span>

        <span class="n">num_buckets</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_per_bucket</span><span class="p">)</span>
        <span class="n">overlap_info</span><span class="o">.</span><span class="n">assigned_ranks_per_bucket</span> <span class="o">=</span> <span class="p">[</span><span class="nb">set</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_buckets</span><span class="p">)]</span>
        <span class="n">assigned_ranks_per_bucket</span> <span class="o">=</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">assigned_ranks_per_bucket</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">overlap_info</span><span class="o">.</span><span class="n">shard_buckets</span><span class="p">:</span>
            <span class="c1"># Assign each DDP bucket entirely to a single rank</span>
            <span class="k">for</span> <span class="n">bucket_index</span><span class="p">,</span> <span class="n">bucket_params</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params_per_bucket</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">bucket_params</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Empty bucket&quot;</span>
                <span class="n">assigned_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_assigned_rank</span><span class="p">(</span><span class="n">bucket_index</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_assign_bucket_subset_to_rank</span><span class="p">(</span>
                    <span class="n">bucket_index</span><span class="p">,</span>
                    <span class="n">bucket_params</span><span class="p">,</span>
                    <span class="mi">0</span><span class="p">,</span>
                    <span class="n">assigned_rank</span><span class="p">,</span>
                    <span class="n">assigned_ranks_per_bucket</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Assign each DDP bucket to possibly multiple ranks</span>
            <span class="c1"># Specifically, sort the DDP buckets by increasing size, and for</span>
            <span class="c1"># each bucket, iteratively assign the maximal unassigned subset</span>
            <span class="c1"># with size less than `threshold` to the rank with the least total</span>
            <span class="c1"># size so far -- each such assignment is represented by a</span>
            <span class="c1"># `_DDPBucketAssignment` instance and only contains parameters from</span>
            <span class="c1"># a single DDP bucket</span>
            <span class="n">params_per_bucket_enum</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
                <span class="nb">enumerate</span><span class="p">(</span><span class="n">params_per_bucket</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">bucket_index</span><span class="p">,</span> <span class="n">bucket_params</span> <span class="ow">in</span> <span class="n">params_per_bucket_enum</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">bucket_params</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Empty bucket&quot;</span>
                <span class="n">bucket_offset</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">assignment_size</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">param_index</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bucket_params</span><span class="p">):</span>
                    <span class="n">param_numel</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">assignment_size</span> <span class="o">+</span> <span class="n">param_numel</span> <span class="o">&gt;=</span> <span class="n">threshold</span>
                        <span class="ow">and</span> <span class="n">param_index</span> <span class="o">&gt;</span> <span class="n">bucket_offset</span>
                    <span class="p">):</span>
                        <span class="n">assigned_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_min_index</span><span class="p">(</span>
                            <span class="n">size_per_rank</span><span class="p">,</span> <span class="n">assigned_ranks_per_bucket</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span>
                        <span class="p">)</span>
                        <span class="c1"># Include up to but not including the parameter that</span>
                        <span class="c1"># exceeded the threshold</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_assign_bucket_subset_to_rank</span><span class="p">(</span>
                            <span class="n">bucket_index</span><span class="p">,</span>
                            <span class="n">bucket_params</span><span class="p">[</span><span class="n">bucket_offset</span><span class="p">:</span><span class="n">param_index</span><span class="p">],</span>
                            <span class="n">bucket_offset</span><span class="p">,</span>
                            <span class="n">assigned_rank</span><span class="p">,</span>
                            <span class="n">assigned_ranks_per_bucket</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="n">size_per_rank</span><span class="p">[</span><span class="n">assigned_rank</span><span class="p">]</span> <span class="o">+=</span> <span class="n">assignment_size</span>
                        <span class="n">bucket_offset</span> <span class="o">=</span> <span class="n">param_index</span>
                        <span class="n">assignment_size</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">assignment_size</span> <span class="o">+=</span> <span class="n">param_numel</span>
                <span class="c1"># Assign the remainder of the bucket so that no assignment</span>
                <span class="c1"># spans across two buckets</span>
                <span class="n">assigned_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_min_index</span><span class="p">(</span>
                    <span class="n">size_per_rank</span><span class="p">,</span> <span class="n">assigned_ranks_per_bucket</span><span class="p">[</span><span class="n">bucket_index</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_assign_bucket_subset_to_rank</span><span class="p">(</span>
                    <span class="n">bucket_index</span><span class="p">,</span>
                    <span class="n">bucket_params</span><span class="p">[</span><span class="n">bucket_offset</span><span class="p">:],</span>
                    <span class="n">bucket_offset</span><span class="p">,</span>
                    <span class="n">assigned_rank</span><span class="p">,</span>
                    <span class="n">assigned_ranks_per_bucket</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">size_per_rank</span><span class="p">[</span><span class="n">assigned_rank</span><span class="p">]</span> <span class="o">+=</span> <span class="n">assignment_size</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank_cache</span>

    <span class="k">def</span> <span class="nf">_local_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">gradients</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">closure</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs a single optimizer step without syncing parameters across</span>
<span class="sd">        ranks.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            gradients (list[Optional[torch.Tensor]], optional): a :class:`list`</span>
<span class="sd">                of length equal to the number of parameters assigned to this</span>
<span class="sd">                rank containing gradient tensors or ``None`` as its elements;</span>
<span class="sd">                a ``None`` in the :class:`list` indicates that the</span>
<span class="sd">                corresponding parameter should not be updated.</span>
<span class="sd">                If the argument itself is ``None``, then all parameters are</span>
<span class="sd">                updated, and the gradients are assumed to be already populated.</span>
<span class="sd">                (default: ``None``)</span>
<span class="sd">            closure (Callable): a closure that re-evaluates the model and</span>
<span class="sd">                returns the loss; optional for most optimizers and should be</span>
<span class="sd">                ``None`` if ``gradients`` is not ``None``; (default: ``None``)</span>
<span class="sd">        Returns:</span>
<span class="sd">            Optional loss depending on the underlying local optimizer.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The argument ``gradients`` should only be specified (i.e. not</span>
<span class="sd">            ``None``) if ``overlap_with_ddp=True``, in which case</span>
<span class="sd">            :class:`ZeroRedundancyOptimizer` wraps a functional optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Join</span><span class="o">.</span><span class="n">notify_join_context</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># Check if the model trainability has changed</span>
        <span class="n">is_trainable_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_is_trainable_mask</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">is_trainable_mask</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_trainable_mask</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;ZeroRedundancyOptimizer with `overlap_with_ddp=True` &quot;</span>
                    <span class="s2">&quot;does not support changing parameter trainability at run &quot;</span>
                    <span class="s2">&quot;time&quot;</span>
                <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;ZeroRedundancyOptimizer detected that the trainable &quot;</span>
                <span class="s2">&quot;parameters changed; rebuilding the parameter buckets if &quot;</span>
                <span class="s2">&quot;enabled&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_build_param_buckets</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_trainable_mask</span> <span class="o">=</span> <span class="n">is_trainable_mask</span>

        <span class="c1"># Sync the exposed `param_groups` attributes to the local optimizer in</span>
        <span class="c1"># case they have been updated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_param_groups</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="c1"># Run the optimizer step on this shard only</span>
        <span class="k">if</span> <span class="n">gradients</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">closure</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;Specifying `gradients` should not &quot;</span>
                <span class="s2">&quot;be used when `overlap_with_ddp=False`&quot;</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="n">closure</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;`closure` is not supported when using &quot;</span> <span class="s2">&quot;a local functional optimizer&quot;</span>
            <span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">gradients</span><span class="o">=</span><span class="n">gradients</span><span class="p">)</span>

        <span class="c1"># Sync any updated attributes in the local optimizer to the exposed</span>
        <span class="c1"># `param_groups`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_param_groups</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

<div class="viewcode-block" id="ZeroRedundancyOptimizer.step"><a class="viewcode-back" href="../../../../distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.step">[docs]</a>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">closure</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs a single optimizer step and syncs parameters across all ranks.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            closure (Callable): a closure that re-evaluates the model and</span>
<span class="sd">                returns the loss; optional for most optimizers.</span>
<span class="sd">        Returns:</span>
<span class="sd">            Optional loss depending on the underlying local optimizer.</span>

<span class="sd">        .. note: Any extra parameters are passed to the base optimizer as-is.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;`step()` should not be included in the training loop when &quot;</span>
                <span class="s2">&quot;`overlap_with_ddp=True`&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># Perform the local optimizer step</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">closure</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Sync all of the updated parameter shards across the ranks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_params</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="ZeroRedundancyOptimizer.join_hook"><a class="viewcode-back" href="../../../../distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.join_hook">[docs]</a>    <span class="k">def</span> <span class="nf">join_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the ZeRO join hook, which enables training on uneven inputs by</span>
<span class="sd">        shadowing the collective communications in the optimizer step.</span>

<span class="sd">        Gradients must be properly set before this hook is called.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            kwargs (dict): a :class:`dict` containing any keyword arguments</span>
<span class="sd">                to modify the behavior of the join hook at run time; all</span>
<span class="sd">                :class:`Joinable` instances sharing the same join context</span>
<span class="sd">                manager are forwarded the same value for ``kwargs``.</span>

<span class="sd">        This hook does not support any keyword arguments; i.e. ``kwargs`` is</span>
<span class="sd">        unused.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_ZeROJoinHook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">join_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_device</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">join_process_group</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>

<div class="viewcode-block" id="ZeroRedundancyOptimizer.load_state_dict"><a class="viewcode-back" href="../../../../distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load the state pertaining to the given rank from the input</span>
<span class="sd">        ``state_dict``, updating the local optimizer as needed.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            state_dict (dict): optimizer state; should be an object returned</span>
<span class="sd">                from a call to :meth:`state_dict`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: if ``overlap_with_ddp=True`` and this method is</span>
<span class="sd">                called before this :class:`ZeroRedundancyOptimizer` instance</span>
<span class="sd">                has been fully initialized, which happens once</span>
<span class="sd">                :class:`DistributedDataParallel` gradient buckets have been</span>
<span class="sd">                rebuilt.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_overlap_initialized</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_index_to_param</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_rank</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">:</span>
                <span class="c1"># Clear any state irrelevant to this rank</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Load the parameter state to the local optimizer</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">_recursive_copy_to_device</span><span class="p">(</span>
                    <span class="n">value</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>
                <span class="c1"># Force zero-dimensional tensors (like Adam &quot;step&quot;) on CPU</span>
                <span class="k">for</span> <span class="n">state_name</span><span class="p">,</span> <span class="n">state_value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">state_value</span><span class="p">)</span> <span class="ow">and</span> <span class="n">state_value</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">param</span><span class="p">][</span><span class="n">state_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_value</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="c1"># Sync the input state with the exposed and local optimizer states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_param_groups</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_param_groups</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span></div>

<div class="viewcode-block" id="ZeroRedundancyOptimizer.state_dict"><a class="viewcode-back" href="../../../../distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the last global optimizer state known to this rank.</span>

<span class="sd">        .. warning:</span>
<span class="sd">            If the state has not been consolidated to this rank, this raises a</span>
<span class="sd">            runtime error, and even if it has, the state may not be up-to-date,</span>
<span class="sd">            depending on when :meth:`consolidate_state_dict` was last called.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: if ``overlap_with_ddp=True`` and this method is</span>
<span class="sd">                called before this :class:`ZeroRedundancyOptimizer` instance</span>
<span class="sd">                has been fully initialized, which happens once</span>
<span class="sd">                :class:`DistributedDataParallel` gradient buckets have been</span>
<span class="sd">                rebuilt; or if this method is called without a preceding call</span>
<span class="sd">                to :meth:`consolidate_state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_overlap_initialized</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_state_dicts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Optimizer state has not been consolidated on this rank. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Please call `consolidate_state_dict(to=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">)` on &quot;</span>
                <span class="s2">&quot;all ranks beforehand if you meant to save the global state.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Get the possibly-stale global optimizer state that uses global</span>
        <span class="c1"># parameter indexing</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="c1"># Update the global optimizer state with local state information,</span>
        <span class="c1"># factoring in the translation from local to global indexing</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">local_state_dict</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_state_dicts</span><span class="p">):</span>
            <span class="n">local_param_groups</span> <span class="o">=</span> <span class="n">local_state_dict</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span>
            <span class="n">global_param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters</span><span class="p">()[</span><span class="n">rank</span><span class="p">]</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">local_param_groups</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">global_param_groups</span>
            <span class="p">),</span> <span class="s2">&quot;Mismatch between number of local and global parameter groups&quot;</span>

            <span class="k">for</span> <span class="n">local_param_group</span><span class="p">,</span> <span class="n">global_param_group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="n">local_param_groups</span><span class="p">,</span> <span class="n">global_param_groups</span>
            <span class="p">):</span>
                <span class="c1"># `local_param_group` stores local indices, while</span>
                <span class="c1"># `global_param_group` stores the tensors directly</span>
                <span class="n">local_param_indices</span> <span class="o">=</span> <span class="n">local_param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
                <span class="n">global_params</span> <span class="o">=</span> <span class="n">global_param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>

                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">local_param_indices</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                    <span class="n">global_params</span>
                <span class="p">),</span> <span class="s2">&quot;Mismatch between number of local and global parameters in parameter group&quot;</span>
                <span class="k">for</span> <span class="n">local_param_index</span><span class="p">,</span> <span class="n">global_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="n">local_param_indices</span><span class="p">,</span> <span class="n">global_params</span>
                <span class="p">):</span>
                    <span class="c1"># Update the global parameter state, if any</span>
                    <span class="k">if</span> <span class="n">local_param_index</span> <span class="ow">in</span> <span class="n">local_state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]:</span>
                        <span class="n">global_param_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_to_index</span><span class="p">[</span><span class="n">global_param</span><span class="p">]</span>
                        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="n">global_param_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">local_state_dict</span><span class="p">[</span>
                            <span class="s2">&quot;state&quot;</span>
                        <span class="p">][</span><span class="n">local_param_index</span><span class="p">]</span>

        <span class="c1"># Sort the parameters in the state</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>
        <span class="k">return</span> <span class="n">state_dict</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_sync_param_groups</span><span class="p">(</span>
        <span class="n">src_param_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">dst_param_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Syncs the attributes from the source parameter groups to the</span>
<span class="sd">        destination parameter groups.</span>

<span class="sd">        Example attributes include learning rate or scheduler attributes. The</span>
<span class="sd">        two parameter groups should have the same length (i.e. same number of</span>
<span class="sd">        parameter groups).</span>

<span class="sd">        Arguments:</span>
<span class="sd">            src_param_groups (list[dict]): parameter groups giving the</span>
<span class="sd">                attribute settings to copy.</span>
<span class="sd">            dst_param_groups (list[dict]): parameter groups giving the</span>
<span class="sd">                attribute settings to set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">src_param_groups</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">dst_param_groups</span>
        <span class="p">),</span> <span class="s2">&quot;Mismatch between number of source and destination parameter groups&quot;</span>
        <span class="k">for</span> <span class="n">src_param_group</span><span class="p">,</span> <span class="n">dst_param_group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">src_param_groups</span><span class="p">,</span> <span class="n">dst_param_groups</span><span class="p">):</span>
            <span class="c1"># Sync all attributes except the parameters</span>
            <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">!=</span> <span class="s2">&quot;params&quot;</span><span class="p">,</span> <span class="n">src_param_group</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="n">dst_param_group</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">src_param_group</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_build_param_buckets</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Builds parameter buckets if ``parameters_as_bucket_view=True`` so</span>
<span class="sd">        that for each device that stores this rank&#39;s parameters, there is a</span>
<span class="sd">        bucket (represented as a tensor) containing all of the parameters on</span>
<span class="sd">        that device that are assigned to a given rank in the parameter update</span>
<span class="sd">        partition.</span>

<span class="sd">        This method is called in the constructor and any time parameter</span>
<span class="sd">        trainability is changed.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The current implementation assumes that all of the parameters in a</span>
<span class="sd">            bucket are of the same dense type when allocating the bucket&#39;s</span>
<span class="sd">            tensor.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            If the model parameters are stored across more than one device,</span>
<span class="sd">            then the storage partitioning must be the same across all</span>
<span class="sd">            processes in order for parameter synchronization to work.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters_as_bucket_view</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># `self._buckets[i][j]` are the parameters stored on device i and</span>
        <span class="c1"># assigned to rank j</span>
        <span class="n">num_devices</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_buckets</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">)]</span>  <span class="c1"># type: ignore[assignment]</span>

        <span class="k">for</span> <span class="n">dev_i</span><span class="p">,</span> <span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">params_per_rank</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device_to_params_per_rank</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">for</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">params_per_rank</span><span class="p">:</span>
                <span class="n">bucket_size</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">trainable_params</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_trainable</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
                        <span class="c1"># Clone in case the parameter was previously part of</span>
                        <span class="c1"># a bucket to avoid the data from being destroyed</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">bucket_size</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                        <span class="n">trainable_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                    <span class="n">dtype</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">dtype</span>  <span class="c1"># assumes all same dtype</span>

                <span class="k">if</span> <span class="n">bucket_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># Create a dummy bucket if there are no parameters</span>
                    <span class="n">bucket</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Construct the bucket (assuming all dense and same dtype)</span>
                    <span class="n">bucket</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">bucket_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">trainable_params</span><span class="p">:</span>
                        <span class="n">offset_next</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                        <span class="n">bucket</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset_next</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
                        <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">bucket</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset_next</span><span class="p">]</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                        <span class="n">offset</span> <span class="o">=</span> <span class="n">offset_next</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_buckets</span><span class="p">[</span><span class="n">dev_i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

    <span class="k">def</span> <span class="nf">_build_ddp_param_buckets</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For each DDP bucket with parameters assigned to this rank, flattens the</span>
<span class="sd">        data of those parameters into a single tensor and saves the tensor to</span>
<span class="sd">        the ``tensor`` attribute in the corresponding</span>
<span class="sd">        :class:`_DDPBucketAssignment` instance stored in</span>
<span class="sd">        ``self._bucket_assignments_per_rank``.</span>

<span class="sd">        :class:`DistributedDataParallel` guarantees that the parameters</span>
<span class="sd">        corresponding to a gradient bucket have the same device and the same</span>
<span class="sd">        dtype.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">bucket_assignments</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">bucket_assignment</span> <span class="ow">in</span> <span class="n">bucket_assignments</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">params</span> <span class="o">=</span> <span class="n">bucket_assignment</span><span class="o">.</span><span class="n">parameters</span>
                <span class="n">bucket_size</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">_is_trainable</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="p">(</span>
                        <span class="s2">&quot;Model parameter &quot;</span>
                        <span class="s2">&quot;corresponding to a gradient in a DDP bucket should &quot;</span>
                        <span class="s2">&quot;require a gradient&quot;</span>
                    <span class="p">)</span>
                    <span class="n">bucket_size</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                    <span class="n">dtype</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">dtype</span>  <span class="c1"># assumes all same dtype</span>
                <span class="k">assert</span> <span class="n">bucket_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Empty bucket&quot;</span>

                <span class="c1"># Construct the bucket tensor (assuming all dense and same dtype)</span>
                <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                    <span class="n">bucket_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">bucket_assignment</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>
                <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
                    <span class="n">offset_next</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                    <span class="n">tensor</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset_next</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset_next</span><span class="p">]</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="n">offset</span> <span class="o">=</span> <span class="n">offset_next</span>
                <span class="n">bucket_assignment</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span>

    <span class="k">def</span> <span class="nf">_verify_and_init_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Verifies the type of ``params`` and initializes ``self._all_params``</span>
<span class="sd">        as a :class:`list` of all parameters if ``params`` is valid.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            params (Any): Candidate parameter list or parameter groups to</span>
<span class="sd">                verify.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: ``params`` has an invalid type.</span>
<span class="sd">            ValueError: ``params`` is empty.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The persistent form of ``params`` to be passed into the parent</span>
<span class="sd">            :class:`Optimizer` constructor -- i.e. returns ``params`` as a</span>
<span class="sd">            :class:`list` to ensure that it can be iterated over again.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;`params` argument should be an iterable of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Tensors, but got </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">params</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">all_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;`params` argument should be an iterable of Tensors&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; or dicts, but got </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">params</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_params</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ZeroRedundancyOptimizer got an empty parameter &quot;</span> <span class="s2">&quot;list&quot;</span><span class="p">)</span>
        <span class="n">all_tensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">all_dicts</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">all_params</span><span class="p">:</span>
            <span class="n">all_tensors</span> <span class="o">&amp;=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="n">all_dicts</span> <span class="o">&amp;=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">all_tensors</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">all_dicts</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;`params` argument should be an iterable of &quot;</span> <span class="s2">&quot;Tensors or dicts&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Ensure that `self._all_params` contains a list of all parameters</span>
        <span class="k">if</span> <span class="n">all_tensors</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span> <span class="o">=</span> <span class="n">all_params</span>
        <span class="k">elif</span> <span class="n">all_dicts</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="c1"># `all_params` contains parameter groups (not parameters)</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">all_params</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;params&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Each parameter group passed-in via `params` must &quot;</span>
                        <span class="s2">&quot;have a &#39;params&#39; key mapping to the parameters in &quot;</span>
                        <span class="s2">&quot;the group&quot;</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">all_params</span>

    <span class="k">def</span> <span class="nf">_verify_same_dense_param_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Verifies that all parameters are of the same dense type.</span>

<span class="sd">        The method assumes that ``self._all_params`` has been initialized</span>
<span class="sd">        and is non-empty.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: ``params`` contains sparse parameters or parameters</span>
<span class="sd">            of varying dense types.</span>

<span class="sd">        NOTE: This method can be removed once support for sparse parameters</span>
<span class="sd">        and varying parameter types is added.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">typename</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;ZeroRedundancyOptimizer only supports using &quot;</span>
                <span class="s2">&quot;the same dense type for all parameters but got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">typename</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">other_typename</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">other_typename</span> <span class="o">!=</span> <span class="n">typename</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;ZeroRedundancyOptimizer only supports &quot;</span>
                    <span class="s2">&quot;using the same dense type for all &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;parameters but got both </span><span class="si">{</span><span class="n">typename</span><span class="si">}</span><span class="s2"> and &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">other_typename</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_is_trainable_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a boolean mask indicating if each parameter is trainable</span>
<span class="sd">        (``requires_grad``) or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">_is_trainable</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_params</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_init_local_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes this rank&#39;s local optimizer, responsible for its subset of</span>
<span class="sd">        the parameters.</span>

<span class="sd">        The local optimizer is saved in ``self.optim``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optim_constructor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;The local optimizer class has not been set&quot;</span>

        <span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters</span><span class="p">()[</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">]</span>
        <span class="c1"># `overlap_with_ddp=True` requires a local functional optimizer</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">:</span>
            <span class="c1"># Functional optimizers only support a single parameter group and</span>
            <span class="c1"># require passing in the parameters as a list</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span>
                <span class="s2">&quot;Initializing the local &quot;</span>
                <span class="s2">&quot;functional optimizer with more than one parameter group&quot;</span>
            <span class="p">)</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
            <span class="c1"># Try to pass `_allow_empty_param_list=True` to avoid erroring</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;_allow_empty_param_list&quot;</span>
                <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_constructor</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optim_constructor</span><span class="p">(</span>
                    <span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_defaults</span><span class="p">,</span> <span class="n">_allow_empty_param_list</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> does not support the argument &quot;</span>
                    <span class="s2">&quot;`_allow_empty_param_list`; ZeroRedundancyOptimizer may &quot;</span>
                    <span class="s2">&quot;error due to an empty parameter list&quot;</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_optim_constructor</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optim_constructor</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_defaults</span><span class="p">)</span>  <span class="c1"># type: ignore[no-redef]</span>

            <span class="c1"># Log information about the DDP and ZeRO bucketing</span>
            <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_debug_level</span><span class="p">()</span> <span class="o">!=</span> <span class="n">dist</span><span class="o">.</span><span class="n">DebugLevel</span><span class="o">.</span><span class="n">OFF</span><span class="p">:</span>
                <span class="n">local_numel</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">)</span>
                <span class="n">num_assigned_buckets</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_bucket_assignments_per_rank</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;rank </span><span class="si">%s</span><span class="s2"> with </span><span class="si">%s</span><span class="s2"> parameters &quot;</span>
                    <span class="s2">&quot;across </span><span class="si">%s</span><span class="s2"> buckets&quot;</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="p">,</span> <span class="n">local_numel</span><span class="p">,</span> <span class="n">num_assigned_buckets</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> DDP &quot;</span>
                        <span class="s2">&quot;buckets and &quot;</span>
                        <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> bucket &quot;</span>
                        <span class="s2">&quot;assignments&quot;</span><span class="p">,</span>
                        <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span><span class="o">.</span><span class="n">params_per_bucket</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span><span class="o">.</span><span class="n">num_bucket_assignments</span>
                    <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># NOTE: Passing `param_groups` into the local optimizer constructor</span>
            <span class="c1"># bypasses the empty parameter list check</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optim_constructor</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_defaults</span><span class="p">)</span>  <span class="c1"># type: ignore[no-redef]</span>

        <span class="c1"># TODO: Manually add `self.param_groups` if using a functional</span>
        <span class="c1"># optimizer; remove this if/when the functional optimizers support</span>
        <span class="c1"># multiple parameter groups</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="s2">&quot;param_groups&quot;</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="s2">&quot;param_group&quot;</span><span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;The functional optimizer should set at least one of the &quot;</span>
                <span class="s2">&quot;attributes `param_group` or `param_groups`&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_group</span><span class="p">]</span>  <span class="c1"># type: ignore[attr-defined]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_param_groups</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_zero_for_overlap</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Performs a delayed initialization of the local optimizer and the</span>
<span class="sd">        supporting data structures.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;`_init_zero_for_overlap()` should only be called when &quot;</span>
            <span class="s2">&quot;`overlap_with_ddp=True`&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span><span class="o">.</span><span class="n">status</span> <span class="o">=</span> <span class="n">_OverlapStatus</span><span class="o">.</span><span class="n">INITIALIZED</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_clear_cache</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_partition_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span><span class="o">.</span><span class="n">params_per_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_ddp_param_buckets</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_local_optimizer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_assigned_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bucket_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the single rank assigned to a :class:`DistributedDataParallel`</span>
<span class="sd">        gradient bucket.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            bucket_index (int): index of the :class:`DistributedDataParallel`</span>
<span class="sd">                bucket for which to get the assigned rank.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span><span class="o">.</span><span class="n">shard_buckets</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;The bucket assignment requires global bucket information and &quot;</span>
            <span class="s2">&quot;will be computed later; there should be no need to use this &quot;</span>
            <span class="s2">&quot;method&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">bucket_index</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span>

    <span class="k">def</span> <span class="nf">_check_overlap_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks that the delayed initialization has occurred (see</span>
<span class="sd">        :meth:`_init_zero_for_overlap`) if ``overlap_with_ddp=True``, and</span>
<span class="sd">        raises a ``RuntimeError`` if not. This should preface methods that</span>
<span class="sd">        should not be run before that delayed initialization.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: if ``overlap_with_ddp=True`` and</span>
<span class="sd">                :meth:`_init_zero_for_overlap` has not been called.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_info</span><span class="o">.</span><span class="n">status</span> <span class="o">!=</span> <span class="n">_OverlapStatus</span><span class="o">.</span><span class="n">INITIALIZED</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;This method should not be called until this &quot;</span>
                <span class="s2">&quot;ZeroRedundancyOptimizer instance has been fully &quot;</span>
                <span class="s2">&quot;initialized&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_optimizer_constructor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_class</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the proper optimizer constructor, performing the necessary</span>
<span class="sd">        validation and transformation depending on ``overlap_with_ddp``.</span>

<span class="sd">        Returns:</span>

<span class="sd">            - ``optimizer_class`` if ``overlap_with_ddp=False`` and</span>
<span class="sd">                ``optimizer_class`` is not a functional optimizer.</span>
<span class="sd">            - ``optimizer_class`` if ``overlap_with_ddp=True`` and</span>
<span class="sd">                ``optimizer_class`` is already a functional optimizer.</span>
<span class="sd">            - The functional equivalent of ``optimizer_class`` if</span>
<span class="sd">                ``overlap_with_ddp=True`` and ``optimizer_class`` is not</span>
<span class="sd">                already a functional optimizer (assuming the equivalent</span>
<span class="sd">                exists).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError:</span>

<span class="sd">                - if ``overlap_with_ddp=True`` but ``optimizer_class`` is</span>
<span class="sd">                    neither a functional optimizer nor translatable to a</span>
<span class="sd">                    functional optimizer.</span>
<span class="sd">                - if ``overlap_with_ddp=False`` and ``optimizer_class`` is a</span>
<span class="sd">                    functional optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">functional_optims</span> <span class="o">=</span> <span class="n">functional_optim_map</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_overlap_with_ddp</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">optimizer_class</span> <span class="ow">in</span> <span class="n">functional_optims</span><span class="p">:</span>
                <span class="c1"># Using a functional optimizer is only supported when</span>
                <span class="c1"># `overlap_with_ddp=True`</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Passing in a functional optimizer </span><span class="si">{</span><span class="n">optimizer_class</span><span class="si">}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;when `overlap_with_ddp=False`&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">optimizer_class</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">optimizer_class</span> <span class="ow">in</span> <span class="n">functional_optims</span><span class="p">:</span>
                <span class="c1"># Already a functional optimizer</span>
                <span class="k">return</span> <span class="n">optimizer_class</span>
            <span class="k">elif</span> <span class="n">optimizer_class</span> <span class="ow">in</span> <span class="n">functional_optim_map</span><span class="p">:</span>
                <span class="c1"># Translate the passed-in optimizer class to its functional</span>
                <span class="c1"># equivalent if `overlap_with_ddp=True`</span>
                <span class="n">optim_constructor</span> <span class="o">=</span> <span class="n">functional_optim_map</span><span class="p">[</span><span class="n">optimizer_class</span><span class="p">]</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;Using the functional optimizer </span><span class="si">%s</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;instead of </span><span class="si">%s</span><span class="s2"> since &quot;</span>
                    <span class="s2">&quot;`overlap_with_ddp=True`&quot;</span><span class="p">,</span>
                    <span class="n">optim_constructor</span><span class="p">,</span> <span class="n">optimizer_class</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">optim_constructor</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Using `ddp_with_overlap=True` requires using a &quot;</span>
                    <span class="s2">&quot;functional optimizer, but there is no supported functional &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;optimizer equivalent for </span><span class="si">{</span><span class="n">optimizer_class</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.distributed_c10d &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.distributed_c10d</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.distributed_c10d</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">collections.abc</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AllreduceCoalescedOptions</span><span class="p">,</span>
    <span class="n">AllreduceOptions</span><span class="p">,</span>
    <span class="n">AllToAllOptions</span><span class="p">,</span>
    <span class="n">_DistributedBackendOptions</span><span class="p">,</span>
    <span class="n">BarrierOptions</span><span class="p">,</span>
    <span class="n">BroadcastOptions</span><span class="p">,</span>
    <span class="n">GatherOptions</span><span class="p">,</span>
    <span class="n">PrefixStore</span><span class="p">,</span>
    <span class="n">ProcessGroup</span><span class="p">,</span>
    <span class="n">ReduceOp</span><span class="p">,</span>
    <span class="n">ReduceOptions</span><span class="p">,</span>
    <span class="n">ReduceScatterOptions</span><span class="p">,</span>
    <span class="n">ScatterOptions</span><span class="p">,</span>
    <span class="n">Store</span><span class="p">,</span>
    <span class="n">DebugLevel</span><span class="p">,</span>
    <span class="n">get_debug_level</span><span class="p">,</span>
    <span class="n">Work</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.autograd.profiler</span> <span class="kn">import</span> <span class="n">record_function</span>
<span class="kn">from</span> <span class="nn">.constants</span> <span class="kn">import</span> <span class="n">default_pg_timeout</span>
<span class="kn">from</span> <span class="nn">.c10d_logger</span> <span class="kn">import</span> <span class="n">_exception_logger</span><span class="p">,</span> <span class="n">_time_logger</span>
<span class="kn">from</span> <span class="nn">.rendezvous</span> <span class="kn">import</span> <span class="n">register_rendezvous_handler</span><span class="p">,</span> <span class="n">rendezvous</span>  <span class="c1"># noqa: F401</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;Backend&#39;</span><span class="p">,</span> <span class="s1">&#39;BackendConfig&#39;</span><span class="p">,</span> <span class="s1">&#39;GroupMember&#39;</span><span class="p">,</span> <span class="s1">&#39;P2POp&#39;</span><span class="p">,</span> <span class="s1">&#39;all_gather&#39;</span><span class="p">,</span> <span class="s1">&#39;all_gather_coalesced&#39;</span><span class="p">,</span>
    <span class="s1">&#39;all_gather_multigpu&#39;</span><span class="p">,</span> <span class="s1">&#39;all_gather_object&#39;</span><span class="p">,</span> <span class="s1">&#39;all_reduce&#39;</span><span class="p">,</span>
    <span class="s1">&#39;all_reduce_coalesced&#39;</span><span class="p">,</span> <span class="s1">&#39;all_reduce_multigpu&#39;</span><span class="p">,</span> <span class="s1">&#39;all_to_all&#39;</span><span class="p">,</span>
    <span class="s1">&#39;all_to_all_single&#39;</span><span class="p">,</span> <span class="s1">&#39;barrier&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_isend_irecv&#39;</span><span class="p">,</span> <span class="s1">&#39;broadcast&#39;</span><span class="p">,</span>
    <span class="s1">&#39;broadcast_multigpu&#39;</span><span class="p">,</span> <span class="s1">&#39;broadcast_object_list&#39;</span><span class="p">,</span> <span class="s1">&#39;destroy_process_group&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dist_backend&#39;</span><span class="p">,</span> <span class="s1">&#39;gather&#39;</span><span class="p">,</span> <span class="s1">&#39;gather_object&#39;</span><span class="p">,</span> <span class="s1">&#39;get_backend_config&#39;</span><span class="p">,</span> <span class="s1">&#39;get_backend&#39;</span><span class="p">,</span> <span class="s1">&#39;get_rank&#39;</span><span class="p">,</span>
    <span class="s1">&#39;get_world_size&#39;</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="s1">&#39;init_process_group&#39;</span><span class="p">,</span> <span class="s1">&#39;irecv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_gloo_available&#39;</span><span class="p">,</span> <span class="s1">&#39;is_initialized&#39;</span><span class="p">,</span> <span class="s1">&#39;is_mpi_available&#39;</span><span class="p">,</span> <span class="s1">&#39;is_backend_available&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_nccl_available&#39;</span><span class="p">,</span> <span class="s1">&#39;is_torchelastic_launched&#39;</span><span class="p">,</span> <span class="s1">&#39;is_ucc_available&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isend&#39;</span><span class="p">,</span> <span class="s1">&#39;monitored_barrier&#39;</span><span class="p">,</span> <span class="s1">&#39;new_group&#39;</span><span class="p">,</span> <span class="s1">&#39;new_subgroups&#39;</span><span class="p">,</span>
    <span class="s1">&#39;new_subgroups_by_enumeration&#39;</span><span class="p">,</span> <span class="s1">&#39;recv&#39;</span><span class="p">,</span> <span class="s1">&#39;reduce&#39;</span><span class="p">,</span> <span class="s1">&#39;reduce_multigpu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reduce_scatter&#39;</span><span class="p">,</span> <span class="s1">&#39;reduce_scatter_multigpu&#39;</span><span class="p">,</span> <span class="s1">&#39;scatter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_object_list&#39;</span><span class="p">,</span> <span class="s1">&#39;send&#39;</span><span class="p">,</span> <span class="s1">&#39;supports_complex&#39;</span><span class="p">,</span>
    <span class="s1">&#39;AllreduceCoalescedOptions&#39;</span><span class="p">,</span> <span class="s1">&#39;AllreduceOptions&#39;</span><span class="p">,</span> <span class="s1">&#39;AllToAllOptions&#39;</span><span class="p">,</span>
    <span class="s1">&#39;BarrierOptions&#39;</span><span class="p">,</span> <span class="s1">&#39;BroadcastOptions&#39;</span><span class="p">,</span> <span class="s1">&#39;GatherOptions&#39;</span><span class="p">,</span> <span class="s1">&#39;PrefixStore&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ProcessGroup&#39;</span><span class="p">,</span> <span class="s1">&#39;ReduceOp&#39;</span><span class="p">,</span> <span class="s1">&#39;ReduceOptions&#39;</span><span class="p">,</span> <span class="s1">&#39;ReduceScatterOptions&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ScatterOptions&#39;</span><span class="p">,</span> <span class="s1">&#39;Store&#39;</span><span class="p">,</span> <span class="s1">&#39;DebugLevel&#39;</span><span class="p">,</span> <span class="s1">&#39;get_debug_level&#39;</span><span class="p">,</span> <span class="s1">&#39;Work&#39;</span><span class="p">,</span>
    <span class="s1">&#39;default_pg_timeout&#39;</span><span class="p">,</span> <span class="s1">&#39;get_group_rank&#39;</span><span class="p">,</span> <span class="s1">&#39;get_global_rank&#39;</span><span class="p">,</span> <span class="s1">&#39;get_process_group_ranks&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reduce_op&#39;</span><span class="p">,</span> <span class="s1">&#39;all_gather_into_tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;reduce_scatter_tensor&#39;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">_MPI_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_NCCL_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_GLOO_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">_UCC_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">_pickler</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">Pickler</span>
<span class="n">_unpickler</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">Unpickler</span>

<span class="c1"># Change __module__ of all imported types from torch._C._distributed_c10d that are public</span>
<span class="k">def</span> <span class="nf">_export_c_types</span><span class="p">():</span>
    <span class="n">_public_types_to_change_module</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">AllreduceCoalescedOptions</span><span class="p">,</span>
        <span class="n">AllreduceOptions</span><span class="p">,</span>
        <span class="n">AllToAllOptions</span><span class="p">,</span>
        <span class="n">BarrierOptions</span><span class="p">,</span>
        <span class="n">BroadcastOptions</span><span class="p">,</span>
        <span class="n">GatherOptions</span><span class="p">,</span>
        <span class="n">PrefixStore</span><span class="p">,</span>
        <span class="n">ProcessGroup</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="p">,</span>
        <span class="n">ReduceOptions</span><span class="p">,</span>
        <span class="n">ReduceScatterOptions</span><span class="p">,</span>
        <span class="n">ScatterOptions</span><span class="p">,</span>
        <span class="n">Store</span><span class="p">,</span>
        <span class="n">DebugLevel</span><span class="p">,</span>
        <span class="n">get_debug_level</span><span class="p">,</span>
        <span class="n">Work</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="nb">type</span> <span class="ow">in</span> <span class="n">_public_types_to_change_module</span><span class="p">:</span>
        <span class="nb">type</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch.distributed.distributed_c10d&quot;</span>
<span class="n">_export_c_types</span><span class="p">()</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">ProcessGroupMPI</span>
    <span class="n">ProcessGroupMPI</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch.distributed.distributed_c10d&quot;</span>
    <span class="n">__all__</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;ProcessGroupMPI&quot;</span><span class="p">]</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_MPI_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">ProcessGroupNCCL</span>
    <span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch.distributed.distributed_c10d&quot;</span>
    <span class="n">__all__</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;ProcessGroupNCCL&quot;</span><span class="p">]</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_NCCL_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">ProcessGroupGloo</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">_ProcessGroupWrapper</span>
    <span class="n">ProcessGroupGloo</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch.distributed.distributed_c10d&quot;</span>
    <span class="n">__all__</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;ProcessGroupGloo&quot;</span><span class="p">]</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_GLOO_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">ProcessGroupUCC</span>
    <span class="n">ProcessGroupUCC</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;torch.distributed.distributed_c10d&quot;</span>
    <span class="n">__all__</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&quot;ProcessGroupUCC&quot;</span><span class="p">]</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_UCC_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">PG_WRAPPER_STORE_PREFIX</span> <span class="o">=</span> <span class="s2">&quot;pg_wrapper&quot;</span>


<span class="c1"># Some reduce ops are not supported by complex numbers and will result in an error.</span>
<span class="c1"># We currently provide complex support to the distributed API by viewing</span>
<span class="c1"># complex tensors as real (torch.view_as_real), meaning that calling</span>
<span class="c1"># these unsupported ops will return garbage values rather than error out.</span>
<span class="c1"># (e.g. max(2+3i, 3+2i) = 3+3i)</span>
<span class="c1"># We&#39;d like calls to unsupported ops to error out accordingly,</span>
<span class="c1"># rather than returning garbage values.</span>
<span class="k">def</span> <span class="nf">supports_complex</span><span class="p">(</span><span class="n">reduceOp</span><span class="p">:</span> <span class="n">ReduceOp</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">denyList</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">MIN</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">PRODUCT</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">BAND</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">BOR</span><span class="p">,</span>
        <span class="n">ReduceOp</span><span class="o">.</span><span class="n">BXOR</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">reduceOp</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">denyList</span>


<div class="viewcode-block" id="Backend"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.Backend">[docs]</a><span class="k">class</span> <span class="nc">Backend</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An enum-like class of available backends: GLOO, NCCL, UCC, MPI, and other registered</span>
<span class="sd">    backends.</span>

<span class="sd">    The values of this class are lowercase strings, e.g., ``&quot;gloo&quot;``. They can</span>
<span class="sd">    be accessed as attributes, e.g., ``Backend.NCCL``.</span>

<span class="sd">    This class can be directly called to parse the string, e.g.,</span>
<span class="sd">    ``Backend(backend_str)`` will check if ``backend_str`` is valid, and</span>
<span class="sd">    return the parsed lowercase string if so. It also accepts uppercase strings,</span>
<span class="sd">    e.g., ``Backend(&quot;GLOO&quot;)`` returns ``&quot;gloo&quot;``.</span>

<span class="sd">    .. note:: The entry ``Backend.UNDEFINED`` is present but only used as</span>
<span class="sd">              initial value of some fields. Users should neither use it directly</span>
<span class="sd">              nor assume its existence.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">UNDEFINED</span> <span class="o">=</span> <span class="s2">&quot;undefined&quot;</span>
    <span class="n">GLOO</span> <span class="o">=</span> <span class="s2">&quot;gloo&quot;</span>
    <span class="n">NCCL</span> <span class="o">=</span> <span class="s2">&quot;nccl&quot;</span>
    <span class="n">UCC</span> <span class="o">=</span> <span class="s2">&quot;ucc&quot;</span>
    <span class="n">MPI</span> <span class="o">=</span> <span class="s2">&quot;mpi&quot;</span>

    <span class="n">_BackendPlugin</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;_BackendPlugin&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;creator_fn&quot;</span><span class="p">,</span> <span class="s2">&quot;extended_api&quot;</span><span class="p">])</span>

    <span class="n">_plugins</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_BackendPlugin</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">backend_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">UNDEFINED</span><span class="p">,</span> <span class="n">GLOO</span><span class="p">,</span> <span class="n">NCCL</span><span class="p">,</span> <span class="n">UCC</span><span class="p">,</span> <span class="n">MPI</span><span class="p">]</span>

    <span class="n">backend_capability</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">GLOO</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">],</span>
        <span class="n">NCCL</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cuda&quot;</span><span class="p">],</span>
        <span class="n">UCC</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">],</span>
        <span class="n">MPI</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">],</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Backend name must be a string, but got: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">Backend</span><span class="p">,</span> <span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UNDEFINED</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UNDEFINED</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">value</span>

<div class="viewcode-block" id="Backend.register_backend"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.Backend.register_backend">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_backend</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">extended_api</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers a new backend with the given name and instantiating function.</span>

<span class="sd">        This class method is used by 3rd party ``ProcessGroup`` extension to</span>
<span class="sd">        register new backends.</span>

<span class="sd">        Args:</span>
<span class="sd">            name (str): Backend name of the ``ProcessGroup`` extension. It</span>
<span class="sd">                        should match the one in ``init_process_group()``.</span>
<span class="sd">            func (function): Function handler that instantiates the backend.</span>
<span class="sd">                             The function should be implemented in the backend</span>
<span class="sd">                             extension and takes four arguments, including</span>
<span class="sd">                             ``store``, ``rank``, ``world_size``, and ``timeout``.</span>
<span class="sd">            extended_api (bool, optional): Whether the backend supports extended argument structure.</span>
<span class="sd">                                           Default: ``False``. If set to ``True``, the backend</span>
<span class="sd">                                           will get an instance of ``c10d::DistributedBackendOptions``, and</span>
<span class="sd">                                           a process group options object as defined by the backend implementation.</span>
<span class="sd">            device (str or list of str, optional): device type this backend</span>
<span class="sd">                            supports, e.g. &quot;cpu&quot;, &quot;cuda&quot;, etc. If `None`,</span>
<span class="sd">                            assuming both &quot;cpu&quot; and &quot;cuda&quot;</span>

<span class="sd">        .. note:: This support of 3rd party backend is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Allow UCC plugin if Pytorch is not built with native support.</span>
        <span class="c1"># TODO: remove this exception once UCC plugin is fully deprecated.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">name</span> <span class="o">!=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UCC</span> <span class="ow">or</span> <span class="p">(</span><span class="n">name</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UCC</span> <span class="ow">and</span> <span class="n">is_ucc_available</span><span class="p">())):</span>
            <span class="k">assert</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">Backend</span><span class="p">,</span> <span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()),</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2"> c10d backend already exist&quot;</span>
            <span class="p">)</span>
        <span class="k">assert</span> <span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">Backend</span><span class="o">.</span><span class="n">_plugins</span><span class="p">,</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2"> c10d backend creator function already exist&quot;</span>
        <span class="p">)</span>

        <span class="nb">setattr</span><span class="p">(</span><span class="n">Backend</span><span class="p">,</span> <span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
        <span class="n">Backend</span><span class="o">.</span><span class="n">backend_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>

        <span class="c1"># Update device capability matrix in Backend class</span>
        <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># This is more of a backward support for groups like `threaded`:</span>
            <span class="c1"># assume default devices &quot;cpu&quot; and &quot;cuda&quot;, but warn</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Device capability of </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> unspecified, assuming `cpu` and &quot;</span>
                <span class="s2">&quot;`cuda`. Please specify it via the `devices` argument of &quot;</span>
                <span class="s2">&quot;`register_backend`.&quot;</span>
            <span class="p">)</span>
            <span class="n">Backend</span><span class="o">.</span><span class="n">backend_capability</span><span class="p">[</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># Single device string specified. Simply convert to list.</span>
            <span class="n">Backend</span><span class="o">.</span><span class="n">backend_capability</span><span class="p">[</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="o">=</span> <span class="p">[</span><span class="n">devices</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Backend</span><span class="o">.</span><span class="n">backend_capability</span><span class="p">[</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="o">=</span> <span class="n">devices</span>

        <span class="n">Backend</span><span class="o">.</span><span class="n">_plugins</span><span class="p">[</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()]</span> <span class="o">=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">_BackendPlugin</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">extended_api</span><span class="p">)</span></div></div>

<span class="k">class</span> <span class="nc">BackendConfig</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backend</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Backend</span><span class="p">]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">Backend</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UNDEFINED</span><span class="p">:</span>
            <span class="c1"># default config when backend is not specified</span>
            <span class="c1"># supported since PyTorch 2.0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cpu&quot;</span><span class="p">:</span> <span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">}</span>
            <span class="k">if</span> <span class="n">is_nccl_available</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span><span class="p">[</span><span class="s2">&quot;cuda&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span>
        <span class="k">elif</span> <span class="n">backend</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">Backend</span><span class="o">.</span><span class="n">backend_list</span><span class="p">:</span>
            <span class="c1"># Cases for when backend is a single string (without device types)</span>
            <span class="c1"># e.g. &quot;nccl&quot;, &quot;gloo&quot;, &quot;ucc&quot;, &quot;mpi&quot;</span>
            <span class="n">supported_devices</span> <span class="o">=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">backend_capability</span><span class="p">[</span><span class="n">backend</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
            <span class="n">backend_val</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">device</span> <span class="p">:</span> <span class="n">backend_val</span> <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">supported_devices</span>
            <span class="p">}</span>
        <span class="k">elif</span> <span class="s2">&quot;:&quot;</span> <span class="ow">in</span> <span class="n">backend</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="c1"># Backend specified in &quot;device:backend&quot; format</span>
            <span class="c1"># make sure the backend string is in the correct format</span>
            <span class="c1"># &quot;{device_type1}:{backend1},{device_type2}:{backend2}&quot;</span>
            <span class="c1"># e.g. &quot;cpu:gloo,cuda:nccl&quot;</span>
            <span class="n">backend_str_error_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;The custom backend string argument is invalid: </span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">                Custom backend string is an experimental feature where the backend string must be in the format:</span>
<span class="s2">                &quot;&lt;device_type1&gt;:&lt;backend1&gt;,&lt;device_type2&gt;:&lt;backend2&gt;...&quot;. e.g. &#39;cpu:gloo,cuda:nccl&#39;&quot;&quot;&quot;</span>

            <span class="c1"># parse the backend string and populate the device_backend_map</span>
            <span class="k">for</span> <span class="n">device_backend_pair_str</span> <span class="ow">in</span> <span class="n">backend</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">):</span>
                <span class="n">device_backend_pair</span> <span class="o">=</span> <span class="n">device_backend_pair_str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_backend_pair</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid device:backend pairing: </span><span class="se">\</span>
<span class="s2">                                     </span><span class="si">{</span><span class="n">device_backend_pair_str</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">backend_str_error_message</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">device</span><span class="p">,</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">device_backend_pair</span>
                <span class="k">if</span> <span class="n">device</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Duplicate device type </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2"> </span><span class="se">\</span>
<span class="s2">                                     in backend string: </span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">backend_str_error_message</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span><span class="p">[</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># User specified a single backend name whose device capability is</span>
            <span class="c1"># unknown, assuming it can support the default devices of PyTorch</span>
            <span class="c1"># (cpu and cuda)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Device capability of </span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2"> unknown, assuming `cpu` and &quot;</span>
                <span class="s2">&quot;`cuda`. You can specify it in `device:backend` format in &quot;</span>
                <span class="s2">&quot;`init_process_group` call.&quot;</span>
            <span class="p">)</span>
            <span class="n">backend_val</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;cpu&quot;</span> <span class="p">:</span> <span class="n">backend_val</span><span class="p">,</span>
                <span class="s2">&quot;cuda&quot;</span> <span class="p">:</span> <span class="n">backend_val</span><span class="p">,</span>
                <span class="s2">&quot;xpu&quot;</span> <span class="p">:</span> <span class="n">backend_val</span><span class="p">,</span>
            <span class="p">}</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Using backend config: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># noqa: G004</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># string with all the device:backend pairs separated by commas</span>
        <span class="k">return</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">device</span><span class="p">,</span> <span class="n">backend</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">get_device_backend_map</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_backend_map</span>

<span class="c1"># `_backend`, `dist_backend`, and `reduce_op` are here to maintain backward</span>
<span class="c1"># compatibility with pre-c10d distributed package.</span>
<span class="c1"># TODO: remove them when users are ready to take a hard dependency on PyTorch 1.</span>
<span class="n">_backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UNDEFINED</span>
<span class="n">dist_backend</span> <span class="o">=</span> <span class="n">Backend</span>


<span class="k">class</span> <span class="nc">_reduce_op</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deprecated enum-like class for reduction operations: ``SUM``, ``PRODUCT``,</span>
<span class="sd">    ``MIN``, and ``MAX``.</span>

<span class="sd">    :class:`~torch.distributed.ReduceOp` is recommended to use instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># __members__ is a dict storing key-value pairs for enum classes</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">ReduceOp</span><span class="o">.</span><span class="n">RedOpType</span><span class="o">.</span><span class="n">__members__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__members__</span> <span class="o">=</span> <span class="n">ReduceOp</span><span class="o">.</span><span class="n">RedOpType</span><span class="o">.</span><span class="n">__members__</span>

    <span class="k">def</span> <span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;torch.distributed.reduce_op is deprecated, please use &quot;</span>
            <span class="s2">&quot;torch.distributed.ReduceOp instead&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>


<span class="n">reduce_op</span> <span class="o">=</span> <span class="n">_reduce_op</span><span class="p">()</span>


<div class="viewcode-block" id="P2POp"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.P2POp">[docs]</a><span class="k">class</span> <span class="nc">P2POp</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class to build point-to-point operations for ``batch_isend_irecv``.</span>

<span class="sd">    This class builds the type of P2P operation, communication buffer, peer rank,</span>
<span class="sd">    Process Group, and tag. Instances of this class will be passed to</span>
<span class="sd">    ``batch_isend_irecv`` for point-to-point communications.</span>

<span class="sd">    Args:</span>
<span class="sd">        op (Callable): A function to send data to or receive data from a peer process.</span>
<span class="sd">            The type of ``op`` is either ``torch.distributed.isend`` or</span>
<span class="sd">            ``torch.distributed.irecv``.</span>
<span class="sd">        tensor (Tensor): Tensor to send or receive.</span>
<span class="sd">        peer (int): Destination or source rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match send with recv.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">peer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">op</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">peer</span> <span class="o">=</span> <span class="n">peer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tag</span> <span class="o">=</span> <span class="n">tag</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">peer</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">_check_op</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
        <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">_CollOp</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class to capture collective operations.</span>

<span class="sd">    Args:</span>
<span class="sd">        op (Callable): A collective function, e.g. ``torch.distributed.all_reduce``.</span>
<span class="sd">        tensor (Tensor): Tensor to operate on.</span>
<span class="sd">        dst_tensor (Tensor, optional): Provided when source and destinaton tensors are not the same.</span>
<span class="sd">        redop (ReduceOp, optional): reduce operation.</span>
<span class="sd">        root (int, optional): root of broadcast or reduce.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dst_tensor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">redop</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ReduceOp</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">root</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">op</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_tensor</span> <span class="o">=</span> <span class="n">dst_tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">redop</span> <span class="o">=</span> <span class="n">redop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root</span> <span class="o">=</span> <span class="n">root</span>


<span class="c1"># DO NOT USE THESE FIELDS DIRECTLY.</span>
<span class="c1"># Use them through the _world object to make sure the _world override mechanism</span>
<span class="n">_pg_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Store</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_pg_names</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_pg_group_ranks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1"># For a pg, it is a map from ProcessGroup to BackendConfig</span>
<span class="n">_pg_backend_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_group_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_tags_to_pg</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">_pg_to_tag</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>


<span class="k">class</span> <span class="nc">_World</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Container class for c10d process group state.</span>
<span class="sd">    This is used during registration and lookup of PG state.</span>

<span class="sd">    .. warning:: This is an experimental API intended to expose the inner workings</span>
<span class="sd">       of c10d and is subject to change..</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_default_pg</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pg_coalesce_state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">_CollOp</span><span class="p">,</span> <span class="n">P2POp</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pg_default_device</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">default_pg</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The default ProcessGroup includes all ranks of the cluster.</span>
<span class="sd">        This is used by c10d APIs when a ProcessGroup is needed but None is provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_default_pg</span>

    <span class="nd">@default_pg</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">default_pg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_default_pg</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pg_map</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Store</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Cached process groups</span>
<span class="sd">        For NCCL and GLOO pg, it is a map from ProcessGroup to (Backend, Store)</span>
<span class="sd">        For MPI pg, it is a map from ProcessGroup to (Backend, None)</span>

<span class="sd">        TODO don&#39;t expose the map, expose fine grained ops</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">_pg_map</span>
        <span class="k">return</span> <span class="n">_pg_map</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pg_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process group&#39;s names, map from ProcessGroup to str.</span>

<span class="sd">        TODO don&#39;t expose the map, expose fine grained ops</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">_pg_names</span>
        <span class="k">return</span> <span class="n">_pg_names</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pg_group_ranks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process group&#39;s global rank to local rank mapping</span>
<span class="sd">        TODO don&#39;t expose the map, expose fine grained ops</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">_pg_group_ranks</span>
        <span class="k">return</span> <span class="n">_pg_group_ranks</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pg_backend_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process group&#39;s backend config</span>
<span class="sd">        TODO don&#39;t expose the map, expose fine grained ops</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">_pg_backend_config</span>
        <span class="k">return</span> <span class="n">_pg_backend_config</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">group_count</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process group count for default naming.</span>

<span class="sd">        TODO don&#39;t expose group_count, use something else instead</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">_group_count</span>
        <span class="k">return</span> <span class="n">_group_count</span>

    <span class="nd">@group_count</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">group_count</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count is used when computing the name of ProcessGroups when using global synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">_group_count</span>
        <span class="n">_group_count</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tags_to_pg</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]]:</span>
        <span class="k">global</span> <span class="n">_tags_to_pg</span>
        <span class="k">return</span> <span class="n">_tags_to_pg</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pg_to_tag</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="k">global</span> <span class="n">_pg_to_tag</span>
        <span class="k">return</span> <span class="n">_pg_to_tag</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pg_coalesce_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">_CollOp</span><span class="p">,</span> <span class="n">P2POp</span><span class="p">]]]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pg_coalesce_state</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pg_default_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pg_default_device</span>

<span class="n">_world</span> <span class="o">=</span> <span class="n">_World</span><span class="p">()</span>
<span class="sd">&quot;&quot;&quot;Holds the singleton instance of ``_World`` used by c10. Experimental extension point to override it&quot;&quot;&quot;</span>

<span class="k">class</span> <span class="nc">_WorldMeta</span><span class="p">(</span><span class="nb">type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Meta class of ``group`` and ``GroupMember`` so they</span>
<span class="sd">    can have the class property ``WORLD``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Points to the default PG once initialized.</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">WORLD</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_world</span><span class="o">.</span><span class="n">default_pg</span>

    <span class="nd">@WORLD</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">WORLD</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]):</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">default_pg</span> <span class="o">=</span> <span class="n">pg</span>

<span class="k">class</span> <span class="nc">group</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">_WorldMeta</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="k">class</span> <span class="nc">GroupMember</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">_WorldMeta</span><span class="p">):</span>
    <span class="n">NON_GROUP_MEMBER</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>


<span class="c1"># Default process group state</span>
<span class="n">_default_pg_init_method</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">STORE_BASED_BARRIER_PREFIX</span> <span class="o">=</span> <span class="s2">&quot;store_based_barrier_key&quot;</span>

<span class="k">def</span> <span class="nf">_get_pg_default_device</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the device to use with ``group`` for control flow usage (object collectives, barrier).</span>
<span class="sd">    There are selection rules:</span>
<span class="sd">        1. If user specifies exactly one backend in ``init_process_group`` call:</span>
<span class="sd">            use that backend</span>
<span class="sd">        2. Else if user specifies multiple &quot;device:backend&quot; pairs in init_process_group:</span>
<span class="sd">            If &quot;cpu&quot; is among those pairs, use &quot;cpu&quot; (because the object is in cpu memory);</span>
<span class="sd">            Otherwise, use the first backend (sort of a random pick).</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.device: The device to use with ``group``.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">group</span> <span class="ow">or</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">:</span>
        <span class="c1"># Previously searched and cached; just return</span>
        <span class="k">return</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">ProcessGroup</span><span class="p">):</span>
        <span class="c1"># Provide backward compatibility to cases where `group` passed in is</span>
        <span class="c1"># actually a Backend (like `ProcessGroupGloo`) rather than a</span>
        <span class="c1"># `ProcessGroup` in PT 2.0 sense</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You are using a Backend </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">group</span><span class="p">)</span><span class="si">}</span><span class="s2"> as a ProcessGroup. &quot;</span>
            <span class="s2">&quot;This usage is deprecated since PyTorch 2.0. Please use a public API &quot;</span>
            <span class="s2">&quot;of PyTorch Distributed instead.&quot;</span>
        <span class="p">)</span>
        <span class="c1"># Most users create Gloo with private API for object collectives</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ``group._device_types`` is a property pybind that returns the devices</span>
<span class="sd">    (&quot;cpu&quot;, &quot;cuda&quot;, etc) supported by ``group``. Can be multiple if the</span>
<span class="sd">    ``group`` supports multiple devices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">_device_types</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># User fixed exactly one backend in `init_process_group`</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">=</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># No backend has been registered with this PG (maybe because no</span>
        <span class="c1"># collective has been run?) We pick cpu as the default and hopefully</span>
        <span class="c1"># this would lazily init Gloo or other available cpu backend.</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
        <span class="c1"># There are multiple backends in this PG and cpu is among them.</span>
        <span class="c1"># cpu is preferred as the object is in cpu memory. No need for device</span>
        <span class="c1"># copy.</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># No cpu in the backend list. Randomly pick the first backend</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span> <span class="o">=</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Using device </span><span class="si">{</span><span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span><span class="si">}</span><span class="s2"> for object &quot;</span>  <span class="c1"># noqa: G004</span>
        <span class="s2">&quot;collectives.&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">group</span><span class="p">]</span>


<span class="nd">@_time_logger</span>
<span class="k">def</span> <span class="nf">_store_based_barrier</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">store</span><span class="p">,</span> <span class="n">group_name</span><span class="p">,</span> <span class="n">rendezvous_count</span><span class="p">,</span> <span class="n">timeout</span><span class="p">,</span> <span class="n">logging_interval</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">10</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Barrier based on store which is used for synchronizing processes after</span>
<span class="sd">    ``init_process_group`` or ``new_group``. Intended to be used only with</span>
<span class="sd">    those two methods and is not a generic alternative to ``barrier()``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">store_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">STORE_BASED_BARRIER_PREFIX</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">group_name</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">store_key</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Added key: </span><span class="si">%s</span><span class="s2"> to store for rank: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">store_key</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>

    <span class="c1"># Now wait for all workers to check in with the store.</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">rendezvous_count</span>
    <span class="n">worker_count</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">store_key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">last_worker_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">store_key</span><span class="si">}</span><span class="s2">:last_worker&quot;</span>
    <span class="k">if</span> <span class="n">worker_count</span> <span class="o">==</span> <span class="n">world_size</span><span class="p">:</span>
        <span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">last_worker_key</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">)</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># This will throw an exception after the logging_interval in which we print out</span>
            <span class="c1"># the status of the group or time out officially, throwing runtime error</span>
            <span class="n">store</span><span class="o">.</span><span class="n">wait</span><span class="p">([</span><span class="n">last_worker_key</span><span class="p">],</span> <span class="n">logging_interval</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">worker_count</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">store_key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Print status periodically to keep track.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Waiting in store based barrier to initialize process group for &quot;</span>
                <span class="s2">&quot;rank: </span><span class="si">%s</span><span class="s2">, key: </span><span class="si">%s</span><span class="s2"> (world_size=</span><span class="si">%s</span><span class="s2">, num_workers_joined=</span><span class="si">%s</span><span class="s2">, timeout=</span><span class="si">%s</span><span class="s2">)&quot;</span><span class="p">,</span>
                <span class="n">rank</span><span class="p">,</span> <span class="n">store_key</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">worker_count</span><span class="p">,</span> <span class="n">timeout</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">timeout</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Timed out initializing process group in store based barrier on &quot;</span>
                    <span class="s2">&quot;rank </span><span class="si">{}</span><span class="s2">, for key: </span><span class="si">{}</span><span class="s2"> (world_size=</span><span class="si">{}</span><span class="s2">, num_workers_joined=</span><span class="si">{}</span><span class="s2">, timeout=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">rank</span><span class="p">,</span> <span class="n">store_key</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">worker_count</span><span class="p">,</span> <span class="n">timeout</span>
                    <span class="p">)</span>
                <span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s2">&quot;Rank </span><span class="si">%s</span><span class="s2">: Completed store-based barrier for key:</span><span class="si">%s</span><span class="s2"> with </span><span class="si">%s</span><span class="s2"> nodes.&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">store_key</span><span class="p">,</span> <span class="n">world_size</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper that checks if the current process&#39;s rank is not in a given group.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">group</span> <span class="o">==</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span>


<span class="k">def</span> <span class="nf">_warn_not_in_group</span><span class="p">(</span><span class="n">op_name</span><span class="p">):</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Running </span><span class="si">{</span><span class="n">op_name</span><span class="si">}</span><span class="s2"> on global rank </span><span class="si">{</span><span class="n">global_rank</span><span class="si">}</span><span class="s2"> which does not &quot;</span>
        <span class="s2">&quot;belong to the given group.&quot;</span>
    <span class="p">)</span>


<div class="viewcode-block" id="get_group_rank"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_group_rank">[docs]</a><span class="k">def</span> <span class="nf">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">global_rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Translate a global rank into a group rank.</span>

<span class="sd">    ``global_rank`` must be part of ``group`` otherwise this raises RuntimeError.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup): ProcessGroup to find the relative rank.</span>
<span class="sd">        global_rank (int): Global rank to query.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Group rank of ``global_rank`` relative to ``group``</span>

<span class="sd">    N.B. calling this function on the default process group returns identity</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">global_rank</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Group </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2"> is not registered, please create group with torch.distributed.new_group API&quot;</span><span class="p">)</span>
    <span class="n">group_ranks</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">[</span><span class="n">group</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">global_rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">group_ranks</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Global rank </span><span class="si">{</span><span class="n">global_rank</span><span class="si">}</span><span class="s2"> is not part of group </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">group_ranks</span><span class="p">[</span><span class="n">global_rank</span><span class="p">]</span></div>

<div class="viewcode-block" id="get_global_rank"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_global_rank">[docs]</a><span class="k">def</span> <span class="nf">get_global_rank</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">group_rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Translate a group rank into a global rank.</span>

<span class="sd">    ``group_rank`` must be part of `group` otherwise this raises RuntimeError.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup): ProcessGroup to find the global rank from.</span>
<span class="sd">        group_rank (int): Group rank to query.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Global rank of ``group_rank`` relative to ``group``</span>

<span class="sd">    N.B. calling this function on the default process group returns identity</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">group_rank</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Group </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2"> is not registered, please create group with torch.distributed.new_group API&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">grp_rank</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">[</span><span class="n">group</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">grp_rank</span> <span class="o">==</span> <span class="n">group_rank</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">rank</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Group rank </span><span class="si">{</span><span class="n">group_rank</span><span class="si">}</span><span class="s2"> is not part of group </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

<span class="c1"># TODO: remove this once the ecosystem moves away from it.</span>
<span class="k">def</span> <span class="nf">_get_global_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">rank</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This method is deprecated, please use get_global_rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed.distributed_c10d._get_global_rank is deprecated &quot;</span>
        <span class="s2">&quot;please use torch.distributed.distributed_c10d.get_global_rank instead&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">get_global_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>


<div class="viewcode-block" id="get_process_group_ranks"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_process_group_ranks">[docs]</a><span class="k">def</span> <span class="nf">get_process_group_ranks</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get all ranks associated with ``group``.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup): ProcessGroup to get all ranks from.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List of global ranks ordered by group rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">[</span><span class="n">group</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span></div>

<span class="k">def</span> <span class="nf">_get_group_size</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper that gets a given group&#39;s world size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_check_single_tensor</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to check that the parameter ``param_name`` is a single tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function argument. Expected parameter `</span><span class="si">{}</span><span class="s2">` &quot;</span>
            <span class="s2">&quot;to be of type torch.Tensor.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_tensor_list</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to check that the parameter ``param_name`` is a list of tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function argument. Expected parameter `</span><span class="si">{}</span><span class="s2">` &quot;</span>
            <span class="s2">&quot;to be of type List[torch.Tensor].&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>
        <span class="p">)</span>

<span class="k">def</span> <span class="nf">_as_iterable</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Iterable</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">obj</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">obj</span><span class="p">,)</span>

<span class="k">def</span> <span class="nf">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">last_dtype</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">_as_iterable</span><span class="p">,</span> <span class="n">tensors</span><span class="p">)):</span>
        <span class="n">tensor_dtype</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span>
        <span class="c1"># Mixing complex and its element type is allowed</span>
        <span class="k">if</span> <span class="n">tensor_dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">tensor_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">tensor_dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex64</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span>

        <span class="k">if</span> <span class="n">last_dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">last_dtype</span> <span class="o">=</span> <span class="n">tensor_dtype</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">last_dtype</span> <span class="o">!=</span> <span class="n">tensor_dtype</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Invalid usage of tensors with different dtypes&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="n">last_dtype</span><span class="si">}</span><span class="s2"> and  </span><span class="si">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_op</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to check that the ``op`` is either isend or irecv.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">op</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">isend</span><span class="p">,</span> <span class="n">irecv</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid ``op``. Expected ``op`` &quot;</span>
            <span class="s2">&quot;to be of type ``torch.distributed.isend`` or &quot;</span>
            <span class="s2">&quot;``torch.distributed.irecv``.&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_p2p_op_list</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper to check that the ``p2p_op_list`` is a list of P2POp instances and</span>
<span class="sd">    all ops use the same group.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">p2p_op</span><span class="p">,</span> <span class="n">P2POp</span><span class="p">)</span> <span class="k">for</span> <span class="n">p2p_op</span> <span class="ow">in</span> <span class="n">p2p_op_list</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid ``p2p_op_list``. Each op is expected to &quot;</span>
            <span class="s2">&quot;to be of type ``torch.distributed.P2POp``.&quot;</span>
        <span class="p">)</span>

    <span class="n">group</span> <span class="o">=</span> <span class="n">p2p_op_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">group</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">group</span> <span class="o">==</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">group</span> <span class="k">for</span> <span class="n">p2p_op</span> <span class="ow">in</span> <span class="n">p2p_op_list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;All ops need to use the same group.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="is_mpi_available"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_mpi_available">[docs]</a><span class="k">def</span> <span class="nf">is_mpi_available</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the MPI backend is available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_MPI_AVAILABLE</span></div>


<div class="viewcode-block" id="is_nccl_available"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_nccl_available">[docs]</a><span class="k">def</span> <span class="nf">is_nccl_available</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the NCCL backend is available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_NCCL_AVAILABLE</span></div>


<div class="viewcode-block" id="is_gloo_available"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_gloo_available">[docs]</a><span class="k">def</span> <span class="nf">is_gloo_available</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the Gloo backend is available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_GLOO_AVAILABLE</span></div>


<span class="k">def</span> <span class="nf">is_ucc_available</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the UCC backend is available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_UCC_AVAILABLE</span>


<span class="k">def</span> <span class="nf">is_backend_available</span><span class="p">(</span><span class="n">backend</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the given backend is available and supports the built-in backends or</span>
<span class="sd">    third-party backends through function ``Backend.register_backend``.</span>

<span class="sd">    Args:</span>
<span class="sd">        backend (str): Backend name.</span>
<span class="sd">    Returns:</span>
<span class="sd">        bool: Returns true if the backend is available otherwise false.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If the backend has an ``is_backend_available`` function, return the result of that function directly</span>
    <span class="n">available_func</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;is_</span><span class="si">{</span><span class="n">backend</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">_available&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">available_func</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">available_func</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">Backend</span><span class="o">.</span><span class="n">backend_list</span>


<div class="viewcode-block" id="is_initialized"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_initialized">[docs]</a><span class="k">def</span> <span class="nf">is_initialized</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checking if the default process group has been initialized</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="is_torchelastic_launched"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.is_torchelastic_launched">[docs]</a><span class="k">def</span> <span class="nf">is_torchelastic_launched</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether this process was launched with ``torch.distributed.elastic``</span>
<span class="sd">    (aka torchelastic). The existence of ``TORCHELASTIC_RUN_ID`` environment</span>
<span class="sd">    variable is used as a proxy to determine whether the current process</span>
<span class="sd">    was launched with torchelastic. This is a reasonable proxy since</span>
<span class="sd">    ``TORCHELASTIC_RUN_ID`` maps to the rendezvous id which is always a</span>
<span class="sd">    non-null value indicating the job id for peer discovery purposes..</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TORCHELASTIC_RUN_ID&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>


<span class="k">def</span> <span class="nf">_is_barrier_after_init</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="c1"># Environment variable to control whether process group should perform a</span>
    <span class="c1"># barrier after its init. Default value is 0, i.e. no barrier. If you</span>
    <span class="c1"># experience issue with this setting, you may set</span>
    <span class="c1"># `TORCH_DIST_INIT_BARRIER=1` to add the barrier.</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TORCH_DIST_INIT_BARRIER&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_get_default_group</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Getting the default process group created by init_process_group</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Default process group has not been initialized, &quot;</span>
            <span class="s2">&quot;please make sure to call init_process_group.&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span>


<span class="k">def</span> <span class="nf">_get_default_store</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Getting the default store created by init_process_group</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Default process group has not been initialized, &quot;</span>
            <span class="s2">&quot;please make sure to call init_process_group.&quot;</span>
        <span class="p">)</span>
    <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">default_store</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="p">[</span><span class="n">default_pg</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">default_store</span>


<span class="k">def</span> <span class="nf">_update_default_pg</span><span class="p">(</span><span class="n">pg</span><span class="p">):</span>
    <span class="n">_world</span><span class="o">.</span><span class="n">default_pg</span> <span class="o">=</span> <span class="n">pg</span>

<span class="k">def</span> <span class="nf">get_backend_config</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">pg</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid process group specified&quot;</span><span class="p">)</span>
    <span class="n">backend_config</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_backend_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">backend_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">backend_config</span><span class="p">)</span>

<div class="viewcode-block" id="get_backend"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_backend">[docs]</a><span class="k">def</span> <span class="nf">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the backend of the given process group.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. The</span>
<span class="sd">            default is the general main process group. If another specific group</span>
<span class="sd">            is specified, the calling process must be part of :attr:`group`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The backend of the given process group as a lower case string.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">pg</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid process group specified&quot;</span><span class="p">)</span>
    <span class="n">pg_store</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">pg_store</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">pg_store</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<span class="n">_exception_logger</span>
<div class="viewcode-block" id="init_process_group"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.init_process_group">[docs]</a><span class="nd">@_time_logger</span>
<span class="k">def</span> <span class="nf">init_process_group</span><span class="p">(</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Backend</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="n">timedelta</span> <span class="o">=</span> <span class="n">default_pg_timeout</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">store</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Store</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">group_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the default distributed process group, and this will also</span>
<span class="sd">    initialize the distributed package.</span>

<span class="sd">    There are 2 main ways to initialize a process group:</span>
<span class="sd">        1. Specify ``store``, ``rank``, and ``world_size`` explicitly.</span>
<span class="sd">        2. Specify ``init_method`` (a URL string) which indicates where/how</span>
<span class="sd">           to discover peers. Optionally specify ``rank`` and ``world_size``,</span>
<span class="sd">           or encode all required parameters in the URL and omit them.</span>

<span class="sd">    If neither is specified, ``init_method`` is assumed to be &quot;env://&quot;.</span>


<span class="sd">    Args:</span>
<span class="sd">        backend (str or Backend, optional): The backend to use. Depending on</span>
<span class="sd">            build-time configurations, valid values include ``mpi``, ``gloo``,</span>
<span class="sd">            ``nccl``, and ``ucc``. If the backend is not provided, then both a ``gloo``</span>
<span class="sd">            and ``nccl`` backend will be created, see notes below for how multiple</span>
<span class="sd">            backends are managed. This field can be given as a lowercase string</span>
<span class="sd">            (e.g., ``&quot;gloo&quot;``), which can also be accessed via</span>
<span class="sd">            :class:`Backend` attributes (e.g., ``Backend.GLOO``). If using</span>
<span class="sd">            multiple processes per machine with ``nccl`` backend, each process</span>
<span class="sd">            must have exclusive access to every GPU it uses, as sharing GPUs</span>
<span class="sd">            between processes can result in deadlocks. ``ucc`` backend is</span>
<span class="sd">            experimental.</span>
<span class="sd">        init_method (str, optional): URL specifying how to initialize the</span>
<span class="sd">                                     process group. Default is &quot;env://&quot; if no</span>
<span class="sd">                                     ``init_method`` or ``store`` is specified.</span>
<span class="sd">                                     Mutually exclusive with ``store``.</span>
<span class="sd">        world_size (int, optional): Number of processes participating in</span>
<span class="sd">                                    the job. Required if ``store`` is specified.</span>
<span class="sd">        rank (int, optional): Rank of the current process (it should be a</span>
<span class="sd">                              number between 0 and ``world_size``-1).</span>
<span class="sd">                              Required if ``store`` is specified.</span>
<span class="sd">        store(Store, optional): Key/value store accessible to all workers, used</span>
<span class="sd">                                to exchange connection/address information.</span>
<span class="sd">                                Mutually exclusive with ``init_method``.</span>
<span class="sd">        timeout (timedelta, optional): Timeout for operations executed against</span>
<span class="sd">            the process group. Default value equals 30 minutes.</span>
<span class="sd">            This is applicable for the ``gloo`` backend. For ``nccl``, this is</span>
<span class="sd">            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            or ``NCCL_ASYNC_ERROR_HANDLING`` is set to 1. When</span>
<span class="sd">            ``NCCL_BLOCKING_WAIT`` is set, this is the duration for which the</span>
<span class="sd">            process will block and wait for collectives to complete before</span>
<span class="sd">            throwing an exception. When ``NCCL_ASYNC_ERROR_HANDLING`` is set,</span>
<span class="sd">            this is the duration after which collectives will be aborted</span>
<span class="sd">            asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            will provide errors to the user which can be caught and handled,</span>
<span class="sd">            but due to its blocking nature, it has a performance overhead. On</span>
<span class="sd">            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little</span>
<span class="sd">            performance overhead, but crashes the process on errors. This is</span>
<span class="sd">            done since CUDA execution is async and it is no longer safe to</span>
<span class="sd">            continue executing user code since failed async NCCL operations</span>
<span class="sd">            might result in subsequent CUDA operations running on corrupted</span>
<span class="sd">            data. Only one of these two environment variables should be set.</span>
<span class="sd">            For ``ucc``, blocking wait is supported similar to NCCL. However,</span>
<span class="sd">            async error handling is done differently since with UCC we have</span>
<span class="sd">            progress thread and not watch-dog thread.</span>
<span class="sd">        group_name (str, optional, deprecated): Group name. This argument is ignored</span>
<span class="sd">        pg_options (ProcessGroupOptions, optional): process group options</span>
<span class="sd">            specifying what additional options need to be passed in during</span>
<span class="sd">            the construction of specific process groups. As of now, the only</span>
<span class="sd">            options we support is ``ProcessGroupNCCL.Options`` for the ``nccl``</span>
<span class="sd">            backend, ``is_high_priority_stream`` can be specified so that</span>
<span class="sd">            the nccl backend can pick up high priority cuda streams when</span>
<span class="sd">            there&#39;re compute kernels waiting.</span>

<span class="sd">    .. note:: To enable ``backend == Backend.MPI``, PyTorch needs to be built from source</span>
<span class="sd">        on a system that supports MPI.</span>

<span class="sd">    .. note:: Support for multiple backends is experimental. Currently when no backend is</span>
<span class="sd">        specified, both ``gloo`` and ``nccl`` backends will be created. The ``gloo`` backend</span>
<span class="sd">        will be used for collectives with CPU tensors and the ``nccl`` backend will be used</span>
<span class="sd">        for collectives with CUDA tensors. A custom backend can be specified by passing in</span>
<span class="sd">        a string with format &quot;&lt;device_type&gt;:&lt;backend_name&gt;,&lt;device_type&gt;:&lt;backend_name&gt;&quot;, e.g.</span>
<span class="sd">        &quot;cpu:gloo,cuda:custom_backend&quot;.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_world</span>

    <span class="k">global</span> <span class="n">_backend</span>
    <span class="k">global</span> <span class="n">_default_pg_init_method</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Expected timeout argument to be of type&quot;</span> <span class="s2">&quot;datetime.timedelta&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trying to initialize the default process group &quot;</span> <span class="s2">&quot;twice!&quot;</span><span class="p">)</span>

    <span class="k">assert</span> <span class="p">(</span><span class="n">store</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="n">init_method</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">&quot;Cannot specify both init_method and store.&quot;</span>

    <span class="k">if</span> <span class="n">store</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;world_size must be positive if using store&quot;</span>
        <span class="k">assert</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;rank must be non-negative if using store&quot;</span>
    <span class="k">elif</span> <span class="n">init_method</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">init_method</span> <span class="o">=</span> <span class="s2">&quot;env://&quot;</span>

    <span class="k">if</span> <span class="n">backend</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="s2">&quot;undefined&quot;</span><span class="p">)</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Group name is not visible to users unless they access</span>
<span class="sd">    internals of c10d. This means we can ignore the value</span>
<span class="sd">    they provide as it not exposed in a public way.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">group_name</span> <span class="o">=</span> <span class="n">_process_group_name</span><span class="p">([],</span> <span class="n">use_hashed_name</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">world_size</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;For MPI backend, world_size (</span><span class="si">{}</span><span class="s2">) and rank (</span><span class="si">{}</span><span class="s2">) &quot;</span>
                <span class="s2">&quot;are ignored since they are assigned by the &quot;</span>
                <span class="s2">&quot;MPI runtime.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="n">default_pg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_new_process_group_helper</span><span class="p">(</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[],</span> <span class="n">backend</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">group_name</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span>
        <span class="p">)</span>
        <span class="n">_update_default_pg</span><span class="p">(</span><span class="n">default_pg</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># backward compatible API</span>
        <span class="k">if</span> <span class="n">store</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rendezvous_iterator</span> <span class="o">=</span> <span class="n">rendezvous</span><span class="p">(</span>
                <span class="n">init_method</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span>
            <span class="p">)</span>
            <span class="n">store</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">rendezvous_iterator</span><span class="p">)</span>
            <span class="n">store</span><span class="o">.</span><span class="n">set_timeout</span><span class="p">(</span><span class="n">timeout</span><span class="p">)</span>

            <span class="c1"># Use a PrefixStore to avoid accidental overrides of keys used by</span>
            <span class="c1"># different systems (e.g. RPC) in case the store is multi-tenant.</span>
            <span class="n">store</span> <span class="o">=</span> <span class="n">PrefixStore</span><span class="p">(</span><span class="s2">&quot;default_pg&quot;</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>

        <span class="n">default_pg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_new_process_group_helper</span><span class="p">(</span>
            <span class="n">world_size</span><span class="p">,</span>
            <span class="n">rank</span><span class="p">,</span>
            <span class="p">[],</span>
            <span class="n">backend</span><span class="p">,</span>
            <span class="n">store</span><span class="p">,</span>
            <span class="n">pg_options</span><span class="o">=</span><span class="n">pg_options</span><span class="p">,</span>
            <span class="n">group_name</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span>
            <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span>
        <span class="p">)</span>
        <span class="n">_update_default_pg</span><span class="p">(</span><span class="n">default_pg</span><span class="p">)</span>

    <span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">[</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="o">.</span><span class="n">size</span><span class="p">())}</span>  <span class="c1"># type: ignore[attr-defined, index]</span>
    <span class="n">_backend</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="p">[</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># type: ignore[index]</span>
    <span class="n">_default_pg_init_method</span> <span class="o">=</span> <span class="n">init_method</span>

    <span class="k">if</span> <span class="n">_is_barrier_after_init</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># barrier at the end to ensure that once we return from this method, all</span>
        <span class="c1"># process groups including global variables (if any) are updated</span>
        <span class="c1"># correctly on all ranks.</span>
        <span class="c1"># Update 04/2023: for large-scale runs, this barrier (esp. store-based</span>
        <span class="c1"># barrier) may be costly and/or unscalable. Also, in a lot of cases,</span>
        <span class="c1"># these barriers may be unnecessary, as proven by a green CI after</span>
        <span class="c1"># removal. An environment variable `TORCH_DIST_INIT_BARRIER` has been</span>
        <span class="c1"># added which enables this barrier only when set to 1.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Performing barrier after ProcessGroup initialization since &quot;</span>
            <span class="s2">&quot;TORCH_DIST_INIT_BARRIER = 1&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
            <span class="c1"># MPI backend doesn&#39;t use store.</span>
            <span class="n">barrier</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use store based barrier here since barrier() used a bunch of</span>
            <span class="c1"># default devices and messes up NCCL internal state.</span>
            <span class="n">_store_based_barrier</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">store</span><span class="p">,</span> <span class="n">group_name</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_new_process_group_helper</span><span class="p">(</span>
    <span class="n">group_size</span><span class="p">,</span>
    <span class="n">group_rank</span><span class="p">,</span>
    <span class="n">global_ranks_in_group</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">,</span>
    <span class="n">store</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">group_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span>
    <span class="n">pg_tag</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a new distributed process group.</span>

<span class="sd">    This function must be called by ALL processes in the global group, even if</span>
<span class="sd">    the calling process is not part of the newly created group. In that case,</span>
<span class="sd">    this function returns GroupMember.NON_GROUP_MEMBER.</span>

<span class="sd">    This function is called with ``global_ranks_in_group == []`` for the default group.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_world</span>

    <span class="k">if</span> <span class="n">group_name</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_names</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;The specified group name has already been &quot;</span>
            <span class="s2">&quot;created, please use a different group name&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Expected timeout argument to be of type&quot;</span> <span class="s2">&quot;datetime.timedelta&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">pg_tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">]:</span>
        <span class="c1"># creating with the same tag and rank set results in the same underlying PG</span>
        <span class="n">existing_group</span> <span class="o">=</span> <span class="n">_find_pg_by_ranks_and_tag</span><span class="p">(</span><span class="n">pg_tag</span><span class="p">,</span> <span class="n">global_ranks_in_group</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">existing_group</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">prefix_store</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="p">[</span><span class="n">existing_group</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">existing_group</span><span class="p">,</span> <span class="n">prefix_store</span>

    <span class="c1"># The list of group ranks is empty if we&#39;re creating the default group.</span>
    <span class="n">is_default_group</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_ranks_in_group</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># If this is a subgroup (which means group_ranks is specified),</span>
    <span class="c1"># we check if the current process is a member of the new group.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_default_group</span><span class="p">:</span>
        <span class="n">global_rank</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">global_rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">global_ranks_in_group</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span><span class="p">,</span> <span class="kc">None</span>

    <span class="n">prefix_store</span> <span class="o">=</span> <span class="n">PrefixStore</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">group_name</span><span class="si">}</span><span class="s2">/&quot;</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>
    <span class="n">base_pg_options</span> <span class="o">=</span> <span class="n">ProcessGroup</span><span class="o">.</span><span class="n">Options</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">backend</span><span class="p">))</span>
    <span class="n">base_pg_options</span><span class="o">.</span><span class="n">_timeout</span> <span class="o">=</span> <span class="n">timeout</span>
    <span class="n">pg</span><span class="p">:</span> <span class="n">ProcessGroup</span> <span class="o">=</span> <span class="n">ProcessGroup</span><span class="p">(</span><span class="n">prefix_store</span><span class="p">,</span> <span class="n">group_rank</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">base_pg_options</span><span class="p">)</span>
    <span class="n">backend_config</span> <span class="o">=</span> <span class="n">BackendConfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">device</span><span class="p">,</span> <span class="n">backend_str</span> <span class="ow">in</span> <span class="n">backend_config</span><span class="o">.</span><span class="n">get_device_backend_map</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Use the group name as prefix in the default store, such that</span>
        <span class="c1"># a single store can be reused by multiple groups.</span>
        <span class="n">backend_prefix_store</span> <span class="o">=</span> <span class="n">PrefixStore</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">/&quot;</span><span class="p">,</span> <span class="n">prefix_store</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">backend_str</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_mpi_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Distributed package doesn&#39;t have MPI built in.&quot;</span>
                    <span class="s2">&quot; MPI is only included if you build PyTorch from&quot;</span>
                    <span class="s2">&quot; source on a host that has MPI installed.&quot;</span>
                <span class="p">)</span>
            <span class="n">backend_class</span> <span class="o">=</span> <span class="n">ProcessGroupMPI</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">global_ranks_in_group</span><span class="p">)</span>
            <span class="n">backend_type</span> <span class="o">=</span> <span class="n">ProcessGroup</span><span class="o">.</span><span class="n">BackendType</span><span class="o">.</span><span class="n">MPI</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">backend_class</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span>
            <span class="c1"># create new process group with accurate rank and size</span>
            <span class="k">if</span> <span class="n">pg</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">pg</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">pg</span> <span class="o">=</span> <span class="n">ProcessGroup</span><span class="p">(</span><span class="n">backend_prefix_store</span><span class="p">,</span> <span class="n">backend_class</span><span class="o">.</span><span class="n">rank</span><span class="p">(),</span> <span class="n">backend_class</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">base_pg_options</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend_str</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">:</span>
            <span class="c1"># TODO: remove this check after lazy initialization is supported</span>
            <span class="c1"># if pg_options is not None:</span>
            <span class="c1">#     raise RuntimeError(&quot;GLOO options not supported&quot;)</span>
            <span class="n">backend_class</span> <span class="o">=</span> <span class="n">ProcessGroupGloo</span><span class="p">(</span><span class="n">backend_prefix_store</span><span class="p">,</span> <span class="n">group_rank</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
            <span class="n">backend_type</span> <span class="o">=</span> <span class="n">ProcessGroup</span><span class="o">.</span><span class="n">BackendType</span><span class="o">.</span><span class="n">GLOO</span>
        <span class="k">elif</span> <span class="n">backend_str</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_nccl_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Distributed package doesn&#39;t have NCCL &quot;</span> <span class="s2">&quot;built in&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">pg_options</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">pg_options</span><span class="p">,</span> <span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="n">Options</span>
                <span class="p">),</span> <span class="s2">&quot;Expected pg_options argument to be of type ProcessGroupNCCL.Options&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># default pg_options for NCCL</span>
                <span class="n">pg_options</span> <span class="o">=</span> <span class="n">ProcessGroupNCCL</span><span class="o">.</span><span class="n">Options</span><span class="p">()</span>
                <span class="n">pg_options</span><span class="o">.</span><span class="n">is_high_priority_stream</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">pg_options</span><span class="o">.</span><span class="n">_timeout</span> <span class="o">=</span> <span class="n">timeout</span>

            <span class="n">backend_class</span> <span class="o">=</span> <span class="n">ProcessGroupNCCL</span><span class="p">(</span><span class="n">backend_prefix_store</span><span class="p">,</span> <span class="n">group_rank</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">pg_options</span><span class="p">)</span>
            <span class="n">backend_type</span> <span class="o">=</span> <span class="n">ProcessGroup</span><span class="o">.</span><span class="n">BackendType</span><span class="o">.</span><span class="n">NCCL</span>
        <span class="k">elif</span> <span class="n">backend_str</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UCC</span> <span class="ow">and</span> <span class="n">is_ucc_available</span><span class="p">():</span>
            <span class="c1"># TODO: once UCC plugin is fully deprecated, remove</span>
            <span class="c1"># is_ucc_available() from above elif-condition and raise</span>
            <span class="c1"># RuntimeError if is_ucc_available() returns false.</span>

            <span class="n">backend_class</span> <span class="o">=</span> <span class="n">ProcessGroupUCC</span><span class="p">(</span><span class="n">backend_prefix_store</span><span class="p">,</span> <span class="n">group_rank</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
            <span class="n">backend_type</span> <span class="o">=</span> <span class="n">ProcessGroup</span><span class="o">.</span><span class="n">BackendType</span><span class="o">.</span><span class="n">UCC</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">backend_str</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="ow">in</span> <span class="n">Backend</span><span class="o">.</span><span class="n">_plugins</span><span class="p">,</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unknown c10d backend type </span><span class="si">{</span><span class="n">backend_str</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

            <span class="n">backend_plugin</span> <span class="o">=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">_plugins</span><span class="p">[</span><span class="n">backend_str</span><span class="o">.</span><span class="n">upper</span><span class="p">()]</span>
            <span class="n">creator_fn</span> <span class="o">=</span> <span class="n">backend_plugin</span><span class="o">.</span><span class="n">creator_fn</span>
            <span class="n">extended_api</span> <span class="o">=</span> <span class="n">backend_plugin</span><span class="o">.</span><span class="n">extended_api</span>
            <span class="n">backend_type</span> <span class="o">=</span> <span class="n">ProcessGroup</span><span class="o">.</span><span class="n">BackendType</span><span class="o">.</span><span class="n">CUSTOM</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">extended_api</span><span class="p">:</span>
                <span class="n">backend_class</span> <span class="o">=</span> <span class="n">creator_fn</span><span class="p">(</span><span class="n">backend_prefix_store</span><span class="p">,</span> <span class="n">group_rank</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dist_backend_opts</span> <span class="o">=</span> <span class="n">_DistributedBackendOptions</span><span class="p">()</span>
                <span class="n">dist_backend_opts</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="n">backend_prefix_store</span>
                <span class="n">dist_backend_opts</span><span class="o">.</span><span class="n">group_rank</span> <span class="o">=</span> <span class="n">group_rank</span>
                <span class="n">dist_backend_opts</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="n">group_size</span>
                <span class="n">dist_backend_opts</span><span class="o">.</span><span class="n">timeout</span> <span class="o">=</span> <span class="n">timeout</span>
                <span class="n">dist_backend_opts</span><span class="o">.</span><span class="n">group_id</span> <span class="o">=</span> <span class="n">group_name</span>
                <span class="n">dist_backend_opts</span><span class="o">.</span><span class="n">global_ranks_in_group</span> <span class="o">=</span> <span class="n">global_ranks_in_group</span>

                <span class="n">backend_class</span> <span class="o">=</span> <span class="n">creator_fn</span><span class="p">(</span><span class="n">dist_backend_opts</span><span class="p">,</span> <span class="n">pg_options</span><span class="p">)</span>

        <span class="c1"># Set sequence numbers for gloo and nccl backends.</span>
        <span class="k">if</span> <span class="n">backend_str</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">,</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">]:</span>
            <span class="n">backend_class</span><span class="o">.</span><span class="n">_set_sequence_number_for_group</span><span class="p">()</span>
        <span class="c1"># If the type is a subclass of ProcessGroup then return this process group immediately</span>
        <span class="c1"># TODO: This defaults to the old behavior for PythonProcessGroups which overwrites the</span>
        <span class="c1"># ProcessGroup instance</span>
        <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">backend_class</span><span class="p">),</span> <span class="n">ProcessGroup</span><span class="p">):</span>
            <span class="n">pg</span> <span class="o">=</span> <span class="n">backend_class</span>
            <span class="k">break</span>

        <span class="c1"># Process group wrapper initialization for supported PGs when TORCH_DISTRIBUTED_DEBUG is set</span>
        <span class="k">if</span> <span class="n">backend_str</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">,</span> <span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">,</span> <span class="n">Backend</span><span class="o">.</span><span class="n">UCC</span><span class="p">]:</span>
            <span class="c1"># In debug mode and if GLOO is available, wrap in a wrapper PG that</span>
            <span class="c1"># enables enhanced collective checking for debuggability.</span>
            <span class="k">if</span> <span class="n">get_debug_level</span><span class="p">()</span> <span class="o">==</span> <span class="n">DebugLevel</span><span class="o">.</span><span class="n">DETAIL</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">_GLOO_AVAILABLE</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sd">&quot;&quot;&quot;TORCH_DISTRIBUTED_DEBUG was set to DETAIL, but</span>
<span class="sd">                                GLOO is not available. Build with Gloo to</span>
<span class="sd">                                create a wrapper process group in debug mode</span>
<span class="sd">                                to aid collective desynchronization debugging.&quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">backend_class</span> <span class="o">=</span> <span class="n">_create_process_group_wrapper</span><span class="p">(</span>
                        <span class="n">wrapped_pg</span><span class="o">=</span><span class="n">backend_class</span><span class="p">,</span>
                        <span class="n">store_prefix</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span>
                        <span class="n">store</span><span class="o">=</span><span class="n">backend_prefix_store</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="n">group_rank</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="n">group_size</span><span class="p">,</span>
                        <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
                    <span class="p">)</span>

        <span class="c1"># register only a single backend when all get_device_backend_map values are the same</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">backend_config</span><span class="o">.</span><span class="n">get_device_backend_map</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">backend_config</span><span class="o">.</span><span class="n">get_device_backend_map</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">pg</span><span class="o">.</span><span class="n">_register_backend</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">backend_type</span><span class="p">,</span> <span class="n">backend_class</span><span class="p">)</span>

            <span class="c1"># break out of outer loop to not create any more backends</span>
            <span class="k">break</span>

        <span class="n">pg</span><span class="o">.</span><span class="n">_register_backend</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">backend_type</span><span class="p">,</span> <span class="n">backend_class</span><span class="p">)</span>

    <span class="c1"># update global state</span>
    <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">prefix_store</span><span class="p">)</span>
    <span class="n">_world</span><span class="o">.</span><span class="n">pg_names</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_name</span>
    <span class="n">_world</span><span class="o">.</span><span class="n">pg_backend_config</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">backend_config</span><span class="p">)</span>
    <span class="c1"># &quot;&quot; is the default tag for user PGs</span>
    <span class="k">if</span> <span class="n">pg_tag</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">]:</span>
        <span class="n">pg_tag</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;ptd:</span><span class="si">{</span><span class="n">group_name</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">tags_to_pg</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg_tag</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;user:</span><span class="si">{</span><span class="n">pg_tag</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="n">_world</span><span class="o">.</span><span class="n">tags_to_pg</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">pg_tag</span><span class="p">,</span> <span class="p">[])</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
    <span class="n">_world</span><span class="o">.</span><span class="n">pg_to_tag</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="n">pg_tag</span>
    <span class="k">return</span> <span class="n">pg</span><span class="p">,</span> <span class="n">prefix_store</span>

<span class="k">def</span> <span class="nf">destroy_process_group</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Destroy a given process group, and deinitialize the distributed package</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to be destroyed, if</span>
<span class="sd">                                        group.WORLD is given, all process</span>
<span class="sd">                                        groups including the default one will</span>
<span class="sd">                                        be destroyed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_world</span>

    <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">NON_GROUP_MEMBER</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">assert</span> <span class="n">pg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid process group specified&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="o">==</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">_update_default_pg</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_names</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_backend_config</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_to_tag</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">tags_to_pg</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="c1"># when process group doesn&#39;t have an explicit name (only WORLD (default)</span>
        <span class="c1"># process group can have an explicit name), we use global _world.group_count</span>
        <span class="c1"># to generate the name. We need to reset the counter on destruction to</span>
        <span class="c1"># allow consistent value to be generated when we re-create process</span>
        <span class="c1"># groups after some trainers recover from failure</span>
        <span class="c1">#</span>
        <span class="c1"># We only reset this when WORLD is being destroyed because if this</span>
        <span class="c1"># process group is in good state, we aren&#39;t dealing with failures.</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">group_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_names</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_backend_config</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">:</span>
            <span class="k">del</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_default_device</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Some coalesced collectives haven&#39;t been launched when &quot;</span>
                <span class="s2">&quot;ProcessGroup is destroyed. They will be cleaned.&quot;</span>
            <span class="p">)</span>
            <span class="k">del</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>

        <span class="n">tag</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_to_tag</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_to_tag</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">tag</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">_world</span><span class="o">.</span><span class="n">tags_to_pg</span><span class="p">[</span><span class="n">tag</span><span class="p">]</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;ptd:&quot;</span><span class="p">):</span>
                    <span class="n">_world</span><span class="o">.</span><span class="n">tags_to_pg</span><span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">pass</span>


<div class="viewcode-block" id="get_rank"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_rank">[docs]</a><span class="k">def</span> <span class="nf">get_rank</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the rank of the current process in the provided ``group`` or the</span>
<span class="sd">    default group if none was provided.</span>

<span class="sd">    Rank is a unique identifier assigned to each process within a distributed</span>
<span class="sd">    process group. They are always consecutive integers ranging from 0 to</span>
<span class="sd">    ``world_size``.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The rank of the process group</span>
<span class="sd">        -1, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>

    <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span></div>


<div class="viewcode-block" id="get_world_size"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.get_world_size">[docs]</a><span class="k">def</span> <span class="nf">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the number of processes in the current process group</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The world size of the process group</span>
<span class="sd">        -1, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">return</span> <span class="n">_get_group_size</span><span class="p">(</span><span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="isend"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.isend">[docs]</a><span class="k">def</span> <span class="nf">isend</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Work</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sends a tensor asynchronously.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Modifying ``tensor`` before the request completes causes undefined</span>
<span class="sd">        behavior.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        ``tag`` is not supported with the NCCL backend.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Tensor to send.</span>
<span class="sd">        dst (int): Destination rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match send with remote recv</span>

<span class="sd">    Returns:</span>
<span class="sd">        A distributed request object.</span>
<span class="sd">        None, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;isend&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">send</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">dst</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">group</span><span class="o">.</span><span class="n">send</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group_dst_rank</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span></div>


<div class="viewcode-block" id="irecv"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.irecv">[docs]</a><span class="k">def</span> <span class="nf">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Work</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Receives a tensor asynchronously.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        ``tag`` is not supported with the NCCL backend.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Tensor to fill with received data.</span>
<span class="sd">        src (int, optional): Source rank. Will receive from any</span>
<span class="sd">            process if unspecified.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match recv with remote send</span>

<span class="sd">    Returns:</span>
<span class="sd">        A distributed request object.</span>
<span class="sd">        None, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;irecv&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pg</span><span class="o">.</span><span class="n">recv_anysource</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">tag</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pg</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pg</span><span class="o">.</span><span class="n">recv</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">src</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pg</span><span class="o">.</span><span class="n">recv</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group_src_rank</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span></div>

<div class="viewcode-block" id="send"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.send">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">send</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sends a tensor synchronously.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Tensor to send.</span>
<span class="sd">        dst (int): Destination rank. Destination rank should not be the same</span>
<span class="sd">        as the rank of the current process.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match send with remote recv</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="n">dst</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid destination rank: destination rank should not be the same as &quot;</span>
            <span class="s2">&quot;the rank of the current process.&quot;</span>
        <span class="p">)</span>

    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;send&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">default_pg</span><span class="o">.</span><span class="n">send</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">dst</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="n">group</span><span class="o">.</span><span class="n">send</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group_dst_rank</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>

<div class="viewcode-block" id="recv"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.recv">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tag</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Receives a tensor synchronously.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Tensor to fill with received data.</span>
<span class="sd">        src (int, optional): Source rank. Will receive from any</span>
<span class="sd">            process if unspecified.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        tag (int, optional): Tag to match recv with remote send</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sender rank</span>
<span class="sd">        -1, if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;recv&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg</span> <span class="o">=</span> <span class="n">group</span>

    <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">pg</span><span class="o">.</span><span class="n">recv_anysource</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">tag</span><span class="p">)</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="n">src_rank</span> <span class="o">=</span> <span class="n">work</span><span class="o">.</span><span class="n">_source_rank</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">src_rank</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">get_global_rank</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">src_rank</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
            <span class="n">pg</span><span class="o">.</span><span class="n">recv</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">src</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">pg</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
            <span class="n">pg</span><span class="o">.</span><span class="n">recv</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group_src_rank</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">src</span></div>


<span class="k">class</span> <span class="nc">_IllegalWork</span><span class="p">(</span><span class="n">Work</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;is_success&quot;</span><span class="p">,</span> <span class="s2">&quot;exception&quot;</span><span class="p">,</span> <span class="s2">&quot;wait&quot;</span><span class="p">,</span> <span class="s2">&quot;source_rank&quot;</span><span class="p">,</span> <span class="s2">&quot;_source_rank&quot;</span><span class="p">,</span> <span class="s2">&quot;result&quot;</span><span class="p">,</span> <span class="s2">&quot;synchronize&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Illegal to call </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> on IllegalWork object&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_CoalescingManager</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">works</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Work</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">work</span><span class="p">:</span> <span class="n">Work</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">work</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">works</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">work</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wait</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">work</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">works</span><span class="p">:</span>
            <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_coalescing_manager</span><span class="p">(</span>
    <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">async_ops</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A context manager used to coalesce collectives or P2P operations when possible.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (`ProcessGroup`, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        device (`torch.device`, optional): Default is None, set to a device if</span>
<span class="sd">            there isn&#39;t a `**_coalesced` implementation by the backend.</span>
<span class="sd">        async_ops (`bool`, optional): whether the coalesced ops are async ops.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;no rank&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Synchronous ops</span>
<span class="sd">        &gt;&gt;&gt; with _coalescing_manager():</span>
<span class="sd">        &gt;&gt;&gt;     for i in range(num_colls):</span>
<span class="sd">        &gt;&gt;&gt;         dist.all_reduce(tensors[i])</span>
<span class="sd">        &gt;&gt;&gt; # Asynchronous ops</span>
<span class="sd">        &gt;&gt;&gt; with _coalescing_manager(async_ops=True) as cm:</span>
<span class="sd">        &gt;&gt;&gt;     for i in range(num_colls):</span>
<span class="sd">        &gt;&gt;&gt;         dist.all_reduce(tensors[i])</span>
<span class="sd">        &gt;&gt;&gt; cm.wait()</span>

<span class="sd">    .. warning::</span>
<span class="sd">       :func:`_coalescing_manager` currently do not support coalescing</span>
<span class="sd">       all-reduces with different reduce operators, e.g.  `ReduceOp.SUM` mixed</span>
<span class="sd">       with `ReduceOp.PRODUCT`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">group</span> <span class="ow">or</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="n">op_list</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="p">[])</span>
    <span class="k">if</span> <span class="n">op_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;ProcessGroup has non-empty op list at the start of coalescing&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device</span><span class="p">:</span>
        <span class="n">group</span><span class="o">.</span><span class="n">_start_coalescing</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">cm</span> <span class="o">=</span> <span class="n">_CoalescingManager</span><span class="p">()</span>
    <span class="k">yield</span> <span class="n">cm</span>
    <span class="n">op_list</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">op_list</span><span class="p">:</span>
        <span class="c1"># Collectives supporting &quot;Fast Path&quot; coalescing are captured.</span>
        <span class="c1"># See implementation in corresponding collective APIs.</span>
        <span class="c1"># Currently supported:</span>
        <span class="c1"># - coalesced `all_reduce`</span>
        <span class="c1"># - coalesced `all_gather_into_tensor`</span>
        <span class="c1"># - coalesced `reduce_scatter_tensor`</span>
        <span class="n">op0</span> <span class="o">=</span> <span class="n">op_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">op</span>
        <span class="k">if</span> <span class="n">op0</span> <span class="o">==</span> <span class="n">all_reduce</span><span class="p">:</span>
            <span class="n">tensors</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">op_list</span><span class="p">:</span>
                <span class="n">tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">opts</span> <span class="o">=</span> <span class="n">AllreduceCoalescedOptions</span><span class="p">()</span>
            <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">redop</span>
            <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allreduce_coalesced</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">op0</span> <span class="o">==</span> <span class="n">all_gather_into_tensor</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">op_list</span><span class="p">:</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
                <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">dst_tensor</span><span class="p">)</span>
            <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allgather_into_tensor_coalesced</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">op0</span> <span class="o">==</span> <span class="n">reduce_scatter_tensor</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">op_list</span><span class="p">:</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
                <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">dst_tensor</span><span class="p">)</span>
                <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceScatterOptions</span><span class="p">()</span>
                <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">redop</span>
            <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce_scatter_tensor_coalesced</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Coalescing manager does not support fast-path coalescing of </span><span class="si">{</span><span class="n">op0</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;yet </span><span class="si">{</span><span class="n">op0</span><span class="si">}</span><span class="s2"> is still recorded in op list. This is an internal error of c10d.&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">device</span><span class="p">:</span>
        <span class="c1"># Old style of letting each coll inside the context manager to call into C++ counterpart via python binding</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">_end_coalescing</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_ops</span><span class="p">:</span>
        <span class="n">cm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">work</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<div class="viewcode-block" id="batch_isend_irecv"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.batch_isend_irecv">[docs]</a><span class="k">def</span> <span class="nf">batch_isend_irecv</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Send or Receive a batch of tensors asynchronously and return a list of requests.</span>

<span class="sd">    Process each of the operations in ``p2p_op_list`` and return the corresponding</span>
<span class="sd">    requests. NCCL, Gloo, and UCC backend are currently supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        p2p_op_list: A list of point-to-point operations(type of each operator is</span>
<span class="sd">            ``torch.distributed.P2POp``). The order of the isend/irecv in the list</span>
<span class="sd">            matters and it needs to match with corresponding isend/irecv on the</span>
<span class="sd">            remote end.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of distributed request objects returned by calling the corresponding</span>
<span class="sd">        op in the op_list.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;no rank&quot;)</span>
<span class="sd">        &gt;&gt;&gt; send_tensor = torch.arange(2) + 2 * rank</span>
<span class="sd">        &gt;&gt;&gt; recv_tensor = torch.randn(2)</span>
<span class="sd">        &gt;&gt;&gt; send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size)</span>
<span class="sd">        &gt;&gt;&gt; recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size)%world_size)</span>
<span class="sd">        &gt;&gt;&gt; reqs = batch_isend_irecv([send_op, recv_op])</span>
<span class="sd">        &gt;&gt;&gt; for req in reqs:</span>
<span class="sd">        &gt;&gt;&gt;     req.wait()</span>
<span class="sd">        &gt;&gt;&gt; recv_tensor</span>
<span class="sd">        tensor([2, 3])     # Rank 0</span>
<span class="sd">        tensor([0, 1])     # Rank 1</span>

<span class="sd">    .. note:: Note that when this API is used with the NCCL PG backend, users must set</span>
<span class="sd">        the current GPU device with `torch.cuda.set_device`, otherwise it will</span>
<span class="sd">        lead to unexpected hang issues.</span>

<span class="sd">        In addition, if this API is the first collective call in the ``group``</span>
<span class="sd">        passed to ``dist.P2POp``, all ranks of the ``group`` must participate in</span>
<span class="sd">        this API call; otherwise, the behavior is undefined. If this API call is</span>
<span class="sd">        not the first collective call in the ``group``, batched P2P operations</span>
<span class="sd">        involving only a subset of ranks of the ``group`` are allowed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_p2p_op_list</span><span class="p">(</span><span class="n">p2p_op_list</span><span class="p">)</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">p2p_op_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">group</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">p2p_op_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">device</span>
    <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="c1"># NCCL style coalescing</span>
        <span class="k">with</span> <span class="n">_coalescing_manager</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">async_ops</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">cm</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p2p_op</span> <span class="ow">in</span> <span class="n">p2p_op_list</span><span class="p">:</span>
                <span class="n">p2p_op</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">p2p_op</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">peer</span><span class="p">,</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">tag</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cm</span><span class="o">.</span><span class="n">works</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Backward support for Gloo</span>
        <span class="n">reqs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">p2p_op</span> <span class="ow">in</span> <span class="n">p2p_op_list</span><span class="p">:</span>
            <span class="n">work</span> <span class="o">=</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">p2p_op</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">peer</span><span class="p">,</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="n">p2p_op</span><span class="o">.</span><span class="n">tag</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">work</span><span class="p">:</span>
                <span class="n">reqs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">work</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reqs</span></div>


<div class="viewcode-block" id="broadcast_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.broadcast_multigpu">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">broadcast_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">src_tensor</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts the tensor to the whole group with multiple GPU tensors</span>
<span class="sd">    per node.</span>

<span class="sd">    ``tensor`` must have the same number of elements in all the GPUs from</span>
<span class="sd">    all processes participating in the collective. each tensor in the list must</span>
<span class="sd">    be on a different GPU</span>

<span class="sd">    Only nccl and gloo backend are currently supported</span>
<span class="sd">    tensors should only be GPU tensors</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_list (List[Tensor]): Tensors that participate in the collective</span>
<span class="sd">            operation. If ``src`` is the rank, then the specified ``src_tensor``</span>
<span class="sd">            element of ``tensor_list`` (``tensor_list[src_tensor]``) will be</span>
<span class="sd">            broadcast to all other tensors (on different GPUs) in the src process</span>
<span class="sd">            and all tensors in ``tensor_list`` of other non-src processes.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same</span>
<span class="sd">            for all the distributed processes calling this function.</span>

<span class="sd">        src (int): Source rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>
<span class="sd">        src_tensor (int, optional): Source tensor rank within ``tensor_list``</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed.broadcast_multigpu will be deprecated. If you must &quot;</span>
        <span class="s2">&quot;use it, please revisit our documentation later at &quot;</span>
        <span class="s2">&quot;https://pytorch.org/docs/master/distributed.html#multi-gpu-collective-functions&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;broadcast_multigpu&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">BroadcastOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">src</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootTensor</span> <span class="o">=</span> <span class="n">src_tensor</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_src_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="broadcast"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.broadcast">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts the tensor to the whole group.</span>

<span class="sd">    ``tensor`` must have the same number of elements in all processes</span>
<span class="sd">    participating in the collective.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Data to be sent if ``src`` is the rank of current</span>
<span class="sd">            process, and tensor to be used to save received data otherwise.</span>
<span class="sd">        src (int): Source rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;broadcast&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">BroadcastOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">src</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootTensor</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_src_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>

<div class="viewcode-block" id="all_reduce_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_reduce_multigpu">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces the tensor data across all machines in such a way that all get</span>
<span class="sd">    the final result. This function reduces a number of tensors on every node,</span>
<span class="sd">    while each tensor resides on different GPUs.</span>
<span class="sd">    Therefore, the input tensor in the tensor list needs to be GPU tensors.</span>
<span class="sd">    Also, each tensor in the tensor list needs to reside on a different GPU.</span>

<span class="sd">    After the call, all ``tensor`` in ``tensor_list`` is going to be bitwise</span>
<span class="sd">    identical in all processes.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Only nccl and gloo backend is currently supported</span>
<span class="sd">    tensors should only be GPU tensors</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_list (List[Tensor]): List of input and output tensors of</span>
<span class="sd">            the collective. The function operates in-place and requires that</span>
<span class="sd">            each tensor to be a GPU tensor on different GPUs.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If</span>
<span class="sd">            ``None``, the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed.all_reduce_multigpu will be deprecated. If you must &quot;</span>
        <span class="s2">&quot;use it, please revisit our documentation later at &quot;</span>
        <span class="s2">&quot;https://pytorch.org/docs/master/distributed.html#multi-gpu-collective-functions&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span>
    <span class="p">]</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllreduceOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>

<div class="viewcode-block" id="all_reduce"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_reduce">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces the tensor data across all machines in such a way that all get</span>
<span class="sd">    the final result.</span>

<span class="sd">    After the call ``tensor`` is going to be bitwise identical in all processes.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;no rank&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.int64 type.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1, 2]) # Rank 0</span>
<span class="sd">        tensor([3, 4]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([4, 6]) # Rank 0</span>
<span class="sd">        tensor([4, 6]) # Rank 1</span>

<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.cfloat type.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1.+1.j, 2.+2.j]) # Rank 0</span>
<span class="sd">        tensor([3.+3.j, 4.+4.j]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([4.+4.j, 6.+6.j]) # Rank 0</span>
<span class="sd">        tensor([4.+4.j, 6.+6.j]) # Rank 1</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_reduce&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">supports_complex</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;all_reduce does not support </span><span class="si">{</span><span class="n">op</span><span class="si">}</span><span class="s2"> on complex tensors&quot;</span><span class="p">)</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllreduceOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="c1"># We are in coalescing context, do not issue single operation, just append a collective representation</span>
        <span class="n">coll</span> <span class="o">=</span> <span class="n">_CollOp</span><span class="p">(</span><span class="n">all_reduce</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="p">[</span><span class="n">group</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">coll</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_IllegalWork</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allreduce</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>

<span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_reduce_coalesced</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    WARNING: at this time individual shape checking is not implemented across nodes.</span>
<span class="sd">    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the</span>
<span class="sd">    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the allreduce</span>
<span class="sd">    operation will proceed without complaint and return erroneous outputs. This lack</span>
<span class="sd">    of shape checking results in significant performance improvements but users of this</span>
<span class="sd">    function should take extra care to ensure that each node passes in tensors whose</span>
<span class="sd">    shapes match across nodes.</span>

<span class="sd">    Reduces each tensor in tensors (residing on the same device) across all machines</span>
<span class="sd">    in such a way that all get the final result.</span>

<span class="sd">    After the call each tensor in tensors is going to bitwise identical</span>
<span class="sd">    in all processes.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (List[Tensor]): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        op (Optional[ReduceOp]): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp`` enum. Specifies an operation used for</span>
<span class="sd">            element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (Optional[bool]): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed.all_reduce_coalesced will be deprecated. If you must &quot;</span>
        <span class="s2">&quot;use it, please revisit our documentation later at &quot;</span>
        <span class="s2">&quot;https://pytorch.org/docs/master/distributed.html#collective-functions&quot;</span>
    <span class="p">)</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_reduce_coalesced&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">supports_complex</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;all_reduce does not support </span><span class="si">{</span><span class="n">op</span><span class="si">}</span><span class="s2"> on complex tensors&quot;</span><span class="p">)</span>

    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllreduceCoalescedOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allreduce_coalesced</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allreduce_coalesced</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span><span class="o">.</span><span class="n">get_future</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

<div class="viewcode-block" id="reduce_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce_multigpu">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">reduce_multigpu</span><span class="p">(</span>
    <span class="n">tensor_list</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dst_tensor</span><span class="o">=</span><span class="mi">0</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces the tensor data on multiple GPUs across all machines. Each tensor</span>
<span class="sd">    in ``tensor_list`` should reside on a separate GPU</span>

<span class="sd">    Only the GPU of ``tensor_list[dst_tensor]`` on the process with rank ``dst``</span>
<span class="sd">    is going to receive the final result.</span>

<span class="sd">    Only nccl backend is currently supported</span>
<span class="sd">    tensors should only be GPU tensors</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_list (List[Tensor]): Input and output GPU tensors of the</span>
<span class="sd">            collective. The function operates in-place.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>
<span class="sd">        dst (int): Destination rank</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>
<span class="sd">        dst_tensor (int, optional): Destination tensor rank within</span>
<span class="sd">                                    ``tensor_list``</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, otherwise</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed.reduce_multigpu will be deprecated. If you must &quot;</span>
        <span class="s2">&quot;use it, please revisit our documentation later at &quot;</span>
        <span class="s2">&quot;https://pytorch.org/docs/master/distributed.html#multi-gpu-collective-functions&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;reduce_multigpu&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">dst</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootTensor</span> <span class="o">=</span> <span class="n">dst_tensor</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_dst_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>

<div class="viewcode-block" id="reduce"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces the tensor data across all machines.</span>

<span class="sd">    Only the process with rank ``dst`` is going to receive the final result.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        dst (int): Destination rank</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;reduce&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">dst</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">reduce</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_dst_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>

<div class="viewcode-block" id="all_gather_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_gather_multigpu">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_gather_multigpu</span><span class="p">(</span>
    <span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers tensors from the whole group in a list.</span>
<span class="sd">    Each tensor in ``tensor_list`` should reside on a separate GPU</span>

<span class="sd">    Only nccl backend is currently supported</span>
<span class="sd">    tensors should only be GPU tensors</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor_lists (List[List[Tensor]]): Output lists. It should</span>
<span class="sd">            contain correctly-sized tensors on each GPU to be used for output</span>
<span class="sd">            of the collective, e.g. ``output_tensor_lists[i]`` contains the</span>
<span class="sd">            all_gather result that resides on the GPU of</span>
<span class="sd">            ``input_tensor_list[i]``.</span>

<span class="sd">            Note that each element of ``output_tensor_lists`` has the size of</span>
<span class="sd">            ``world_size * len(input_tensor_list)``, since the function all</span>
<span class="sd">            gathers the result from every single GPU in the group. To interpret</span>
<span class="sd">            each element of ``output_tensor_lists[i]``, note that</span>
<span class="sd">            ``input_tensor_list[j]`` of rank k will be appear in</span>
<span class="sd">            ``output_tensor_lists[i][k * world_size + j]``</span>

<span class="sd">            Also note that ``len(output_tensor_lists)``, and the size of each</span>
<span class="sd">            element in ``output_tensor_lists`` (each element is a list,</span>
<span class="sd">            therefore ``len(output_tensor_lists[i])``) need to be the same</span>
<span class="sd">            for all the distributed processes calling this function.</span>

<span class="sd">        input_tensor_list (List[Tensor]): List of tensors(on different GPUs) to</span>
<span class="sd">            be broadcast from current process.</span>
<span class="sd">            Note that ``len(input_tensor_list)`` needs to be the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>

<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed.all_gather_multigpu will be deprecated. If you must &quot;</span>
        <span class="s2">&quot;use it, please revisit our documentation later at &quot;</span>
        <span class="s2">&quot;https://pytorch.org/docs/master/distributed.html#multi-gpu-collective-functions&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_gather_multigpu&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">output_tensor_lists</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">output_tensor_lists</span>
    <span class="p">]</span>
    <span class="n">input_tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_tensor_list</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allgather</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allgather</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
    <span class="n">_pickler</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="n">byte_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteStorage</span><span class="o">.</span><span class="n">_from_buffer</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="c1"># Do not replace `torch.ByteTensor` or `torch.LongTensor` with torch.tensor and specifying dtype.</span>
    <span class="c1"># Otherwise, it will casue 100X slowdown.</span>
    <span class="c1"># See: https://github.com/pytorch/pytorch/issues/65696</span>
    <span class="n">byte_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">(</span><span class="n">byte_storage</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">local_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">byte_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">byte_tensor</span><span class="p">,</span> <span class="n">local_size</span>


<span class="k">def</span> <span class="nf">_tensor_to_object</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor_size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">buf</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tobytes</span><span class="p">()[:</span><span class="n">tensor_size</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">_unpickler</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">buf</span><span class="p">))</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>


<div class="viewcode-block" id="all_gather_object"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_gather_object">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_gather_object</span><span class="p">(</span><span class="n">object_list</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers picklable objects from the whole group into a list. Similar to</span>
<span class="sd">    :func:`all_gather`, but Python objects can be passed in. Note that the object</span>
<span class="sd">    must be picklable in order to be gathered.</span>

<span class="sd">    Args:</span>
<span class="sd">        object_list (list[Any]): Output list. It should be correctly sized as the</span>
<span class="sd">            size of the group for this collective and will contain the output.</span>
<span class="sd">        obj (Any): Pickable Python object to be broadcast from current process.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used. Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None. If the calling rank is part of this group, the output of the</span>
<span class="sd">        collective will be populated into the input ``object_list``. If the</span>
<span class="sd">        calling rank is not part of the group, the passed in ``object_list`` will</span>
<span class="sd">        be unmodified.</span>

<span class="sd">    .. note:: Note that this API differs slightly from the :func:`all_gather`</span>
<span class="sd">        collective since it does not provide an ``async_op`` handle and thus</span>
<span class="sd">        will be a blocking call.</span>

<span class="sd">    .. note:: For NCCL-based processed groups, internal tensor representations</span>
<span class="sd">        of objects must be moved to the GPU device before communication takes</span>
<span class="sd">        place. In this case, the device used is given by</span>
<span class="sd">        ``torch.cuda.current_device()`` and it is the user&#39;s responsiblity to</span>
<span class="sd">        ensure that this is set so that each rank has an individual GPU, via</span>
<span class="sd">        ``torch.cuda.set_device()``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`all_gather_object` uses ``pickle`` module implicitly, which is</span>
<span class="sd">        known to be insecure. It is possible to construct malicious pickle data</span>
<span class="sd">        which will execute arbitrary code during unpickling. Only call this</span>
<span class="sd">        function with data you trust.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Calling :func:`all_gather_object` with GPU tensors is not well supported</span>
<span class="sd">        and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be</span>
<span class="sd">        pickled. Please consider using :func:`all_gather` instead.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; # Assumes world_size of 3.</span>
<span class="sd">        &gt;&gt;&gt; gather_objects = [&quot;foo&quot;, 12, {1: 2}] # any picklable object</span>
<span class="sd">        &gt;&gt;&gt; output = [None for _ in gather_objects]</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather_object(output, gather_objects[dist.get_rank()])</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [&#39;foo&#39;, 12, {1: 2}]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_gather_object&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">current_device</span> <span class="o">=</span> <span class="n">_get_pg_default_device</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="n">input_tensor</span><span class="p">,</span> <span class="n">local_size</span> <span class="o">=</span> <span class="n">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">current_device</span><span class="p">)</span>

    <span class="c1"># Gather all local sizes. This is so that we can find the max size, and index</span>
    <span class="c1"># until the correct size when deserializing the tensors.</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="n">group_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
    <span class="p">)</span>
    <span class="n">object_size_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">object_sizes_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="c1"># Allgather tensor sizes</span>
    <span class="n">all_gather</span><span class="p">(</span><span class="n">object_size_list</span><span class="p">,</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="n">max_object_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">object_size_list</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># type: ignore[type-var]</span>
    <span class="c1"># Resize tensor to max size across all ranks.</span>
    <span class="n">input_tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_object_size</span><span class="p">)</span>
    <span class="n">coalesced_output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
        <span class="n">max_object_size</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
    <span class="p">)</span>
    <span class="c1"># Output tensors are nonoverlapping views of coalesced_output_tensor</span>
    <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">coalesced_output_tensor</span><span class="p">[</span><span class="n">max_object_size</span> <span class="o">*</span> <span class="n">i</span> <span class="p">:</span> <span class="n">max_object_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">all_gather</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="c1"># Deserialize outputs back to object.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">):</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="n">tensor_size</span> <span class="o">=</span> <span class="n">object_size_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">object_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">_tensor_to_object</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_object"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.gather_object">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">gather_object</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">object_gather_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers picklable objects from the whole group in a single process.</span>
<span class="sd">    Similar to :func:`gather`, but Python objects can be passed in. Note that the</span>
<span class="sd">    object must be picklable in order to be gathered.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Any): Input object. Must be picklable.</span>
<span class="sd">        object_gather_list (list[Any]): Output list. On the ``dst`` rank, it</span>
<span class="sd">            should be correctly sized as the size of the group for this</span>
<span class="sd">            collective and will contain the output. Must be ``None`` on non-dst</span>
<span class="sd">            ranks. (default is ``None``)</span>
<span class="sd">        dst (int, optional): Destination rank. (default is 0)</span>
<span class="sd">        group: (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used. Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None. On the ``dst`` rank, ``object_gather_list`` will contain the</span>
<span class="sd">        output of the collective.</span>

<span class="sd">    .. note:: Note that this API differs slightly from the gather collective</span>
<span class="sd">        since it does not provide an async_op handle and thus will be a blocking</span>
<span class="sd">        call.</span>

<span class="sd">    .. note:: For NCCL-based processed groups, internal tensor representations</span>
<span class="sd">        of objects must be moved to the GPU device before communication takes</span>
<span class="sd">        place. In this case, the device used is given by</span>
<span class="sd">        ``torch.cuda.current_device()`` and it is the user&#39;s responsiblity to</span>
<span class="sd">        ensure that this is set so that each rank has an individual GPU, via</span>
<span class="sd">        ``torch.cuda.set_device()``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`gather_object` uses ``pickle`` module implicitly, which is</span>
<span class="sd">        known to be insecure. It is possible to construct malicious pickle data</span>
<span class="sd">        which will execute arbitrary code during unpickling. Only call this</span>
<span class="sd">        function with data you trust.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Calling :func:`gather_object` with GPU tensors is not well supported</span>
<span class="sd">        and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be</span>
<span class="sd">        pickled. Please consider using :func:`gather` instead.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; # Assumes world_size of 3.</span>
<span class="sd">        &gt;&gt;&gt; gather_objects = [&quot;foo&quot;, 12, {1: 2}] # any picklable object</span>
<span class="sd">        &gt;&gt;&gt; output = [None for _ in gather_objects]</span>
<span class="sd">        &gt;&gt;&gt; dist.gather_object(</span>
<span class="sd">        ...     gather_objects[dist.get_rank()],</span>
<span class="sd">        ...     output if dist.get_rank() == 0 else None,</span>
<span class="sd">        ...     dst=0</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; # On rank 0</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [&#39;foo&#39;, 12, {1: 2}]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;gather_object&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Ensure object_gather_list is specified appropriately.</span>
    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">_validate_output_list_for_rank</span><span class="p">(</span><span class="n">my_rank</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">object_gather_list</span><span class="p">)</span>
    <span class="n">current_device</span> <span class="o">=</span> <span class="n">_get_pg_default_device</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="n">input_tensor</span><span class="p">,</span> <span class="n">local_size</span> <span class="o">=</span> <span class="n">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">current_device</span><span class="p">)</span>

    <span class="c1"># Gather all local sizes. This is so that we can find the max size, and index</span>
    <span class="c1"># until the correct size when deserializing the tensors.</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="n">group_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
    <span class="p">)</span>
    <span class="n">object_size_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">object_sizes_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="c1"># Allgather tensor sizes. An all-gather is needed here despite this being a</span>
    <span class="c1"># gather, since each rank needs to broadcast a tensor of the same (maximal)</span>
    <span class="c1"># size.</span>
    <span class="n">all_gather</span><span class="p">(</span><span class="n">object_size_list</span><span class="p">,</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="n">max_object_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">object_size_list</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># type: ignore[type-var]</span>
    <span class="c1"># Resize tensor to max size across all ranks.</span>
    <span class="n">input_tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_object_size</span><span class="p">)</span>
    <span class="c1"># Avoid populating output tensors if the result won&#39;t be gathered on this rank.</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">dst</span><span class="p">:</span>
        <span class="n">coalesced_output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="n">max_object_size</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
        <span class="p">)</span>
        <span class="c1"># Output tensors are nonoverlapping views of coalesced_output_tensor</span>
        <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">coalesced_output_tensor</span><span class="p">[</span><span class="n">max_object_size</span> <span class="o">*</span> <span class="n">i</span> <span class="p">:</span> <span class="n">max_object_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="c1"># All ranks call gather with equal-sized tensors.</span>
    <span class="n">gather</span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">,</span>
        <span class="n">gather_list</span><span class="o">=</span><span class="n">output_tensors</span> <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">dst</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dst</span><span class="o">=</span><span class="n">dst</span><span class="p">,</span>
        <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="n">dst</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">):</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
        <span class="n">tensor_size</span> <span class="o">=</span> <span class="n">object_size_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">object_gather_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">_tensor_to_object</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_object_list"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.broadcast_object_list">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">broadcast_object_list</span><span class="p">(</span><span class="n">object_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts picklable objects in ``object_list`` to the whole group. Similar</span>
<span class="sd">    to :func:`broadcast`, but Python objects can be passed in.</span>
<span class="sd">    Note that all objects in ``object_list`` must be picklable in order to be</span>
<span class="sd">    broadcasted.</span>

<span class="sd">    Args:</span>
<span class="sd">        object_list (List[Any]): List of input objects to broadcast.</span>
<span class="sd">            Each object must be picklable. Only objects on the ``src`` rank will</span>
<span class="sd">            be broadcast, but each rank must provide lists of equal sizes.</span>
<span class="sd">        src (int): Source rank from which to broadcast ``object_list``.</span>
<span class="sd">        group: (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used. Default is ``None``.</span>
<span class="sd">        device (``torch.device``, optional): If not None, the objects are</span>
<span class="sd">            serialized and converted to tensors which are moved to the</span>
<span class="sd">            ``device`` before broadcasting. Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ``None``. If rank is part of the group, ``object_list`` will contain the</span>
<span class="sd">        broadcasted objects from ``src`` rank.</span>

<span class="sd">    .. note:: For NCCL-based process groups, internal tensor representations</span>
<span class="sd">        of objects must be moved to the GPU device before communication takes</span>
<span class="sd">        place. In this case, the device used is given by</span>
<span class="sd">        ``torch.cuda.current_device()`` and it is the user&#39;s responsibility to</span>
<span class="sd">        ensure that this is set so that each rank has an individual GPU, via</span>
<span class="sd">        ``torch.cuda.set_device()``.</span>

<span class="sd">    .. note:: Note that this API differs slightly from the :func:`all_gather`</span>
<span class="sd">        collective since it does not provide an ``async_op`` handle and thus</span>
<span class="sd">        will be a blocking call.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`broadcast_object_list` uses ``pickle`` module implicitly, which</span>
<span class="sd">        is known to be insecure. It is possible to construct malicious pickle</span>
<span class="sd">        data which will execute arbitrary code during unpickling. Only call this</span>
<span class="sd">        function with data you trust.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Calling :func:`broadcast_object_list` with GPU tensors is not well supported</span>
<span class="sd">        and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be</span>
<span class="sd">        pickled. Please consider using :func:`broadcast` instead.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     # Assumes world_size of 3.</span>
<span class="sd">        &gt;&gt;&gt;     objects = [&quot;foo&quot;, 12, {1: 2}] # any picklable object</span>
<span class="sd">        &gt;&gt;&gt; else:</span>
<span class="sd">        &gt;&gt;&gt;     objects = [None, None, None]</span>
<span class="sd">        &gt;&gt;&gt; # Assumes backend is not NCCL</span>
<span class="sd">        &gt;&gt;&gt; device = torch.device(&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dist.broadcast_object_list(objects, src=0, device=device)</span>
<span class="sd">        &gt;&gt;&gt; objects</span>
<span class="sd">        [&#39;foo&#39;, 12, {1: 2}]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;broadcast_object_list&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Current device selection.</span>
    <span class="c1"># To preserve backwards compatibility, ``device`` is default to ``None``</span>
    <span class="c1"># in which case we run current logic of device selection, i.e.</span>
    <span class="c1"># ``current_device`` is CUDA if backend is NCCL otherwise CPU device. In the</span>
    <span class="c1"># case it is not ``None`` we move the size and object tensors to be</span>
    <span class="c1"># broadcasted to this device.</span>
    <span class="n">current_device</span> <span class="o">=</span> <span class="n">device</span> <span class="ow">or</span> <span class="n">_get_pg_default_device</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="c1"># Serialize object_list elements to tensors on src rank.</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">tensor_list</span><span class="p">,</span> <span class="n">size_list</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">current_device</span><span class="p">)</span> <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">object_list</span><span class="p">])</span>
        <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">size_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">object_sizes_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">object_list</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">)</span>

    <span class="c1"># Broadcast object sizes</span>
    <span class="n">broadcast</span><span class="p">(</span><span class="n">object_sizes_tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>

    <span class="c1"># Concatenate and broadcast serialized object tensors</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">object_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">object_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>  <span class="c1"># type: ignore[call-overload]</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">object_sizes_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">current_device</span>
        <span class="p">)</span>

    <span class="n">broadcast</span><span class="p">(</span><span class="n">object_tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="c1"># Deserialize objects using their stored sizes.</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="n">src</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">obj_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">object_sizes_tensor</span><span class="p">):</span>
            <span class="n">obj_view</span> <span class="o">=</span> <span class="n">object_tensor</span><span class="p">[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">obj_size</span><span class="p">]</span>
            <span class="n">obj_view</span> <span class="o">=</span> <span class="n">obj_view</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">obj_view</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
                <span class="n">obj_view</span> <span class="o">=</span> <span class="n">obj_view</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="n">offset</span> <span class="o">+=</span> <span class="n">obj_size</span>
            <span class="n">object_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">_tensor_to_object</span><span class="p">(</span><span class="n">obj_view</span><span class="p">,</span> <span class="n">obj_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_object_list"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.scatter_object_list">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">scatter_object_list</span><span class="p">(</span>
    <span class="n">scatter_object_output_list</span><span class="p">,</span> <span class="n">scatter_object_input_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters picklable objects in ``scatter_object_input_list`` to the whole</span>
<span class="sd">    group. Similar to :func:`scatter`, but Python objects can be passed in. On</span>
<span class="sd">    each rank, the scattered object will be stored as the first element of</span>
<span class="sd">    ``scatter_object_output_list``. Note that all objects in</span>
<span class="sd">    ``scatter_object_input_list`` must be picklable in order to be scattered.</span>

<span class="sd">    Args:</span>
<span class="sd">        scatter_object_output_list (List[Any]): Non-empty list whose first</span>
<span class="sd">            element will store the object scattered to this rank.</span>
<span class="sd">        scatter_object_input_list (List[Any]): List of input objects to scatter.</span>
<span class="sd">            Each object must be picklable. Only objects on the ``src`` rank will</span>
<span class="sd">            be scattered, and the argument can be ``None`` for non-src ranks.</span>
<span class="sd">        src (int): Source rank from which to scatter</span>
<span class="sd">            ``scatter_object_input_list``.</span>
<span class="sd">        group: (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used. Default is ``None``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ``None``. If rank is part of the group, ``scatter_object_output_list``</span>
<span class="sd">        will have its first element set to the scattered object for this rank.</span>

<span class="sd">    .. note:: Note that this API differs slightly from the scatter collective</span>
<span class="sd">        since it does not provide an ``async_op`` handle and thus will be a</span>
<span class="sd">        blocking call.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :func:`scatter_object_list` uses ``pickle`` module implicitly, which</span>
<span class="sd">        is known to be insecure. It is possible to construct malicious pickle</span>
<span class="sd">        data which will execute arbitrary code during unpickling. Only call this</span>
<span class="sd">        function with data you trust.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Calling :func:`scatter_object_list` with GPU tensors is not well supported</span>
<span class="sd">        and inefficient as it incurs GPU -&gt; CPU transfer since tensors would be</span>
<span class="sd">        pickled. Please consider using :func:`scatter` instead.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     # Assumes world_size of 3.</span>
<span class="sd">        &gt;&gt;&gt;     objects = [&quot;foo&quot;, 12, {1: 2}] # any picklable object</span>
<span class="sd">        &gt;&gt;&gt; else:</span>
<span class="sd">        &gt;&gt;&gt;     # Can be any list on non-src ranks, elements are not used.</span>
<span class="sd">        &gt;&gt;&gt;     objects = [None, None, None]</span>
<span class="sd">        &gt;&gt;&gt; output_list = [None]</span>
<span class="sd">        &gt;&gt;&gt; dist.scatter_object_list(output_list, objects, src=0)</span>
<span class="sd">        &gt;&gt;&gt; # Rank i gets objects[i]. For example, on rank 2:</span>
<span class="sd">        &gt;&gt;&gt; output_list</span>
<span class="sd">        [{1: 2}]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;scatter_object_list&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scatter_object_output_list</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
        <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">scatter_object_output_list</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Expected argument scatter_object_output_list to be a list of size at least 1.&quot;</span>
        <span class="p">)</span>

    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">pg_device</span> <span class="o">=</span> <span class="n">_get_pg_default_device</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor_sizes</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">_object_to_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">pg_device</span><span class="p">)</span> <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">scatter_object_input_list</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">)</span>

    <span class="c1"># Src rank broadcasts the maximum tensor size. This is because all ranks are</span>
    <span class="c1"># expected to call into scatter() with equal-sized tensors.</span>
    <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
        <span class="n">max_tensor_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">tensor_sizes</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensor_list</span><span class="p">:</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">max_tensor_size</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">max_tensor_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pg_device</span><span class="p">)</span>
    <span class="n">broadcast</span><span class="p">(</span><span class="n">max_tensor_size</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>

    <span class="c1"># Scatter actual serialized objects</span>
    <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">max_tensor_size</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pg_device</span><span class="p">)</span>
    <span class="n">scatter</span><span class="p">(</span>
        <span class="n">output_tensor</span><span class="p">,</span>
        <span class="n">scatter_list</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="n">src</span> <span class="k">else</span> <span class="n">tensor_list</span><span class="p">,</span>
        <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span>
        <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Scatter per-object sizes to trim tensors when deserializing back to object</span>
    <span class="n">obj_tensor_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pg_device</span><span class="p">)</span>
    <span class="n">scatter</span><span class="p">(</span>
        <span class="n">obj_tensor_size</span><span class="p">,</span>
        <span class="n">scatter_list</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">my_rank</span> <span class="o">!=</span> <span class="n">src</span> <span class="k">else</span> <span class="n">tensor_sizes</span><span class="p">,</span>
        <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span>
        <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Deserialize back to object</span>
    <span class="n">scatter_object_output_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">_tensor_to_object</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">obj_tensor_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="all_gather"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_gather">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers tensors from the whole group in a list.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor_list (list[Tensor]): Output list. It should contain</span>
<span class="sd">            correctly-sized tensors to be used for output of the collective.</span>
<span class="sd">        tensor (Tensor): Tensor to be broadcast from current process.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.int64 dtype.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]</span>
<span class="sd">        &gt;&gt;&gt; tensor_list</span>
<span class="sd">        [tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1, 2]) # Rank 0</span>
<span class="sd">        tensor([3, 4]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather(tensor_list, tensor)</span>
<span class="sd">        &gt;&gt;&gt; tensor_list</span>
<span class="sd">        [tensor([1, 2]), tensor([3, 4])] # Rank 0</span>
<span class="sd">        [tensor([1, 2]), tensor([3, 4])] # Rank 1</span>

<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.cfloat dtype.</span>
<span class="sd">        &gt;&gt;&gt; # We have 2 process groups, 2 ranks.</span>
<span class="sd">        &gt;&gt;&gt; tensor_list = [torch.zeros(2, dtype=torch.cfloat) for _ in range(2)]</span>
<span class="sd">        &gt;&gt;&gt; tensor_list</span>
<span class="sd">        [tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([1.+1.j, 2.+2.j]) # Rank 0</span>
<span class="sd">        tensor([3.+3.j, 4.+4.j]) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather(tensor_list, tensor)</span>
<span class="sd">        &gt;&gt;&gt; tensor_list</span>
<span class="sd">        [tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0</span>
<span class="sd">        [tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="s2">&quot;tensor_list&quot;</span><span class="p">)</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
    <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_gather&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensor_list</span>
    <span class="p">]</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allgather</span><span class="p">([</span><span class="n">tensor_list</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allgather</span><span class="p">([</span><span class="n">tensor_list</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="all_gather_into_tensor"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_gather_into_tensor">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_gather_into_tensor</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gather tensors from all ranks and put them in a single output tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor (Tensor): Output tensor to accommodate tensor elements</span>
<span class="sd">            from all ranks. It must be correctly sized to have one of the</span>
<span class="sd">            following forms:</span>
<span class="sd">            (i) a concatenation of all the input tensors along the primary</span>
<span class="sd">            dimension; for definition of &quot;concatenation&quot;, see ``torch.cat()``;</span>
<span class="sd">            (ii) a stack of all the input tensors along the primary dimension;</span>
<span class="sd">            for definition of &quot;stack&quot;, see ``torch.stack()``.</span>
<span class="sd">            Examples below may better explain the supported output forms.</span>
<span class="sd">        input_tensor (Tensor): Tensor to be gathered from current rank.</span>
<span class="sd">            Different from the ``all_gather`` API, the input tensors in this</span>
<span class="sd">            API must have the same size across all ranks.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.int64 dtype and on CUDA devices.</span>
<span class="sd">        &gt;&gt;&gt; # We have two ranks.</span>
<span class="sd">        &gt;&gt;&gt; device = torch.device(f&#39;cuda:{rank}&#39;)</span>
<span class="sd">        &gt;&gt;&gt; tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank</span>
<span class="sd">        &gt;&gt;&gt; tensor_in</span>
<span class="sd">        tensor([1, 2], device=&#39;cuda:0&#39;) # Rank 0</span>
<span class="sd">        tensor([3, 4], device=&#39;cuda:1&#39;) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; # Output in concatenation form</span>
<span class="sd">        &gt;&gt;&gt; tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather_into_tensor(tensor_out, tensor_in)</span>
<span class="sd">        &gt;&gt;&gt; tensor_out</span>
<span class="sd">        tensor([1, 2, 3, 4], device=&#39;cuda:0&#39;) # Rank 0</span>
<span class="sd">        tensor([1, 2, 3, 4], device=&#39;cuda:1&#39;) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; # Output in stack form</span>
<span class="sd">        &gt;&gt;&gt; tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)</span>
<span class="sd">        &gt;&gt;&gt; dist.all_gather_into_tensor(tensor_out2, tensor_in)</span>
<span class="sd">        &gt;&gt;&gt; tensor_out2</span>
<span class="sd">        tensor([[1, 2],</span>
<span class="sd">                [3, 4]], device=&#39;cuda:0&#39;) # Rank 0</span>
<span class="sd">        tensor([[1, 2],</span>
<span class="sd">                [3, 4]], device=&#39;cuda:1&#39;) # Rank 1</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The Gloo backend does not support this API.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="s2">&quot;input_tensor&quot;</span><span class="p">)</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="s2">&quot;output_tensor&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_gather_into_tensor&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">output_tensor</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_tensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">output_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">input_tensor</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">group</span> <span class="o">=</span> <span class="n">group</span> <span class="ow">or</span> <span class="n">_get_default_group</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="c1"># We are in coalescing context, do not issue single operation, just append a collective representation</span>
        <span class="n">coll</span> <span class="o">=</span> <span class="n">_CollOp</span><span class="p">(</span><span class="n">all_gather_into_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">)</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="p">[</span><span class="n">group</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">coll</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_IllegalWork</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">_allgather_base</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">_all_gather_base</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Single tensor all gather. Gathers a single tensor from all ranks, and puts them in a single output tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor (Tensor): Output tensor. It should contain</span>
<span class="sd">            correctly-sized tensors to be used for output of the collective.</span>
<span class="sd">        input_tensor (Tensor): Tensor to be broadcast from current process.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `_all_gather_base` is a private function. Users should use</span>
<span class="sd">        `all_gather_into_tensor` instead.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed._all_gather_base is a private function and will be &quot;</span>
        <span class="s2">&quot;deprecated. Please use torch.distributed.all_gather_into_tensor &quot;</span>
        <span class="s2">&quot;instead.&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">async_op</span><span class="p">)</span>


<span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_gather_coalesced</span><span class="p">(</span>
    <span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers input tensors from the whole group in a list in a coalesced manner.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor_lists (list[list[Tensor]]): Output list. It should contain</span>
<span class="sd">            correctly-sized tensors to be used for output of the collective.</span>
<span class="sd">        input_tensor_list (list[Tensor]): Tensors to be broadcast from</span>
<span class="sd">            current process. At least one tensor has to be non empty.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    Example:</span>
<span class="sd">        we have 2 process groups, 2 ranks.</span>
<span class="sd">        rank 0 passes:</span>
<span class="sd">            input_tensor_list = [[[1, 1], [1, 1]], [2], [3, 3]]</span>
<span class="sd">            output_tensor_lists =</span>
<span class="sd">               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],</span>
<span class="sd">                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]</span>
<span class="sd">        rank 1 passes:</span>
<span class="sd">            input_tensor_list = [[[3, 3], [3, 3]], [5], [1, 1]]</span>
<span class="sd">            output_tensor_lists =</span>
<span class="sd">               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],</span>
<span class="sd">                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]</span>
<span class="sd">        both rank 0 and 1 get:</span>
<span class="sd">            output_tensor_lists =</span>
<span class="sd">               [[[1, 1], [1, 1]], [2], [3, 3]],</span>
<span class="sd">                [[3, 3], [3, 3]], [5], [1, 1]]].</span>

<span class="sd">    WARNING: at this time individual shape checking is not implemented across nodes.</span>
<span class="sd">    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the</span>
<span class="sd">    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the</span>
<span class="sd">    all_gather_coalesced operation will proceed without complaint and return</span>
<span class="sd">    erroneous outputs. This lack of shape checking results in significant</span>
<span class="sd">    performance improvements but users of this function should take extra care</span>
<span class="sd">    to ensure that each node passes in tensors whose shapes match across nodes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed.all_gather_coalesced will be deprecated. If you must &quot;</span>
        <span class="s2">&quot;use it, please revisit our documentation later at &quot;</span>
        <span class="s2">&quot;https://pytorch.org/docs/master/distributed.html#collective-functions&quot;</span>
    <span class="p">)</span>
    <span class="c1"># We only check basic compatibility with C++ params here, C++ code will</span>
    <span class="c1"># do shape and type checking.</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_gather_coalesced&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">input_tensor_list</span><span class="p">,</span> <span class="s2">&quot;input_tensor_list&quot;</span><span class="p">)</span>
    <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">input_tensor_list</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid function argument: &quot;</span> <span class="s2">&quot;output_tensor_lists should be a list&quot;</span>
        <span class="p">)</span>
    <span class="k">for</span> <span class="n">output_tensor_list</span> <span class="ow">in</span> <span class="n">output_tensor_lists</span><span class="p">:</span>
        <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="s2">&quot;output_tensor_lists&quot;</span><span class="p">)</span>
        <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">)</span>

    <span class="n">output_tensor_lists</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">output_tensor_lists</span>
    <span class="p">]</span>
    <span class="n">input_tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_tensor_list</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allgather_coalesced</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">allgather_coalesced</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span><span class="o">.</span><span class="n">get_future</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_validate_output_list_for_rank</span><span class="p">(</span><span class="n">my_rank</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">gather_list</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dst</span> <span class="o">==</span> <span class="n">my_rank</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">gather_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument ``gather_list`` must be specified on destination rank.&quot;</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="n">gather_list</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Argument ``gather_list`` must NOT be specified &quot;</span>
            <span class="s2">&quot;on non-destination ranks.&quot;</span>
        <span class="p">)</span>


<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.gather">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">gather_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers a list of tensors in a single process.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Input tensor.</span>
<span class="sd">        gather_list (list[Tensor], optional): List of appropriately-sized</span>
<span class="sd">            tensors to use for gathered data (default is None, must be specified</span>
<span class="sd">            on the destination rank)</span>
<span class="sd">        dst (int, optional): Destination rank (default is 0)</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>

    <span class="c1"># Parameter ``gather_list`` may be left unspecified on non-dst ranks.</span>
    <span class="k">if</span> <span class="n">gather_list</span><span class="p">:</span>
        <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">gather_list</span><span class="p">,</span> <span class="s2">&quot;gather_list&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gather_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">gather_list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;gather&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">_validate_output_list_for_rank</span><span class="p">(</span><span class="n">my_rank</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">gather_list</span><span class="p">)</span>
    <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">gather_list</span><span class="p">]</span> <span class="k">if</span> <span class="n">dst</span> <span class="o">==</span> <span class="n">my_rank</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">GatherOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">dst</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_dst_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_dst_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="scatter"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.scatter">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">scatter_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters a list of tensors to all processes in a group.</span>

<span class="sd">    Each process will receive exactly one tensor and store its data in the</span>
<span class="sd">    ``tensor`` argument.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): Output tensor.</span>
<span class="sd">        scatter_list (list[Tensor]): List of tensors to scatter (default is</span>
<span class="sd">            None, must be specified on the source rank)</span>
<span class="sd">        src (int): Source rank (default is 0)</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>

<span class="sd">    .. note:: Note that all Tensors in scatter_list must have the same size.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; tensor_size = 2</span>
<span class="sd">        &gt;&gt;&gt; t_ones = torch.ones(tensor_size)</span>
<span class="sd">        &gt;&gt;&gt; t_fives = torch.ones(tensor_size) * 5</span>
<span class="sd">        &gt;&gt;&gt; output_tensor = torch.zeros(tensor_size)</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     # Assumes world_size of 2.</span>
<span class="sd">        &gt;&gt;&gt;     # Only tensors, all of which must be the same size.</span>
<span class="sd">        &gt;&gt;&gt;     scatter_list = [t_ones, t_fives]</span>
<span class="sd">        &gt;&gt;&gt; else:</span>
<span class="sd">        &gt;&gt;&gt;     scatter_list = None</span>
<span class="sd">        &gt;&gt;&gt; dist.scatter(output_tensor, scatter_list, src=0)</span>
<span class="sd">        &gt;&gt;&gt; # Rank i gets scatter_list[i]. For example, on rank 1:</span>
<span class="sd">        &gt;&gt;&gt; output_tensor</span>
<span class="sd">        tensor([5., 5.])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>

    <span class="c1"># Parameter ``scatter_list`` may be left unspecified on non-src ranks.</span>
    <span class="k">if</span> <span class="n">scatter_list</span><span class="p">:</span>
        <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">scatter_list</span><span class="p">,</span> <span class="s2">&quot;scatter_list&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scatter_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">scatter_list</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;scatter&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="n">scatter_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">scatter_list</span>
    <span class="p">]</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">src</span> <span class="o">==</span> <span class="n">my_rank</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">scatter_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument ``scatter_list`` must be specified &quot;</span> <span class="s2">&quot;on source rank.&quot;</span>
            <span class="p">)</span>
        <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">scatter_list</span><span class="p">]</span>
        <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scatter_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument ``scatter_list`` must NOT be specified &quot;</span>
                <span class="s2">&quot;on non-source ranks.&quot;</span>
            <span class="p">)</span>
        <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">]</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ScatterOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">src</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">group</span> <span class="ow">is</span> <span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">group_src_rank</span> <span class="o">=</span> <span class="n">get_group_rank</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
        <span class="n">opts</span><span class="o">.</span><span class="n">rootRank</span> <span class="o">=</span> <span class="n">group_src_rank</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="reduce_scatter_multigpu"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce_scatter_multigpu">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">reduce_scatter_multigpu</span><span class="p">(</span>
    <span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_lists</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduce and scatter a list of tensors to the whole group.  Only nccl backend</span>
<span class="sd">    is currently supported.</span>

<span class="sd">    Each tensor in ``output_tensor_list`` should reside on a separate GPU, as</span>
<span class="sd">    should each list of tensors in ``input_tensor_lists``.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor_list (List[Tensor]): Output tensors (on different GPUs)</span>
<span class="sd">            to receive the result of the operation.</span>

<span class="sd">            Note that ``len(output_tensor_list)`` needs to be the same for all</span>
<span class="sd">            the distributed processes calling this function.</span>

<span class="sd">        input_tensor_lists (List[List[Tensor]]): Input lists.  It should</span>
<span class="sd">            contain correctly-sized tensors on each GPU to be used for input of</span>
<span class="sd">            the collective, e.g. ``input_tensor_lists[i]`` contains the</span>
<span class="sd">            reduce_scatter input that resides on the GPU of</span>
<span class="sd">            ``output_tensor_list[i]``.</span>

<span class="sd">            Note that each element of ``input_tensor_lists`` has the size of</span>
<span class="sd">            ``world_size * len(output_tensor_list)``, since the function</span>
<span class="sd">            scatters the result from every single GPU in the group.  To</span>
<span class="sd">            interpret each element of ``input_tensor_lists[i]``, note that</span>
<span class="sd">            ``output_tensor_list[j]`` of rank k receives the reduce-scattered</span>
<span class="sd">            result from ``input_tensor_lists[i][k * world_size + j]``</span>

<span class="sd">            Also note that ``len(input_tensor_lists)``, and the size of each</span>
<span class="sd">            element in ``input_tensor_lists`` (each element is a list,</span>
<span class="sd">            therefore ``len(input_tensor_lists[i])``) need to be the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>

<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed.reduce_scatter_multigpu will be deprecated. If you must &quot;</span>
        <span class="s2">&quot;use it, please revisit our documentation later at &quot;</span>
        <span class="s2">&quot;https://pytorch.org/docs/master/distributed.html#multi-gpu-collective-functions&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;reduce_scatter_multigpu&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceScatterOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">reduce_scatter</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_lists</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce_scatter</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_lists</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="reduce_scatter"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce_scatter">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">reduce_scatter</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces, then scatters a list of tensors to all processes in a group.</span>

<span class="sd">    Args:</span>
<span class="sd">        output (Tensor): Output tensor.</span>
<span class="sd">        input_list (list[Tensor]): List of tensors to reduce and scatter.</span>
<span class="sd">        op (optional): One of the values from</span>
<span class="sd">            ``torch.distributed.ReduceOp``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="s2">&quot;input_list&quot;</span><span class="p">)</span>
    <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">input_list</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;reduce_scatter&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceScatterOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">reduce_scatter</span><span class="p">([</span><span class="n">output</span><span class="p">],</span> <span class="p">[</span><span class="n">input_list</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">reduce_scatter</span><span class="p">([</span><span class="n">output</span><span class="p">],</span> <span class="p">[</span><span class="n">input_list</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="reduce_scatter_tensor"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.reduce_scatter_tensor">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">reduce_scatter_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces, then scatters a tensor to all ranks in a group.</span>

<span class="sd">    Args:</span>
<span class="sd">        output (Tensor): Output tensor. It should have the same size across all</span>
<span class="sd">            ranks.</span>
<span class="sd">        input (Tensor): Input tensor to be reduced and scattered. Its size</span>
<span class="sd">            should be output tensor size times the world size. The input tensor</span>
<span class="sd">            can have one of the following shapes:</span>
<span class="sd">            (i) a concatenation of the output tensors along the primary</span>
<span class="sd">            dimension, or</span>
<span class="sd">            (ii) a stack of the output tensors along the primary dimension.</span>
<span class="sd">            For definition of &quot;concatenation&quot;, see ``torch.cat()``.</span>
<span class="sd">            For definition of &quot;stack&quot;, see ``torch.stack()``.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # All tensors below are of torch.int64 dtype and on CUDA devices.</span>
<span class="sd">        &gt;&gt;&gt; # We have two ranks.</span>
<span class="sd">        &gt;&gt;&gt; device = torch.device(f&#39;cuda:{rank}&#39;)</span>
<span class="sd">        &gt;&gt;&gt; tensor_out = torch.zeros(2, dtype=torch.int64, device=device)</span>
<span class="sd">        &gt;&gt;&gt; # Input in concatenation form</span>
<span class="sd">        &gt;&gt;&gt; tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)</span>
<span class="sd">        &gt;&gt;&gt; tensor_in</span>
<span class="sd">        tensor([0, 1, 2, 3], device=&#39;cuda:0&#39;) # Rank 0</span>
<span class="sd">        tensor([0, 1, 2, 3], device=&#39;cuda:1&#39;) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.reduce_scatter_tensor(tensor_out, tensor_in)</span>
<span class="sd">        &gt;&gt;&gt; tensor_out</span>
<span class="sd">        tensor([0, 2], device=&#39;cuda:0&#39;) # Rank 0</span>
<span class="sd">        tensor([4, 6], device=&#39;cuda:1&#39;) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; # Input in stack form</span>
<span class="sd">        &gt;&gt;&gt; tensor_in = torch.reshape(tensor_in, (world_size, 2))</span>
<span class="sd">        &gt;&gt;&gt; tensor_in</span>
<span class="sd">        tensor([[0, 1],</span>
<span class="sd">                [2, 3]], device=&#39;cuda:0&#39;) # Rank 0</span>
<span class="sd">        tensor([[0, 1],</span>
<span class="sd">                [2, 3]], device=&#39;cuda:1&#39;) # Rank 1</span>
<span class="sd">        &gt;&gt;&gt; dist.reduce_scatter_tensor(tensor_out, tensor_in)</span>
<span class="sd">        &gt;&gt;&gt; tensor_out</span>
<span class="sd">        tensor([0, 2], device=&#39;cuda:0&#39;) # Rank 0</span>
<span class="sd">        tensor([4, 6], device=&#39;cuda:1&#39;) # Rank 1</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The Gloo backend does not support this API.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;reduce_scatter_tensor&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">ReduceScatterOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">reduceOp</span> <span class="o">=</span> <span class="n">op</span>

    <span class="n">group</span> <span class="o">=</span> <span class="n">group</span> <span class="ow">or</span> <span class="n">_get_default_group</span><span class="p">()</span>

    <span class="c1"># Check if we are in coalescing context</span>
    <span class="c1"># If we are, do not issue single operation, just append a collective representation</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">coll</span> <span class="o">=</span> <span class="n">_CollOp</span><span class="p">(</span><span class="n">reduce_scatter_tensor</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">pg_coalesce_state</span><span class="p">[</span><span class="n">group</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">coll</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_IllegalWork</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">_reduce_scatter_base</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_reduce_scatter_base</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces, then scatters a flattened tensor to all processes in a group.</span>

<span class="sd">    Args:</span>
<span class="sd">        output (Tensor): Output tensor.</span>
<span class="sd">        input (Tensor): Input tensor that is of size output tensor size times world size</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `_reduce_scatter_base` is a private function. Users should use</span>
<span class="sd">        `reduce_scatter_tensor` instead.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.distributed._reduce_scatter_base is a private function and will &quot;</span>
        <span class="s2">&quot;be deprecated. Please use torch.distributed.reduce_scatter_tensor &quot;</span>
        <span class="s2">&quot;instead.&quot;</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_scatter_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">async_op</span><span class="p">)</span>


<div class="viewcode-block" id="all_to_all_single"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_to_all_single">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_to_all_single</span><span class="p">(</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="nb">input</span><span class="p">,</span>
    <span class="n">output_split_sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_split_sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Each process splits input tensor and then scatters the split list</span>
<span class="sd">    to all processes in a group. Then concatenate the received tensors from all</span>
<span class="sd">    the processes in the group and return single output tensor.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        output (Tensor): Gathered concatenated output tensor.</span>
<span class="sd">        input (Tensor): Input tensor to scatter.</span>
<span class="sd">        output_split_sizes: (list[Int], optional): Output split sizes for dim 0</span>
<span class="sd">            if specified None or empty, dim 0 of ``output`` tensor must divide</span>
<span class="sd">            equally by ``world_size``.</span>
<span class="sd">        input_split_sizes: (list[Int], optional): Input split sizes for dim 0</span>
<span class="sd">            if specified None or empty, dim 0 of ``input`` tensor must divide</span>
<span class="sd">            equally by ``world_size``.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `all_to_all_single` is experimental and subject to change.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Undefined rank&quot;)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.arange(4) + rank * 4</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([0, 1, 2, 3])     # Rank 0</span>
<span class="sd">        tensor([4, 5, 6, 7])     # Rank 1</span>
<span class="sd">        tensor([8, 9, 10, 11])   # Rank 2</span>
<span class="sd">        tensor([12, 13, 14, 15]) # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = torch.empty([4], dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all_single(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([0, 4, 8, 12])    # Rank 0</span>
<span class="sd">        tensor([1, 5, 9, 13])    # Rank 1</span>
<span class="sd">        tensor([2, 6, 10, 14])   # Rank 2</span>
<span class="sd">        tensor([3, 7, 11, 15])   # Rank 3</span>

<span class="sd">        &gt;&gt;&gt; # Essentially, it is similar to following operation:</span>
<span class="sd">        &gt;&gt;&gt; scatter_list = list(input.chunk(world_size))</span>
<span class="sd">        &gt;&gt;&gt; gather_list  = list(output.chunk(world_size))</span>
<span class="sd">        &gt;&gt;&gt; for i in range(world_size):</span>
<span class="sd">        &gt;&gt;&gt;     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)</span>

<span class="sd">        &gt;&gt;&gt; # Another example with uneven split</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0</span>
<span class="sd">        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1</span>
<span class="sd">        tensor([20, 21, 22, 23, 24])                                     # Rank 2</span>
<span class="sd">        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; input_splits</span>
<span class="sd">        [2, 2, 1, 1]                                                     # Rank 0</span>
<span class="sd">        [3, 2, 2, 2]                                                     # Rank 1</span>
<span class="sd">        [2, 1, 1, 1]                                                     # Rank 2</span>
<span class="sd">        [2, 2, 2, 1]                                                     # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output_splits</span>
<span class="sd">        [2, 3, 2, 2]                                                     # Rank 0</span>
<span class="sd">        [2, 2, 1, 2]                                                     # Rank 1</span>
<span class="sd">        [1, 2, 1, 2]                                                     # Rank 2</span>
<span class="sd">        [1, 2, 1, 1]                                                     # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = ...</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all_single(output, input, output_splits, input_splits)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0</span>
<span class="sd">        tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1</span>
<span class="sd">        tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2</span>
<span class="sd">        tensor([ 5, 17, 18, 24, 36])                                     # Rank 3</span>


<span class="sd">        &gt;&gt;&gt; # Another example with tensors of torch.cfloat type.</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0</span>
<span class="sd">        tensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1</span>
<span class="sd">        tensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2</span>
<span class="sd">        tensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = torch.empty([4], dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all_single(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0</span>
<span class="sd">        tensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1</span>
<span class="sd">        tensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2</span>
<span class="sd">        tensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_to_all_single&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllToAllOptions</span><span class="p">()</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>
    <span class="n">_check_single_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
    <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="n">output_split_sizes</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">output_split_sizes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output_split_sizes</span>
    <span class="n">input_split_sizes</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">input_split_sizes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_split_sizes</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">alltoall_base</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_split_sizes</span><span class="p">,</span> <span class="n">input_split_sizes</span><span class="p">,</span> <span class="n">opts</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">alltoall_base</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_split_sizes</span><span class="p">,</span> <span class="n">input_split_sizes</span><span class="p">,</span> <span class="n">opts</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="all_to_all"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.all_to_all">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">all_to_all</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Each process scatters list of input tensors to all processes in a group and</span>
<span class="sd">    return gathered list of tensors in output list.</span>

<span class="sd">    Complex tensors are supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_tensor_list (list[Tensor]): List of tensors to be gathered one</span>
<span class="sd">            per rank.</span>
<span class="sd">        input_tensor_list (list[Tensor]): List of tensors to scatter one per rank.</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        `all_to_all` is experimental and subject to change.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Undefined rank&quot;)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.arange(4) + rank * 4</span>
<span class="sd">        &gt;&gt;&gt; input = list(input.chunk(4))</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        [tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0</span>
<span class="sd">        [tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1</span>
<span class="sd">        [tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2</span>
<span class="sd">        [tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = list(torch.empty([4], dtype=torch.int64).chunk(4))</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0</span>
<span class="sd">        [tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1</span>
<span class="sd">        [tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2</span>
<span class="sd">        [tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3</span>

<span class="sd">        &gt;&gt;&gt; # Essentially, it is similar to following operation:</span>
<span class="sd">        &gt;&gt;&gt; scatter_list = input</span>
<span class="sd">        &gt;&gt;&gt; gather_list  = output</span>
<span class="sd">        &gt;&gt;&gt; for i in range(world_size):</span>
<span class="sd">        &gt;&gt;&gt;     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)</span>

<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0</span>
<span class="sd">        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1</span>
<span class="sd">        tensor([20, 21, 22, 23, 24])                                     # Rank 2</span>
<span class="sd">        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; input_splits</span>
<span class="sd">        [2, 2, 1, 1]                                                     # Rank 0</span>
<span class="sd">        [3, 2, 2, 2]                                                     # Rank 1</span>
<span class="sd">        [2, 1, 1, 1]                                                     # Rank 2</span>
<span class="sd">        [2, 2, 2, 1]                                                     # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output_splits</span>
<span class="sd">        [2, 3, 2, 2]                                                     # Rank 0</span>
<span class="sd">        [2, 2, 1, 2]                                                     # Rank 1</span>
<span class="sd">        [1, 2, 1, 2]                                                     # Rank 2</span>
<span class="sd">        [1, 2, 1, 1]                                                     # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; input = list(input.split(input_splits))</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        [tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0</span>
<span class="sd">        [tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1</span>
<span class="sd">        [tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2</span>
<span class="sd">        [tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = ...</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0</span>
<span class="sd">        [tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1</span>
<span class="sd">        [tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2</span>
<span class="sd">        [tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3</span>

<span class="sd">        &gt;&gt;&gt; # Another example with tensors of torch.cfloat type.</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)</span>
<span class="sd">        &gt;&gt;&gt; input = list(input.chunk(4))</span>
<span class="sd">        &gt;&gt;&gt; input</span>
<span class="sd">        [tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0</span>
<span class="sd">        [tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1</span>
<span class="sd">        [tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2</span>
<span class="sd">        [tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3</span>
<span class="sd">        &gt;&gt;&gt; output = list(torch.empty([4], dtype=torch.int64).chunk(4))</span>
<span class="sd">        &gt;&gt;&gt; dist.all_to_all(output, input)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        [tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0</span>
<span class="sd">        [tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1</span>
<span class="sd">        [tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2</span>
<span class="sd">        [tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;all_to_all&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">AllToAllOptions</span><span class="p">()</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="s2">&quot;output_tensor_list&quot;</span><span class="p">)</span>
    <span class="n">_check_tensor_list</span><span class="p">(</span><span class="n">input_tensor_list</span><span class="p">,</span> <span class="s2">&quot;input_tensor_list&quot;</span><span class="p">)</span>
    <span class="n">_ensure_all_tensors_same_dtype</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">)</span>

    <span class="n">input_tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">input_tensor_list</span>
    <span class="p">]</span>
    <span class="n">output_tensor_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">t</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">output_tensor_list</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">alltoall</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">alltoall</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>

<div class="viewcode-block" id="barrier"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.barrier">[docs]</a><span class="nd">@_exception_logger</span>
<span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Synchronizes all processes.</span>

<span class="sd">    This collective blocks processes until the whole group enters this function,</span>
<span class="sd">    if async_op is False, or if async work handle is called on wait().</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If None,</span>
<span class="sd">            the default process group will be used.</span>
<span class="sd">        async_op (bool, optional): Whether this op should be an async op</span>
<span class="sd">        device_ids ([int], optional): List of device/GPU ids.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Async work handle, if async_op is set to True.</span>
<span class="sd">        None, if not async_op or if not part of the group</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;barrier&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="n">opts</span> <span class="o">=</span> <span class="n">BarrierOptions</span><span class="p">()</span>
    <span class="n">opts</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">_get_pg_default_device</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">opts</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="n">device_ids</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid function argument: &quot;</span> <span class="s2">&quot;device_ids type should be List[int]&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">opts</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">opts</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">async_op</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">work</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">work</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span></div>


<div class="viewcode-block" id="monitored_barrier"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.monitored_barrier">[docs]</a><span class="k">def</span> <span class="nf">monitored_barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">GroupMember</span><span class="o">.</span><span class="n">WORLD</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wait_all_ranks</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Synchronizes all processes similar to ``torch.distributed.barrier``, but takes</span>
<span class="sd">    a configurable timeout and is able to report ranks that did not pass this</span>
<span class="sd">    barrier within that timeout. Specifically, for non-zero ranks, will block</span>
<span class="sd">    until a send/recv is processed from rank 0. Rank 0 will block until all send</span>
<span class="sd">    /recv from other ranks are processed, and will report failures for ranks</span>
<span class="sd">    that failed to respond in time. Note that if one rank does not reach the</span>
<span class="sd">    monitored_barrier (for example due to a hang), all other ranks would fail</span>
<span class="sd">    in monitored_barrier.</span>

<span class="sd">    This collective will block all processes/ranks in the group, until the</span>
<span class="sd">    whole group exits the function successfully, making it useful for debugging</span>
<span class="sd">    and synchronizing. However, it can have a performance impact and should only</span>
<span class="sd">    be used for debugging or scenarios that require full synchronization points</span>
<span class="sd">    on the host-side. For debugging purposes, this barrier can be inserted</span>
<span class="sd">    before the application&#39;s collective calls to check if any ranks are</span>
<span class="sd">    desynchronized.</span>

<span class="sd">    .. note:: Note that this collective is only supported with the GLOO backend.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (ProcessGroup, optional): The process group to work on. If</span>
<span class="sd">            ``None``, the default process group will be used.</span>
<span class="sd">        timeout (datetime.timedelta, optional): Timeout for monitored_barrier.</span>
<span class="sd">            If ``None``, the default process group timeout will be used.</span>
<span class="sd">        wait_all_ranks (bool, optional): Whether to collect all failed ranks or</span>
<span class="sd">            not. By default, this is ``False`` and ``monitored_barrier`` on rank 0</span>
<span class="sd">            will throw on the first failed rank it encounters in order to fail</span>
<span class="sd">            fast. By setting ``wait_all_ranks=True`` ``monitored_barrier`` will</span>
<span class="sd">            collect all failed ranks and throw an error containing information</span>
<span class="sd">            about all failed ranks.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ``None``.</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # Note: Process group initialization omitted on each rank.</span>
<span class="sd">        &gt;&gt;&gt; import torch.distributed as dist</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() != 1:</span>
<span class="sd">        &gt;&gt;&gt;     dist.monitored_barrier() # Raises exception indicating that</span>
<span class="sd">        &gt;&gt;&gt; # rank 1 did not call into monitored_barrier.</span>
<span class="sd">        &gt;&gt;&gt; # Example with wait_all_ranks=True</span>
<span class="sd">        &gt;&gt;&gt; if dist.get_rank() == 0:</span>
<span class="sd">        &gt;&gt;&gt;     dist.monitored_barrier(wait_all_ranks=True) # Raises exception</span>
<span class="sd">        &gt;&gt;&gt; # indicating that ranks 1, 2, ... world_size - 1 did not call into</span>
<span class="sd">        &gt;&gt;&gt; # monitored_barrier.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Need to call rank not in group before using the group, otherwise</span>
    <span class="c1"># &quot;Invalid process group&quot; error is raised.</span>
    <span class="k">if</span> <span class="n">_rank_not_in_group</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
        <span class="n">_warn_not_in_group</span><span class="p">(</span><span class="s2">&quot;monitored_barrier&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span> <span class="o">!=</span> <span class="n">Backend</span><span class="o">.</span><span class="n">GLOO</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;monitored_barrier is only implemented for GLOO backend.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">timeout</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">timeout</span> <span class="o">=</span> <span class="n">default_pg_timeout</span>

    <span class="n">group_to_use</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span> <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">group</span>
    <span class="k">return</span> <span class="n">group_to_use</span><span class="o">.</span><span class="n">monitored_barrier</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">wait_all_ranks</span><span class="o">=</span><span class="n">wait_all_ranks</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_create_process_group_wrapper</span><span class="p">(</span>
    <span class="n">wrapped_pg</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">,</span>
    <span class="n">store_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">store</span><span class="p">:</span> <span class="n">Store</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="n">timedelta</span> <span class="o">=</span> <span class="n">default_pg_timeout</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Create a separate prefix store for the helper process group.</span>
    <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">PG_WRAPPER_STORE_PREFIX</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">store_prefix</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">store</span> <span class="o">=</span> <span class="n">PrefixStore</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">store</span><span class="p">)</span>
    <span class="n">helper_pg</span> <span class="o">=</span> <span class="n">ProcessGroupGloo</span><span class="p">(</span><span class="n">store</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">)</span>
    <span class="c1"># Wrap the underlying pg with ProcessGroupWrapper.</span>
    <span class="n">wrapped_pg</span> <span class="o">=</span> <span class="n">_ProcessGroupWrapper</span><span class="p">(</span><span class="n">wrapped_pg</span><span class="p">,</span> <span class="n">helper_pg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">wrapped_pg</span>


<span class="k">def</span> <span class="nf">_process_group_name</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">use_hashed_name</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">_world</span>
    <span class="k">if</span> <span class="n">use_hashed_name</span><span class="p">:</span>
        <span class="n">pg_name</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">sha1</span><span class="p">(</span><span class="nb">bytes</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">ranks</span><span class="p">)),</span> <span class="s2">&quot;utf-8&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span>
        <span class="k">while</span> <span class="n">pg_name</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_names</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">pg_name</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">sha1</span><span class="p">(</span><span class="nb">bytes</span><span class="p">(</span><span class="n">pg_name</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="s2">&quot;utf-8&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pg_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">_world</span><span class="o">.</span><span class="n">group_count</span><span class="p">)</span>
        <span class="n">_world</span><span class="o">.</span><span class="n">group_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">pg_name</span>

<span class="k">def</span> <span class="nf">_get_backend_from_str</span><span class="p">(</span><span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Backend</span><span class="p">:</span>
    <span class="c1"># Default to the same backend as the global process group</span>
    <span class="c1">#  if backend is not specified.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">backend</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">get_backend</span><span class="p">(</span><span class="n">_get_default_group</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>


<div class="viewcode-block" id="new_group"><a class="viewcode-back" href="../../../distributed.html#torch.distributed.new_group">[docs]</a><span class="nd">@_time_logger</span>
<span class="k">def</span> <span class="nf">new_group</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_local_synchronization</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new distributed group.</span>

<span class="sd">    This function requires that all processes in the main group (i.e. all</span>
<span class="sd">    processes that are part of the distributed job) enter this function, even</span>
<span class="sd">    if they are not going to be members of the group. Additionally, groups</span>
<span class="sd">    should be created in the same order in all processes.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Using multiple process groups with the ``NCCL`` backend concurrently</span>
<span class="sd">        is not safe and the user should perform explicit synchronization in</span>
<span class="sd">        their application to ensure only one process group is used at a time.</span>
<span class="sd">        This means collectives from one process group should have completed</span>
<span class="sd">        execution on the device (not just enqueued since CUDA execution is</span>
<span class="sd">        async) before collectives from another process group are enqueued.</span>
<span class="sd">        See `Using multiple NCCL communicators concurrently &lt;https://docs.nvid</span>
<span class="sd">        ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using</span>
<span class="sd">        -multiple-nccl-communicators-concurrently&gt;`_ for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        ranks (list[int]): List of ranks of group members. If ``None``, will be</span>
<span class="sd">            set to all ranks. Default is ``None``.</span>
<span class="sd">        timeout (timedelta, optional): Timeout for operations executed against</span>
<span class="sd">            the process group. Default value equals 30 minutes.</span>
<span class="sd">            This is applicable for the ``gloo`` backend. For ``nccl``, this is</span>
<span class="sd">            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            or ``NCCL_ASYNC_ERROR_HANDLING`` is set to 1. When</span>
<span class="sd">            ``NCCL_BLOCKING_WAIT`` is set, this is the duration for which the</span>
<span class="sd">            process will block and wait for collectives to complete before</span>
<span class="sd">            throwing an exception. When ``NCCL_ASYNC_ERROR_HANDLING`` is set,</span>
<span class="sd">            this is the duration after which collectives will be aborted</span>
<span class="sd">            asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            will provide errors to the user which can be caught and handled,</span>
<span class="sd">            but due to its blocking nature, it has a performance overhead. On</span>
<span class="sd">            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little</span>
<span class="sd">            performance overhead, but crashes the process on errors. This is</span>
<span class="sd">            done since CUDA execution is async and it is no longer safe to</span>
<span class="sd">            continue executing user code since failed async NCCL operations</span>
<span class="sd">            might result in subsequent CUDA operations running on corrupted</span>
<span class="sd">            data. Only one of these two environment variables should be set.</span>
<span class="sd">        backend (str or Backend, optional): The backend to use. Depending on</span>
<span class="sd">            build-time configurations, valid values are ``gloo`` and ``nccl``.</span>
<span class="sd">            By default uses the same backend as the global group. This field</span>
<span class="sd">            should be given as a lowercase string (e.g., ``&quot;gloo&quot;``), which can</span>
<span class="sd">            also be accessed via :class:`Backend` attributes (e.g.,</span>
<span class="sd">            ``Backend.GLOO``). If ``None`` is passed in, the backend</span>
<span class="sd">            corresponding to the default process group will be used. Default is</span>
<span class="sd">            ``None``.</span>
<span class="sd">        pg_options (ProcessGroupOptions, optional): process group options</span>
<span class="sd">            specifying what additional options need to be passed in during</span>
<span class="sd">            the construction of specific process groups. i.e. for the ``nccl``</span>
<span class="sd">            backend, ``is_high_priority_stream`` can be specified so that</span>
<span class="sd">            process group can pick up high priority cuda streams.</span>
<span class="sd">        use_local_synchronization (bool, optional): perform a group-local</span>
<span class="sd">            barrier at the end of the process group creation. This is different</span>
<span class="sd">            in that non-member ranks don&#39;t need to call into API and don&#39;t</span>
<span class="sd">            join the barrier.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A handle of distributed group that can be given to collective calls or None if the rank is not part of ``ranks``.</span>

<span class="sd">    N.B. use_local_synchronization doesn&#39;t work with MPI.</span>

<span class="sd">    N.B. While use_local_synchronization=True can be significantly faster with larger</span>
<span class="sd">    clusters and small process groups, care must be taken since it changes cluster behavior</span>
<span class="sd">    as non-member ranks don&#39;t join the group barrier().</span>

<span class="sd">    N.B. use_local_synchronization=True can lead to deadlocks when each rank creates</span>
<span class="sd">    multiple overlaping process groups. To avoid that, make sure all ranks follow the</span>
<span class="sd">    same global creation order.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_new_group_with_tag</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">timeout</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">pg_options</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">use_local_synchronization</span><span class="o">=</span><span class="n">use_local_synchronization</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_new_group_with_tag</span><span class="p">(</span>
    <span class="n">ranks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pg_tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_local_synchronization</span><span class="o">=</span><span class="kc">False</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is a variant of ``new_group`` that exposes tag creation.</span>

<span class="sd">    :: N.B. The mechanism is experimental and tied to the functional collectives effort, see</span>
<span class="sd">    ``torch.distributed._functional_collectives`` for reference on how to use it.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_world</span>

    <span class="n">default_pg</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
    <span class="n">default_backend</span><span class="p">,</span> <span class="n">default_store</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_map</span><span class="p">[</span><span class="n">default_pg</span><span class="p">]</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span>
    <span class="n">global_world_size</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="c1"># Default to the same backend as the global process group</span>
    <span class="c1"># if the backend is not specified.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">backend</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">default_backend</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">Backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_local_synchronization</span><span class="p">:</span>
        <span class="c1"># MPI backend doesn&#39;t have have a way for us to perform a partial sync</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;MPI backend doesn&#39;t support use_local_synchronization=True&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ranks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">get_rank</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ranks</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># checks the input ranks</span>
    <span class="k">if</span> <span class="n">ranks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ranks</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
        <span class="n">group_world_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">group_world_size</span> <span class="o">&gt;</span> <span class="n">global_world_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;the new group&#39;s world size should be less or &quot;</span>
                <span class="s2">&quot;equal to the world size set by &quot;</span>
                <span class="s2">&quot;init_process_group&quot;</span>
            <span class="p">)</span>
        <span class="c1"># check ranks&#39; sanity</span>
        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">ranks</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="n">global_world_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;The new group&#39;s rank should be within the &quot;</span>
                    <span class="s2">&quot;the world_size set by init_process_group&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">global_rank</span> <span class="ow">in</span> <span class="n">ranks</span><span class="p">:</span>
            <span class="n">group_rank</span> <span class="o">=</span> <span class="n">ranks</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">global_rank</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">group_rank</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ranks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">global_world_size</span><span class="p">))</span>
        <span class="n">group_world_size</span> <span class="o">=</span> <span class="n">global_world_size</span>
        <span class="n">group_rank</span> <span class="o">=</span> <span class="n">global_rank</span>

    <span class="n">group_name</span> <span class="o">=</span> <span class="n">_process_group_name</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">use_hashed_name</span><span class="o">=</span><span class="n">use_local_synchronization</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;## process_group:init with ranks: </span><span class="si">{</span><span class="n">ranks</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">):</span>
        <span class="n">pg</span><span class="p">,</span> <span class="n">pg_store</span> <span class="o">=</span> <span class="n">_new_process_group_helper</span><span class="p">(</span>
            <span class="n">group_world_size</span><span class="p">,</span>
            <span class="n">group_rank</span><span class="p">,</span>
            <span class="n">ranks</span><span class="p">,</span>
            <span class="n">backend</span><span class="p">,</span>
            <span class="n">default_store</span><span class="p">,</span>
            <span class="n">group_name</span><span class="o">=</span><span class="n">group_name</span><span class="p">,</span>
            <span class="n">pg_options</span><span class="o">=</span><span class="n">pg_options</span><span class="p">,</span>
            <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
            <span class="n">pg_tag</span><span class="o">=</span><span class="n">pg_tag</span>
        <span class="p">)</span>

    <span class="c1"># Create the global rank to group rank mapping</span>
    <span class="n">_world</span><span class="o">.</span><span class="n">pg_group_ranks</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">global_rank</span><span class="p">:</span> <span class="n">group_rank</span> <span class="k">for</span> <span class="n">group_rank</span><span class="p">,</span> <span class="n">global_rank</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">_is_barrier_after_init</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># barrier at the end to ensure that once we return from this method, all</span>
        <span class="c1"># process groups including global variables (if any) are updated</span>
        <span class="c1"># correctly on all ranks.</span>
        <span class="c1"># Update 04/2023: for large-scale runs, this barrier (esp. store-based</span>
        <span class="c1"># barrier) may be costly and/or unscalable. Also, in a lot of cases,</span>
        <span class="c1"># these barriers may be unnecessary, as proven by a green CI after</span>
        <span class="c1"># removal. An environment variable `TORCH_DIST_INIT_BARRIER` has been</span>
        <span class="c1"># added which enables this barrier only when set to 1.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Performing barrier after ProcessGroup initialization since &quot;</span>
            <span class="s2">&quot;TORCH_DIST_INIT_BARRIER = 1&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">Backend</span><span class="o">.</span><span class="n">MPI</span><span class="p">:</span>
            <span class="c1"># MPI doesn&#39;t have store.</span>
            <span class="n">barrier</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">barrier_store</span> <span class="o">=</span> <span class="n">pg_store</span> <span class="k">if</span> <span class="n">use_local_synchronization</span> <span class="k">else</span> <span class="n">default_store</span>
            <span class="n">world_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span> <span class="k">if</span> <span class="n">use_local_synchronization</span> <span class="k">else</span> <span class="n">get_world_size</span><span class="p">()</span>
            <span class="c1"># Use store based barrier here since barrier() used a bunch of</span>
            <span class="c1"># default devices and messes up NCCL internal state.</span>
            <span class="n">_store_based_barrier</span><span class="p">(</span><span class="n">global_rank</span><span class="p">,</span> <span class="n">barrier_store</span><span class="p">,</span> <span class="n">group_name</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pg</span>


<span class="k">def</span> <span class="nf">new_subgroups</span><span class="p">(</span>
    <span class="n">group_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates subgroups of equal size. By default, it creates intra-machine subgroups,</span>
<span class="sd">    where each of which contains all the ranks of a machine, based on the assumption</span>
<span class="sd">    that each machine has the same number of devices.</span>

<span class="sd">    This is a convenience API that calls ``new_group`` to generate multiple subgroups.</span>
<span class="sd">    It requires that all processes in the main group (i.e. all</span>
<span class="sd">    processes that are part of the distributed job) enter this function, even</span>
<span class="sd">    if they are not going to be members of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If ``group_size`` is passed in, the world size must be divisible by ``group_size``.</span>
<span class="sd">        If no ``group_size`` is passed in, it believe that you are creating a group based</span>
<span class="sd">        on CUDA and determining the group size by number of CUDA devices, and if not all</span>
<span class="sd">        the machines have the same number of devices, the subgroup division will be</span>
<span class="sd">        different across nodes and can cause unexpected behaviors. Therefore, if you are</span>
<span class="sd">        creating a subgroup that does not depend on CUDA (such as Gloo on CPU), please</span>
<span class="sd">        pass in ``group_size`` correctly.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Using multiple process groups with the ``NCCL`` backend concurrently</span>
<span class="sd">        is not safe and the user should perform explicit synchronization in</span>
<span class="sd">        their application to ensure only one process group is used at a time.</span>
<span class="sd">        This means collectives from one process group should have completed</span>
<span class="sd">        execution on the device (not just enqueued since CUDA execution is</span>
<span class="sd">        async) before collectives from another process group are enqueued.</span>
<span class="sd">        See `Using multiple NCCL communicators concurrently &lt;https://docs.nvid</span>
<span class="sd">        ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using</span>
<span class="sd">        -multiple-nccl-communicators-concurrently&gt;`_ for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        group_size (int, optional): The size of each subgroup. If ``None``,</span>
<span class="sd">            the default subgroup size is equal to the number of devices on each machine,</span>
<span class="sd">            based on the assumption that each machine has exactly the same</span>
<span class="sd">            number of devices. Default is ``None``.</span>
<span class="sd">        timeout (timedelta, optional): Timeout for operations executed against</span>
<span class="sd">            the process group. Default value equals 30 minutes.</span>
<span class="sd">            This is applicable for the ``gloo`` backend. For ``nccl``, this is</span>
<span class="sd">            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            or ``NCCL_ASYNC_ERROR_HANDLING`` is set to 1. When</span>
<span class="sd">            ``NCCL_BLOCKING_WAIT`` is set, this is the duration for which the</span>
<span class="sd">            process will block and wait for collectives to complete before</span>
<span class="sd">            throwing an exception. When ``NCCL_ASYNC_ERROR_HANDLING`` is set,</span>
<span class="sd">            this is the duration after which collectives will be aborted</span>
<span class="sd">            asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            will provide errors to the user which can be caught and handled,</span>
<span class="sd">            but due to its blocking nature, it has a performance overhead. On</span>
<span class="sd">            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little</span>
<span class="sd">            performance overhead, but crashes the process on errors. This is</span>
<span class="sd">            done since CUDA execution is async and it is no longer safe to</span>
<span class="sd">            continue executing user code since failed async NCCL operations</span>
<span class="sd">            might result in subsequent CUDA operations running on corrupted</span>
<span class="sd">            data. Only one of these two environment variables should be set.</span>
<span class="sd">        backend (str or Backend, optional): The backend to use. Depending on</span>
<span class="sd">            build-time configurations, valid values are ``gloo`` and ``nccl``.</span>
<span class="sd">            By default uses the same backend as the global group. This field</span>
<span class="sd">            should be given as a lowercase string (e.g., ``&quot;gloo&quot;``), which can</span>
<span class="sd">            also be accessed via :class:`Backend` attributes (e.g.,</span>
<span class="sd">            ``Backend.GLOO``). If ``None`` is passed in, the backend</span>
<span class="sd">            corresponding to the default process group will be used. Default is</span>
<span class="sd">            ``None``.</span>
<span class="sd">        pg_options (ProcessGroupOptions, optional): process group options</span>
<span class="sd">            specifying what additional options need to be passed in during</span>
<span class="sd">            the construction of specific process groups. i.e. for the ``nccl``</span>
<span class="sd">            backend, ``is_high_priority_stream`` can be specified so that</span>
<span class="sd">            process group can pick up high priority cuda streams.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The subgroup containing the current rank, and all the subgroups used for cleanup.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Create intra-machine subgroups.</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; cur_subgroup, subgroups = dist.new_subgroups()</span>
<span class="sd">        &gt;&gt;&gt; # Allreduce within the machine.</span>
<span class="sd">        &gt;&gt;&gt; rank = dist.get_rank()</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.ones(1, device=rank) * rank</span>
<span class="sd">        &gt;&gt;&gt; dist.all_reduce(tensor, group=cur_subgroup)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([8])     # Assume 8 is the number of CUDA devices per machine.</span>
<span class="sd">        &gt;&gt;&gt; # Cleanup.</span>
<span class="sd">        &gt;&gt;&gt; for subgroup in subgroups:</span>
<span class="sd">        &gt;&gt;&gt;     dist.destroy_process_group(subgroup)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Default group size only takes effect when CUDA is available.&quot;</span>
                             <span class="s2">&quot;If your subgroup using a backend that does not depend on CUDA,&quot;</span>
                             <span class="s2">&quot;please pass in &#39;group_size&#39; correctly.&quot;</span><span class="p">)</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">group_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The arg &#39;group_size&#39; (</span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">) must be positive&quot;</span><span class="p">)</span>

    <span class="n">world_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">world_size</span> <span class="o">&lt;</span> <span class="n">group_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The arg &#39;group_size&#39; (</span><span class="si">{</span><span class="n">group_size</span><span class="si">}</span><span class="s2">) must not exceed the world size (</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">world_size</span> <span class="o">%</span> <span class="n">group_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The world size must be divisible by &#39;group_size&#39;&quot;</span><span class="p">)</span>

    <span class="n">subgroups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cur_subgroup</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">subgroup_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span> <span class="o">//</span> <span class="n">group_size</span><span class="p">):</span>
        <span class="n">start_rank</span> <span class="o">=</span> <span class="n">subgroup_id</span> <span class="o">*</span> <span class="n">group_size</span>
        <span class="n">end_rank</span> <span class="o">=</span> <span class="n">start_rank</span> <span class="o">+</span> <span class="n">group_size</span>
        <span class="n">ranks_in_subgroup</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start_rank</span><span class="p">,</span> <span class="n">end_rank</span><span class="p">))</span>
        <span class="n">subgroup</span> <span class="o">=</span> <span class="n">new_group</span><span class="p">(</span>
            <span class="n">ranks</span><span class="o">=</span><span class="n">ranks_in_subgroup</span><span class="p">,</span>
            <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
            <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
            <span class="n">pg_options</span><span class="o">=</span><span class="n">pg_options</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">subgroups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">subgroup</span><span class="p">)</span>

        <span class="n">rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">ranks_in_subgroup</span><span class="p">:</span>
            <span class="n">cur_subgroup</span> <span class="o">=</span> <span class="n">subgroup</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;Rank </span><span class="si">%s</span><span class="s2"> is assigned to subgroup </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">rank</span><span class="p">,</span> <span class="n">ranks_in_subgroup</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">cur_subgroup</span><span class="p">,</span> <span class="n">subgroups</span>


<span class="k">def</span> <span class="nf">new_subgroups_by_enumeration</span><span class="p">(</span>
    <span class="n">ranks_per_subgroup_list</span><span class="p">,</span>
    <span class="n">timeout</span><span class="o">=</span><span class="n">default_pg_timeout</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">pg_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates subgroups by dividing the global world, where the division is specified by</span>
<span class="sd">    a nested list of ranks. The subgroups cannot have overlap, and some ranks may not have</span>
<span class="sd">    to be in any subgroup.</span>

<span class="sd">    This is a convenience API that calls ``new_group`` to generate multiple subgroups.</span>
<span class="sd">    It requires that all processes in the main group (i.e. all</span>
<span class="sd">    processes that are part of the distributed job) enter this function, even</span>
<span class="sd">    if they are not going to be members of the group.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Using multiple process groups with the ``NCCL`` backend concurrently</span>
<span class="sd">        is not safe and the user should perform explicit synchronization in</span>
<span class="sd">        their application to ensure only one process group is used at a time.</span>
<span class="sd">        This means collectives from one process group should have completed</span>
<span class="sd">        execution on the device (not just enqueued since CUDA execution is</span>
<span class="sd">        async) before collectives from another process group are enqueued.</span>
<span class="sd">        See `Using multiple NCCL communicators concurrently &lt;https://docs.nvid</span>
<span class="sd">        ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using</span>
<span class="sd">        -multiple-nccl-communicators-concurrently&gt;`_ for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        ranks_per_subgroup_list (list[list[int]]): A nested list of ranks of</span>
<span class="sd">            group members.</span>
<span class="sd">        timeout (timedelta, optional): Timeout for operations executed against</span>
<span class="sd">            the process group. Default value equals 30 minutes.</span>
<span class="sd">            This is applicable for the ``gloo`` backend. For ``nccl``, this is</span>
<span class="sd">            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            or ``NCCL_ASYNC_ERROR_HANDLING`` is set to 1. When</span>
<span class="sd">            ``NCCL_BLOCKING_WAIT`` is set, this is the duration for which the</span>
<span class="sd">            process will block and wait for collectives to complete before</span>
<span class="sd">            throwing an exception. When ``NCCL_ASYNC_ERROR_HANDLING`` is set,</span>
<span class="sd">            this is the duration after which collectives will be aborted</span>
<span class="sd">            asynchronously and the process will crash. ``NCCL_BLOCKING_WAIT``</span>
<span class="sd">            will provide errors to the user which can be caught and handled,</span>
<span class="sd">            but due to its blocking nature, it has a performance overhead. On</span>
<span class="sd">            the other hand, ``NCCL_ASYNC_ERROR_HANDLING`` has very little</span>
<span class="sd">            performance overhead, but crashes the process on errors. This is</span>
<span class="sd">            done since CUDA execution is async and it is no longer safe to</span>
<span class="sd">            continue executing user code since failed async NCCL operations</span>
<span class="sd">            might result in subsequent CUDA operations running on corrupted</span>
<span class="sd">            data. Only one of these two environment variables should be set.</span>
<span class="sd">         backend (str or Backend, optional): The backend to use. Depending on</span>
<span class="sd">             build-time configurations, valid values are ``gloo`` and ``nccl``.</span>
<span class="sd">             By default uses the same backend as the global group. This field</span>
<span class="sd">             should be given as a lowercase string (e.g., ``&quot;gloo&quot;``), which can</span>
<span class="sd">             also be accessed via :class:`Backend` attributes (e.g.,</span>
<span class="sd">             ``Backend.GLOO``). If ``None`` is passed in, the backend</span>
<span class="sd">             corresponding to the default process group will be used. Default is</span>
<span class="sd">             ``None``.</span>
<span class="sd">        pg_options (ProcessGroupOptions, optional): process group options</span>
<span class="sd">            specifying what additional options need to be passed in during</span>
<span class="sd">            the construction of specific process groups. i.e. for the ``nccl``</span>
<span class="sd">            backend, ``is_high_priority_stream`` can be specified so that</span>
<span class="sd">            process group can pick up high priority cuda streams.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The subgroup containing the current rank, and all the subgroups used for cleanup.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Create two subgroups, where each has 2 processes.</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;need process group init&quot;)</span>
<span class="sd">        &gt;&gt;&gt; cur_subgroup, subgroups = dist.new_subgroups(ranks=[[0, 2], [1, 3]])</span>
<span class="sd">        &gt;&gt;&gt; rank = dist.get_rank()</span>
<span class="sd">        &gt;&gt;&gt; tensor = torch.ones(1, device=rank) * rank</span>
<span class="sd">        &gt;&gt;&gt; dist.all_reduce(tensor, group=cur_subgroup)</span>
<span class="sd">        &gt;&gt;&gt; tensor</span>
<span class="sd">        tensor([2])     # Subgroup 0: ranks 0 and 2</span>
<span class="sd">        tensor([4])     # Subgroup 1: ranks 1 and 3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">ranks_per_subgroup_list</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks_per_subgroup_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The arg &#39;ranks_per_subgroup_list&#39; cannot be empty&quot;</span><span class="p">)</span>

    <span class="n">subgroups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cur_subgroup</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Create a mapping from rank to subgroup to check if there is any subgroup overlap.</span>
    <span class="n">rank_to_ranks_dict</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># type: ignore[var-annotated]</span>
    <span class="k">for</span> <span class="n">ranks</span> <span class="ow">in</span> <span class="n">ranks_per_subgroup_list</span><span class="p">:</span>
        <span class="n">subgroup</span> <span class="o">=</span> <span class="n">new_group</span><span class="p">(</span>
            <span class="n">ranks</span><span class="o">=</span><span class="n">ranks</span><span class="p">,</span>
            <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span><span class="p">,</span>
            <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
            <span class="n">pg_options</span><span class="o">=</span><span class="n">pg_options</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">subgroups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">subgroup</span><span class="p">)</span>
        <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">ranks</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">rank_to_ranks_dict</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> has appeared in both subgroup </span><span class="si">{</span><span class="n">rank_to_ranks_dict</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">ranks</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">rank_to_ranks_dict</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="n">ranks</span>
            <span class="k">if</span> <span class="n">my_rank</span> <span class="o">==</span> <span class="n">rank</span><span class="p">:</span>
                <span class="n">cur_subgroup</span> <span class="o">=</span> <span class="n">subgroup</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Rank </span><span class="si">%s</span><span class="s2"> is assigned to subgroup </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">ranks</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cur_subgroup</span><span class="p">,</span> <span class="n">subgroups</span>


<span class="k">def</span> <span class="nf">_find_pg_by_ranks_and_tag</span><span class="p">(</span><span class="n">tag</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">ranks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ProcessGroup</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;ptd:&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;user:&quot;</span><span class="p">):</span>
        <span class="n">tag</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;user:</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">_world</span><span class="o">.</span><span class="n">tags_to_pg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="p">[]):</span>
        <span class="k">if</span> <span class="n">group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">):</span>
            <span class="k">continue</span>

        <span class="n">group_ranks</span> <span class="o">=</span> <span class="n">get_process_group_ranks</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="n">good</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">r</span> <span class="ow">in</span> <span class="n">group_ranks</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ranks</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">good</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">group</span>
    <span class="k">return</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">_find_or_create_pg_by_ranks_and_tag</span><span class="p">(</span><span class="n">tag</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">ranks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ProcessGroup</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span> <span class="o">%</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Ranks length (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span><span class="si">}</span><span class="s2">) must be divisible by stride (</span><span class="si">{</span><span class="n">stride</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">my_ranks</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">stride</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">):</span>
        <span class="n">my_ranks</span> <span class="o">=</span> <span class="n">ranks</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">my_rank</span> <span class="ow">in</span> <span class="n">my_ranks</span><span class="p">,</span> <span class="s2">&quot;rankset doesn&#39;t include the current node&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">),</span> <span class="n">stride</span><span class="p">):</span>
            <span class="n">rank_set</span> <span class="o">=</span> <span class="n">ranks</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">stride</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">my_rank</span> <span class="ow">in</span> <span class="n">rank_set</span><span class="p">:</span>
                <span class="n">my_ranks</span> <span class="o">=</span> <span class="n">rank_set</span>
        <span class="k">assert</span> <span class="n">my_ranks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;rankset doesn&#39;t include the current node&quot;</span>

    <span class="n">my_ranks</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="n">pg</span> <span class="o">=</span> <span class="n">_find_pg_by_ranks_and_tag</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">my_ranks</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pg</span>
    <span class="k">if</span> <span class="n">tag</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot automatically create PG with empty tag&quot;</span><span class="p">)</span>
    <span class="c1"># TODO copy settings and timeout from default PG</span>
    <span class="k">return</span> <span class="n">_new_group_with_tag</span><span class="p">(</span><span class="n">my_ranks</span><span class="p">,</span> <span class="n">pg_tag</span><span class="o">=</span><span class="n">tag</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_get_group_tag</span><span class="p">(</span><span class="n">pg</span><span class="p">:</span> <span class="n">ProcessGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the tag associated with ``pg``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tag</span> <span class="o">=</span> <span class="n">_world</span><span class="o">.</span><span class="n">pg_to_tag</span><span class="p">[</span><span class="n">pg</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;user:&quot;</span><span class="p">):</span>
        <span class="n">tag</span> <span class="o">=</span> <span class="n">tag</span><span class="p">[</span><span class="mi">5</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">tag</span>


<span class="c1"># This ops are not friently to TorchDynamo. So, we decide to disallow these ops</span>
<span class="c1"># in FX graph, allowing them to run them on eager, with torch.compile.</span>
<span class="n">dynamo_unsupported_distributed_c10d_ops</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">all_reduce_multigpu</span><span class="p">,</span>
    <span class="n">recv</span><span class="p">,</span>
    <span class="n">all_gather_object</span><span class="p">,</span>
    <span class="n">all_gather_coalesced</span><span class="p">,</span>
    <span class="n">all_to_all_single</span><span class="p">,</span>
    <span class="n">all_reduce</span><span class="p">,</span>
    <span class="n">gather_object</span><span class="p">,</span>
    <span class="n">all_to_all</span><span class="p">,</span>
    <span class="n">all_reduce_coalesced</span><span class="p">,</span>
    <span class="n">gather</span><span class="p">,</span>
    <span class="n">broadcast_object_list</span><span class="p">,</span>
    <span class="n">barrier</span><span class="p">,</span>
    <span class="n">reduce_multigpu</span><span class="p">,</span>
    <span class="n">scatter</span><span class="p">,</span>
    <span class="n">scatter_object_list</span><span class="p">,</span>
    <span class="n">reduce</span><span class="p">,</span>
    <span class="n">reduce_scatter_multigpu</span><span class="p">,</span>
    <span class="n">all_gather</span><span class="p">,</span>
    <span class="n">broadcast_multigpu</span><span class="p">,</span>
    <span class="n">all_gather_multigpu</span><span class="p">,</span>
    <span class="n">reduce_scatter</span><span class="p">,</span>
    <span class="n">all_gather_into_tensor</span><span class="p">,</span>
    <span class="n">broadcast</span><span class="p">,</span>
    <span class="n">reduce_scatter_tensor</span><span class="p">,</span>
    <span class="n">send</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.fsdp.fully_sharded_data_parallel &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.fsdp.fully_sharded_data_parallel</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.fsdp.fully_sharded_data_parallel</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">auto</span><span class="p">,</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">Generator</span><span class="p">,</span>
    <span class="n">Iterable</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.distributed.fsdp._traversal_utils</span> <span class="k">as</span> <span class="nn">traversal_utils</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms._checkpoint.checkpoint_wrapper</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_CHECKPOINT_WRAPPED_MODULE</span><span class="p">,</span>
    <span class="n">ActivationWrapper</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.algorithms._comm_hooks</span> <span class="kn">import</span> <span class="n">LOW_PRECISION_HOOKS</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp._common_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_FSDPState</span><span class="p">,</span>
    <span class="n">_get_param_to_fqns</span><span class="p">,</span>
    <span class="n">FSDP_PREFIX</span><span class="p">,</span>
    <span class="n">FSDP_WRAPPED_MODULE</span><span class="p">,</span>
    <span class="n">TrainingState</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp._dynamo_utils</span> <span class="kn">import</span> <span class="n">_annotate_modules_for_dynamo</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp._init_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_check_orig_params_flattened</span><span class="p">,</span>
    <span class="n">_get_default_comm_hook</span><span class="p">,</span>
    <span class="n">_init_buffer_state</span><span class="p">,</span>
    <span class="n">_init_core_state</span><span class="p">,</span>
    <span class="n">_init_device_handle</span><span class="p">,</span>
    <span class="n">_init_ignored_module_states</span><span class="p">,</span>
    <span class="n">_init_param_handle_from_module</span><span class="p">,</span>
    <span class="n">_init_prefetching_state</span><span class="p">,</span>
    <span class="n">_init_process_group_state</span><span class="p">,</span>
    <span class="n">_init_runtime_state</span><span class="p">,</span>
    <span class="n">_init_state_dict_state</span><span class="p">,</span>
    <span class="n">HYBRID_SHARDING_STRATEGIES</span><span class="p">,</span>
    <span class="n">ProcessGroupType</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp._runtime_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_get_fsdp_root_states</span><span class="p">,</span>
    <span class="n">_is_fsdp_root</span><span class="p">,</span>
    <span class="n">_lazy_init</span><span class="p">,</span>
    <span class="n">_post_forward</span><span class="p">,</span>
    <span class="n">_post_forward_reshard</span><span class="p">,</span>
    <span class="n">_pre_forward</span><span class="p">,</span>
    <span class="n">_pre_forward_unshard</span><span class="p">,</span>
    <span class="n">_root_pre_forward</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp._wrap_utils</span> <span class="kn">import</span> <span class="n">_auto_wrap</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.api</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BackwardPrefetch</span><span class="p">,</span>
    <span class="n">CPUOffload</span><span class="p">,</span>
    <span class="n">FullOptimStateDictConfig</span><span class="p">,</span>
    <span class="n">FullStateDictConfig</span><span class="p">,</span>
    <span class="n">LocalOptimStateDictConfig</span><span class="p">,</span>
    <span class="n">LocalStateDictConfig</span><span class="p">,</span>
    <span class="n">MixedPrecision</span><span class="p">,</span>
    <span class="n">OptimStateDictConfig</span><span class="p">,</span>
    <span class="n">ShardedOptimStateDictConfig</span><span class="p">,</span>
    <span class="n">ShardedStateDictConfig</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span><span class="p">,</span>
    <span class="n">StateDictConfig</span><span class="p">,</span>
    <span class="n">StateDictSettings</span><span class="p">,</span>
    <span class="n">StateDictType</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.distributed.utils</span> <span class="kn">import</span> <span class="n">_p_assert</span>

<span class="kn">from</span> <span class="nn">._optim_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_flatten_optim_state_dict</span><span class="p">,</span>
    <span class="n">_get_param_id_to_param_from_optim_input</span><span class="p">,</span>
    <span class="n">_get_param_key_to_param</span><span class="p">,</span>
    <span class="n">_get_param_to_param_id_from_optim_input</span><span class="p">,</span>
    <span class="n">_get_param_to_param_key</span><span class="p">,</span>
    <span class="n">_optim_state_dict</span><span class="p">,</span>
    <span class="n">_rekey_sharded_optim_state_dict</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._state_dict_utils</span> <span class="kn">import</span> <span class="n">_register_all_state_dict_hooks</span>
<span class="kn">from</span> <span class="nn">._unshard_param_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_deregister_orig_params</span><span class="p">,</span>
    <span class="n">_register_flat_param</span><span class="p">,</span>
    <span class="n">_register_orig_params</span><span class="p">,</span>
    <span class="n">_unshard_params</span><span class="p">,</span>
    <span class="n">_unshard_params_recurse</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.flat_param</span> <span class="kn">import</span> <span class="n">FlatParameter</span>
<span class="kn">from</span> <span class="nn">.wrap</span> <span class="kn">import</span> <span class="n">_FSDPPolicy</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">,</span>
    <span class="s2">&quot;OptimStateKeyType&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="n">FLAT_PARAM</span> <span class="o">=</span> <span class="s2">&quot;_flat_param&quot;</span>


<span class="k">class</span> <span class="nc">OptimStateKeyType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">PARAM_NAME</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>
    <span class="n">PARAM_ID</span> <span class="o">=</span> <span class="n">auto</span><span class="p">()</span>


<div class="viewcode-block" id="FullyShardedDataParallel"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel">[docs]</a><span class="k">class</span> <span class="nc">FullyShardedDataParallel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">_FSDPState</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper for sharding module parameters across data parallel workers. This</span>
<span class="sd">    is inspired by `Xu et al.`_ as well as the ZeRO Stage 3 from DeepSpeed_.</span>
<span class="sd">    FullyShardedDataParallel is commonly shortened to FSDP.</span>

<span class="sd">    .. _`Xu et al.`: https://arxiv.org/abs/2004.13336</span>
<span class="sd">    .. _DeepSpeed: https://www.deepspeed.ai/</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(device_id)</span>
<span class="sd">        &gt;&gt;&gt; sharded_module = FSDP(my_module)</span>
<span class="sd">        &gt;&gt;&gt; optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)</span>
<span class="sd">        &gt;&gt;&gt; x = sharded_module(x, y=3, z=torch.Tensor([1]))</span>
<span class="sd">        &gt;&gt;&gt; loss = x.sum()</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt; optim.step()</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The optimizer must be initialized *after* the module has been wrapped</span>
<span class="sd">        with FSDP since FSDP will shard and transform the module&#39;s parameters</span>
<span class="sd">        in a way that may not preserve the original parameter variables. Thus,</span>
<span class="sd">        the previously initialized optimizer may have stale references to the</span>
<span class="sd">        parameters.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the destination CUDA device has ID ``dev_id``, either (1)</span>
<span class="sd">        ``module`` should already be placed on that device, (2) the device</span>
<span class="sd">        should be set using ``torch.cuda.set_device(dev_id)``, or (3)</span>
<span class="sd">        ``dev_id`` should be passed into the ``device_id`` constructor</span>
<span class="sd">        argument. This FSDP instance&#39;s compute device will be that destination</span>
<span class="sd">        device. For (1) and (3), the FSDP initialization always occurs on GPU.</span>
<span class="sd">        For (2), the FSDP initialization happens on ``module`` &#39;s current</span>
<span class="sd">        device, which may be CPU.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        FSDP currently does not support gradient accumulation outside</span>
<span class="sd">        ``no_sync()`` when using CPU offloading. Trying to do so yields</span>
<span class="sd">        incorrect results since FSDP will use the newly-reduced gradient</span>
<span class="sd">        instead of accumulating with any existing gradient.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Changing the original parameter variable names after construction will</span>
<span class="sd">        lead to undefined behavior.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Passing in the ``sync_module_states=True`` flag requires ``module`` to</span>
<span class="sd">        be on GPU or to use the ``device_id`` argument to specify a CUDA device</span>
<span class="sd">        that FSDP will move ``module`` to in the FSDP constructor. This is</span>
<span class="sd">        because ``sync_module_states=True`` requires GPU communication.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        As of PyTorch 1.12, FSDP only offers limited support for shared parameters</span>
<span class="sd">        (for example, setting one ``Linear`` layer&#39;s weight to another&#39;s). In</span>
<span class="sd">        particular, modules that share parameters must be wrapped as part of the</span>
<span class="sd">        same FSDP unit. If enhanced shared parameter support is needed for your</span>
<span class="sd">        use case, please ping https://github.com/pytorch/pytorch/issues/77724</span>

<span class="sd">    .. note:</span>
<span class="sd">        Attempting to run the forward pass of a submodule that is contained in an</span>
<span class="sd">        FSDP instance is not supported and will result in errors. This is because the</span>
<span class="sd">        submodule&#39;s parameters will be sharded, but it itself is not an FSDP instance,</span>
<span class="sd">        so its forward pass will not all-gather the full parameters appropriately.</span>
<span class="sd">        This could potentially happen when attempting to run only the encoder of a</span>
<span class="sd">        encoder-decoder model, and the encoder is not wrapped in its own FSDP instance. To</span>
<span class="sd">        resolve this, please wrap the submodule in its own FSDP unit.</span>

<span class="sd">    .. note::</span>
<span class="sd">        FSDP moves input tensors to the ``forward`` method to the GPU compute</span>
<span class="sd">        device, so the user does not need to manually move them from CPU.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The user should not modify the parameters between forward and backward</span>
<span class="sd">        without using the :meth:`summon_full_params` context since the</span>
<span class="sd">        modifications may not persist. Moreover, for ``use_orig_params=False``,</span>
<span class="sd">        accessing the original parameters between forward and backward may</span>
<span class="sd">        raise an illegal memory access.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        For ``use_orig_params=True``, ``ShardingStrategy.SHARD_GRAD_OP``</span>
<span class="sd">        exposes the unsharded parameters, not the sharded parameters, after</span>
<span class="sd">        forward since it does not free the unsharded ones, unlike</span>
<span class="sd">        ``ShardingStrategy.FULL_SHARD``. One caveat is that, since gradients</span>
<span class="sd">        are always sharded or ``None``, ``ShardingStrategy.SHARD_GRAD_OP`` will</span>
<span class="sd">        not expose the sharded gradients with the unsharded parameters after</span>
<span class="sd">        forward. If you want to inspect the gradients, try</span>
<span class="sd">        :meth:`summon_full_params` with ``with_grads=True``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        FSDP replaces managed modules&#39; parameters with ``torch.Tensor`` views</span>
<span class="sd">        during forward and backward computation for autograd-related reasons.</span>
<span class="sd">        If your module&#39;s forward relies on saved references to the parameters</span>
<span class="sd">        instead of reacquiring the references each iteration, then it will not</span>
<span class="sd">        see FSDP&#39;s newly created views, and autograd will not work correctly.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module):</span>
<span class="sd">            This is the module to be wrapped with FSDP.</span>
<span class="sd">        process_group (Optional[Union[ProcessGroup, Tuple[ProcessGroup, ProcessGroup]]]):</span>
<span class="sd">            This is the process group used for collective communications and</span>
<span class="sd">            the one over which the model is sharded. For hybrid sharding strategies such as</span>
<span class="sd">            ``ShardingStrategy.HYBRID_SHARD`` users can</span>
<span class="sd">            pass in a tuple of process groups representing the groups to shard and replicate across,</span>
<span class="sd">            respectively.</span>
<span class="sd">        sharding_strategy (Optional[ShardingStrategy]):</span>
<span class="sd">            This configures the sharding strategy used by FSDP, which may trade</span>
<span class="sd">            off memory saving and communication overhead. See</span>
<span class="sd">            :class:`ShardingStrategy` for details. (Default: ``FULL_SHARD``)</span>
<span class="sd">        cpu_offload (Optional[CPUOffload]):</span>
<span class="sd">            This configures CPU offloading. If this is set to ``None``, then</span>
<span class="sd">            no CPU offloading happens. See :class:`CPUOffload` for details.</span>
<span class="sd">            (Default: ``None``)</span>
<span class="sd">        auto_wrap_policy (Optional[Union[Callable[[nn.Module, bool, int], bool], _FSDPPolicy]]):</span>
<span class="sd">            This is either ``None``, an ``_FSDPPolicy``, or a callable of</span>
<span class="sd">            a fixed signature. If it is ``None``, then ``module`` is wrapped</span>
<span class="sd">            with only a top-level FSDP instance without any nested wrapping. If</span>
<span class="sd">            it is an ``_FSDPPolicy``, then the wrapping follows the given</span>
<span class="sd">            policy. ``ModuleWrapPolicy`` in ``torch.distributed.fsdp.wrap.py``</span>
<span class="sd">            is an example. If it is a callable, then it should take in three</span>
<span class="sd">            arguments ``module: nn.Module``, ``recurse: bool``, and</span>
<span class="sd">            ``nonwrapped_numel: int`` and should return a ``bool`` specifying</span>
<span class="sd">            whether the passed-in ``module`` should be wrapped if</span>
<span class="sd">            ``recurse=False`` or if the traversal should continue down the</span>
<span class="sd">            subtree if ``recurse=True``. Additional custom arguments may be</span>
<span class="sd">            added to the callable. The ``size_based_auto_wrap_policy`` in</span>
<span class="sd">            ``torch.distributed.fsdp.wrap.py`` gives an example callable that</span>
<span class="sd">            wraps a module if the parameters in its subtree exceed 100M numel.</span>
<span class="sd">            A good practice is to print the model after wrapping and adjust as</span>
<span class="sd">            needed.</span>

<span class="sd">            Example::</span>

<span class="sd">                &gt;&gt;&gt; def custom_auto_wrap_policy(</span>
<span class="sd">                &gt;&gt;&gt;     module: nn.Module,</span>
<span class="sd">                &gt;&gt;&gt;     recurse: bool,</span>
<span class="sd">                &gt;&gt;&gt;     nonwrapped_numel: int,</span>
<span class="sd">                &gt;&gt;&gt;     # Additional custom arguments</span>
<span class="sd">                &gt;&gt;&gt;     min_num_params: int = int(1e8),</span>
<span class="sd">                &gt;&gt;&gt; ) -&gt; bool:</span>
<span class="sd">                &gt;&gt;&gt;     return nonwrapped_numel &gt;= min_num_params</span>
<span class="sd">                &gt;&gt;&gt; # Configure a custom `min_num_params`</span>
<span class="sd">                &gt;&gt;&gt; my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5))</span>

<span class="sd">        backward_prefetch (Optional[BackwardPrefetch]):</span>
<span class="sd">            This configures explicit backward prefetching of all-gathers. See</span>
<span class="sd">            :class:`BackwardPrefetch` for details. (Default: ``BACKWARD_PRE``)</span>
<span class="sd">        mixed_precision (Optional[MixedPrecision]):</span>
<span class="sd">            This configures native mixed precision for FSDP. If this is set to</span>
<span class="sd">            ``None``, then no mixed precision is used. Otherwise, parameter,</span>
<span class="sd">            buffer, and gradient reduction dtypes can be set. See</span>
<span class="sd">            :class:`MixedPrecision` for details. (Default: ``None``)</span>
<span class="sd">        ignored_modules (Optional[Iterable[torch.nn.Module]]): Modules whose</span>
<span class="sd">            own parameters and child modules&#39; parameters and buffers are</span>
<span class="sd">            ignored by this instance. None of the modules directly in</span>
<span class="sd">            ``ignored_modules`` should be :class:`FullyShardedDataParallel`</span>
<span class="sd">            instances, and any child modules that are already-constructed</span>
<span class="sd">            :class:`FullyShardedDataParallel` instances will not be ignored if</span>
<span class="sd">            they are nested under this instance. This argument may be used to</span>
<span class="sd">            avoid sharding specific parameters at module granularity when using an</span>
<span class="sd">            ``auto_wrap_policy`` or if parameters&#39; sharding is not managed by</span>
<span class="sd">            FSDP. (Default: ``None``)</span>
<span class="sd">        param_init_fn (Optional[Callable[[nn.Module], None]]):</span>
<span class="sd">            A ``Callable[torch.nn.Module] -&gt; None`` that</span>
<span class="sd">            specifies how modules that are currently on the meta device should</span>
<span class="sd">            be initialized onto an actual device. As of v1.12, FSDP detects</span>
<span class="sd">            modules with parameters or buffers on meta device via ``is_meta``</span>
<span class="sd">            and either applies ``param_init_fn`` if specified or calls</span>
<span class="sd">            ``nn.Module.reset_parameters()`` otherwise. For both cases, the</span>
<span class="sd">            implementation should *only* initialize the parameters/buffers of</span>
<span class="sd">            the module, not those of its submodules. This is to avoid</span>
<span class="sd">            re-initialization. In addition, FSDP also supports deferred</span>
<span class="sd">            initialization via torchdistX&#39;s (https://github.com/pytorch/torchdistX)</span>
<span class="sd">            ``deferred_init()`` API, where the deferred modules are initialized</span>
<span class="sd">            by calling ``param_init_fn`` if specified or torchdistX&#39;s default</span>
<span class="sd">            ``materialize_module()`` otherwise. If ``param_init_fn`` is</span>
<span class="sd">            specified, then it is applied to all meta-device modules, meaning</span>
<span class="sd">            that it should probably case on the module type. FSDP calls the</span>
<span class="sd">            initialization function before parameter flattening and sharding.</span>

<span class="sd">            Example::</span>

<span class="sd">                &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">                &gt;&gt;&gt; module = MyModule(device=&quot;meta&quot;)</span>
<span class="sd">                &gt;&gt;&gt; def my_init_fn(module: nn.Module):</span>
<span class="sd">                &gt;&gt;&gt;     # E.g. initialize depending on the module type</span>
<span class="sd">                &gt;&gt;&gt;     ...</span>
<span class="sd">                &gt;&gt;&gt; fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)</span>
<span class="sd">                &gt;&gt;&gt; print(next(fsdp_model.parameters()).device) # current CUDA device</span>
<span class="sd">                &gt;&gt;&gt; # With torchdistX</span>
<span class="sd">                &gt;&gt;&gt; module = deferred_init.deferred_init(MyModule, device=&quot;cuda&quot;)</span>
<span class="sd">                &gt;&gt;&gt; # Will initialize via deferred_init.materialize_module().</span>
<span class="sd">                &gt;&gt;&gt; fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)</span>

<span class="sd">        device_id (Optional[Union[int, torch.device]]): An ``int`` or ``torch.device``</span>
<span class="sd">            describing the CUDA device the FSDP module should be moved to determining where</span>
<span class="sd">            initialization such as sharding takes place. If this argument is not specified</span>
<span class="sd">            and ``module`` is on CPU, we issue a warning mentioning that this argument can</span>
<span class="sd">            be specified for faster initialization. If specified, resulting FSDP instances</span>
<span class="sd">            will reside on this device, including moving ignored modules&#39; parameters if</span>
<span class="sd">            needed. Note that if ``device_id`` is specified but ``module`` is already on a</span>
<span class="sd">            different CUDA device, an error will be thrown. (Default: ``None``)</span>
<span class="sd">        sync_module_states (bool): If ``True``, each individually wrapped FSDP unit will broadcast</span>
<span class="sd">            module parameters from rank 0 to ensure they are the same across all ranks after</span>
<span class="sd">            initialization. This helps ensure model parameters are the same across ranks</span>
<span class="sd">            before starting training, but adds communication overhead to ``__init__``, as at least</span>
<span class="sd">            one broadcast is triggered per individually wrapped FSDP unit.</span>
<span class="sd">            This can also help load checkpoints taken by ``state_dict`` and to be loaded by</span>
<span class="sd">            ``load_state_dict`` in a memory efficient way. See documentation for</span>
<span class="sd">            :class:`FullStateDictConfig` for an example of this. (Default: ``False``)</span>
<span class="sd">        forward_prefetch (bool): If ``True``, then FSDP *explicitly* prefetches</span>
<span class="sd">            the next upcoming all-gather while executing in the forward pass.</span>
<span class="sd">            This may improve communication and computation overlap for CPU</span>
<span class="sd">            bound workloads. This should only be used for static graph models</span>
<span class="sd">            since the forward order is fixed based on the first iteration&#39;s</span>
<span class="sd">            execution. (Default: ``False``)</span>
<span class="sd">        limit_all_gathers (bool): If ``False``, then FSDP allows the CPU</span>
<span class="sd">            thread to schedule all-gathers without any extra synchronization.</span>
<span class="sd">            If ``True``, then FSDP explicitly synchronizes the CPU thread to</span>
<span class="sd">            prevent too many in-flight all-gathers. This ``bool`` only affects</span>
<span class="sd">            the sharded strategies that schedule all-gathers. Enabling this can</span>
<span class="sd">            help lower the number of CUDA malloc retries.</span>
<span class="sd">        use_orig_params (bool): Setting this to ``True`` has FSDP use</span>
<span class="sd">            ``module`` &#39;s original parameters. FSDP exposes those original</span>
<span class="sd">            parameters to the user via :meth:`nn.Module.named_parameters`</span>
<span class="sd">            instead of FSDP&#39;s internal :class:`FlatParameter` s. This means</span>
<span class="sd">            that the optimizer step runs on the original parameters, enabling</span>
<span class="sd">            per-original-parameter hyperparameters. FSDP preserves the original</span>
<span class="sd">            parameter variables and manipulates their data between unsharded</span>
<span class="sd">            and sharded forms, where they are always views into the underlying</span>
<span class="sd">            unsharded or sharded :class:`FlatParameter`, respectively. With the</span>
<span class="sd">            current algorithm, the sharded form is always 1D, losing the</span>
<span class="sd">            original tensor structure. An original parameter may have all,</span>
<span class="sd">            some, or none of its data present for a given rank. In the none</span>
<span class="sd">            case, its data will be like a size-0 empty tensor. Users should not</span>
<span class="sd">            author programs relying on what data is present for a given</span>
<span class="sd">            original parameter in its sharded form. ``True`` is required to</span>
<span class="sd">            use ``torch.compile()``. Setting this to ``False`` exposes FSDP&#39;s</span>
<span class="sd">            internal :class:`FlatParameter` s to the user via</span>
<span class="sd">            :meth:`nn.Module.named_parameters`. (Default: ``False``)</span>
<span class="sd">        ignored_states (Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]):</span>
<span class="sd">            Ignored parameters or modules that will not be managed by this FSDP</span>
<span class="sd">            instance, meaning that the parameters are not sharded and their</span>
<span class="sd">            gradients are not reduced across ranks. This argument unifies with</span>
<span class="sd">            the existing ``ignored_modules`` argument, and we may deprecate</span>
<span class="sd">            ``ignored_modules`` soon. For backward compatibility, we keep both</span>
<span class="sd">            ``ignored_states`` and `ignored_modules``, but FSDP only allows one</span>
<span class="sd">            of them to be specified as not ``None``.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">process_group</span><span class="p">:</span> <span class="n">ProcessGroupType</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sharding_strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShardingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cpu_offload</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CPUOffload</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">auto_wrap_policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">_FSDPPolicy</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">backward_prefetch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BackwardPrefetch</span><span class="p">]</span> <span class="o">=</span> <span class="n">BackwardPrefetch</span><span class="o">.</span><span class="n">BACKWARD_PRE</span><span class="p">,</span>
        <span class="n">mixed_precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MixedPrecision</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ignored_modules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">param_init_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sync_module_states</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">forward_prefetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">limit_all_gathers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_orig_params</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">ignored_states</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">Optional</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.distributed.fsdp&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_init_ignored_module_states</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">ignored_modules</span><span class="p">,</span> <span class="n">ignored_states</span><span class="p">)</span>
        <span class="n">_init_device_handle</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_params</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>

        <span class="c1"># Add module annotations for Dynamo support (see function for details)</span>
        <span class="n">_annotate_modules_for_dynamo</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_modules</span><span class="p">,</span> <span class="n">use_orig_params</span><span class="p">)</span>

        <span class="c1"># Initializes self.process_group, along with rank and world size. This will</span>
        <span class="c1"># also set another attribute, _inter_node_pg, to control the process group</span>
        <span class="c1"># over which sharding occurs, if sharding_strategy is {HYBRID_SHARD, _HYBRID_SHARD_ZERO2}.</span>
        <span class="c1"># Note that this is done before auto_wrapping, so that child FSDP modules simply pick up</span>
        <span class="c1"># the same process group state as the root FSDP module.</span>
        <span class="n">_init_process_group_state</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">process_group</span><span class="p">,</span> <span class="n">sharding_strategy</span><span class="p">,</span> <span class="n">auto_wrap_policy</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">auto_wrap_policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">auto_wrap_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;module&quot;</span><span class="p">:</span> <span class="n">module</span><span class="p">,</span>
                <span class="s2">&quot;auto_wrap_policy&quot;</span><span class="p">:</span> <span class="n">auto_wrap_policy</span><span class="p">,</span>
                <span class="s2">&quot;wrapper_cls&quot;</span><span class="p">:</span> <span class="n">FullyShardedDataParallel</span><span class="p">,</span>
                <span class="s2">&quot;ignored_modules&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_modules</span><span class="p">,</span>
                <span class="s2">&quot;ignored_params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_params</span><span class="p">,</span>
                <span class="s2">&quot;only_wrap_children&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># avoid double wrapping the root</span>
            <span class="p">}</span>
            <span class="n">fsdp_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;process_group&quot;</span><span class="p">:</span> <span class="n">process_group</span><span class="p">,</span>
                <span class="s2">&quot;sharding_strategy&quot;</span><span class="p">:</span> <span class="n">sharding_strategy</span><span class="p">,</span>
                <span class="s2">&quot;cpu_offload&quot;</span><span class="p">:</span> <span class="n">cpu_offload</span><span class="p">,</span>
                <span class="s2">&quot;backward_prefetch&quot;</span><span class="p">:</span> <span class="n">backward_prefetch</span><span class="p">,</span>
                <span class="s2">&quot;mixed_precision&quot;</span><span class="p">:</span> <span class="n">mixed_precision</span><span class="p">,</span>
                <span class="s2">&quot;param_init_fn&quot;</span><span class="p">:</span> <span class="n">param_init_fn</span><span class="p">,</span>
                <span class="s2">&quot;device_id&quot;</span><span class="p">:</span> <span class="n">device_id</span><span class="p">,</span>
                <span class="s2">&quot;sync_module_states&quot;</span><span class="p">:</span> <span class="n">sync_module_states</span><span class="p">,</span>
                <span class="s2">&quot;forward_prefetch&quot;</span><span class="p">:</span> <span class="n">forward_prefetch</span><span class="p">,</span>
                <span class="s2">&quot;limit_all_gathers&quot;</span><span class="p">:</span> <span class="n">limit_all_gathers</span><span class="p">,</span>
                <span class="s2">&quot;use_orig_params&quot;</span><span class="p">:</span> <span class="n">use_orig_params</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">if</span> <span class="n">sharding_strategy</span> <span class="ow">in</span> <span class="n">HYBRID_SHARDING_STRATEGIES</span><span class="p">:</span>
                <span class="c1"># Share root process groups with children to maintain</span>
                <span class="c1"># the invariant that all FSDP modules will have the same</span>
                <span class="c1"># process groups.</span>
                <span class="n">fsdp_kwargs</span><span class="p">[</span><span class="s2">&quot;process_group&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inter_node_pg</span><span class="p">)</span>

            <span class="n">_auto_wrap</span><span class="p">(</span><span class="n">auto_wrap_kwargs</span><span class="p">,</span> <span class="n">fsdp_kwargs</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">)</span>

        <span class="n">backward_prefetch_limit</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">forward_prefetch_limit</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">_init_core_state</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">sharding_strategy</span><span class="p">,</span>
            <span class="n">mixed_precision</span><span class="p">,</span>
            <span class="n">cpu_offload</span><span class="p">,</span>
            <span class="n">limit_all_gathers</span><span class="p">,</span>
            <span class="n">use_orig_params</span><span class="p">,</span>
            <span class="n">backward_prefetch_limit</span><span class="p">,</span>
            <span class="n">forward_prefetch_limit</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">_init_runtime_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">_init_prefetching_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backward_prefetch</span><span class="p">,</span> <span class="n">forward_prefetch</span><span class="p">)</span>
        <span class="n">_init_buffer_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
        <span class="n">_init_param_handle_from_module</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">module</span><span class="p">,</span>
            <span class="n">device_id</span><span class="p">,</span>
            <span class="n">param_init_fn</span><span class="p">,</span>
            <span class="n">sync_module_states</span><span class="p">,</span>
            <span class="n">FullyShardedDataParallel</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">use_orig_params</span><span class="p">:</span>
            <span class="n">_check_orig_params_flattened</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ignored_params</span><span class="p">)</span>
            <span class="n">_register_flat_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

        <span class="c1"># `_state_dict_type` controls the `state_dict()` behavior, which is</span>
        <span class="c1"># implemented using post-save and pre-load hooks</span>
        <span class="n">_init_state_dict_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">_register_all_state_dict_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">module</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the wrapped module (like :class:`DistributedDataParallel`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># FSDP&#39;s `.module` must refer to the innermost wrapped module when</span>
        <span class="c1"># composing with other module wrappers in order for state dict to work</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">ActivationWrapper</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">_CHECKPOINT_WRAPPED_MODULE</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_has_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Returns whether this FSDP instance manages any parameters.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_handles&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_flat_param</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FlatParameter</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flat_param</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Forward missing attributes to the wrapped module.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>  <span class="c1"># defer to nn.Module&#39;s logic</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Forward indexing calls in case the module is an ``nn.Sequential``.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">FSDP_WRAPPED_MODULE</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_is_root</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_is_fsdp_root</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.fsdp_modules"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">fsdp_modules</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">root_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns all nested FSDP instances, possibly including ``module`` itself</span>
<span class="sd">        and only including FSDP root modules if ``root_only=True``.</span>

<span class="sd">        Args:</span>
<span class="sd">            module (torch.nn.Module): Root module, which may or may not be an</span>
<span class="sd">                ``FSDP`` module.</span>
<span class="sd">            root_only (bool): Whether to return only FSDP root modules.</span>
<span class="sd">                (Default: ``False``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[FullyShardedDataParallel]: FSDP modules that are nested in</span>
<span class="sd">            the input ``module``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">root_only</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_get_fsdp_root_states</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">traversal_utils</span><span class="o">.</span><span class="n">_get_fsdp_states</span><span class="p">(</span><span class="n">module</span><span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.apply"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.apply">[docs]</a>    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;FullyShardedDataParallel&quot;</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies ``fn`` recursively to every submodule (as returned by ``.children()``)</span>
<span class="sd">        as well as self. Typical use includes initializing the parameters of a model</span>
<span class="sd">        (see also :ref:`nn-init-doc`).</span>

<span class="sd">        Compared to ``torch.nn.Module.apply``, this version additionally gathers</span>
<span class="sd">        the full parameters before applying ``fn``. It should not be called from</span>
<span class="sd">        within another ``summon_full_params`` context.</span>

<span class="sd">        Args:</span>
<span class="sd">            fn (:class:`Module` -&gt; None): function to be applied to each submodule</span>

<span class="sd">        Returns:</span>
<span class="sd">            Module: self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">uninitialized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="c1"># Use `_unshard_params_recurse()` with `recurse=False` instead of</span>
        <span class="c1"># `_unshard_fsdp_state_params()` directly to perform lazy</span>
        <span class="c1"># initialization, which is needed to initialize `FlatParameter`</span>
        <span class="c1"># parameter attributes as required by the unshard logic</span>
        <span class="k">with</span> <span class="n">_unshard_params_recurse</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">recurse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">writeback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">with_grads</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

        <span class="c1"># Reset lazy init called in `_unshard_params_recurse()` since `apply()`</span>
        <span class="c1"># may have been called on FSDP instance that is not truly a root, in</span>
        <span class="c1"># which case it will be incorrectly marked as one.</span>
        <span class="k">if</span> <span class="n">uninitialized</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">traversal_utils</span><span class="o">.</span><span class="n">_get_fsdp_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_reset_lazy_init</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">ret</span></div>

    <span class="k">def</span> <span class="nf">_mixed_precision_enabled_for_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns if the user explicitly enabled buffer mixed precision.</span>

<span class="sd">        NOTE: Unlike parameters and gradient reduction, buffer mixed precision</span>
<span class="sd">        is applied at the FSDP instance level, not the ``FlatParameter`` level,</span>
<span class="sd">        which may be different for the composable code path.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">buffer_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_low_precision_hook_enabled</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Whether a low precision hook is registered or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="ow">in</span> <span class="n">LOW_PRECISION_HOOKS</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_reset_lazy_init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reset instance so :func:`_lazy_init` will run on the next forward.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="FullyShardedDataParallel.set_state_dict_type"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">set_state_dict_type</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">state_dict_type</span><span class="p">:</span> <span class="n">StateDictType</span><span class="p">,</span>
        <span class="n">state_dict_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StateDictConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim_state_dict_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">OptimStateDictConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">StateDictSettings</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the ``state_dict_type`` and the corresponding (optional)</span>
<span class="sd">        configurations of all the descendant FSDP modules of the target module.</span>
<span class="sd">        The target module does not have to be a FSDP module. If the target</span>
<span class="sd">        module is a FSDP module, its ``state_dict_type`` will also be changed.</span>

<span class="sd">        .. note:: This API should be called for only the top-level (root)</span>
<span class="sd">            module.</span>

<span class="sd">        .. note:: This API enables users to transparently use the conventional</span>
<span class="sd">            ``state_dict`` API to take model checkpoints in cases where the</span>
<span class="sd">            root FSDP module is wrapped by another ``nn.Module``. For example,</span>
<span class="sd">            the following will ensure ``state_dict`` is called on all non-FSDP</span>
<span class="sd">            instances, while dispatching into `sharded_state_dict` implementation</span>
<span class="sd">            for FSDP:</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = DDP(FSDP(...))</span>
<span class="sd">            &gt;&gt;&gt; FSDP.set_state_dict_type(</span>
<span class="sd">            &gt;&gt;&gt;     model,</span>
<span class="sd">            &gt;&gt;&gt;     StateDictType.SHARDED_STATE_DICT,</span>
<span class="sd">            &gt;&gt;&gt;     state_dict_config = ShardedStateDictConfig(offload_to_cpu=True),</span>
<span class="sd">            &gt;&gt;&gt;     optim_state_dict_config = OptimStateDictConfig(offload_to_cpu=True),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; param_state_dict = model.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; optim_state_dict = FSDP.optim_state_dict(model, optim)</span>

<span class="sd">        Args:</span>
<span class="sd">            module (torch.nn.Module): Root module.</span>
<span class="sd">            state_dict_type (StateDictType): the desired ``state_dict_type`` to set.</span>
<span class="sd">            state_dict_config (Optional[StateDictConfig]): the configuration for the</span>
<span class="sd">                target ``state_dict_type``.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A StateDictSettings that include the previous state_dict type and</span>
<span class="sd">            configuration for the module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_state_dict_type_to_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="n">FullStateDictConfig</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="n">LocalStateDictConfig</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="n">ShardedStateDictConfig</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">_optim_state_dict_type_to_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">:</span> <span class="n">FullOptimStateDictConfig</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">LOCAL_STATE_DICT</span><span class="p">:</span> <span class="n">LocalOptimStateDictConfig</span><span class="p">,</span>
            <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">:</span> <span class="n">ShardedOptimStateDictConfig</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Use the default config if a state_dict config is not set.</span>
        <span class="n">state_dict_config_type</span> <span class="o">=</span> <span class="n">_state_dict_type_to_config</span><span class="p">[</span><span class="n">state_dict_type</span><span class="p">]</span>
        <span class="n">optim_state_dict_config_type</span> <span class="o">=</span> <span class="n">_optim_state_dict_type_to_config</span><span class="p">[</span><span class="n">state_dict_type</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">state_dict_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict_config</span> <span class="o">=</span> <span class="n">state_dict_config_type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">optim_state_dict_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optim_state_dict_config</span> <span class="o">=</span> <span class="n">optim_state_dict_config_type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">state_dict_config_type</span> <span class="o">!=</span> <span class="nb">type</span><span class="p">(</span><span class="n">state_dict_config</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected state_dict_config of type </span><span class="si">{</span><span class="n">state_dict_config_type</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">state_dict_config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">optim_state_dict_config_type</span> <span class="o">!=</span> <span class="nb">type</span><span class="p">(</span><span class="n">optim_state_dict_config</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected optim_state_dict_config of type </span><span class="si">{</span><span class="n">optim_state_dict_config_type</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">optim_state_dict_config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Set the state_dict type and configurations.</span>
        <span class="n">prev_state_dict_type</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">prev_state_dict_config</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">prev_optim_state_dict_config</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">traversal_utils</span><span class="o">.</span><span class="n">_get_fsdp_states</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">prev_state_dict_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prev_state_dict_type</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">prev_state_dict_type</span> <span class="o">==</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span>
                <span class="p">),</span> <span class="s2">&quot;All FSDP modules should have the same state_dict_type.&quot;</span>
            <span class="k">if</span> <span class="n">prev_state_dict_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prev_state_dict_config</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">prev_state_dict_config</span><span class="p">)</span>
                <span class="p">),</span> <span class="s2">&quot;All FSDP modules must have the same type of state_dict_config.&quot;</span>
            <span class="k">if</span> <span class="n">prev_optim_state_dict_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prev_optim_state_dict_config</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_optim_state_dict_config</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">submodule</span><span class="o">.</span><span class="n">_optim_state_dict_config</span><span class="p">,</span>
                    <span class="nb">type</span><span class="p">(</span><span class="n">prev_optim_state_dict_config</span><span class="p">),</span>
                <span class="p">),</span> <span class="s2">&quot;All FSDP modules must have the same type of optim_state_dict_config.&quot;</span>

            <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span> <span class="o">=</span> <span class="n">state_dict_type</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span> <span class="o">=</span> <span class="n">state_dict_config</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_optim_state_dict_config</span> <span class="o">=</span> <span class="n">optim_state_dict_config</span>

        <span class="k">return</span> <span class="n">StateDictSettings</span><span class="p">(</span>
            <span class="n">prev_state_dict_type</span><span class="p">,</span> <span class="n">prev_state_dict_config</span><span class="p">,</span> <span class="n">prev_optim_state_dict_config</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.get_state_dict_type"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_state_dict_type</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">StateDictSettings</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the state_dict_type and the corresponding configurations</span>
<span class="sd">        for the FSDP modules rooted at ``module``. The target module</span>
<span class="sd">        does not have to be an FSDP module.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A ``StateDictSettings`` containing the state_dict_type and</span>
<span class="sd">            state_dict / optim_state_dict configs that are currently set.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ``AssertionError`` if the ``StateDictSettings`` for different</span>
<span class="sd">            FSDP submodules differ.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StateDictSettings</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">state_dict_settings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">state_dict_settings</span> <span class="o">=</span> <span class="n">StateDictSettings</span><span class="p">(</span>
                    <span class="n">state_dict_type</span><span class="o">=</span><span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">,</span>
                    <span class="n">state_dict_config</span><span class="o">=</span><span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span><span class="p">,</span>
                    <span class="n">optim_state_dict_config</span><span class="o">=</span><span class="n">submodule</span><span class="o">.</span><span class="n">_optim_state_dict_config</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">submodule_settings</span> <span class="o">=</span> <span class="n">StateDictSettings</span><span class="p">(</span>
                    <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_type</span><span class="p">,</span>
                    <span class="n">submodule</span><span class="o">.</span><span class="n">_state_dict_config</span><span class="p">,</span>
                    <span class="n">submodule</span><span class="o">.</span><span class="n">_optim_state_dict_config</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">assert</span> <span class="n">state_dict_settings</span> <span class="o">==</span> <span class="n">submodule_settings</span><span class="p">,</span> <span class="p">(</span>
                    <span class="s2">&quot;All FSDP modules must have the same state dict settings.&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">submodule_settings</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">state_dict_settings</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">state_dict_settings</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.state_dict_type"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">state_dict_type</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">state_dict_type</span><span class="p">:</span> <span class="n">StateDictType</span><span class="p">,</span>
        <span class="n">state_dict_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StateDictConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim_state_dict_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">OptimStateDictConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to set the ``state_dict_type`` of all the descendant</span>
<span class="sd">        FSDP modules of the target module. This context manager has the same</span>
<span class="sd">        functions as :meth:`set_state_dict_type`. Read the document of</span>
<span class="sd">        :meth:`set_state_dict_type` for the detail.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; model = DDP(FSDP(...))</span>
<span class="sd">            &gt;&gt;&gt; with FSDP.state_dict_type(</span>
<span class="sd">            &gt;&gt;&gt;     model,</span>
<span class="sd">            &gt;&gt;&gt;     StateDictType.SHARDED_STATE_DICT,</span>
<span class="sd">            &gt;&gt;&gt; ):</span>
<span class="sd">            &gt;&gt;&gt;     checkpoint = model.state_dict()</span>

<span class="sd">        Args:</span>
<span class="sd">            module (torch.nn.Module): Root module.</span>
<span class="sd">            state_dict_type (StateDictType): the desired ``state_dict_type`` to set.</span>
<span class="sd">            state_dict_config (Optional[StateDictConfig]): the model ``state_dict``</span>
<span class="sd">                configuration for the target ``state_dict_type``.</span>
<span class="sd">            optim_state_dict_config (Optional[OptimStateDictConfig]): the optimizer</span>
<span class="sd">               ``state_dict`` configuration for the target ``state_dict_type``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prev_state_dict_settings</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span>
            <span class="n">state_dict_type</span><span class="p">,</span>
            <span class="n">state_dict_config</span><span class="p">,</span>
            <span class="n">optim_state_dict_config</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">yield</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span>
            <span class="n">prev_state_dict_settings</span><span class="o">.</span><span class="n">state_dict_type</span><span class="p">,</span>
            <span class="n">prev_state_dict_settings</span><span class="o">.</span><span class="n">state_dict_config</span><span class="p">,</span>
            <span class="n">prev_state_dict_settings</span><span class="o">.</span><span class="n">optim_state_dict_config</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.forward"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runs the forward pass for the wrapped module, inserting FSDP-specific</span>
<span class="sd">        pre- and post-forward sharding logic.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span>
            <span class="s2">&quot;FullyShardedDataParallel.forward&quot;</span>
        <span class="p">):</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">_root_pre_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
            <span class="n">unused</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">unshard_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_pre_forward_unshard</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">)</span>
            <span class="n">reshard_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_post_forward_reshard</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">)</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">_pre_forward</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">,</span> <span class="n">unshard_fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">:</span>
                <span class="n">_p_assert</span><span class="p">(</span>
                    <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="p">,</span>
                    <span class="s2">&quot;Expected `FlatParameter` to be on the compute device &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span><span class="si">}</span><span class="s2"> but got </span><span class="si">{</span><span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fsdp_wrapped_module</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">_post_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handles</span><span class="p">,</span> <span class="n">reshard_fn</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">unused</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.summon_full_params"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">summon_full_params</span><span class="p">(</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">writeback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">offload_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A context manager to expose full params for FSDP instances.</span>
<span class="sd">        Can be useful *after* forward/backward for a model to get</span>
<span class="sd">        the params for additional processing or checking. It can take a non-FSDP</span>
<span class="sd">        module and will summon full params for all contained FSDP modules as</span>
<span class="sd">        well as their children, depending on the ``recurse`` argument.</span>

<span class="sd">        .. note:: This can be used on inner FSDPs.</span>
<span class="sd">        .. note:: This can *not* be used within a forward or backward pass. Nor</span>
<span class="sd">            can forward and backward be started from within this context.</span>
<span class="sd">        .. note:: Parameters will revert to their local shards after the context</span>
<span class="sd">            manager exits, storage behavior is the same as forward.</span>
<span class="sd">        .. note:: The full parameters can be modified, but only the portion</span>
<span class="sd">            corresponding to the local param shard will persist after the</span>
<span class="sd">            context manager exits (unless ``writeback=False``, in which case</span>
<span class="sd">            changes will be discarded). In the case where FSDP does not shard</span>
<span class="sd">            the parameters, currently only when ``world_size == 1``, or ``NO_SHARD``</span>
<span class="sd">            config, the modification is persisted regardless of ``writeback``.</span>
<span class="sd">        .. note:: This method works on modules which are not FSDP themselves but</span>
<span class="sd">            may contain multiple independent FSDP units. In that case, the given</span>
<span class="sd">            arguments will apply to all contained FSDP units.</span>

<span class="sd">        .. warning:: Note that ``rank0_only=True`` in conjunction with</span>
<span class="sd">            ``writeback=True`` is not currently supported and will raise an</span>
<span class="sd">            error. This is because model parameter shapes would be different</span>
<span class="sd">            across ranks within the context, and writing to them can lead to</span>
<span class="sd">            inconsistency across ranks when the context is exited.</span>

<span class="sd">        .. warning:: Note that ``offload_to_cpu`` and ``rank0_only=False`` will</span>
<span class="sd">            result in full parameters being redundantly copied to CPU memory for</span>
<span class="sd">            GPUs that reside on the same machine, which may incur the risk of</span>
<span class="sd">            CPU OOM. It is recommended to use ``offload_to_cpu`` with</span>
<span class="sd">            ``rank0_only=True``.</span>

<span class="sd">        Args:</span>
<span class="sd">            recurse (bool, Optional): recursively summon all params for nested</span>
<span class="sd">                FSDP instances (default: True).</span>
<span class="sd">            writeback (bool, Optional): if ``False``, modifications to params are</span>
<span class="sd">                discarded after the context manager exits;</span>
<span class="sd">                disabling this can be slightly more efficient (default: True)</span>
<span class="sd">            rank0_only (bool, Optional): if ``True``, full parameters are</span>
<span class="sd">                materialized on only global rank 0. This means that within the</span>
<span class="sd">                context, only rank 0 will have full parameters and the other</span>
<span class="sd">                ranks will have sharded parameters. Note that setting</span>
<span class="sd">                ``rank0_only=True`` with ``writeback=True`` is not supported,</span>
<span class="sd">                as model parameter shapes will be different across ranks</span>
<span class="sd">                within the context, and writing to them can lead to</span>
<span class="sd">                inconsistency across ranks when the context is exited.</span>
<span class="sd">            offload_to_cpu (bool, Optional): If ``True``, full parameters are</span>
<span class="sd">                offloaded to CPU. Note that this offloading currently only</span>
<span class="sd">                occurs if the parameter is sharded (which is only not the case</span>
<span class="sd">                for world_size = 1 or ``NO_SHARD`` config). It is recommended</span>
<span class="sd">                to use ``offload_to_cpu`` with ``rank0_only=True`` to avoid</span>
<span class="sd">                redundant copies of model parameters being offloaded to the same CPU memory.</span>
<span class="sd">            with_grads (bool, Optional): If ``True``, gradients are also</span>
<span class="sd">                unsharded with the parameters. Currently, this is only</span>
<span class="sd">                supported when passing ``use_orig_params=True`` to the FSDP</span>
<span class="sd">                constructor and ``offload_to_cpu=False`` to this method.</span>
<span class="sd">                (Default: ``False``)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">_unshard_params</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span> <span class="n">recurse</span><span class="p">,</span> <span class="n">writeback</span><span class="p">,</span> <span class="n">rank0_only</span><span class="p">,</span> <span class="n">offload_to_cpu</span><span class="p">,</span> <span class="n">with_grads</span>
        <span class="p">):</span>
            <span class="k">yield</span></div>

    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">_deregister_orig_params_ctx</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This deregisters the original parameters and exposes the</span>
<span class="sd">        :class:`FlatParameter` s. If a :class:`FlatParameter` is sharded, then</span>
<span class="sd">        this refreshes the sharded views before exiting. This method should</span>
<span class="sd">        only be called when using the original parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_p_assert</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_use_orig_params</span><span class="p">,</span>
            <span class="s2">&quot;`_deregister_orig_params_ctx()` should only be called when &quot;</span>
            <span class="s2">&quot;`_use_orig_params=True`&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">fsdp_module</span> <span class="ow">in</span> <span class="n">traversal_utils</span><span class="o">.</span><span class="n">_get_fsdp_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">_deregister_orig_params</span><span class="p">(</span><span class="n">fsdp_module</span><span class="p">,</span> <span class="n">fsdp_module</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">fsdp_module</span> <span class="ow">in</span> <span class="n">traversal_utils</span><span class="o">.</span><span class="n">_get_fsdp_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="n">_register_orig_params</span><span class="p">(</span><span class="n">fsdp_module</span><span class="p">,</span> <span class="n">fsdp_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        When using the original parameters, this deregisters the original</span>
<span class="sd">        parameters and exposes the :class:`FlatParameter` s before calling</span>
<span class="sd">        ``_apply()``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># When using the original parameters: Since (1) the `FlatParameter`s</span>
        <span class="c1"># own the storage and (2) `_apply()` is the subroutine underlying the</span>
        <span class="c1"># most common storage-changing ops like `to()` and `cuda()`, we</span>
        <span class="c1"># override `_apply()` to have the storage change directly performed on</span>
        <span class="c1"># the `FlatParameter`s instead of applying to the original parameters</span>
        <span class="c1"># and then writing back to the `FlatParameter`s.</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_deregister_orig_params_ctx</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_orig_params</span>
            <span class="k">else</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="n">context</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.named_buffers"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers">[docs]</a>    <span class="k">def</span> <span class="nf">named_buffers</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overrides :meth:`named_buffers()` to intercept buffer names and</span>
<span class="sd">        remove all occurrences of the FSDP-specific flattened buffer prefix</span>
<span class="sd">        when inside the :meth:`summon_full_params` context manager.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">should_clean_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">==</span> <span class="n">TrainingState</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>
        <span class="k">for</span> <span class="n">buffer_name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">should_clean_name</span><span class="p">:</span>
                <span class="c1"># Remove any instances of the FSDP-specific prefix; there can</span>
                <span class="c1"># be multiple in the case of nested FSDP modules</span>
                <span class="n">buffer_name</span> <span class="o">=</span> <span class="n">buffer_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FSDP_PREFIX</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">buffer_name</span><span class="p">,</span> <span class="n">buffer</span><span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.named_parameters"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Overrides :meth:`named_parameters()` to intercept parameter names and</span>
<span class="sd">        remove all occurrences of the FSDP-specific flattened parameter prefix</span>
<span class="sd">        when inside the :meth:`summon_full_params` context manager.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">should_clean_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="o">==</span> <span class="n">TrainingState</span><span class="o">.</span><span class="n">SUMMON_FULL_PARAMS</span>
        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">should_clean_name</span><span class="p">:</span>
                <span class="c1"># Remove any instances of the FSDP-specific prefix; there can</span>
                <span class="c1"># be multiple in the case of nested FSDP modules</span>
                <span class="n">param_name</span> <span class="o">=</span> <span class="n">param_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">FSDP_PREFIX</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_assert_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TrainingState</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TrainingState</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Assert we are in the given state.&quot;&quot;&quot;</span>
        <span class="c1"># Since assert can be turned off and this error checking</span>
        <span class="c1"># is really important, we use explicit error checking</span>
        <span class="c1"># and raise a ValueError if needed.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">TrainingState</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_state</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;expected to be in states </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2"> but current state &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">training_state</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="c1"># In case we are failing in the context of autograd hook, asserting</span>
            <span class="c1"># may not generate useful msg. So, let&#39;s print it to be sure.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Asserting FSDP instance is: </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ERROR: </span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">traceback</span><span class="o">.</span><span class="n">print_stack</span><span class="p">()</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.no_sync"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.no_sync">[docs]</a>    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">no_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A context manager to disable gradient synchronizations across FSDP</span>
<span class="sd">        instances. Within this context, gradients will be accumulated in module</span>
<span class="sd">        variables, which will later be synchronized in the first</span>
<span class="sd">        forward-backward pass after exiting the context. This should only be</span>
<span class="sd">        used on the root FSDP instance and will recursively apply to all</span>
<span class="sd">        children FSDP instances.</span>

<span class="sd">        .. note:: This likely results in higher memory usage because FSDP will</span>
<span class="sd">            accumulate the full model gradients (instead of gradient shards)</span>
<span class="sd">            until the eventual sync.</span>

<span class="sd">        .. note:: When used with CPU offloading, the gradients will not be</span>
<span class="sd">            offloaded to CPU when inside the context manager. Instead, they</span>
<span class="sd">            will only be offloaded right after the eventual sync.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_lazy_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;`no_sync()` on inner FSDP instances is not supported. Please call `no_sync()` on root FSDP module.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="n">old_flags</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">FullyShardedDataParallel</span><span class="p">):</span>
                <span class="n">old_flags</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">_sync_gradients</span><span class="p">))</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_sync_gradients</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">old_flag</span> <span class="ow">in</span> <span class="n">old_flags</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">m</span><span class="o">.</span><span class="n">_sync_gradients</span><span class="p">,</span> <span class="p">(</span>
                    <span class="s2">&quot;`_sync_gradients` was incorrectly set to &quot;</span>
                    <span class="s2">&quot;`True` while in the `no_sync()` context manager&quot;</span>
                <span class="p">)</span>
                <span class="n">m</span><span class="o">.</span><span class="n">_sync_gradients</span> <span class="o">=</span> <span class="n">old_flag</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.clip_grad_norm_"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">clip_grad_norm_</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clips the gradient norm of all parameters. The norm is computed over</span>
<span class="sd">        all parameters&#39; gradients as viewed as a single vector, and the</span>
<span class="sd">        gradients are modified in-place.</span>

<span class="sd">        Args:</span>
<span class="sd">            max_norm (float or int): max norm of the gradients</span>
<span class="sd">            norm_type (float or int): type of the used p-norm. Can be ``&#39;inf&#39;``</span>
<span class="sd">                for infinity norm.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Total norm of the parameters (viewed as a single vector).</span>

<span class="sd">        .. note:: If every FSDP instance uses ``NO_SHARD``, meaning that no</span>
<span class="sd">            gradients are sharded across ranks, then you may directly use</span>
<span class="sd">            :func:`torch.nn.utils.clip_grad_norm_`.</span>

<span class="sd">        .. note:: If at least some FSDP instance uses a sharded strategy (i.e.</span>
<span class="sd">            one other than ``NO_SHARD``), then you should use this method</span>
<span class="sd">            instead of :func:`torch.nn.utils.clip_grad_norm_` since this method</span>
<span class="sd">            handles the fact that gradients are sharded across ranks.</span>

<span class="sd">        .. note:: The total norm returned will have the &quot;largest&quot; dtype across</span>
<span class="sd">            all parameters/gradients as defined by PyTorch&#39;s type promotion</span>
<span class="sd">            semantics. For example, if *all* parameters/gradients use a low</span>
<span class="sd">            precision dtype, then the returned norm&#39;s dtype will be that low</span>
<span class="sd">            precision dtype, but if there exists at least one parameter/</span>
<span class="sd">            gradient using FP32, then the returned norm&#39;s dtype will be FP32.</span>

<span class="sd">        .. warning:: This needs to be called on all ranks since it uses</span>
<span class="sd">            collective communications.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_lazy_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_root</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;`clip_grad_norm_()` should only be called on the root FSDP instance&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_state</span><span class="p">(</span><span class="n">TrainingState</span><span class="o">.</span><span class="n">IDLE</span><span class="p">)</span>
        <span class="c1"># If every FSDP instance uses `NO_SHARD`, then we can directly use</span>
        <span class="c1"># the normal `nn.utils` one targeting local gradients</span>
        <span class="n">all_no_shard</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="ow">not</span> <span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span> <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_handles</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">all_no_shard</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span>
            <span class="p">)</span>
        <span class="c1"># Otherwise, there exists some FSDP instance using a sharded strategy,</span>
        <span class="c1"># where sharded and non-sharded parameters must be handled separately</span>
        <span class="n">max_norm</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">max_norm</span><span class="p">)</span>
        <span class="n">norm_type</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">norm_type</span><span class="p">)</span>
        <span class="n">sharded_params</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">nonsharded_params</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>  <span class="c1"># `NO_SHARD` or not FSDP-managed</span>
        <span class="n">grads</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">handle</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_all_handles</span><span class="p">:</span>
            <span class="n">target_set</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">sharded_params</span> <span class="k">if</span> <span class="n">handle</span><span class="o">.</span><span class="n">uses_sharded_strategy</span> <span class="k">else</span> <span class="n">nonsharded_params</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">handle</span><span class="o">.</span><span class="n">_use_orig_params</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">_params</span><span class="p">:</span>
                    <span class="n">target_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">target_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">flat_param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">not_fsdp_managed</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sharded_params</span> <span class="ow">and</span> <span class="n">param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">nonsharded_params</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">not_fsdp_managed</span><span class="p">:</span>
                <span class="n">nonsharded_params</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="c1"># Compute local norms (forced to be in FP32)</span>
        <span class="n">local_sharded_norm</span> <span class="o">=</span> <span class="n">_get_grad_norm</span><span class="p">(</span><span class="n">sharded_params</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span>
        <span class="p">)</span>
        <span class="n">local_nonsharded_norm</span> <span class="o">=</span> <span class="n">_get_grad_norm</span><span class="p">(</span><span class="n">nonsharded_params</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_device</span>
        <span class="p">)</span>
        <span class="c1"># Reconstruct the total gradient norm depending on the norm type</span>
        <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">local_sharded_norm</span><span class="p">,</span> <span class="n">local_nonsharded_norm</span><span class="p">)</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span>
                <span class="n">total_norm</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">local_sharded_norm</span><span class="o">**</span><span class="n">norm_type</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">total_norm</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">)</span>
            <span class="c1"># All-reducing the local non-sharded norm would count it an extra</span>
            <span class="c1"># world-size-many times</span>
            <span class="n">total_norm</span> <span class="o">+=</span> <span class="n">local_nonsharded_norm</span><span class="o">**</span><span class="n">norm_type</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">total_norm</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu_offload</span><span class="o">.</span><span class="n">offload_params</span><span class="p">:</span>
            <span class="n">total_norm</span> <span class="o">=</span> <span class="n">total_norm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="n">clip_coef</span> <span class="o">=</span> <span class="n">max_norm</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="c1"># Multiplying by the clamped coefficient is meaningless when it is</span>
        <span class="c1"># equal to 1, but it avoids the host-device sync that would result from</span>
        <span class="c1"># `if clip_coef &lt; 1`</span>
        <span class="n">clip_coef_clamped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">clip_coef</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">clip_coef_clamped</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="c1"># Use the &quot;largest&quot; dtype by type promotion semantics to use the same</span>
        <span class="c1"># dtype as if we did not force local norm computation to be in FP32</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># If this rank has no gradients, then we must default to FP32</span>
            <span class="c1"># unless we use additional communication, which we prefer to avoid</span>
            <span class="c1"># since `clip_grad_norm_()` is called in the training loop</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Called FSDP.clip_grad_norm_() on rank </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2"> with no &quot;</span>
                <span class="s2">&quot;gradients -- returning the total norm in the default dtype &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total_norm</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>  <span class="c1"># warn since this is generally unexpected</span>
            <span class="k">return</span> <span class="n">total_norm</span>
        <span class="n">total_norm_dtype</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">promote_types</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">),</span>
            <span class="p">[</span><span class="n">grad</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">total_norm</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">total_norm_dtype</span><span class="p">)</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">optim_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The `optim_input` argument is deprecated and will be removed after PyTorch 1.13. You may remove it &quot;</span>
                <span class="s2">&quot;from your code without changing its functionality.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_is_using_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">optim_input</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">optim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use the default behavior of `optim_input``</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">optim_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use the `optim_input` code path</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="c1"># Use the `optim` code path</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_warn_legacy_optim_state_dict</span><span class="p">(</span><span class="n">curr</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">new</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;``FullyShardedDataParallel.</span><span class="si">{</span><span class="n">curr</span><span class="si">}</span><span class="s2">``is being deprecated and is &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;replaced by ``FullyShardedDataParallel.</span><span class="si">{</span><span class="n">new</span><span class="si">}</span><span class="s2">``. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;``FullyShardedDataParallel.</span><span class="si">{</span><span class="n">curr</span><span class="si">}</span><span class="s2">`` may be removed after PyTorch 2.2.&quot;</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_optim_state_dict_impl</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
                <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">full_state_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The internal API that is used by all the optim_state_dict implementations.</span>
<span class="sd">        Given model, optim, the original optim_state_dict, this API removes the</span>
<span class="sd">        FSDP internal information and internal sharding from the optim_state_dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">full_state_dict</span><span class="p">:</span>
            <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
            <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
                <span class="n">optim_input</span><span class="p">,</span>
                <span class="n">optim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">using_optim_input</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">assert</span> <span class="n">optim_input</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">rank0_only</span>

        <span class="n">use_orig_params</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">model</span><span class="p">)[</span>
            <span class="mi">0</span>
        <span class="p">]</span><span class="o">.</span><span class="n">_use_orig_params</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">use_orig_params</span> <span class="o">==</span> <span class="n">m</span><span class="o">.</span><span class="n">_use_orig_params</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;Not all FSDP modules have the same _use_orig_params value&quot;</span>

        <span class="k">return</span> <span class="n">_optim_state_dict</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">optim_state_dict</span><span class="o">=</span><span class="n">optim_state_dict</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="n">optim_input</span><span class="p">,</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span><span class="p">,</span>
            <span class="n">shard_state</span><span class="o">=</span><span class="ow">not</span> <span class="n">full_state_dict</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
            <span class="n">using_optim_input</span><span class="o">=</span><span class="n">using_optim_input</span><span class="p">,</span>
            <span class="n">use_orig_params</span><span class="o">=</span><span class="n">use_orig_params</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_optim_state_dict_to_load_impl</span><span class="p">(</span>
        <span class="n">optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
                <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">full_state_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">is_named_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The internal API that is used by all the load optim_state_dict implementations.</span>
<span class="sd">        Given model, optim, and the saved optim_state_dict, this API adds the FSDP</span>
<span class="sd">        internal information and internal sharding to the optim_state_dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">full_state_dict</span><span class="p">:</span>
            <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
            <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
                <span class="n">optim_input</span><span class="p">,</span>
                <span class="n">optim</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">using_optim_input</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">assert</span> <span class="n">optim_input</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">rank0_only</span>

        <span class="n">use_orig_params</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">model</span><span class="p">)[</span>
            <span class="mi">0</span>
        <span class="p">]</span><span class="o">.</span><span class="n">_use_orig_params</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">use_orig_params</span> <span class="o">==</span> <span class="n">m</span><span class="o">.</span><span class="n">_use_orig_params</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">fsdp_modules</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="p">),</span> <span class="s2">&quot;Not all FSDP modules have the same _use_orig_params value&quot;</span>

        <span class="k">if</span> <span class="n">rank0_only</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="n">group</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">optim_state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">sharded_osd</span> <span class="o">=</span> <span class="n">_flatten_optim_state_dict</span><span class="p">(</span>
            <span class="n">optim_state_dict</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">use_orig_params</span><span class="o">=</span><span class="n">use_orig_params</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="p">(</span><span class="n">optim</span> <span class="k">if</span> <span class="n">is_named_optimizer</span> <span class="k">else</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_rekey_sharded_optim_state_dict</span><span class="p">(</span>
            <span class="n">sharded_osd</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="n">optim_input</span><span class="p">,</span>
            <span class="n">using_optim_input</span><span class="o">=</span><span class="n">using_optim_input</span><span class="p">,</span>
            <span class="n">is_named_optimizer</span><span class="o">=</span><span class="n">is_named_optimizer</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="FullyShardedDataParallel.full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">full_optim_state_dict</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
                <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rank0_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Consolidates the full optimizer state on rank 0 and returns it</span>
<span class="sd">        as a :class:`dict` following the convention of</span>
<span class="sd">        :meth:`torch.optim.Optimizer.state_dict`, i.e. with keys ``&quot;state&quot;``</span>
<span class="sd">        and ``&quot;param_groups&quot;``. The flattened parameters in ``FSDP`` modules</span>
<span class="sd">        contained in ``model`` are mapped back to their unflattened parameters.</span>

<span class="sd">        .. warning:: This needs to be called on all ranks since it uses</span>
<span class="sd">            collective communications. However, if ``rank0_only=True``, then</span>
<span class="sd">            the state dict is only populated on rank 0, and all other ranks</span>
<span class="sd">            return an empty :class:`dict`.</span>

<span class="sd">        .. warning:: Unlike ``torch.optim.Optimizer.state_dict()``, this method</span>
<span class="sd">            uses full parameter names as keys instead of parameter IDs.</span>

<span class="sd">        .. note:: Like in :meth:`torch.optim.Optimizer.state_dict`, the tensors</span>
<span class="sd">            contained in the optimizer state dict are not cloned, so there may</span>
<span class="sd">            be aliasing surprises. For best practices, consider saving the</span>
<span class="sd">            returned optimizer state dict immediately, e.g. using</span>
<span class="sd">            ``torch.save()``.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                were passed into the optimizer ``optim``.</span>
<span class="sd">            optim (torch.optim.Optimizer): Optimizer for ``model`` &#39;s</span>
<span class="sd">                parameters.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer ``optim`` representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``. This argument is deprecated, and there</span>
<span class="sd">                is no need to pass it in anymore. (Default: ``None``)</span>
<span class="sd">            rank0_only (bool): If ``True``, saves the populated :class:`dict`</span>
<span class="sd">                only on rank 0; if ``False``, saves it on all ranks. (Default:</span>
<span class="sd">                ``True``)</span>
<span class="sd">            group (dist.ProcessGroup): Model&#39;s process group or ``None`` if using</span>
<span class="sd">                the default process group. (Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: A :class:`dict` containing the optimizer state for</span>
<span class="sd">            ``model`` &#39;s original unflattened parameters and including keys</span>
<span class="sd">            &quot;state&quot; and &quot;param_groups&quot; following the convention of</span>
<span class="sd">            :meth:`torch.optim.Optimizer.state_dict`. If ``rank0_only=True``,</span>
<span class="sd">            then nonzero ranks return an empty :class:`dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_legacy_optim_state_dict</span><span class="p">(</span>
            <span class="s2">&quot;full_optim_state_dict&quot;</span><span class="p">,</span> <span class="s2">&quot;optim_state_dict&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_optim_state_dict_impl</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">optim_state_dict</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="n">optim_input</span><span class="p">,</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="n">rank0_only</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
            <span class="n">full_state_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.sharded_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">sharded_optim_state_dict</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The API is similar to :meth:`full_optim_state_dict` but this API chunks</span>
<span class="sd">        all non-zero-dimension states to :class:`ShardedTensor` to save memory.</span>
<span class="sd">        This API should only be used when the model ``state_dict`` is derived</span>
<span class="sd">        with the context manager ``with state_dict_type(SHARDED_STATE_DICT):``.</span>

<span class="sd">        For the detailed usage, refer to :meth:`full_optim_state_dict`.</span>

<span class="sd">        .. warning:: The returned state dict contains ``ShardedTensor`` and</span>
<span class="sd">            cannot be directly used by the regular ``optim.load_state_dict``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_legacy_optim_state_dict</span><span class="p">(</span>
            <span class="s2">&quot;sharded_optim_state_dict&quot;</span><span class="p">,</span> <span class="s2">&quot;optim_state_dict&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_optim_state_dict_impl</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">optim_state_dict</span><span class="o">=</span><span class="n">optim</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">full_state_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.shard_full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">shard_full_optim_state_dict</span><span class="p">(</span>
        <span class="n">full_optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
                <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Shards the full optimizer state dict ``full_optim_state_dict`` by</span>
<span class="sd">        remapping the state to flattened parameters instead of unflattened</span>
<span class="sd">        parameters and restricting to only this rank&#39;s part of the optimizer</span>
<span class="sd">        state. The first argument should be the return value of</span>
<span class="sd">        :meth:`full_optim_state_dict`.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(model, optim)</span>
<span class="sd">            &gt;&gt;&gt; torch.save(full_osd, PATH)</span>
<span class="sd">            &gt;&gt;&gt; # Define new model with possibly different world size</span>
<span class="sd">            &gt;&gt;&gt; new_model, new_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = torch.load(PATH)</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)</span>
<span class="sd">            &gt;&gt;&gt; new_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        .. note:: Both :meth:`shard_full_optim_state_dict` and</span>
<span class="sd">            :meth:`scatter_full_optim_state_dict` may be used to get the</span>
<span class="sd">            sharded optimizer state dict to load. Assuming that the full</span>
<span class="sd">            optimizer state dict resides in CPU memory, the former requires</span>
<span class="sd">            each rank to have the full dict in CPU memory, where each rank</span>
<span class="sd">            individually shards the dict without any communication, while the</span>
<span class="sd">            latter requires only rank 0 to have the full dict in CPU memory,</span>
<span class="sd">            where rank 0 moves each shard to GPU memory (for NCCL) and</span>
<span class="sd">            communicates it to ranks appropriately. Hence, the former has</span>
<span class="sd">            higher aggregate CPU memory cost, while the latter has higher</span>
<span class="sd">            communication cost.</span>

<span class="sd">        Args:</span>
<span class="sd">            full_optim_state_dict (Dict[str, Any]): Optimizer state dict</span>
<span class="sd">                corresponding to the unflattened parameters and holding the</span>
<span class="sd">                full non-sharded optimizer state.</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                correspond to the optimizer state in ``full_optim_state_dict``.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``. This argument is deprecated, and there</span>
<span class="sd">                is no need to pass it in anymore. (Default: ``None``)</span>
<span class="sd">            optim (Optional[torch.optim.Optimizer]): Optimizer that will load</span>
<span class="sd">                the state dict returned by this method. This is the preferred</span>
<span class="sd">                argument to use over ``optim_input``. (Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The full optimizer state dict now remapped to</span>
<span class="sd">            flattened parameters instead of unflattened parameters and</span>
<span class="sd">            restricted to only include this rank&#39;s part of the optimizer state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_legacy_optim_state_dict</span><span class="p">(</span>
            <span class="s2">&quot;shard_full_optim_state_dict&quot;</span><span class="p">,</span> <span class="s2">&quot;optim_state_dict_to_load&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_optim_state_dict_to_load_impl</span><span class="p">(</span>
            <span class="n">optim_state_dict</span><span class="o">=</span><span class="n">full_optim_state_dict</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="n">optim_input</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">full_state_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">is_named_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.flatten_sharded_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">flatten_sharded_optim_state_dict</span><span class="p">(</span>
        <span class="n">sharded_optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The API is similar to :meth:`shard_full_optim_state_dict`. The only</span>
<span class="sd">        difference is that the input ``sharded_optim_state_dict`` should be</span>
<span class="sd">        returned from :meth:`sharded_optim_state_dict`. Therefore, there will</span>
<span class="sd">        be all-gather calls on each rank to gather ``ShardedTensor`` s.</span>

<span class="sd">        Args:</span>
<span class="sd">            sharded_optim_state_dict (Dict[str, Any]): Optimizer state dict</span>
<span class="sd">                corresponding to the unflattened parameters and holding the</span>
<span class="sd">                sharded optimizer state.</span>
<span class="sd">            model (torch.nn.Module):</span>
<span class="sd">                Refer to :meth:`shard_full_optim_state_dict`.</span>
<span class="sd">            optim (torch.optim.Optimizer): Optimizer for ``model`` &#39;s</span>
<span class="sd">                parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Refer to :meth:`shard_full_optim_state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_legacy_optim_state_dict</span><span class="p">(</span>
            <span class="s2">&quot;flatten_sharded_optim_state_dict&quot;</span><span class="p">,</span> <span class="s2">&quot;optim_state_dict_to_load&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_optim_state_dict_to_load_impl</span><span class="p">(</span>
            <span class="n">optim_state_dict</span><span class="o">=</span><span class="n">sharded_optim_state_dict</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">full_state_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">is_named_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.scatter_full_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">scatter_full_optim_state_dict</span><span class="p">(</span>
        <span class="n">full_optim_state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
                <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Scatters the full optimizer state dict from rank 0 to all other ranks,</span>
<span class="sd">        returning the sharded optimizer state dict on each rank. The return</span>
<span class="sd">        value is the same as :meth:`shard_full_optim_state_dict`, and on rank</span>
<span class="sd">        0, the first argument should be the return value of</span>
<span class="sd">        :meth:`full_optim_state_dict`.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0</span>
<span class="sd">            &gt;&gt;&gt; # Define new model with possibly different world size</span>
<span class="sd">            &gt;&gt;&gt; new_model, new_optim, new_group = ...</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)</span>
<span class="sd">            &gt;&gt;&gt; new_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        .. note:: Both :meth:`shard_full_optim_state_dict` and</span>
<span class="sd">            :meth:`scatter_full_optim_state_dict` may be used to get the</span>
<span class="sd">            sharded optimizer state dict to load. Assuming that the full</span>
<span class="sd">            optimizer state dict resides in CPU memory, the former requires</span>
<span class="sd">            each rank to have the full dict in CPU memory, where each rank</span>
<span class="sd">            individually shards the dict without any communication, while the</span>
<span class="sd">            latter requires only rank 0 to have the full dict in CPU memory,</span>
<span class="sd">            where rank 0 moves each shard to GPU memory (for NCCL) and</span>
<span class="sd">            communicates it to ranks appropriately. Hence, the former has</span>
<span class="sd">            higher aggregate CPU memory cost, while the latter has higher</span>
<span class="sd">            communication cost.</span>

<span class="sd">        Args:</span>
<span class="sd">            full_optim_state_dict (Optional[Dict[str, Any]]): Optimizer state</span>
<span class="sd">                dict corresponding to the unflattened parameters and holding</span>
<span class="sd">                the full non-sharded optimizer state if on rank 0; the argument</span>
<span class="sd">                is ignored on nonzero ranks.</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                correspond to the optimizer state in ``full_optim_state_dict``.</span>
<span class="sd">            optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]):</span>
<span class="sd">                Input passed into the optimizer representing either a</span>
<span class="sd">                :class:`list` of parameter groups or an iterable of parameters;</span>
<span class="sd">                if ``None``, then this method assumes the input was</span>
<span class="sd">                ``model.parameters()``. This argument is deprecated, and there</span>
<span class="sd">                is no need to pass it in anymore. (Default: ``None``)</span>
<span class="sd">            optim (Optional[torch.optim.Optimizer]): Optimizer that will load</span>
<span class="sd">                the state dict returned by this method. This is the preferred</span>
<span class="sd">                argument to use over ``optim_input``. (Default: ``None``)</span>
<span class="sd">            group (dist.ProcessGroup): Model&#39;s process group or ``None`` if</span>
<span class="sd">                using the default process group. (Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The full optimizer state dict now remapped to</span>
<span class="sd">            flattened parameters instead of unflattened parameters and</span>
<span class="sd">            restricted to only include this rank&#39;s part of the optimizer state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_legacy_optim_state_dict</span><span class="p">(</span>
            <span class="s2">&quot;scatter_full_optim_state_dict&quot;</span><span class="p">,</span> <span class="s2">&quot;optim_state_dict_to_load&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_optim_state_dict_to_load_impl</span><span class="p">(</span>
            <span class="n">optim_state_dict</span><span class="o">=</span><span class="n">full_optim_state_dict</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="n">optim_input</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">full_state_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">is_named_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.rekey_optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">rekey_optim_state_dict</span><span class="p">(</span>
        <span class="n">optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">optim_state_key_type</span><span class="p">:</span> <span class="n">OptimStateKeyType</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim_input</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
                <span class="n">Iterable</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Re-keys the optimizer state dict ``optim_state_dict`` to use the key</span>
<span class="sd">        type ``optim_state_key_type``. This can be used to achieve</span>
<span class="sd">        compatibility between optimizer state dicts from models with FSDP</span>
<span class="sd">        instances and ones without.</span>

<span class="sd">        To re-key an FSDP full optimizer state dict (i.e. from</span>
<span class="sd">        :meth:`full_optim_state_dict`) to use parameter IDs and be loadable to</span>
<span class="sd">        a non-wrapped model::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; wrapped_model, wrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)</span>
<span class="sd">            &gt;&gt;&gt; nonwrapped_model, nonwrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; nonwrapped_optim.load_state_dict(rekeyed_osd)</span>

<span class="sd">        To re-key a normal optimizer state dict from a non-wrapped model to be</span>
<span class="sd">        loadable to a wrapped model::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; nonwrapped_model, nonwrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; osd = nonwrapped_optim.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; wrapped_model, wrapped_optim = ...</span>
<span class="sd">            &gt;&gt;&gt; sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)</span>
<span class="sd">            &gt;&gt;&gt; wrapped_optim.load_state_dict(sharded_osd)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: The optimizer state dict re-keyed using the</span>
<span class="sd">            parameter keys specified by ``optim_state_key_type``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_warn_optim_input</span><span class="p">(</span><span class="n">optim_input</span><span class="p">)</span>
        <span class="n">using_optim_input</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_is_using_optim_input</span><span class="p">(</span>
            <span class="n">optim_input</span><span class="p">,</span>
            <span class="n">optim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">optim_state_key_type</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span><span class="p">,</span>
            <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">osd</span> <span class="o">=</span> <span class="n">optim_state_dict</span>  <span class="c1"># alias</span>
        <span class="c1"># Validate that the existing parameter keys are uniformly typed</span>
        <span class="n">uses_param_name_mask</span> <span class="o">=</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">param_key</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span> <span class="k">for</span> <span class="n">param_key</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]]</span>
        <span class="n">uses_param_id_mask</span> <span class="o">=</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">param_key</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span> <span class="k">for</span> <span class="n">param_key</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]]</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">any</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">any</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">error_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Invalid parameter keys: </span><span class="si">{</span><span class="n">osd</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">error_msg</span><span class="p">)</span>
        <span class="c1"># Return directly if the existing key type matches the target key type</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_name_mask</span><span class="p">)</span>
        <span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span>
            <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="n">uses_param_id_mask</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="n">osd</span>
        <span class="c1"># Otherwise, actually perform the re-keying</span>
        <span class="n">new_osd</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span><span class="p">:</span>  <span class="c1"># ID -&gt; name</span>
            <span class="n">param_id_to_param</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_get_param_id_to_param_from_optim_input</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">using_optim_input</span>
                <span class="k">else</span> <span class="n">_get_param_key_to_param</span><span class="p">(</span><span class="n">optim</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="n">_get_param_to_fqn</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">param_id_to_param_name</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">param_to_param_name</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_id_to_param</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">]</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_id_to_param_name</span><span class="p">[</span><span class="n">param_id</span><span class="p">]:</span> <span class="n">param_state</span>
                <span class="k">for</span> <span class="n">param_id</span><span class="p">,</span> <span class="n">param_state</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">param_id_to_param_name</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">param_id</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_osd</span>
        <span class="k">elif</span> <span class="n">optim_state_key_type</span> <span class="o">==</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span><span class="p">:</span>  <span class="c1"># name -&gt; ID</span>
            <span class="n">param_name_to_param</span> <span class="o">=</span> <span class="n">_get_fqn_to_param</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">param_to_param_id</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">_get_param_to_param_id_from_optim_input</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim_input</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">using_optim_input</span>
                <span class="k">else</span> <span class="n">_get_param_to_param_key</span><span class="p">(</span><span class="n">optim</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># Because not all model parameters may be passed as the optimizer</span>
            <span class="c1"># input, we may need to drop some parameters from this mapping</span>
            <span class="n">param_name_to_param_id</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_name</span><span class="p">:</span> <span class="n">param_to_param_id</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_name_to_param</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_to_param_id</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">param_name_to_param_id</span><span class="p">[</span><span class="n">param_name</span><span class="p">]:</span> <span class="n">param_state</span>
                <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_state</span> <span class="ow">in</span> <span class="n">osd</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">new_osd</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">param_name_to_param_id</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">new_osd</span>
        <span class="k">return</span> <span class="n">new_osd</span>  <span class="c1"># should never reach here</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.optim_state_dict"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">optim_state_dict</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optim_state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transforms the state_dict of ``optim`` for the ``model`` that is sharded</span>
<span class="sd">        by FSDP to one of the three types: 1) full optimizer state_dict, 2)</span>
<span class="sd">        sharded optimizer state_dict, 3) local optimizer state_dict.</span>

<span class="sd">        For full optimizer state_dict, all states are unflattened and not sharded.</span>
<span class="sd">        Rank0 only and CPU only can be specified via :meth:`state_dict_type` to</span>
<span class="sd">        avoid OOM.</span>

<span class="sd">        For sharded optimizer state_dict, all states are unflattend but sharded.</span>
<span class="sd">        CPU only can be specified via :meth:`state_dict_type` to further save</span>
<span class="sd">        memory.</span>

<span class="sd">        For local state_dict, no transformation will be performed. But a state</span>
<span class="sd">        will be converted from nn.Tensor to ShardedTensor to represent its sharding</span>
<span class="sd">        nature (this is not supported yet).</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import StateDictType</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullStateDictConfig</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullOptimStateDictConfig</span>
<span class="sd">            &gt;&gt;&gt; # Save a checkpoint</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; FSDP.set_state_dict_type(</span>
<span class="sd">            &gt;&gt;&gt;     model,</span>
<span class="sd">            &gt;&gt;&gt;     StateDictType.FULL_STATE_DICT,</span>
<span class="sd">            &gt;&gt;&gt;     FullStateDictConfig(rank0_only=False),</span>
<span class="sd">            &gt;&gt;&gt;     FullOptimStateDictConfig(rank0_only=False),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; state_dict = model.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; optim_state_dict = FSDP.optim_state_dict(model, optim)</span>
<span class="sd">            &gt;&gt;&gt; save_a_checkpoint(state_dict, optim_state_dict)</span>
<span class="sd">            &gt;&gt;&gt; # Load a checkpoint</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; state_dict, optim_state_dict = load_a_checkpoint()</span>
<span class="sd">            &gt;&gt;&gt; FSDP.set_state_dict_type(</span>
<span class="sd">            &gt;&gt;&gt;     model,</span>
<span class="sd">            &gt;&gt;&gt;     StateDictType.FULL_STATE_DICT,</span>
<span class="sd">            &gt;&gt;&gt;     FullStateDictConfig(rank0_only=False),</span>
<span class="sd">            &gt;&gt;&gt;     FullOptimStateDictConfig(rank0_only=False),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; model.load_state_dict(state_dict)</span>
<span class="sd">            &gt;&gt;&gt; optim_state_dict = FSDP.optim_state_dict_to_load(</span>
<span class="sd">            &gt;&gt;&gt;     optim_state_dict, model, optim</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; optim.load_state_dict(optim_state_dict)</span>

<span class="sd">        Args:</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                were passed into the optimizer ``optim``.</span>
<span class="sd">            optim (torch.optim.Optimizer): Optimizer for ``model`` &#39;s</span>
<span class="sd">                parameters.</span>
<span class="sd">            optim_state_dict (Dict[str, Any]): the target optimizer state_dict to</span>
<span class="sd">                transform. If the value is None, optim.state_dict() will be used. (</span>
<span class="sd">                Default: ``None``)</span>
<span class="sd">            group (dist.ProcessGroup): Model&#39;s process group across which parameters</span>
<span class="sd">                are sharded or ``None`` if using the default process group. (</span>
<span class="sd">                Default: ``None``)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Any]: A :class:`dict` containing the optimizer state for</span>
<span class="sd">            ``model``. The sharding of the optimizer state is based on</span>
<span class="sd">            ``state_dict_type``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict_settings</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">get_state_dict_type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">optim_state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_optim_state_dict_impl</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">optim_state_dict</span><span class="o">=</span><span class="n">optim_state_dict</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span>
                <span class="n">state_dict_settings</span><span class="o">.</span><span class="n">optim_state_dict_config</span><span class="p">,</span> <span class="s2">&quot;rank0_only&quot;</span><span class="p">,</span> <span class="kc">False</span>
            <span class="p">),</span>
            <span class="n">full_state_dict</span><span class="o">=</span><span class="n">state_dict_settings</span><span class="o">.</span><span class="n">state_dict_type</span>
            <span class="o">==</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.optim_state_dict_to_load"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">optim_state_dict_to_load</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">optim</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">optim_state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">is_named_optimizer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">load_directly</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a ``optim_state_dict`` that is transformed through</span>
<span class="sd">        :meth:`optim_state_dict`, converts it to the flattened optimizer</span>
<span class="sd">        state_dict that can be loaded to ``optim`` which is the optimizer for</span>
<span class="sd">        ``model``.  ``model`` must be sharded by FullyShardedDataParallel.</span>

<span class="sd">            &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullyShardedDataParallel as FSDP</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import StateDictType</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullStateDictConfig</span>
<span class="sd">            &gt;&gt;&gt; from torch.distributed.fsdp import FullOptimStateDictConfig</span>
<span class="sd">            &gt;&gt;&gt; # Save a checkpoint</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; FSDP.set_state_dict_type(</span>
<span class="sd">            &gt;&gt;&gt;     model,</span>
<span class="sd">            &gt;&gt;&gt;     StateDictType.FULL_STATE_DICT,</span>
<span class="sd">            &gt;&gt;&gt;     FullStateDictConfig(rank0_only=False),</span>
<span class="sd">            &gt;&gt;&gt;     FullOptimStateDictConfig(rank0_only=False),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; state_dict = model.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; original_osd = optim.state_dict()</span>
<span class="sd">            &gt;&gt;&gt; optim_state_dict = FSDP.optim_state_dict(</span>
<span class="sd">            &gt;&gt;&gt;     model,</span>
<span class="sd">            &gt;&gt;&gt;     optim,</span>
<span class="sd">            &gt;&gt;&gt;     optim_state_dict=original_osd</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; save_a_checkpoint(state_dict, optim_state_dict)</span>
<span class="sd">            &gt;&gt;&gt; # Load a checkpoint</span>
<span class="sd">            &gt;&gt;&gt; model, optim = ...</span>
<span class="sd">            &gt;&gt;&gt; state_dict, optim_state_dict = load_a_checkpoint()</span>
<span class="sd">            &gt;&gt;&gt; FSDP.set_state_dict_type(</span>
<span class="sd">            &gt;&gt;&gt;     model,</span>
<span class="sd">            &gt;&gt;&gt;     StateDictType.FULL_STATE_DICT,</span>
<span class="sd">            &gt;&gt;&gt;     FullStateDictConfig(rank0_only=False),</span>
<span class="sd">            &gt;&gt;&gt;     FullOptimStateDictConfig(rank0_only=False),</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; model.load_state_dict(state_dict)</span>
<span class="sd">            &gt;&gt;&gt; optim_state_dict = FSDP.optim_state_dict_to_load(</span>
<span class="sd">            &gt;&gt;&gt;     optim_state_dict, model, optim</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; optim.load_state_dict(optim_state_dict)</span>

<span class="sd">        Args:</span>
<span class="sd">            model (torch.nn.Module): Root module (which may or may not be a</span>
<span class="sd">                :class:`FullyShardedDataParallel` instance) whose parameters</span>
<span class="sd">                were passed into the optimizer ``optim``.</span>
<span class="sd">            optim (torch.optim.Optimizer): Optimizer for ``model`` &#39;s</span>
<span class="sd">                parameters.</span>
<span class="sd">            optim_state_dict (Dict[str, Any]): The optimizer states to be loaded.</span>
<span class="sd">            is_named_optimizer (bool): Is this optimizer a NamedOptimizer or</span>
<span class="sd">                KeyedOptimizer. Only set to True if ``optim`` is TorchRec&#39;s</span>
<span class="sd">                KeyedOptimizer or torch.distributed&#39;s NamedOptimizer.</span>
<span class="sd">            load_directly (bool): If this is set to True, this API will also</span>
<span class="sd">                call optim.load_state_dict(result) before returning the result.</span>
<span class="sd">                Otherwise, users are responsible to call ``optim.load_state_dict()``</span>
<span class="sd">                (Default: ``False``)</span>
<span class="sd">            group (dist.ProcessGroup): Model&#39;s process group across which parameters</span>
<span class="sd">                are sharded or ``None`` if using the default process group. (</span>
<span class="sd">                Default: ``None``)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict_settings</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">get_state_dict_type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">FullyShardedDataParallel</span><span class="o">.</span><span class="n">_optim_state_dict_to_load_impl</span><span class="p">(</span>
            <span class="n">optim_state_dict</span><span class="o">=</span><span class="n">optim_state_dict</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">optim_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">optim</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span>
            <span class="n">full_state_dict</span><span class="o">=</span><span class="p">(</span>
                <span class="n">state_dict_settings</span><span class="o">.</span><span class="n">state_dict_type</span> <span class="o">==</span> <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span>
            <span class="p">),</span>
            <span class="n">rank0_only</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span>
                <span class="n">state_dict_settings</span><span class="o">.</span><span class="n">optim_state_dict_config</span><span class="p">,</span> <span class="s2">&quot;rank0_only&quot;</span><span class="p">,</span> <span class="kc">False</span>
            <span class="p">),</span>
            <span class="n">is_named_optimizer</span><span class="o">=</span><span class="n">is_named_optimizer</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">load_directly</span><span class="p">:</span>
            <span class="n">optim</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span></div>

<div class="viewcode-block" id="FullyShardedDataParallel.register_comm_hook"><a class="viewcode-back" href="../../../../fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook">[docs]</a>    <span class="k">def</span> <span class="nf">register_comm_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">callable</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Registers a communication hook which is an enhancement that provides a</span>
<span class="sd">        flexible hook to users where they can specify how FSDP aggregates gradients</span>
<span class="sd">        across multiple workers.</span>
<span class="sd">        This hook can be used to implement several algorithms like</span>
<span class="sd">        `GossipGrad &lt;https://arxiv.org/abs/1803.05880&gt;`_ and gradient compression</span>
<span class="sd">        which involve different communication strategies for</span>
<span class="sd">        parameter syncs while training with :class:`FullyShardedDataParallel`.</span>

<span class="sd">        .. warning ::</span>
<span class="sd">            FSDP communication hook should be registered before running an initial forward pass</span>
<span class="sd">            and only once.</span>

<span class="sd">        Args:</span>
<span class="sd">            state (object): Passed to the hook to maintain any state information during the training process.</span>
<span class="sd">                            Examples include error feedback in gradient compression,</span>
<span class="sd">                            peers to communicate with next in `GossipGrad &lt;https://arxiv.org/abs/1803.05880&gt;`_, etc.</span>
<span class="sd">                            It is locally stored by each worker</span>
<span class="sd">                            and shared by all the gradient tensors on the worker.</span>
<span class="sd">            hook (Callable): Callable, which has one of the following signatures:</span>
<span class="sd">                            1) ``hook: Callable[torch.Tensor] -&gt; None``:</span>
<span class="sd">                            This function takes in a Python tensor, which represents</span>
<span class="sd">                            the full, flattened, unsharded gradient with respect to all variables</span>
<span class="sd">                            corresponding to the model this FSDP unit is wrapping</span>
<span class="sd">                            (that are not wrapped by other FSDP sub-units).</span>
<span class="sd">                            It then performs all necessary processing and returns ``None``;</span>
<span class="sd">                            2) ``hook: Callable[torch.Tensor, torch.Tensor] -&gt; None``:</span>
<span class="sd">                            This function takes in two Python tensors, the first one represents</span>
<span class="sd">                            the full, flattened, unsharded gradient with respect to all variables</span>
<span class="sd">                            corresponding to the model this FSDP unit is wrapping</span>
<span class="sd">                            (that are not wrapped by other FSDP sub-units). The latter</span>
<span class="sd">                            represents a pre-sized tensor to store a chunk of a sharded gradient after</span>
<span class="sd">                            reduction.</span>
<span class="sd">                            In both cases, callable performs all necessary processing and returns ``None``.</span>
<span class="sd">                            Callables with signature 1 are expected to handle gradient communication for a `NO_SHARD` case.</span>
<span class="sd">                            Callables with signature 2 are expected to handle gradient communication for sharded cases.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_is_root</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;register_comm_hook can only be called on a root instance.&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">traversal_utils</span><span class="o">.</span><span class="n">_get_fsdp_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_hook_registered</span>
            <span class="p">),</span> <span class="s2">&quot;communication hook can be only registered once&quot;</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_hook_registered</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">assert</span> <span class="n">submodule</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="o">==</span> <span class="n">_get_default_comm_hook</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sharding_strategy</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;communication hook should be default, but it is </span><span class="si">{</span><span class="n">submodule</span><span class="o">.</span><span class="n">_communication_hook</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> instead&quot;</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_communication_hook_state</span> <span class="o">=</span> <span class="n">state</span>
            <span class="n">submodule</span><span class="o">.</span><span class="n">_communication_hook</span> <span class="o">=</span> <span class="n">hook</span></div></div>


<span class="k">def</span> <span class="nf">_get_grad_norm</span><span class="p">(</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the gradient norm of parameters ``param`` s, where the gradients</span>
<span class="sd">    are viewed as a single vector. The returned norm is in FP32 even if</span>
<span class="sd">    parameters/gradients are in a low precision. This is because the downstream</span>
<span class="sd">    use of this return value is a reduction across ranks.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params_with_grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_with_grad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params_with_grad</span><span class="p">]</span>
    <span class="n">grad_dtypes</span> <span class="o">=</span> <span class="p">{</span><span class="n">grad</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">grad_dtypes</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Requires uniform dtype across all gradients but got </span><span class="si">{</span><span class="n">grad_dtypes</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="c1"># Compute the gradient norm in FP32, where we treat the gradients as a</span>
    <span class="c1"># single vector</span>
    <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">vector_norm</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">grads</span>
            <span class="p">],</span>
        <span class="p">),</span>
        <span class="n">norm_type</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_norm</span>


<span class="k">def</span> <span class="nf">_get_param_to_fqn</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a mapping from parameters to their parameter names. ``model``</span>
<span class="sd">    should not contain any :class:`FullyShardedDataParallel` instances, which</span>
<span class="sd">    means that none of the parameters should be ``FlatParameter`` s. As a</span>
<span class="sd">    result, compared to :meth:`_get_param_to_fqns`, the mapped</span>
<span class="sd">    values may be flattened from singleton :class:`list` s to the contained</span>
<span class="sd">    names themselves.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): Root module, which should not contain any</span>
<span class="sd">            :class:`FullyShardedDataParallel` instances.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">param_to_param_names</span> <span class="o">=</span> <span class="n">_get_param_to_fqns</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">param_names</span> <span class="ow">in</span> <span class="n">param_to_param_names</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;`_get_param_to_fqns()` &quot;</span> <span class="s2">&quot;should not construct empty lists&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Each parameter should only map to one parameter name but got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">param_names</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">param</span><span class="p">:</span> <span class="n">param_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">param_names</span> <span class="ow">in</span> <span class="n">param_to_param_names</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">param_to_param_name</span>


<span class="k">def</span> <span class="nf">_get_fqn_to_param</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Constructs the inverse mapping of :meth:`_get_param_to_fqn`.&quot;&quot;&quot;</span>
    <span class="n">param_to_param_name</span> <span class="o">=</span> <span class="n">_get_param_to_fqn</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">param_to_param_name</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">param_to_param_name</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.jit._trace &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/jit/_trace.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/jit/_trace.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../jit.html">torch.jit</a> &gt;</li>
        
      <li>torch.jit._trace</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.jit._trace</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Tracing</span>

<span class="sd">This module contains functionality to support the JIT&#39;s tracing frontend, notably:</span>
<span class="sd">    * torch.jit.trace</span>
<span class="sd">    * torch.jit.trace_module</span>

<span class="sd">This is not intended to be imported directly; please use the exposed</span>
<span class="sd">functionalities in `torch.jit`.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span>

<span class="kn">from</span> <span class="nn">torch.jit._state</span> <span class="kn">import</span> <span class="n">_python_cu</span><span class="p">,</span> <span class="n">_enabled</span>
<span class="kn">from</span> <span class="nn">torch.jit._script</span> <span class="kn">import</span> <span class="n">ScriptModule</span><span class="p">,</span> <span class="n">_CachedForward</span><span class="p">,</span> <span class="n">script</span>
<span class="kn">from</span> <span class="nn">torch._jit_internal</span> <span class="kn">import</span> <span class="n">_qualified_name</span><span class="p">,</span> <span class="n">is_scripting</span><span class="p">,</span> <span class="n">get_callable_argument_names</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">function</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Module</span>

<span class="kn">from</span> <span class="nn">torch.testing._comparison</span> <span class="kn">import</span> <span class="n">default_tolerances</span>

<span class="n">_flatten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_flatten</span>
<span class="n">_unflatten</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_unflatten</span>


<span class="k">def</span> <span class="nf">_create_interpreter_name_lookup_fn</span><span class="p">(</span><span class="n">frames_up</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_get_interpreter_name_for_var</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">frame</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;failed to inspect frame&quot;</span><span class="p">)</span>

        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">frames_up</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">frame</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">f_back</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">frame</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;failed to get frame&quot;</span><span class="p">)</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">f_locals</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">f_locals</span>
        <span class="n">f_globals</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">f_globals</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">f_locals</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">var</span> <span class="ow">is</span> <span class="n">v</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">k</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s2">&quot;self&quot;</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span>

    <span class="k">return</span> <span class="n">_get_interpreter_name_for_var</span>


<span class="k">def</span> <span class="nf">_unique_state_dict</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># since Parameter.detach() always creates a new torch.Tensor instance,</span>
    <span class="c1"># id(v) doesn&#39;t work with it. So we always get the Parameter or Buffer</span>
    <span class="c1"># as values, and deduplicate the params using Parameters and Buffers</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">keep_vars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">filtered_dict</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)()</span>
    <span class="n">seen_ids</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="n">seen_ids</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">seen_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">keep_vars</span><span class="p">:</span>
            <span class="n">filtered_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">filtered_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">filtered_dict</span>


<span class="k">class</span> <span class="nc">ONNXTracedModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inner</span><span class="p">,</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">force_outplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_inputs_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># inner may be a Module, or it may be an arbitrary callable</span>
        <span class="c1"># If it&#39;s a Module, we get its parameters automatically, which lets</span>
        <span class="c1"># us avoid a special casing functions versus modules.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inner</span> <span class="o">=</span> <span class="n">inner</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strict</span> <span class="o">=</span> <span class="n">strict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_force_outplace</span> <span class="o">=</span> <span class="n">force_outplace</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_return_inputs</span> <span class="o">=</span> <span class="n">return_inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_return_inputs_states</span> <span class="o">=</span> <span class="n">return_inputs_states</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">in_vars</span><span class="p">,</span> <span class="n">in_desc</span> <span class="o">=</span> <span class="n">_flatten</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="c1"># NOTE: use full state, because we need it for BatchNorm export</span>
        <span class="c1"># This differs from the compiler path, which doesn&#39;t support it at the moment.</span>
        <span class="n">module_state</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">_unique_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_vars</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

        <span class="n">ret_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">inputs_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
            <span class="n">in_args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">in_vars</span><span class="p">)):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Expected Tensor argument&#39;</span><span class="p">)</span>
                <span class="n">in_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

            <span class="n">trace_inputs</span> <span class="o">=</span> <span class="n">_unflatten</span><span class="p">(</span><span class="n">in_args</span><span class="p">,</span> <span class="n">in_desc</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_return_inputs</span><span class="p">:</span>
                <span class="n">ret_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_return_inputs_states</span><span class="p">:</span>
                <span class="n">inputs_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_unflatten</span><span class="p">(</span><span class="n">in_args</span><span class="p">,</span> <span class="n">in_desc</span><span class="p">))</span>
            <span class="n">outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inner</span><span class="p">(</span><span class="o">*</span><span class="n">trace_inputs</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_return_inputs_states</span><span class="p">:</span>
                <span class="n">inputs_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs_states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">trace_inputs</span><span class="p">)</span>
            <span class="n">out_vars</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_flatten</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_vars</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">out_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out_vars</span><span class="p">)</span>

        <span class="n">graph</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_create_graph_by_tracing</span><span class="p">(</span>
            <span class="n">wrapper</span><span class="p">,</span>
            <span class="n">in_vars</span> <span class="o">+</span> <span class="n">module_state</span><span class="p">,</span>
            <span class="n">_create_interpreter_name_lookup_fn</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strict</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_force_outplace</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_return_inputs</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">graph</span><span class="p">,</span> <span class="n">outs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ret_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_return_inputs_states</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">graph</span><span class="p">,</span> <span class="n">outs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">graph</span><span class="p">,</span> <span class="n">outs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_clone_inputs</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">clone_input</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="c1"># TODO: figure out one liner to .clone() and set requires_grad</span>
            <span class="n">v</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                <span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="kc">None</span> <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">is_mkldnn</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>
                <span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">v</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">clone_input</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">v</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">function</span><span class="o">.</span><span class="n">_nested_map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">),</span> <span class="n">clone_input</span><span class="p">,</span> <span class="n">condition_msg</span><span class="o">=</span><span class="s2">&quot;tensors&quot;</span>
    <span class="p">)(</span><span class="n">args</span><span class="p">)</span>


<span class="c1"># This is purely for developer debugging.  We are not going to advertise it.</span>
<span class="n">_JIT_TIME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;PYTORCH_JIT_TIME&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>  <span class="c1"># CUDA-only timing</span>
<span class="n">_JIT_DISABLE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;PYTORCH_JIT_DISABLE&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">_JIT_STATS</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;PYTORCH_JIT_STATS&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_time</span><span class="p">(</span><span class="n">trace_name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">time</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">_JIT_TIME</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">time</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">yield</span>
        <span class="k">return</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">stream</span><span class="o">.</span><span class="n">record_event</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">stream</span><span class="o">.</span><span class="n">record_event</span><span class="p">(</span><span class="n">end</span><span class="p">)</span>
        <span class="n">end</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> time: </span><span class="si">{}</span><span class="s2"> ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">trace_name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">start</span><span class="o">.</span><span class="n">elapsed_time</span><span class="p">(</span><span class="n">end</span><span class="p">)))</span>


<span class="k">def</span> <span class="nf">verify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Verify that a JIT compiled model has the same behavior as its uncompiled</span>
<span class="sd">    version along with its backwards pass.  If your model returns multiple</span>
<span class="sd">    outputs, you must also specify a `loss_fn` to produce a loss for which</span>
<span class="sd">    the backwards will be computed.</span>

<span class="sd">    This function has side-effects (e.g., it executes your model / saves and loads</span>
<span class="sd">    parameters), so don&#39;t expect the model to come out exactly the same as what</span>
<span class="sd">    you passed in.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (compiled torch.nn.Module or function): the module/function to be</span>
<span class="sd">            verified.  The module/function definition MUST have been decorated with</span>
<span class="sd">            `@torch.jit.compile`.</span>
<span class="sd">        args (tuple or Tensor): the positional arguments to pass to the</span>
<span class="sd">            compiled function/module to be verified.  A non-tuple is assumed to</span>
<span class="sd">            be a single positional argument to be passed to the model.</span>
<span class="sd">        loss_fn (function, optional): the loss function to be applied to</span>
<span class="sd">            the output of the model, before backwards is invoked.  By default,</span>
<span class="sd">            we assume that a model returns a single result, and we :func:`torch.sum`</span>
<span class="sd">            before calling backwards; if this is inappropriate, you can pass your</span>
<span class="sd">            own loss function.  Note that if a model returns a tuple of results,</span>
<span class="sd">            these are passed as separate positional arguments to `loss_fn`.</span>
<span class="sd">        devices (iterable of device IDs, optional): the GPU devices which the</span>
<span class="sd">            compiled module will be run on.  This determines the RNG state we</span>
<span class="sd">            must save when running both compiled and uncompiled versions of the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: In principle, we track device information in our trace, so it</span>
    <span class="c1"># should be possible to check if our execution actually obeyed the &#39;devices&#39;</span>
    <span class="c1"># the user provided.</span>

    <span class="c1"># TODO: Consider adding a utility function to torch.jit to test</span>
    <span class="c1"># for this case</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">CompiledFunction</span><span class="p">):</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;Cannot verify an uncompiled module.  Add @torch.jit.compile to compile it&quot;</span>
        <span class="p">)</span>
    <span class="n">is_module</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Module</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">args</span><span class="p">,)</span>

    <span class="n">saved_args</span> <span class="o">=</span> <span class="n">_clone_inputs</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_module</span><span class="p">:</span>
        <span class="n">saved_state</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">run_fwd_bwd</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">force_trace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">assert_compiled</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="k">if</span> <span class="n">is_module</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="n">in_vars</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_flatten</span><span class="p">((</span><span class="n">args</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>
        <span class="c1"># We use a special API to reset the trace and compile it from scratch.</span>
        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">model</span>
        <span class="k">if</span> <span class="n">force_trace</span><span class="p">:</span>
            <span class="n">compiled_fn</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">assert_compiled</span><span class="p">:</span>
            <span class="n">hits</span> <span class="o">=</span> <span class="n">compiled_fn</span><span class="o">.</span><span class="n">hits</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">assert_compiled</span> <span class="ow">and</span> <span class="n">compiled_fn</span><span class="o">.</span><span class="n">hits</span> <span class="o">==</span> <span class="n">hits</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;failed to use the compiled function&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">out</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s2">&quot;Model returns </span><span class="si">{}</span><span class="s2"> outputs, but default loss function &quot;</span>
                    <span class="s2">&quot;(torch.sum) can only handle a single output&quot;</span>
                <span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="n">out_vars</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_flatten</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">saved_outs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">v</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">out_vars</span>
        <span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="o">*</span><span class="n">out</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">([</span><span class="n">loss</span><span class="p">],</span> <span class="n">in_vars</span><span class="p">)</span>
        <span class="c1"># TODO: I&#39;m not sure if the clone here is necessary but it is safer</span>
        <span class="n">saved_grads</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">v</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">grads</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">saved_outs</span><span class="p">,</span> <span class="n">saved_grads</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">fork_rng</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="n">_caller</span><span class="o">=</span><span class="s2">&quot;torch.jit.verify&quot;</span><span class="p">):</span>
        <span class="n">uncompiled_outs</span><span class="p">,</span> <span class="n">uncompiled_grads</span> <span class="o">=</span> <span class="n">run_fwd_bwd</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">force_trace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">has_trace_for</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_module</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">saved_state</span><span class="p">)</span>
    <span class="n">compiled_outs</span><span class="p">,</span> <span class="n">compiled_grads</span> <span class="o">=</span> <span class="n">run_fwd_bwd</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">assert_compiled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">_verify_equal</span><span class="p">(</span><span class="n">uncompiled_outs</span><span class="p">,</span> <span class="n">compiled_outs</span><span class="p">)</span>
    <span class="n">_verify_equal</span><span class="p">(</span><span class="n">uncompiled_grads</span><span class="p">,</span> <span class="n">compiled_grads</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_verify_equal</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">1e-6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;JIT and real computation mismatch&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">indent</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">line</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()])</span>


<span class="k">class</span> <span class="nc">TracingCheckError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_diff_error</span><span class="p">,</span> <span class="n">tensor_compare_error</span><span class="p">,</span> <span class="n">extra_msg</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;Tracing failed sanity checks!</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">extra_msg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">message</span> <span class="o">+=</span> <span class="n">extra_msg</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">graph_diff_error</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;ERROR: Graphs differed across invocations!</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">message</span> <span class="o">+=</span> <span class="n">indent</span><span class="p">(</span><span class="n">graph_diff_error</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">tensor_compare_error</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">message</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="s2">&quot;ERROR: Tensor-valued Constant nodes differed in value &quot;</span>
                <span class="s2">&quot;across invocations. This often indicates that the tracer has&quot;</span>
                <span class="s2">&quot; encountered untraceable code.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">message</span> <span class="o">+=</span> <span class="n">indent</span><span class="p">(</span><span class="n">tensor_compare_error</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">message</span><span class="p">)</span>


<span class="c1"># Check the traced module against a set of user-provided validation inputs</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">_check_trace</span><span class="p">(</span>
    <span class="n">check_inputs</span><span class="p">,</span>
    <span class="n">func</span><span class="p">,</span>
    <span class="n">traced_func</span><span class="p">,</span>
    <span class="n">check_tolerance</span><span class="p">,</span>
    <span class="n">strict</span><span class="p">,</span>
    <span class="n">force_outplace</span><span class="p">,</span>
    <span class="n">is_trace_module</span><span class="p">,</span>
    <span class="n">_module_class</span><span class="p">,</span>
    <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Note: tracing is independent of optimizations, which consume the trace</span>
    <span class="k">for</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="n">check_inputs</span><span class="p">:</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,)</span>

        <span class="k">if</span> <span class="n">is_trace_module</span><span class="p">:</span>
            <span class="n">copied_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">copied_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">_clone_inputs</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">check_mod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace_module</span><span class="p">(</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="s2">&quot;__self__&quot;</span><span class="p">,</span> <span class="n">func</span><span class="p">),</span>
                <span class="n">copied_dict</span><span class="p">,</span>
                <span class="n">check_trace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span>
                <span class="n">_force_outplace</span><span class="o">=</span><span class="n">force_outplace</span><span class="p">,</span>
                <span class="n">_module_class</span><span class="o">=</span><span class="n">_module_class</span><span class="p">,</span>
                <span class="n">_compilation_unit</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">CompilationUnit</span><span class="p">(),</span>
                <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="n">example_inputs_is_kwarg</span><span class="p">,</span>
                <span class="n">_store_inputs</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">check_mod_func</span> <span class="o">=</span> <span class="n">check_mod</span><span class="o">.</span><span class="n">_c</span><span class="o">.</span><span class="n">_get_method</span><span class="p">(</span><span class="n">traced_func</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">traced_func</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">example_inputs_is_kwarg</span><span class="p">:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">example_inputs_is_kwarg</span><span class="p">:</span>
                <span class="n">check_mod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span>
                    <span class="n">func</span><span class="p">,</span>
                    <span class="n">check_trace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span>
                    <span class="n">_force_outplace</span><span class="o">=</span><span class="n">force_outplace</span><span class="p">,</span>
                    <span class="n">_module_class</span><span class="o">=</span><span class="n">_module_class</span><span class="p">,</span>
                    <span class="n">example_kwarg_inputs</span><span class="o">=</span><span class="n">_clone_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
                    <span class="n">_store_inputs</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">check_mod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span>
                    <span class="n">func</span><span class="p">,</span>
                    <span class="n">_clone_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
                    <span class="n">check_trace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span>
                    <span class="n">_force_outplace</span><span class="o">=</span><span class="n">force_outplace</span><span class="p">,</span>
                    <span class="n">_module_class</span><span class="o">=</span><span class="n">_module_class</span><span class="p">,</span>
                    <span class="n">_store_inputs</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>
            <span class="n">check_mod_func</span> <span class="o">=</span> <span class="n">check_mod</span>

        <span class="k">def</span> <span class="nf">graph_diagnostic_info</span><span class="p">():</span>
            <span class="n">mod_canonicalized</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_pass_canonicalize</span><span class="p">(</span><span class="n">traced_func</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_pass_inline</span><span class="p">(</span><span class="n">mod_canonicalized</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_pass_erase_shape_information</span><span class="p">(</span><span class="n">mod_canonicalized</span><span class="p">)</span>
            <span class="n">mod_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">mod_canonicalized</span><span class="p">)</span>
            <span class="n">mod_str</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;___torch_mangle_[0-9]+\.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">mod_str</span><span class="p">)</span>
            <span class="n">check_canonicalized</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_pass_canonicalize</span><span class="p">(</span><span class="n">check_mod_func</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_pass_inline</span><span class="p">(</span><span class="n">check_canonicalized</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_pass_erase_shape_information</span><span class="p">(</span><span class="n">check_canonicalized</span><span class="p">)</span>
            <span class="n">check_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">check_canonicalized</span><span class="p">)</span>
            <span class="n">check_str</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;___torch_mangle_[0-9]+\.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">check_str</span><span class="p">)</span>

            <span class="n">graph_diff_errors</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">mod_str</span> <span class="o">!=</span> <span class="n">check_str</span><span class="p">:</span>
                <span class="kn">import</span> <span class="nn">difflib</span>

                <span class="n">graph_diff</span> <span class="o">=</span> <span class="n">difflib</span><span class="o">.</span><span class="n">ndiff</span><span class="p">(</span>
                    <span class="n">mod_str</span><span class="o">.</span><span class="n">splitlines</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span> <span class="n">check_str</span><span class="o">.</span><span class="n">splitlines</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">graph_diff_errors</span> <span class="o">=</span> <span class="s2">&quot;Graph diff:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">indent</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">graph_diff</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>

                <span class="k">for</span> <span class="n">n_mod</span><span class="p">,</span> <span class="n">n_check</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="n">mod_canonicalized</span><span class="o">.</span><span class="n">nodes</span><span class="p">(),</span> <span class="n">check_canonicalized</span><span class="o">.</span><span class="n">nodes</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_mod</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_check</span><span class="p">):</span>
                        <span class="n">graph_diff_errors</span> <span class="o">+=</span> <span class="s2">&quot;First diverging operator:</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="n">node_diff</span> <span class="o">=</span> <span class="n">difflib</span><span class="o">.</span><span class="n">ndiff</span><span class="p">(</span>
                            <span class="nb">str</span><span class="p">(</span><span class="n">n_mod</span><span class="p">)</span><span class="o">.</span><span class="n">splitlines</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_check</span><span class="p">)</span><span class="o">.</span><span class="n">splitlines</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                        <span class="p">)</span>
                        <span class="n">source_printout</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="s2">&quot;Node diff:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">indent</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">node_diff</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="n">mod_stack</span> <span class="o">=</span> <span class="n">n_mod</span><span class="o">.</span><span class="n">sourceRange</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">mod_stack</span><span class="p">:</span>
                            <span class="n">source_printout</span> <span class="o">+=</span> <span class="p">(</span>
                                <span class="s2">&quot;Trace source location:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">indent</span><span class="p">(</span><span class="n">mod_stack</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                            <span class="p">)</span>
                        <span class="n">check_stack</span> <span class="o">=</span> <span class="n">n_check</span><span class="o">.</span><span class="n">sourceRange</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">check_stack</span><span class="p">:</span>
                            <span class="n">source_printout</span> <span class="o">+=</span> <span class="p">(</span>
                                <span class="s2">&quot;Check source location:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">indent</span><span class="p">(</span><span class="n">check_stack</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                            <span class="p">)</span>
                        <span class="n">graph_diff_errors</span> <span class="o">+=</span> <span class="n">source_printout</span>

                        <span class="k">break</span>  <span class="c1"># For now, only print out the first pair of nodes that diverges</span>

            <span class="n">tensor_compare_errors</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># Check Tensor-valued constant nodes</span>
            <span class="k">for</span> <span class="n">n_mod</span><span class="p">,</span> <span class="n">n_check</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="n">mod_canonicalized</span><span class="o">.</span><span class="n">nodes</span><span class="p">(),</span> <span class="n">check_canonicalized</span><span class="o">.</span><span class="n">nodes</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">n_mod</span><span class="o">.</span><span class="n">kind</span><span class="p">()</span> <span class="o">!=</span> <span class="n">n_check</span><span class="o">.</span><span class="n">kind</span><span class="p">():</span>
                    <span class="k">break</span>  <span class="c1"># Graphs have already diverged</span>

                <span class="k">if</span> <span class="n">n_mod</span><span class="o">.</span><span class="n">kind</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;prim::Constant&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span>
                    <span class="n">n_mod</span><span class="o">.</span><span class="n">mustBeNone</span><span class="p">()</span> <span class="ow">or</span> <span class="n">n_check</span><span class="o">.</span><span class="n">mustBeNone</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">n_mod</span><span class="o">.</span><span class="n">hasAttribute</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">):</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="n">n_mod</span><span class="o">.</span><span class="n">kindOf</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;t&quot;</span> <span class="ow">or</span> <span class="n">n_check</span><span class="o">.</span><span class="n">kindOf</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;t&quot;</span><span class="p">:</span>
                        <span class="k">continue</span>

                    <span class="n">mod_tensor_val</span> <span class="o">=</span> <span class="n">n_mod</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>
                    <span class="n">check_tensor_val</span> <span class="o">=</span> <span class="n">n_check</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">)</span>

                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">mod_tensor_val</span><span class="p">,</span> <span class="n">check_tensor_val</span><span class="p">,</span> <span class="n">equal_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="k">except</span> <span class="p">(</span><span class="ne">RuntimeError</span><span class="p">,</span> <span class="ne">AssertionError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">tensor_compare_errors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="n">tensor_compare_errors</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                        <span class="n">tensor_compare_errors</span> <span class="o">+=</span> <span class="s2">&quot;Node:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">indent</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">n_mod</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="n">compare_stack</span> <span class="o">=</span> <span class="n">n_mod</span><span class="o">.</span><span class="n">sourceRange</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">compare_stack</span><span class="p">:</span>
                            <span class="n">tensor_compare_errors</span> <span class="o">+=</span> <span class="p">(</span>
                                <span class="s2">&quot;Source Location:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">indent</span><span class="p">(</span><span class="n">compare_stack</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                            <span class="p">)</span>
                        <span class="n">tensor_compare_errors</span> <span class="o">+=</span> <span class="s2">&quot;Comparison exception: &quot;</span> <span class="o">+</span> <span class="n">indent</span><span class="p">(</span>
                            <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
                        <span class="p">)</span>

                        <span class="k">break</span>  <span class="c1"># For now, only print the first diverging pair</span>

            <span class="k">return</span> <span class="n">graph_diff_errors</span><span class="p">,</span> <span class="n">tensor_compare_errors</span>

        <span class="k">def</span> <span class="nf">wrap_retval</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">x</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">x</span><span class="p">,)</span>

        <span class="k">def</span> <span class="nf">run_mod_and_filter_tensor_outputs</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">running_what</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="n">example_inputs_is_kwarg</span><span class="p">:</span>
                    <span class="n">outs</span> <span class="o">=</span> <span class="n">wrap_retval</span><span class="p">(</span><span class="n">mod</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">outs</span> <span class="o">=</span> <span class="n">wrap_retval</span><span class="p">(</span><span class="n">mod</span><span class="p">(</span><span class="o">*</span><span class="n">_clone_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
                <span class="n">outs</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outs</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)]</span>
                <span class="k">return</span> <span class="n">outs</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">graph_diff_errors</span><span class="p">,</span> <span class="n">tensor_compare_errors</span> <span class="o">=</span> <span class="n">graph_diagnostic_info</span><span class="p">()</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;encountered an exception while running the </span><span class="si">{</span><span class="n">running_what</span><span class="si">}</span><span class="s2"> with test inputs.</span><span class="se">\n</span><span class="s2">Exception:</span><span class="se">\n</span><span class="si">{</span><span class="n">indent</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="k">raise</span> <span class="n">TracingCheckError</span><span class="p">(</span>
                    <span class="n">graph_diff_errors</span><span class="p">,</span>
                    <span class="n">tensor_compare_errors</span><span class="p">,</span>
                    <span class="n">extra_msg</span><span class="o">=</span><span class="n">msg</span><span class="p">,</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="n">has_warned</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span>

        <span class="k">def</span> <span class="nf">maybe_warn_nondeterministic</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">has_warned</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">return</span>
            <span class="n">has_warned</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">nondeterm_ops</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">op</span> <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">traced_func</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">()</span> <span class="k">if</span> <span class="n">op</span><span class="o">.</span><span class="n">isNondeterministic</span><span class="p">()</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">nondeterm_ops</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">nondeterministic_ops_warning</span> <span class="o">=</span> <span class="s2">&quot;Trace had nondeterministic nodes. &quot;</span>
                <span class="n">nondeterministic_ops_warning</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="s2">&quot;Did you forget call .eval() on your model? Nodes:</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="n">nondeterministic_ops_warning</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">indent</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">op</span><span class="p">))</span> <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">nondeterm_ops</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="n">nondeterministic_ops_warning</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">This may cause errors in trace checking. To disable trace checking,&quot;</span>
                    <span class="s2">&quot; pass check_trace=False to torch.jit.trace()&quot;</span>
                <span class="p">)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="n">nondeterministic_ops_warning</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">TracerWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">5</span>
                <span class="p">)</span>

        <span class="k">def</span> <span class="nf">compare_outputs</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">reference</span><span class="p">,</span> <span class="n">match_what</span><span class="p">):</span>
            <span class="n">all_ok</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">ref</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">reference</span><span class="p">)):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">orig</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
                        <span class="n">orig</span> <span class="o">=</span> <span class="n">orig</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">ref</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
                        <span class="n">ref</span> <span class="o">=</span> <span class="n">ref</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">orig</span><span class="o">.</span><span class="n">is_mkldnn</span><span class="p">:</span>
                        <span class="n">orig</span> <span class="o">=</span> <span class="n">orig</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">ref</span><span class="o">.</span><span class="n">is_mkldnn</span><span class="p">:</span>
                        <span class="n">ref</span> <span class="o">=</span> <span class="n">ref</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">ref</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="ow">or</span> <span class="n">orig</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
                            <span class="n">orig</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">),</span>
                            <span class="n">ref</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">),</span>
                            <span class="n">rtol</span><span class="o">=</span><span class="n">check_tolerance</span><span class="p">,</span>
                            <span class="n">atol</span><span class="o">=</span><span class="n">default_tolerances</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">ref</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">equal_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">orig</span><span class="o">.</span><span class="n">is_mps</span> <span class="ow">or</span> <span class="n">ref</span><span class="o">.</span><span class="n">is_mps</span><span class="p">:</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
                                <span class="n">orig</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
                                <span class="n">ref</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span>
                                <span class="n">rtol</span><span class="o">=</span><span class="n">check_tolerance</span><span class="p">,</span>
                                <span class="n">atol</span><span class="o">=</span><span class="n">default_tolerances</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">ref</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
                                <span class="n">equal_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
                                <span class="n">orig</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
                                <span class="n">ref</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
                                <span class="n">rtol</span><span class="o">=</span><span class="n">check_tolerance</span><span class="p">,</span>
                                <span class="n">atol</span><span class="o">=</span><span class="n">default_tolerances</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">ref</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
                                <span class="n">equal_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="p">)</span>
                <span class="k">except</span> <span class="ne">AssertionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">maybe_warn_nondeterministic</span><span class="p">()</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                        <span class="s2">&quot;Output nr &quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="o">+</span> <span class="s2">&quot;. of the traced function does not match &quot;</span>
                        <span class="s2">&quot;the corresponding output of the &quot;</span>
                        <span class="o">+</span> <span class="n">match_what</span>
                        <span class="o">+</span> <span class="s2">&quot;. Detailed error:</span><span class="se">\n</span><span class="s2">&quot;</span>
                        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
                        <span class="n">category</span><span class="o">=</span><span class="n">TracerWarning</span><span class="p">,</span>
                        <span class="n">stacklevel</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">all_ok</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="k">return</span> <span class="n">all_ok</span>

        <span class="n">traced_outs</span> <span class="o">=</span> <span class="n">run_mod_and_filter_tensor_outputs</span><span class="p">(</span><span class="n">traced_func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;trace&quot;</span><span class="p">)</span>
        <span class="n">fn_outs</span> <span class="o">=</span> <span class="n">run_mod_and_filter_tensor_outputs</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;Python function&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">compare_outputs</span><span class="p">(</span><span class="n">traced_outs</span><span class="p">,</span> <span class="n">fn_outs</span><span class="p">,</span> <span class="s2">&quot;Python function&quot;</span><span class="p">):</span>
            <span class="n">check_outs</span> <span class="o">=</span> <span class="n">run_mod_and_filter_tensor_outputs</span><span class="p">(</span>
                <span class="n">check_mod_func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="s2">&quot;repeated trace&quot;</span>
            <span class="p">)</span>
            <span class="n">compare_outputs</span><span class="p">(</span><span class="n">traced_outs</span><span class="p">,</span> <span class="n">check_outs</span><span class="p">,</span> <span class="s2">&quot;repeated trace&quot;</span><span class="p">)</span>

        <span class="n">diag_info</span> <span class="o">=</span> <span class="n">graph_diagnostic_info</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">diag_info</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">TracingCheckError</span><span class="p">(</span><span class="o">*</span><span class="n">diag_info</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TracerWarning</span><span class="p">(</span><span class="ne">Warning</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">ignore_lib_warnings</span><span class="p">():</span>
        <span class="c1"># We ignore warnings from all submodules excluding the JIT, because we need them e.g. for _check_trace</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
            <span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">TracerWarning</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s2">&quot;torch.(?!jit)&quot;</span>
        <span class="p">)</span>


<span class="c1"># We ignore the tracer warnings coming form inside the library, because all our shape</span>
<span class="c1"># checks in nn will trigger them.</span>
<span class="n">TracerWarning</span><span class="o">.</span><span class="n">ignore_lib_warnings</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_tracer_warn_use_python</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">make_tuple</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">example_inputs</span><span class="p">,)</span>
    <span class="c1"># done primarily so that weird iterables fail here and not pybind11 code</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">example_inputs</span>


<span class="k">def</span> <span class="nf">make_module</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">_module_class</span><span class="p">,</span> <span class="n">_compilation_unit</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">ScriptModule</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mod</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">module_has_exports</span><span class="p">(</span><span class="n">mod</span><span class="p">):</span>

        <span class="n">infer_methods_stubs_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_recursive</span><span class="o">.</span><span class="n">make_stubs_from_exported_methods</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_recursive</span><span class="o">.</span><span class="n">create_script_module</span><span class="p">(</span>
            <span class="n">mod</span><span class="p">,</span>
            <span class="n">infer_methods_stubs_fn</span><span class="p">,</span>
            <span class="n">share_types</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">is_tracing</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_module_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_module_class</span> <span class="o">=</span> <span class="n">TopLevelTracedModule</span>
        <span class="k">return</span> <span class="n">_module_class</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">_compilation_unit</span><span class="o">=</span><span class="n">_compilation_unit</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">wrap_check_inputs</span><span class="p">(</span><span class="n">check_inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">check_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="p">[{</span><span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="n">c</span><span class="p">}</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">check_inputs</span><span class="p">]</span>


<div class="viewcode-block" id="trace"><a class="viewcode-back" href="../../../generated/torch.jit.trace.html#torch.jit.trace">[docs]</a><span class="k">def</span> <span class="nf">trace</span><span class="p">(</span>
    <span class="n">func</span><span class="p">,</span>
    <span class="n">example_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">optimize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">check_trace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">check_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">check_tolerance</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">_force_outplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">_module_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_compilation_unit</span><span class="o">=</span><span class="n">_python_cu</span><span class="p">,</span>
    <span class="n">example_kwarg_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_store_inputs</span><span class="o">=</span><span class="kc">True</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trace a function and return an executable  or :class:`ScriptFunction`</span>
<span class="sd">    that will be optimized using just-in-time compilation. Tracing is ideal for</span>
<span class="sd">    code that operates only on ``Tensor``\\s and lists, dictionaries, and</span>
<span class="sd">    tuples of ``Tensor``\\s.</span>

<span class="sd">    Using `torch.jit.trace` and `torch.jit.trace_module`, you can turn an</span>
<span class="sd">    existing module or Python function into a TorchScript</span>
<span class="sd">    :class:`ScriptFunction` or :class:`ScriptModule`. You must provide example</span>
<span class="sd">    inputs, and we run the function, recording the operations performed on all</span>
<span class="sd">    the tensors.</span>

<span class="sd">    * The resulting recording of a standalone function produces `ScriptFunction`.</span>
<span class="sd">    * The resulting recording of `nn.Module.forward` or `nn.Module` produces</span>
<span class="sd">      `ScriptModule`.</span>

<span class="sd">    This module also contains any parameters that the original</span>
<span class="sd">    module had as well.</span>

<span class="sd">    Warning:</span>
<span class="sd">        Tracing only correctly records functions and modules which are not data</span>
<span class="sd">        dependent (e.g., do not have conditionals on data in tensors) and do not have</span>
<span class="sd">        any untracked external dependencies (e.g., perform input/output or</span>
<span class="sd">        access global variables). Tracing only records operations done when the given</span>
<span class="sd">        function is run on the given tensors. Therefore, the returned</span>
<span class="sd">        `ScriptModule` will always run the same traced graph on any input. This</span>
<span class="sd">        has some important implications when your module is expected to run</span>
<span class="sd">        different sets of operations, depending on the input and/or the module</span>
<span class="sd">        state. For example,</span>

<span class="sd">        * Tracing will not record any control-flow like if-statements or loops.</span>
<span class="sd">          When this control-flow is constant across your module, this is fine</span>
<span class="sd">          and it often inlines the control-flow decisions. But sometimes the</span>
<span class="sd">          control-flow is actually part of the model itself. For instance, a</span>
<span class="sd">          recurrent network is a loop over the (possibly dynamic) length of an</span>
<span class="sd">          input sequence.</span>
<span class="sd">        * In the returned :class:`ScriptModule`, operations that have different</span>
<span class="sd">          behaviors in ``training`` and ``eval`` modes will always behave as if</span>
<span class="sd">          it is in the mode it was in during tracing, no matter which mode the</span>
<span class="sd">          `ScriptModule` is in.</span>

<span class="sd">        In cases like these, tracing would not be appropriate and</span>
<span class="sd">        :func:`scripting &lt;torch.jit.script&gt;` is a better choice. If you trace</span>
<span class="sd">        such models, you may silently get incorrect results on subsequent</span>
<span class="sd">        invocations of the model. The tracer will try to emit warnings when</span>
<span class="sd">        doing something that may cause an incorrect trace to be produced.</span>

<span class="sd">    Args:</span>
<span class="sd">        func (callable or torch.nn.Module):  A Python function or `torch.nn.Module`</span>
<span class="sd">            that will be run with `example_inputs`. `func` arguments and return</span>
<span class="sd">            values  must be tensors or (possibly nested) tuples that contain</span>
<span class="sd">            tensors. When a module is passed `torch.jit.trace`, only the</span>
<span class="sd">            ``forward`` method is run and traced (see :func:`torch.jit.trace</span>
<span class="sd">            &lt;torch.jit.trace_module&gt;` for details).</span>

<span class="sd">    Keyword arguments:</span>
<span class="sd">        example_inputs (tuple or torch.Tensor or None, optional): A tuple of example</span>
<span class="sd">            inputs that will be passed to the function while tracing.</span>
<span class="sd">            Default: ``None``. Either this argument or ``example_kwarg_inputs``</span>
<span class="sd">            should be specified. The resulting trace can be run with inputs of</span>
<span class="sd">            different types and shapes assuming the traced operations support those</span>
<span class="sd">            types and shapes. `example_inputs` may also be a single Tensor in which</span>
<span class="sd">            case it is automatically wrapped in a tuple. When the value is None,</span>
<span class="sd">            ``example_kwarg_inputs`` should be specified.</span>

<span class="sd">        check_trace (``bool``, optional): Check if the same inputs run through</span>
<span class="sd">            traced code produce the same outputs. Default: ``True``. You might want</span>
<span class="sd">            to disable this if, for example, your network contains non-</span>
<span class="sd">            deterministic ops or if you are sure that the network is correct despite</span>
<span class="sd">            a checker failure.</span>

<span class="sd">        check_inputs (list of tuples, optional): A list of tuples of input</span>
<span class="sd">            arguments that should be used to check the trace against what is</span>
<span class="sd">            expected. Each tuple is equivalent to a set of input arguments that</span>
<span class="sd">            would be specified in ``example_inputs``. For best results, pass in</span>
<span class="sd">            a set of checking inputs representative of the space of shapes and</span>
<span class="sd">            types of inputs you expect the network to see.  If not specified,</span>
<span class="sd">            the original ``example_inputs`` are used for checking</span>
<span class="sd">        check_tolerance (float, optional): Floating-point comparison tolerance</span>
<span class="sd">            to use in the checker procedure.  This can be used to relax the</span>
<span class="sd">            checker strictness in the event that results diverge numerically</span>
<span class="sd">            for a known reason, such as operator fusion.</span>
<span class="sd">        strict (``bool``, optional): run the tracer in a strict mode or not</span>
<span class="sd">            (default: ``True``). Only turn this off when you want the tracer to</span>
<span class="sd">            record your mutable container types (currently ``list``/``dict``)</span>
<span class="sd">            and you are sure that the container you are using in your</span>
<span class="sd">            problem is a ``constant`` structure and does not get used as</span>
<span class="sd">            control flow (if, for) conditions.</span>
<span class="sd">        example_kwarg_inputs (dict, optional): This parameter is a pack of keyword</span>
<span class="sd">            arguments of example inputs that will be passed to the function while</span>
<span class="sd">            tracing. Default: ``None``. Either this argument or ``example_inputs``</span>
<span class="sd">            should be specified. The dict will be unpacking by the arguments name</span>
<span class="sd">            of the traced function. If the keys of the dict don&#39;t not match with</span>
<span class="sd">            the traced function&#39;s arguments name, a runtime exception will be raised.</span>

<span class="sd">    Returns:</span>
<span class="sd">        If `func` is `nn.Module` or ``forward`` of `nn.Module`, `trace` returns</span>
<span class="sd">        a :class:`ScriptModule` object with a single ``forward`` method</span>
<span class="sd">        containing the traced code.  The returned `ScriptModule` will</span>
<span class="sd">        have the same set of sub-modules and parameters as the original</span>
<span class="sd">        ``nn.Module``.  If ``func`` is a standalone function, ``trace``</span>
<span class="sd">        returns `ScriptFunction`.</span>

<span class="sd">    Example (tracing a function):</span>

<span class="sd">    .. testcode::</span>

<span class="sd">        import torch</span>

<span class="sd">        def foo(x, y):</span>
<span class="sd">            return 2 * x + y</span>

<span class="sd">        # Run `foo` with the provided inputs and record the tensor operations</span>
<span class="sd">        traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))</span>

<span class="sd">        # `traced_foo` can now be run with the TorchScript interpreter or saved</span>
<span class="sd">        # and loaded in a Python-free environment</span>

<span class="sd">    Example (tracing an existing module)::</span>

<span class="sd">        import torch</span>
<span class="sd">        import torch.nn as nn</span>

<span class="sd">        class Net(nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super().__init__()</span>
<span class="sd">                self.conv = nn.Conv2d(1, 1, 3)</span>

<span class="sd">            def forward(self, x):</span>
<span class="sd">                return self.conv(x)</span>

<span class="sd">        n = Net()</span>
<span class="sd">        example_weight = torch.rand(1, 1, 3, 3)</span>
<span class="sd">        example_forward_input = torch.rand(1, 1, 3, 3)</span>

<span class="sd">        # Trace a specific method and construct `ScriptModule` with</span>
<span class="sd">        # a single `forward` method</span>
<span class="sd">        module = torch.jit.trace(n.forward, example_forward_input)</span>

<span class="sd">        # Trace a module (implicitly traces `forward`) and construct a</span>
<span class="sd">        # `ScriptModule` with a single `forward` method</span>
<span class="sd">        module = torch.jit.trace(n, example_forward_input)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_enabled</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">func</span>
    <span class="k">if</span> <span class="n">optimize</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
        <span class="c1"># it is hard to trace it because the forward method on ScriptModule is already defined, so it</span>
        <span class="c1"># would result in an error.</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">func</span>


    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">example_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_kwarg_inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">example_inputs</span> <span class="o">=</span> <span class="n">example_kwarg_inputs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;example_kwarg_inputs should be a dict&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">trace_module</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span>
            <span class="p">{</span><span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="n">example_inputs</span><span class="p">},</span>
            <span class="kc">None</span><span class="p">,</span>
            <span class="n">check_trace</span><span class="p">,</span>
            <span class="n">wrap_check_inputs</span><span class="p">(</span><span class="n">check_inputs</span><span class="p">),</span>
            <span class="n">check_tolerance</span><span class="p">,</span>
            <span class="n">strict</span><span class="p">,</span>
            <span class="n">_force_outplace</span><span class="p">,</span>
            <span class="n">_module_class</span><span class="p">,</span>
            <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">example_kwarg_inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span>
            <span class="n">_store_inputs</span><span class="o">=</span><span class="n">_store_inputs</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">hasattr</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="s2">&quot;__self__&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="vm">__self__</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;forward&quot;</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">example_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_kwarg_inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">example_inputs</span> <span class="o">=</span> <span class="n">example_kwarg_inputs</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;example_kwarg_inputs should be a dict&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">trace_module</span><span class="p">(</span>
            <span class="n">func</span><span class="o">.</span><span class="vm">__self__</span><span class="p">,</span>
            <span class="p">{</span><span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="n">example_inputs</span><span class="p">},</span>
            <span class="kc">None</span><span class="p">,</span>
            <span class="n">check_trace</span><span class="p">,</span>
            <span class="n">wrap_check_inputs</span><span class="p">(</span><span class="n">check_inputs</span><span class="p">),</span>
            <span class="n">check_tolerance</span><span class="p">,</span>
            <span class="n">strict</span><span class="p">,</span>
            <span class="n">_force_outplace</span><span class="p">,</span>
            <span class="n">_module_class</span><span class="p">,</span>
            <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">example_kwarg_inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span>
            <span class="n">_store_inputs</span><span class="o">=</span><span class="n">_store_inputs</span>
        <span class="p">)</span>

    <span class="c1"># Special case for common case of passing a single Tensor</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">))</span> <span class="ow">and</span> <span class="n">example_kwarg_inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">example_inputs</span><span class="p">,)</span>
    <span class="c1"># done primarily so that weird iterables fail here and not pybind11 code</span>
    <span class="k">elif</span> <span class="n">example_kwarg_inputs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">example_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">)</span>

    <span class="n">var_lookup_fn</span> <span class="o">=</span> <span class="n">_create_interpreter_name_lookup_fn</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="s2">&quot;__self__&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="vm">__self__</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;trace doesn&#39;t support compiling individual module&#39;s functions.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;Please use trace_module&quot;</span>
        <span class="p">)</span>

    <span class="n">name</span> <span class="o">=</span> <span class="n">_qualified_name</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_kwarg_inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">example_inputs</span> <span class="o">=</span> <span class="n">example_kwarg_inputs</span>
        <span class="n">traced</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_create_function_from_trace_with_dict</span><span class="p">(</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="n">func</span><span class="p">,</span>
            <span class="n">example_kwarg_inputs</span><span class="p">,</span>
            <span class="n">var_lookup_fn</span><span class="p">,</span>
            <span class="n">strict</span><span class="p">,</span>
            <span class="n">_force_outplace</span><span class="p">,</span>
            <span class="n">get_callable_argument_names</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">traced</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_create_function_from_trace</span><span class="p">(</span>
            <span class="n">name</span><span class="p">,</span>
            <span class="n">func</span><span class="p">,</span>
            <span class="n">example_inputs</span><span class="p">,</span>
            <span class="n">var_lookup_fn</span><span class="p">,</span>
            <span class="n">strict</span><span class="p">,</span>
            <span class="n">_force_outplace</span><span class="p">,</span>
            <span class="n">get_callable_argument_names</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Check the trace against new traces created from user-specified inputs</span>
    <span class="k">if</span> <span class="n">check_trace</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">check_inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_trace</span><span class="p">(</span>
                <span class="n">check_inputs</span><span class="p">,</span>
                <span class="n">func</span><span class="p">,</span>
                <span class="n">traced</span><span class="p">,</span>
                <span class="n">check_tolerance</span><span class="p">,</span>
                <span class="n">strict</span><span class="p">,</span>
                <span class="n">_force_outplace</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="n">_module_class</span><span class="p">,</span>
                <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">example_kwarg_inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_trace</span><span class="p">(</span>
                <span class="p">[</span><span class="n">example_inputs</span><span class="p">],</span>
                <span class="n">func</span><span class="p">,</span>
                <span class="n">traced</span><span class="p">,</span>
                <span class="n">check_tolerance</span><span class="p">,</span>
                <span class="n">strict</span><span class="p">,</span>
                <span class="n">_force_outplace</span><span class="p">,</span>
                <span class="kc">False</span><span class="p">,</span>
                <span class="n">_module_class</span><span class="p">,</span>
                <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">example_kwarg_inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span>
            <span class="p">)</span>

    <span class="c1"># Allow torch.compile() to inline</span>
    <span class="n">traced</span><span class="o">.</span><span class="n">_torchdynamo_inline</span> <span class="o">=</span> <span class="n">func</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="k">return</span> <span class="n">traced</span></div>


<span class="n">_trace_module_map</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>


<div class="viewcode-block" id="trace_module"><a class="viewcode-back" href="../../../generated/torch.jit.trace_module.html#torch.jit.trace_module">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span>
<span class="k">def</span> <span class="nf">trace_module</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">optimize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">check_trace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">check_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">check_tolerance</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">_force_outplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">_module_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_compilation_unit</span><span class="o">=</span><span class="n">_python_cu</span><span class="p">,</span>
    <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">_store_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trace a module and return an executable :class:`ScriptModule` that will be optimized</span>
<span class="sd">    using just-in-time compilation. When a module is passed to :func:`torch.jit.trace &lt;torch.jit.trace&gt;`, only</span>
<span class="sd">    the ``forward`` method is run and traced. With ``trace_module``, you can specify a dictionary of</span>
<span class="sd">    method names to example inputs to trace (see the ``inputs``) argument below.</span>

<span class="sd">    See :func:`torch.jit.trace &lt;torch.jit.trace&gt;` for more information on tracing.</span>

<span class="sd">    Args:</span>
<span class="sd">        mod (torch.nn.Module):  A ``torch.nn.Module`` containing methods whose names are</span>
<span class="sd">                                specified in ``inputs``. The given methods will be compiled</span>
<span class="sd">                                as a part of a single `ScriptModule`.</span>
<span class="sd">        inputs (dict):  A dict containing sample inputs indexed by method names in ``mod``.</span>
<span class="sd">                                The inputs will be passed to methods whose names correspond to inputs&#39;</span>
<span class="sd">                                keys while tracing.</span>
<span class="sd">                                ``{ &#39;forward&#39; : example_forward_input, &#39;method2&#39;: example_method2_input}``</span>
<span class="sd">    Keyword arguments:</span>
<span class="sd">        check_trace (``bool``, optional): Check if the same inputs run through</span>
<span class="sd">                                      traced code produce the same outputs. Default: ``True``. You might want</span>
<span class="sd">                                      to disable this if, for example, your network contains non-</span>
<span class="sd">                                      deterministic ops or if you are sure that the network is correct despite</span>
<span class="sd">                                      a checker failure.</span>

<span class="sd">        check_inputs (list of dicts, optional): A list of dicts of input arguments that should be used</span>
<span class="sd">                                                 to check the trace against what is expected. Each tuple</span>
<span class="sd">                                                 is equivalent to a set of input arguments that would</span>
<span class="sd">                                                 be specified in ``inputs``. For best results, pass in a</span>
<span class="sd">                                                 set of checking inputs representative of the space of</span>
<span class="sd">                                                 shapes and types of inputs you expect the network to see.</span>
<span class="sd">                                                 If not specified, the original ``inputs`` are used for checking</span>
<span class="sd">        check_tolerance (float, optional): Floating-point comparison tolerance to use in the checker procedure.</span>
<span class="sd">                                           This can be used to relax the checker strictness in the event that</span>
<span class="sd">                                           results diverge numerically for a known reason, such as operator fusion.</span>
<span class="sd">        example_inputs_is_kwarg (``bool``, optional): This parameter indicate whether the example inputs is a pack</span>
<span class="sd">                                           pack of keyword arguments. Default: ``False``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A :class:`ScriptModule` object with a single ``forward`` method containing the traced code.</span>
<span class="sd">        When ``func`` is a ``torch.nn.Module``, the returned :class:`ScriptModule` will have the same set of</span>
<span class="sd">        sub-modules and parameters as ``func``.</span>

<span class="sd">    Example (tracing a module with multiple methods)::</span>

<span class="sd">        import torch</span>
<span class="sd">        import torch.nn as nn</span>

<span class="sd">        class Net(nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super().__init__()</span>
<span class="sd">                self.conv = nn.Conv2d(1, 1, 3)</span>

<span class="sd">            def forward(self, x):</span>
<span class="sd">                return self.conv(x)</span>

<span class="sd">            def weighted_kernel_sum(self, weight):</span>
<span class="sd">                return weight * self.conv.weight</span>


<span class="sd">        n = Net()</span>
<span class="sd">        example_weight = torch.rand(1, 1, 3, 3)</span>
<span class="sd">        example_forward_input = torch.rand(1, 1, 3, 3)</span>

<span class="sd">        # Trace a specific method and construct `ScriptModule` with</span>
<span class="sd">        # a single `forward` method</span>
<span class="sd">        module = torch.jit.trace(n.forward, example_forward_input)</span>

<span class="sd">        # Trace a module (implicitly traces `forward`) and construct a</span>
<span class="sd">        # `ScriptModule` with a single `forward` method</span>
<span class="sd">        module = torch.jit.trace(n, example_forward_input)</span>

<span class="sd">        # Trace specific methods on a module (specified in `inputs`), constructs</span>
<span class="sd">        # a `ScriptModule` with `forward` and `weighted_kernel_sum` methods</span>
<span class="sd">        inputs = {&#39;forward&#39; : example_forward_input, &#39;weighted_kernel_sum&#39; : example_weight}</span>
<span class="sd">        module = torch.jit.trace_module(n, inputs)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_enabled</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mod</span>
    <span class="k">if</span> <span class="n">optimize</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead&quot;</span>
        <span class="p">)</span>

    <span class="n">var_lookup_fn</span> <span class="o">=</span> <span class="n">_create_interpreter_name_lookup_fn</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;expected torch.nn.Module as the first argument&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;expected a dictionary of (method_name, input) pairs&quot;</span><span class="p">)</span>

    <span class="n">old_module_map</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_trace</span><span class="o">.</span><span class="n">_trace_module_map</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">trace_module_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">def</span> <span class="nf">register_submods</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
                <span class="n">submod_qualname</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">name</span>
                <span class="n">trace_module_map</span><span class="p">[</span><span class="n">child</span><span class="p">]</span> <span class="o">=</span> <span class="n">submod_qualname</span>
                <span class="n">register_submods</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">submod_qualname</span><span class="p">)</span>

        <span class="n">trace_module_map</span><span class="p">[</span><span class="s2">&quot;__module&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mod</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_trace</span><span class="o">.</span><span class="n">_trace_module_map</span> <span class="o">=</span> <span class="n">trace_module_map</span>
        <span class="n">register_submods</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&quot;__module&quot;</span><span class="p">)</span>

        <span class="n">module</span> <span class="o">=</span> <span class="n">make_module</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">_module_class</span><span class="p">,</span> <span class="n">_compilation_unit</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">example_inputs</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">method_name</span> <span class="o">==</span> <span class="s2">&quot;forward&quot;</span><span class="p">:</span>
                <span class="c1"># &quot;forward&quot; is a special case because we need to trace</span>
                <span class="c1"># `Module.__call__`, which sets up some extra tracing, but uses</span>
                <span class="c1"># argument names of the real `Module.forward` method.</span>
                <span class="n">func</span> <span class="o">=</span> <span class="n">mod</span>
                <span class="n">forward_method</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">method_name</span><span class="p">)</span>
                <span class="n">argument_names</span> <span class="o">=</span> <span class="n">get_callable_argument_names</span><span class="p">(</span><span class="n">forward_method</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">func</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">method_name</span><span class="p">)</span>
                <span class="n">argument_names</span> <span class="o">=</span> <span class="n">get_callable_argument_names</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="n">example_inputs_is_kwarg</span><span class="p">:</span>
                <span class="c1"># Raise exception when the user provided key names are not aligned with forward() method&#39;s arguments&#39; name/</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">example_inputs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">argument_names</span><span class="p">:</span>
                        <span class="n">valid_arguments</span> <span class="o">=</span> <span class="s2">&quot;[&quot;</span> <span class="o">+</span> <span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">argument_names</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span>
                        <span class="k">raise</span> <span class="ne">NameError</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is not in forward() method&#39;s arguments,</span>
<span class="s2">                         valid arguments name are </span><span class="si">{}</span><span class="s2">&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">valid_arguments</span><span class="p">))</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_c</span><span class="o">.</span><span class="n">_create_method_from_trace_with_dict</span><span class="p">(</span>
                    <span class="n">method_name</span><span class="p">,</span>
                    <span class="n">func</span><span class="p">,</span>
                    <span class="n">example_inputs</span><span class="p">,</span>
                    <span class="n">var_lookup_fn</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">,</span>
                    <span class="n">_force_outplace</span><span class="p">,</span>
                    <span class="n">argument_names</span><span class="p">,</span>
                    <span class="n">_store_inputs</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">example_inputs</span> <span class="o">=</span> <span class="n">make_tuple</span><span class="p">(</span><span class="n">example_inputs</span><span class="p">)</span>
                <span class="n">module</span><span class="o">.</span><span class="n">_c</span><span class="o">.</span><span class="n">_create_method_from_trace</span><span class="p">(</span>
                    <span class="n">method_name</span><span class="p">,</span>
                    <span class="n">func</span><span class="p">,</span>
                    <span class="n">example_inputs</span><span class="p">,</span>
                    <span class="n">var_lookup_fn</span><span class="p">,</span>
                    <span class="n">strict</span><span class="p">,</span>
                    <span class="n">_force_outplace</span><span class="p">,</span>
                    <span class="n">argument_names</span><span class="p">,</span>
                    <span class="n">_store_inputs</span>
                <span class="p">)</span>

            <span class="n">check_trace_method</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">_c</span><span class="o">.</span><span class="n">_get_method</span><span class="p">(</span><span class="n">method_name</span><span class="p">)</span>

            <span class="c1"># Check the trace against new traces created from user-specified inputs</span>
            <span class="k">if</span> <span class="n">check_trace</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">check_inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">_check_trace</span><span class="p">(</span>
                        <span class="n">check_inputs</span><span class="p">,</span>
                        <span class="n">func</span><span class="p">,</span>
                        <span class="n">check_trace_method</span><span class="p">,</span>
                        <span class="n">check_tolerance</span><span class="p">,</span>
                        <span class="n">strict</span><span class="p">,</span>
                        <span class="n">_force_outplace</span><span class="p">,</span>
                        <span class="kc">True</span><span class="p">,</span>
                        <span class="n">_module_class</span><span class="p">,</span>
                        <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="n">example_inputs_is_kwarg</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">_check_trace</span><span class="p">(</span>
                        <span class="p">[</span><span class="n">inputs</span><span class="p">],</span>
                        <span class="n">func</span><span class="p">,</span>
                        <span class="n">check_trace_method</span><span class="p">,</span>
                        <span class="n">check_tolerance</span><span class="p">,</span>
                        <span class="n">strict</span><span class="p">,</span>
                        <span class="n">_force_outplace</span><span class="p">,</span>
                        <span class="kc">True</span><span class="p">,</span>
                        <span class="n">_module_class</span><span class="p">,</span>
                        <span class="n">example_inputs_is_kwarg</span><span class="o">=</span><span class="n">example_inputs_is_kwarg</span><span class="p">,</span>
                    <span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_trace</span><span class="o">.</span><span class="n">_trace_module_map</span> <span class="o">=</span> <span class="n">old_module_map</span>

    <span class="k">return</span> <span class="n">module</span></div>


<div class="viewcode-block" id="is_tracing"><a class="viewcode-back" href="../../../jit_language_reference.html#torch.jit.is_tracing">[docs]</a><span class="k">def</span> <span class="nf">is_tracing</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns ``True`` in tracing (if a function is called during the tracing of</span>
<span class="sd">    code with ``torch.jit.trace``) and ``False`` otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_is_tracing</span><span class="p">()</span></div>


<span class="k">class</span> <span class="nc">TracedModule</span><span class="p">(</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="n">_disable_script_meta</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">orig</span><span class="p">,</span> <span class="n">id_set</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_compilation_unit</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># XXX: orig can be a nn.Module or a function!</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span>

        <span class="c1"># Copy a subset of `orig` to a temporary nn.Module.</span>
        <span class="c1"># This is a way to customize what will actually get compiled by create_script_module</span>
        <span class="n">id_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

        <span class="c1"># This allows us to preserve the original module&#39;s qualified name by defining a new</span>
        <span class="c1"># type with the attribute _jit_override_qualname. In torch._jit_internal._qualified_name</span>
        <span class="c1"># we have a special case that will look up this attribute to override whatever qualname</span>
        <span class="c1"># we would get from the python type system</span>
        <span class="k">class</span> <span class="nc">QualnameWrapper</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">pass</span>

        <span class="n">QualnameWrapper</span><span class="o">.</span><span class="n">_jit_override_qualname</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">_qualified_name</span><span class="p">(</span>  <span class="c1"># type: ignore[attr-defined]</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">orig</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">tmp_module</span> <span class="o">=</span> <span class="n">QualnameWrapper</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">check_unique</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">id_set</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;TracedModules don&#39;t support parameter sharing between modules&quot;</span>
                <span class="p">)</span>
            <span class="n">id_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
        <span class="n">tmp_module</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">orig</span><span class="o">.</span><span class="n">training</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">orig</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tmp_module</span><span class="o">.</span><span class="n">_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span>
                <span class="n">check_unique</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">orig</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">buf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tmp_module</span><span class="o">.</span><span class="n">_buffers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">buf</span>
                <span class="n">check_unique</span><span class="p">(</span><span class="n">buf</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">orig</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_is_script_object</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">orig</span><span class="o">.</span><span class="n">_parameters</span>
                <span class="ow">and</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">orig</span><span class="o">.</span><span class="n">_buffers</span>
            <span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">tmp_module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">orig</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Modules that have backward hooks assigned can&#39;t be compiled: &quot;</span>
                <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">orig</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">orig</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">submodule</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">tmp_module</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">make_module</span><span class="p">(</span>
                <span class="n">submodule</span><span class="p">,</span> <span class="n">TracedModule</span><span class="p">,</span> <span class="n">_compilation_unit</span><span class="o">=</span><span class="kc">None</span>
            <span class="p">)</span>

        <span class="n">script_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_recursive</span><span class="o">.</span><span class="n">create_script_module</span><span class="p">(</span>
            <span class="n">tmp_module</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="p">(),</span> <span class="n">share_types</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_tracing</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">orig</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_actual_script_module&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">script_module</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;_parameters&quot;</span><span class="p">,</span> <span class="s2">&quot;_buffers&quot;</span><span class="p">,</span> <span class="s2">&quot;_modules&quot;</span><span class="p">,</span> <span class="s2">&quot;training&quot;</span><span class="p">):</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Trace submodules cannot be called.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;_actual_script_module&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getattr__</span><span class="p">(</span><span class="n">attr</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actual_script_module</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;_actual_script_module&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actual_script_module</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;original_name=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TopLevelTracedModule</span><span class="p">(</span><span class="n">TracedModule</span><span class="p">):</span>
    <span class="n">forward</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">_CachedForward</span><span class="p">()</span>  <span class="c1"># type: ignore[assignment]</span>

    <span class="k">def</span> <span class="nf">_reconstruct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cpp_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Re-construct an instance of TopLevelTracedModule using an instance of a C++ module.</span>

<span class="sd">        Args:</span>
<span class="sd">            cpp_module: The C++ module that this TopLevelTracedModule will be rebuilt around.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_actual_script_module&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">_reconstruct</span><span class="p">(</span><span class="n">cpp_module</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_script_if_tracing</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_tracing</span><span class="p">():</span>
            <span class="c1"># Not tracing, don&#39;t do anything</span>
            <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">script</span><span class="p">(</span><span class="n">wrapper</span><span class="o">.</span><span class="n">__original_fn</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">return</span> <span class="n">compiled_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">wrapper</span><span class="o">.</span><span class="n">__original_fn</span> <span class="o">=</span> <span class="n">fn</span>  <span class="c1"># type: ignore[attr-defined]</span>
    <span class="n">wrapper</span><span class="o">.</span><span class="n">__script_if_tracing_wrapper</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">return</span> <span class="n">wrapper</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_disable_dynamo</span>
<span class="k">def</span> <span class="nf">_get_trace_graph</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">_force_outplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">return_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">_return_inputs_states</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. warning::</span>
<span class="sd">        This function is internal-only and should only be used by the ONNX</span>
<span class="sd">        exporter. If you are trying to get a graph through tracing, please go</span>
<span class="sd">        through the public API instead::</span>

<span class="sd">            trace = torch.jit.trace(nn.LSTMCell(), (input, hidden))</span>
<span class="sd">            trace_graph = trace.graph</span>

<span class="sd">    Trace a function or model, returning a tuple consisting of the both the</span>
<span class="sd">    *trace* of an execution, as well as the original return value. If return_inputs,</span>
<span class="sd">    also returns the trace inputs as part of the tuple</span>

<span class="sd">    Tracing is guaranteed not to change the semantics of the function/module</span>
<span class="sd">    that is traced.</span>

<span class="sd">    Args:</span>
<span class="sd">        f (torch.nn.Module or function): the function or module</span>
<span class="sd">            to be traced.</span>
<span class="sd">        args (tuple or Tensor): the positional arguments to pass to the</span>
<span class="sd">            function/module to be traced.  A non-tuple is assumed to</span>
<span class="sd">            be a single positional argument to be passed to the model.</span>
<span class="sd">        kwargs (dict): the keyword arguments to pass to the function/module</span>
<span class="sd">            to be traced.</span>

<span class="sd">    Example (trace a cell):</span>

<span class="sd">    .. testcode::</span>

<span class="sd">        trace = torch.jit.trace(nn.LSTMCell(), (input, hidden))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">args</span><span class="p">,)</span>
    <span class="n">outs</span> <span class="o">=</span> <span class="n">ONNXTracedModule</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">_force_outplace</span><span class="p">,</span> <span class="n">return_inputs</span><span class="p">,</span> <span class="n">_return_inputs_states</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outs</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
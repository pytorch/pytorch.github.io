


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.cuda.graphs &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/cuda/graphs.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/cuda/graphs.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../cuda.html">torch.cuda</a> &gt;</li>
        
      <li>torch.cuda.graphs</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.cuda.graphs</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">gc</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_dummy_type</span>
<span class="kn">from</span> <span class="nn">torch.utils._pytree</span> <span class="kn">import</span> <span class="n">tree_flatten</span> <span class="k">as</span> <span class="n">_tree_flatten</span>
<span class="kn">from</span> <span class="nn">torch.utils._pytree</span> <span class="kn">import</span> <span class="n">tree_unflatten</span> <span class="k">as</span> <span class="n">_tree_unflatten</span>

<span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s1">&#39;_CudaStreamBase&#39;</span><span class="p">):</span>
    <span class="c1"># Define dummy base classes</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;_CUDAGraph&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s1">&#39;_CUDAGraph&#39;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;_graph_pool_handle&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s1">&#39;_graph_pool_handle&#39;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;_cuda_isCurrentStreamCapturing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s1">&#39;_cuda_isCurrentStreamCapturing&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_CUDAGraph</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_graph_pool_handle</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_cuda_isCurrentStreamCapturing</span>


<div class="viewcode-block" id="is_current_stream_capturing"><a class="viewcode-back" href="../../../generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing">[docs]</a><span class="k">def</span> <span class="nf">is_current_stream_capturing</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.</span>

<span class="sd">    If a CUDA context does not exist on the current device, returns False without initializing the context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_cuda_isCurrentStreamCapturing</span><span class="p">()</span></div>

<span class="c1"># Python shim helps Sphinx process docstrings more reliably.</span>
<div class="viewcode-block" id="graph_pool_handle"><a class="viewcode-back" href="../../../generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle">[docs]</a><span class="k">def</span> <span class="nf">graph_pool_handle</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an opaque token representing the id of a graph memory pool.</span>
<span class="sd">    See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_graph_pool_handle</span><span class="p">()</span></div>


<span class="c1"># Python shim helps Sphinx process docstrings more reliably.</span>
<div class="viewcode-block" id="CUDAGraph"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph">[docs]</a><span class="k">class</span> <span class="nc">CUDAGraph</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_CUDAGraph</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Wrapper around a CUDA graph.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">CUDAGraph</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

<div class="viewcode-block" id="CUDAGraph.capture_begin"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_begin">[docs]</a>    <span class="k">def</span> <span class="nf">capture_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Begins capturing CUDA work on the current stream.</span>

<span class="sd">        Typically, you shouldn&#39;t call ``capture_begin`` yourself.</span>
<span class="sd">        Use :class:`~torch.cuda.graph` or :func:`~torch.cuda.make_graphed_callables`,</span>
<span class="sd">        which call ``capture_begin`` internally.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            pool (optional): Token (returned by :func:`~torch.cuda.graph_pool_handle` or</span>
<span class="sd">                :meth:`other_Graph_instance.pool()&lt;torch.cuda.CUDAGraph.pool&gt;`) that hints this graph may share memory</span>
<span class="sd">                with the indicated pool.  See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># I&#39;m not sure if pybind11 converts a None arg to the default defined on the C++ side,</span>
        <span class="c1"># so I&#39;m not taking any chances.</span>
        <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">capture_begin</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">capture_begin</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span></div>

<div class="viewcode-block" id="CUDAGraph.capture_end"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_end">[docs]</a>    <span class="k">def</span> <span class="nf">capture_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Ends CUDA graph capture on the current stream.</span>
<span class="sd">        After ``capture_end``, ``replay`` may be called on this instance.</span>

<span class="sd">        Typically, you shouldn&#39;t call ``capture_end`` yourself.</span>
<span class="sd">        Use :class:`~torch.cuda.graph` or :func:`~torch.cuda.make_graphed_callables`,</span>
<span class="sd">        which call ``capture_end`` internally.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">capture_end</span><span class="p">()</span></div>

<div class="viewcode-block" id="CUDAGraph.replay"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.replay">[docs]</a>    <span class="k">def</span> <span class="nf">replay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replays the CUDA work captured by this graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span></div>

<div class="viewcode-block" id="CUDAGraph.reset"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Deletes the graph currently held by this instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span></div>

<div class="viewcode-block" id="CUDAGraph.pool"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.pool">[docs]</a>    <span class="k">def</span> <span class="nf">pool</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns an opaque token representing the id of this graph&#39;s memory pool.</span>
<span class="sd">        This id can optionally be passed to another graph&#39;s ``capture_begin``,</span>
<span class="sd">        which hints the other graph may share the same memory pool.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">pool</span><span class="p">()</span></div>

<div class="viewcode-block" id="CUDAGraph.enable_debug_mode"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.enable_debug_mode">[docs]</a>    <span class="k">def</span> <span class="nf">enable_debug_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enables debugging mode for CUDAGraph.debug_dump.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">enable_debug_mode</span><span class="p">()</span></div>

<div class="viewcode-block" id="CUDAGraph.debug_dump"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.debug_dump">[docs]</a>    <span class="k">def</span> <span class="nf">debug_dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">debug_path</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Arguments:</span>
<span class="sd">            debug_path (required): Path to dump the graph to.</span>

<span class="sd">        Calls a debugging function to dump the graph if the debugging is</span>
<span class="sd">        enabled via CUDAGraph.enable_debug_mode()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">debug_dump</span><span class="p">(</span><span class="n">debug_path</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="graph"><a class="viewcode-back" href="../../../generated/torch.cuda.graph.html#torch.cuda.graph">[docs]</a><span class="k">class</span> <span class="nc">graph</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Context-manager that captures CUDA work into a :class:`torch.cuda.CUDAGraph`</span>
<span class="sd">    object for later replay.</span>

<span class="sd">    See :ref:`CUDA Graphs &lt;cuda-graph-semantics&gt;` for a general introduction,</span>
<span class="sd">    detailed use, and constraints.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        cuda_graph (torch.cuda.CUDAGraph): Graph object used for capture.</span>
<span class="sd">        pool (optional): Opaque token (returned by a call to :func:`~torch.cuda.graph_pool_handle()` or</span>
<span class="sd">            :meth:`other_Graph_instance.pool()&lt;torch.cuda.CUDAGraph.pool&gt;`) hinting this graph&#39;s capture</span>
<span class="sd">            may share memory from the specified pool. See :ref:`Graph memory management&lt;graph-memory-management&gt;`.</span>
<span class="sd">        stream (torch.cuda.Stream, optional): If supplied, will be set as the current stream in the context.</span>
<span class="sd">            If not supplied, ``graph`` sets its own internal side stream as the current stream in the context.</span>

<span class="sd">    .. note::</span>
<span class="sd">        For effective memory sharing, if you pass a ``pool`` used by a previous capture and the previous capture</span>
<span class="sd">        used an explicit ``stream`` argument, you should pass the same ``stream`` argument to this capture.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">default_capture_stream</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">cuda_graph</span><span class="p">,</span>
                 <span class="n">pool</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Lazy-init of default_capture_stream helps avoid circular-import errors.</span>
        <span class="c1"># Not thread safe, but graphs already have the general (explicitly documented)</span>
        <span class="c1"># restriction that only one capture may be underway at a time in the process.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">pool</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">pool</span><span class="p">,)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span> <span class="o">=</span> <span class="n">stream</span> <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="n">default_capture_stream</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capture_stream</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span> <span class="o">=</span> <span class="n">cuda_graph</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Free as much memory as we can for the graph</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="c1"># Stackoverflow seems comfortable with this pattern</span>
        <span class="c1"># https://stackoverflow.com/questions/26635684/calling-enter-and-exit-manually#39172487</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span><span class="o">.</span><span class="n">capture_begin</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">)</span>


    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cuda_graph</span><span class="o">.</span><span class="n">capture_end</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stream_ctx</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_value</span><span class="p">,</span> <span class="n">traceback</span><span class="p">)</span></div>
        <span class="c1"># returning None should propagate exceptions from either capture_end or stream_ctx.__exit__()</span>


<div class="viewcode-block" id="make_graphed_callables"><a class="viewcode-back" href="../../../generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables">[docs]</a><span class="k">def</span> <span class="nf">make_graphed_callables</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="n">sample_args</span><span class="p">,</span> <span class="n">num_warmup_iters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">allow_unused_input</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Accepts callables (functions or :class:`nn.Module&lt;torch.nn.Module&gt;`\ s)</span>
<span class="sd">    and returns graphed versions.</span>

<span class="sd">    Each graphed callable&#39;s forward pass runs its source callable&#39;s</span>
<span class="sd">    forward CUDA work as a CUDA graph inside a single autograd node.</span>

<span class="sd">    The graphed callable&#39;s forward pass also appends</span>
<span class="sd">    a backward node to the autograd graph. During backward, this node runs the</span>
<span class="sd">    callable&#39;s backward work as a CUDA graph.</span>

<span class="sd">    Therefore, each graphed callable should be a drop-in replacement for its source callable</span>
<span class="sd">    in an autograd-enabled training loop.</span>

<span class="sd">    See :ref:`Partial-network capture&lt;partial-network-capture&gt;` for detailed use and constraints.</span>

<span class="sd">    If you pass a tuple of several callables, their captures will use the same memory pool.</span>
<span class="sd">    See :ref:`Graph memory management&lt;graph-memory-management&gt;` for when this is appropriate.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        callables (torch.nn.Module or Python function, or tuple of these): Callable or callables to graph.</span>
<span class="sd">            See :ref:`Graph memory management&lt;graph-memory-management&gt;` for when passing a tuple of callables</span>
<span class="sd">            is appropriate.  If you pass a tuple of callables, their order in the tuple must be the same order</span>
<span class="sd">            they&#39;ll run in the live workload.</span>
<span class="sd">        sample_args (tuple of Tensors, or tuple of tuples of Tensors): Samples args for each callable.</span>
<span class="sd">            If a single callable was passed, ``sample_args`` must be a single tuple of argument Tensors.</span>
<span class="sd">            If a tuple of callables was passed, ``sample_args`` must be tuple of tuples of argument Tensors.</span>
<span class="sd">        num_warmup_iters (int): The number of warmup iterations. Currently, ``DataDistributedParallel`` needs</span>
<span class="sd">            11 iterations for warm up. Default: ``3``.</span>
<span class="sd">        allow_unused_input (bool): If False, specifying inputs that were not used when computing outputs</span>
<span class="sd">            (and therefore their grad is always zero) is an error. Defaults to False.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The ``requires_grad`` state of each Tensor in ``sample_args`` must match the state</span>
<span class="sd">        that&#39;s expected for the corresponding real input in the training loop.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This API is in beta and may change in future releases.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        ``sample_args`` for each callable must contain only Tensors. Other types are not allowed.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Returned callables do not support higher order differentiation (e.g., double backward).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In any :class:`~torch.nn.Module` passed to :func:`~make_graphed_callables`, only parameters</span>
<span class="sd">        may be trainable. Buffers must have ``requires_grad=False``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        After you pass a :class:`torch.nn.Module` through :func:`~make_graphed_callables`,</span>
<span class="sd">        you may not add or remove any of that Module&#39;s parameters or buffers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :class:`torch.nn.Module`\s passed to :func:`~torch.cuda.make_graphed_callables` must not have module hooks</span>
<span class="sd">        registered on them at the time they are passed. However, registering hooks on modules *after* passing them</span>
<span class="sd">        through :func:`~torch.cuda.make_graphed_callables` is allowed.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        When running a graphed callable, you must pass its arguments in the same order and format</span>
<span class="sd">        they appeared in that callable&#39;s ``sample_args``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The automatic mixed precision is supported in :func:`~torch.cuda.make_graphed_callables` only with disabled</span>
<span class="sd">        caching. The context manager `torch.cuda.amp.autocast()` must have `cache_enabled=False`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_autocast_enabled</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_autocast_cache_enabled</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;make_graphed_callables does not support the autocast caching. Please set `cache_enabled=False`.&quot;</span><span class="p">)</span>

    <span class="n">just_one_callable</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">just_one_callable</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">callables</span> <span class="o">=</span> <span class="p">(</span><span class="n">callables</span><span class="p">,)</span>
        <span class="n">sample_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_args</span><span class="p">,)</span>

    <span class="n">flatten_sample_args</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">args</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span> <span class="n">sample_args</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_forward_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">_forward_pre_hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> \
                <span class="s2">&quot;Modules must not have hooks registered at the time they are passed. However, registering hooks &quot;</span> <span class="o">+</span> \
                <span class="s2">&quot;on modules after passing them through make_graphed_callables is allowed.&quot;</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">is</span> <span class="kc">False</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">c</span><span class="o">.</span><span class="n">buffers</span><span class="p">()),</span> <span class="s2">&quot;In any :class:`~torch.nn.Module` passed to &quot;</span> <span class="o">+</span> \
                <span class="s2">&quot;:func:`~make_graphed_callables`, only parameters may be trainable. All buffers must have &quot;</span> <span class="o">+</span> \
                <span class="s2">&quot;``requires_grad=False``.&quot;</span>
        <span class="n">flatten_arg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_tree_flatten</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">flatten_sample_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_arg</span><span class="p">))</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">flatten_arg</span><span class="p">),</span> <span class="s2">&quot;In the beta API, sample_args &quot;</span> <span class="o">+</span> \
            <span class="s2">&quot;for each callable must contain only Tensors. Other types are not allowed.&quot;</span>


    <span class="c1"># If a callable is an nn.Module, its graph&#39;s full input surface is the args the user explicitly</span>
    <span class="c1"># passes to forward (ie, its sample_args) AND the module&#39;s parameter attributes.</span>
    <span class="n">per_callable_len_user_args</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="k">for</span> <span class="n">args</span> <span class="ow">in</span> <span class="n">flatten_sample_args</span><span class="p">]</span>
    <span class="n">per_callable_module_params</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="k">else</span> <span class="p">()</span>
                                  <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">callables</span><span class="p">]</span>
    <span class="n">per_callable_static_input_surfaces</span> <span class="o">=</span> <span class="p">[</span><span class="n">flatten_sample_args</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">per_callable_module_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                                          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))]</span>

    <span class="n">fwd_graphs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">CUDAGraph</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))]</span>
    <span class="n">bwd_graphs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">CUDAGraph</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">callables</span><span class="p">))]</span>

    <span class="n">mempool</span> <span class="o">=</span> <span class="n">graph_pool_handle</span><span class="p">()</span>

    <span class="c1"># Warmup</span>
    <span class="c1"># Hopefully prevents cudnn benchmarking and other lazy-initialization cuda work</span>
    <span class="c1"># from ending up in any captures.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">static_input_surface</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span>
                                                    <span class="n">sample_args</span><span class="p">,</span>
                                                    <span class="n">per_callable_static_input_surfaces</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_warmup_iters</span><span class="p">):</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_tree_flatten</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
                <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
                                                  <span class="n">inputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">static_input_surface</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
                                                  <span class="n">grad_outputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
                                                  <span class="n">only_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                  <span class="n">allow_unused</span><span class="o">=</span><span class="n">allow_unused_input</span><span class="p">)</span>
            <span class="k">del</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">grad_inputs</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="c1"># All captures here share a mempool. To avoid replays corrupting each other&#39;s memory,</span>
    <span class="c1"># the safest approach is to capture all passes in the same order they&#39;ll run:</span>
    <span class="c1"># fwd 1, fwd 2, ... fwd N, then bwd N, bwd N-1, ... bwd 1.</span>

    <span class="c1"># Capture forward graphs</span>
    <span class="n">per_callable_static_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">per_callable_output_unflatten_spec</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">fwd_graph</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">callables</span><span class="p">,</span>
                                     <span class="n">sample_args</span><span class="p">,</span>
                                     <span class="n">fwd_graphs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">graph</span><span class="p">(</span><span class="n">fwd_graph</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="n">mempool</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

        <span class="n">flatten_outputs</span><span class="p">,</span> <span class="n">spec</span> <span class="o">=</span> <span class="n">_tree_flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="n">per_callable_static_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_outputs</span><span class="p">))</span>
        <span class="n">per_callable_output_unflatten_spec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>


    <span class="c1"># Capture backward graphs in reverse order</span>
    <span class="n">per_callable_static_grad_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">per_callable_static_grad_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">static_input_surface</span><span class="p">,</span> <span class="n">static_outputs</span><span class="p">,</span> <span class="n">bwd_graph</span><span class="p">,</span> <span class="n">module_params</span> <span class="ow">in</span> \
            <span class="nb">zip</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_input_surfaces</span><span class="p">),</span>
                <span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_outputs</span><span class="p">),</span>
                <span class="nb">reversed</span><span class="p">(</span><span class="n">bwd_graphs</span><span class="p">),</span>
                <span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_module_params</span><span class="p">)):</span>

        <span class="c1"># For now, assumes all static_outputs require grad</span>
        <span class="c1"># assert all(o.requires_grad for o in static_outputs), &quot;Outputs of graphed callables must require grad.&quot;</span>
        <span class="n">static_grad_outputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">graph</span><span class="p">(</span><span class="n">bwd_graph</span><span class="p">,</span> <span class="n">pool</span><span class="o">=</span><span class="n">mempool</span><span class="p">):</span>
            <span class="n">grad_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span> <span class="k">if</span> <span class="n">o</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
                                              <span class="n">inputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">static_input_surface</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span>
                                              <span class="n">grad_outputs</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">o</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_grad_outputs</span> <span class="k">if</span> <span class="n">o</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">),</span>
                                              <span class="n">only_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                              <span class="n">allow_unused</span><span class="o">=</span><span class="n">allow_unused_input</span><span class="p">)</span>

        <span class="c1"># Constructs a tuple suitable for returning from Graphed.backward:</span>
        <span class="c1"># Pads out the actually-needed grads with Nones in gradient slots for inputs that don&#39;t require grad.</span>
        <span class="c1"># I couldn&#39;t think of a slick one-liner for this pattern.</span>
        <span class="n">static_grad_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">grad_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">static_input_surface</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">arg</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_inputs</span><span class="p">[</span><span class="n">grad_idx</span><span class="p">])</span>
                <span class="n">grad_idx</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="n">static_grad_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>

        <span class="n">per_callable_static_grad_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">static_grad_outputs</span><span class="p">)</span>
        <span class="n">per_callable_static_grad_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">)</span>

    <span class="c1"># Reverses the most recent two lists</span>
    <span class="n">per_callable_static_grad_outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_grad_outputs</span><span class="p">))</span>
    <span class="n">per_callable_static_grad_inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">per_callable_static_grad_inputs</span><span class="p">))</span>
    <span class="c1"># Now for every per_callable list, per_callable_*[i] holds the stuff for the ith callable.</span>

    <span class="k">def</span> <span class="nf">make_graphed_autograd_function</span><span class="p">(</span><span class="n">fwd_graph</span><span class="p">,</span>
                                       <span class="n">bwd_graph</span><span class="p">,</span>
                                       <span class="n">module_params</span><span class="p">,</span>
                                       <span class="n">len_user_args</span><span class="p">,</span>
                                       <span class="n">output_unflatten_spec</span><span class="p">,</span>
                                       <span class="n">static_input_surface</span><span class="p">,</span>
                                       <span class="n">static_outputs</span><span class="p">,</span>
                                       <span class="n">static_grad_outputs</span><span class="p">,</span>
                                       <span class="n">static_grad_inputs</span><span class="p">):</span>
        <span class="k">class</span> <span class="nc">Graphed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
            <span class="nd">@staticmethod</span>
            <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
                <span class="c1"># At this stage, only the user args may (potentially) be new tensors.</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_user_args</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">static_input_surface</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">():</span>
                        <span class="n">static_input_surface</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">fwd_graph</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">static_outputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">static_outputs</span><span class="p">)</span>

            <span class="nd">@staticmethod</span>
            <span class="nd">@torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="n">once_differentiable</span>
            <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">grads</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">static_grad_outputs</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">static_grad_outputs</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="c1"># don&#39;t copy if autograd gods have been kind and the</span>
                        <span class="c1"># incoming grad is already in the right place</span>
                        <span class="k">if</span> <span class="n">g</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">!=</span> <span class="n">grad</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">():</span>
                            <span class="n">g</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">bwd_graph</span><span class="o">.</span><span class="n">replay</span><span class="p">()</span>

                <span class="c1"># Input args that didn&#39;t require grad expect a None gradient.</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">static_grad_inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">static_grad_inputs</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">functionalized</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">):</span>
            <span class="c1"># Runs the autograd function with inputs == all inputs to the graph that might require grad</span>
            <span class="c1"># (explicit user args + module parameters)</span>
            <span class="c1"># Assumes module params didn&#39;t change since capture.</span>
            <span class="n">flatten_user_args</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_tree_flatten</span><span class="p">(</span><span class="n">user_args</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">Graphed</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">flatten_user_args</span><span class="p">)</span> <span class="o">+</span> <span class="n">module_params</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">_tree_unflatten</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">output_unflatten_spec</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">functionalized</span>

    <span class="c1"># Put together the final graphed callables</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">callables</span><span class="p">):</span>
        <span class="n">graphed</span> <span class="o">=</span> <span class="n">make_graphed_autograd_function</span><span class="p">(</span><span class="n">fwd_graphs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">bwd_graphs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_module_params</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_len_user_args</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_output_unflatten_spec</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_static_input_surfaces</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_static_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_static_grad_outputs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                                 <span class="n">per_callable_static_grad_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">def</span> <span class="nf">make_graphed_forward</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">graph_training_state</span><span class="p">,</span> <span class="n">graphed</span><span class="p">,</span> <span class="n">orig_fwd</span><span class="p">):</span>
                <span class="k">def</span> <span class="nf">new_fwd</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">):</span>
                    <span class="c1"># If the module&#39;s training-or-eval state matches what we graphed,</span>
                    <span class="c1"># run the graph, otherwise run the original forward method</span>
                    <span class="k">if</span> <span class="n">func</span><span class="o">.</span><span class="n">training</span> <span class="o">==</span> <span class="n">graph_training_state</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">graphed</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">orig_fwd</span><span class="p">(</span><span class="o">*</span><span class="n">user_args</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">new_fwd</span>
            <span class="n">func</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">make_graphed_forward</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">training</span><span class="p">,</span> <span class="n">graphed</span><span class="p">,</span> <span class="n">func</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">graphed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">just_one_callable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
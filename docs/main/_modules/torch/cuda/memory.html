


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.cuda.memory &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/cuda/memory.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/cuda/memory.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../cuda.html">torch.cuda</a> &gt;</li>
        
      <li>torch.cuda.memory</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.cuda.memory</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">ctypes</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">is_initialized</span><span class="p">,</span> <span class="n">_get_device_index</span><span class="p">,</span> <span class="n">_lazy_init</span><span class="p">,</span> <span class="n">_get_nvml_device_index</span>
<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_dummy_type</span>

<span class="kn">from</span> <span class="nn">._memory_viz</span> <span class="kn">import</span> <span class="n">segments</span> <span class="k">as</span> <span class="n">_segments</span><span class="p">,</span> <span class="n">memory</span> <span class="k">as</span> <span class="n">_memory</span><span class="p">,</span> <span class="n">segment_plot</span><span class="p">,</span> <span class="n">trace_plot</span>

<span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">Device</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">_C</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;caching_allocator_alloc&quot;</span><span class="p">,</span> <span class="s2">&quot;caching_allocator_delete&quot;</span><span class="p">,</span> <span class="s2">&quot;set_per_process_memory_fraction&quot;</span><span class="p">,</span>
           <span class="s2">&quot;empty_cache&quot;</span><span class="p">,</span> <span class="s2">&quot;memory_stats&quot;</span><span class="p">,</span> <span class="s2">&quot;memory_stats_as_nested_dict&quot;</span><span class="p">,</span> <span class="s2">&quot;reset_accumulated_memory_stats&quot;</span><span class="p">,</span>
           <span class="s2">&quot;reset_peak_memory_stats&quot;</span><span class="p">,</span> <span class="s2">&quot;reset_max_memory_allocated&quot;</span><span class="p">,</span> <span class="s2">&quot;reset_max_memory_cached&quot;</span><span class="p">,</span>
           <span class="s2">&quot;memory_allocated&quot;</span><span class="p">,</span> <span class="s2">&quot;max_memory_allocated&quot;</span><span class="p">,</span> <span class="s2">&quot;memory_reserved&quot;</span><span class="p">,</span> <span class="s2">&quot;max_memory_reserved&quot;</span><span class="p">,</span>
           <span class="s2">&quot;memory_cached&quot;</span><span class="p">,</span> <span class="s2">&quot;max_memory_cached&quot;</span><span class="p">,</span> <span class="s2">&quot;memory_snapshot&quot;</span><span class="p">,</span> <span class="s2">&quot;memory_summary&quot;</span><span class="p">,</span> <span class="s2">&quot;list_gpu_processes&quot;</span><span class="p">,</span>
           <span class="s2">&quot;mem_get_info&quot;</span><span class="p">,</span> <span class="s2">&quot;get_allocator_backend&quot;</span><span class="p">,</span> <span class="s2">&quot;CUDAPluggableAllocator&quot;</span><span class="p">,</span> <span class="s2">&quot;change_current_allocator&quot;</span><span class="p">]</span>


<span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="p">,</span> <span class="s1">&#39;_cuda_CUDAAllocator&#39;</span><span class="p">):</span>
    <span class="c1"># Define dummy base classes</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;_cuda_CUDAAllocator&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_dummy_type</span><span class="p">(</span><span class="s1">&#39;_cuda_CUDAAllocator&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_host_allocator</span><span class="p">():</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_cudaHostAllocator</span><span class="p">()</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_free_mutex</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_lock_mutex</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_unlock_mutex</span><span class="p">()</span>


<div class="viewcode-block" id="caching_allocator_alloc"><a class="viewcode-back" href="../../../generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc">[docs]</a><span class="k">def</span> <span class="nf">caching_allocator_alloc</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Performs a memory allocation using the CUDA memory allocator.</span>

<span class="sd">    Memory is allocated for a given device and a stream, this</span>
<span class="sd">    function is intended to be used for interoperability with other</span>
<span class="sd">    frameworks. Allocated memory is released through</span>
<span class="sd">    :func:`~torch.cuda.caching_allocator_delete`.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (int): number of bytes to be allocated.</span>
<span class="sd">        device (torch.device or int, optional): selected device. If it is</span>
<span class="sd">            ``None`` the default CUDA device is used.</span>
<span class="sd">        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then</span>
<span class="sd">            the default stream for the selected device is used.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">Stream</span><span class="p">):</span>
        <span class="n">stream</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">cuda_stream</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Invalid type for stream argument, must be &#39;</span>
                        <span class="s1">&#39;`torch.cuda.Stream` or `int` representing a pointer &#39;</span>
                        <span class="s1">&#39;to a existing stream&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_cudaCachingAllocator_raw_alloc</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span></div>


<div class="viewcode-block" id="caching_allocator_delete"><a class="viewcode-back" href="../../../generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete">[docs]</a><span class="k">def</span> <span class="nf">caching_allocator_delete</span><span class="p">(</span><span class="n">mem_ptr</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Deletes memory allocated using the CUDA memory allocator.</span>

<span class="sd">    Memory allocated with :func:`~torch.cuda.caching_allocator_alloc`.</span>
<span class="sd">    is freed here. The associated device and stream are tracked inside</span>
<span class="sd">    the allocator.</span>

<span class="sd">    Args:</span>
<span class="sd">        mem_ptr (int): memory address to be freed by the allocator.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_cudaCachingAllocator_raw_delete</span><span class="p">(</span><span class="n">mem_ptr</span><span class="p">)</span></div>


<div class="viewcode-block" id="set_per_process_memory_fraction"><a class="viewcode-back" href="../../../generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction">[docs]</a><span class="k">def</span> <span class="nf">set_per_process_memory_fraction</span><span class="p">(</span><span class="n">fraction</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set memory fraction for a process.</span>
<span class="sd">    The fraction is used to limit an caching allocator to allocated memory on a CUDA device.</span>
<span class="sd">    The allowed value equals the total visible memory multiplied fraction.</span>
<span class="sd">    If trying to allocate more than the allowed value in a process, will raise an out of</span>
<span class="sd">    memory error in allocator.</span>

<span class="sd">    Args:</span>
<span class="sd">        fraction(float): Range: 0~1. Allowed memory equals total_memory * fraction.</span>
<span class="sd">        device (torch.device or int, optional): selected device. If it is</span>
<span class="sd">            ``None`` the default CUDA device is used.</span>
<span class="sd">    .. note::</span>
<span class="sd">        In general, the total available free memory is less than the total capacity.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fraction</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Invalid type for fraction argument, must be `float`&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fraction</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">fraction</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid fraction value: </span><span class="si">{}</span><span class="s1">. &#39;</span>
                         <span class="s1">&#39;Allowed range: 0~1&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fraction</span><span class="p">))</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_setMemoryFraction</span><span class="p">(</span><span class="n">fraction</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="empty_cache"><a class="viewcode-back" href="../../../generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache">[docs]</a><span class="k">def</span> <span class="nf">empty_cache</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Releases all unoccupied cached memory currently held by the caching</span>
<span class="sd">    allocator so that those can be used in other GPU application and visible in</span>
<span class="sd">    `nvidia-smi`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        :func:`~torch.cuda.empty_cache` doesn&#39;t increase the amount of GPU</span>
<span class="sd">        memory available for PyTorch. However, it may help reduce fragmentation</span>
<span class="sd">        of GPU memory in certain cases. See :ref:`cuda-memory-management` for</span>
<span class="sd">        more details about GPU memory management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">is_initialized</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_emptyCache</span><span class="p">()</span></div>


<div class="viewcode-block" id="memory_stats"><a class="viewcode-back" href="../../../generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats">[docs]</a><span class="k">def</span> <span class="nf">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary of CUDA memory allocator statistics for a</span>
<span class="sd">    given device.</span>

<span class="sd">    The return value of this function is a dictionary of statistics, each of</span>
<span class="sd">    which is a non-negative integer.</span>

<span class="sd">    Core statistics:</span>

<span class="sd">    - ``&quot;allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of allocation requests received by the memory allocator.</span>
<span class="sd">    - ``&quot;allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      amount of allocated memory.</span>
<span class="sd">    - ``&quot;segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of reserved segments from ``cudaMalloc()``.</span>
<span class="sd">    - ``&quot;reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      amount of reserved memory.</span>
<span class="sd">    - ``&quot;active.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of active memory blocks.</span>
<span class="sd">    - ``&quot;active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      amount of active memory.</span>
<span class="sd">    - ``&quot;inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of inactive, non-releasable memory blocks.</span>
<span class="sd">    - ``&quot;inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      amount of inactive, non-releasable memory.</span>

<span class="sd">    For these core statistics, values are broken down as follows.</span>

<span class="sd">    Pool type:</span>

<span class="sd">    - ``all``: combined statistics across all memory pools.</span>
<span class="sd">    - ``large_pool``: statistics for the large allocation pool</span>
<span class="sd">      (as of October 2019, for size &gt;= 1MB allocations).</span>
<span class="sd">    - ``small_pool``: statistics for the small allocation pool</span>
<span class="sd">      (as of October 2019, for size &lt; 1MB allocations).</span>

<span class="sd">    Metric type:</span>

<span class="sd">    - ``current``: current value of this metric.</span>
<span class="sd">    - ``peak``: maximum value of this metric.</span>
<span class="sd">    - ``allocated``: historical total increase in this metric.</span>
<span class="sd">    - ``freed``: historical total decrease in this metric.</span>

<span class="sd">    In addition to the core statistics, we also provide some simple event</span>
<span class="sd">    counters:</span>

<span class="sd">    - ``&quot;num_alloc_retries&quot;``: number of failed ``cudaMalloc`` calls that</span>
<span class="sd">      result in a cache flush and retry.</span>
<span class="sd">    - ``&quot;num_ooms&quot;``: number of out-of-memory errors thrown.</span>

<span class="sd">    The caching allocator can be configured via ENV to not split blocks larger than a</span>
<span class="sd">    defined size (see Memory Management section of the Cuda Semantics documentation).</span>
<span class="sd">    This helps avoid memory fragmentation but may have a performance</span>
<span class="sd">    penalty. Additional outputs to assist with tuning and evaluating impact:</span>

<span class="sd">    - ``&quot;max_split_size&quot;``: blocks above this size will not be split.</span>
<span class="sd">    - ``&quot;oversize_allocations.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of over-size allocation requests received by the memory allocator.</span>
<span class="sd">    - ``&quot;oversize_segments.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of over-size reserved segments from ``cudaMalloc()``.</span>

<span class="sd">    The caching allocator can be configured via ENV to round memory allocations in order</span>
<span class="sd">    to reduce fragmentation. Sometimes the overhead from rounding can be higher than</span>
<span class="sd">    the fragmentation it helps reduce. The following stat can be used to check if</span>
<span class="sd">    rounding adds too much overhead:</span>

<span class="sd">    - ``&quot;requested_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      memory requested by client code, compare this with allocated_bytes to check if</span>
<span class="sd">      allocation rounding adds too much overhead.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistics for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>

<span class="sd">    .. note::</span>
<span class="sd">        With :ref:`backend:cudaMallocAsync&lt;cuda-memory-envvars&gt;`, some stats are not</span>
<span class="sd">        meaningful, and are always reported as zero.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">_recurse_add_to_result</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">obj</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">prefix</span> <span class="o">+=</span> <span class="s2">&quot;.&quot;</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">_recurse_add_to_result</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">prefix</span><span class="p">,</span> <span class="n">obj</span><span class="p">))</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="n">memory_stats_as_nested_dict</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">_recurse_add_to_result</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">stats</span><span class="p">)</span>
    <span class="n">result</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">(</span><span class="n">result</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">memory_stats_as_nested_dict</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the result of :func:`~torch.cuda.memory_stats` as a nested dictionary.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">{}</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_memoryStats</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reset_accumulated_memory_stats</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets the &quot;accumulated&quot; (historical) stats tracked by the CUDA memory allocator.</span>

<span class="sd">    See :func:`~torch.cuda.memory_stats` for details. Accumulated stats correspond to</span>
<span class="sd">    the `&quot;allocated&quot;` and `&quot;freed&quot;` keys in each individual stat dict, as well as</span>
<span class="sd">    `&quot;num_alloc_retries&quot;` and `&quot;num_ooms&quot;`.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_resetAccumulatedMemoryStats</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<div class="viewcode-block" id="reset_peak_memory_stats"><a class="viewcode-back" href="../../../generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats">[docs]</a><span class="k">def</span> <span class="nf">reset_peak_memory_stats</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets the &quot;peak&quot; stats tracked by the CUDA memory allocator.</span>

<span class="sd">    See :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the</span>
<span class="sd">    `&quot;peak&quot;` key in each individual stat dict.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_resetPeakMemoryStats</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="reset_max_memory_allocated"><a class="viewcode-back" href="../../../generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated">[docs]</a><span class="k">def</span> <span class="nf">reset_max_memory_allocated</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets the starting point in tracking maximum GPU memory occupied by</span>
<span class="sd">    tensors for a given device.</span>

<span class="sd">    See :func:`~torch.cuda.max_memory_allocated` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets</span>
<span class="sd">        /all/ peak memory stats.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, &quot;</span>
        <span class="s2">&quot;which resets /all/ peak memory stats.&quot;</span><span class="p">,</span>
        <span class="ne">FutureWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reset_peak_memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="reset_max_memory_cached"><a class="viewcode-back" href="../../../generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached">[docs]</a><span class="k">def</span> <span class="nf">reset_max_memory_cached</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets the starting point in tracking maximum GPU memory managed by the</span>
<span class="sd">    caching allocator for a given device.</span>

<span class="sd">    See :func:`~torch.cuda.max_memory_cached` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets</span>
<span class="sd">        /all/ peak memory stats.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, &quot;</span>
        <span class="s2">&quot;which resets /all/ peak memory stats.&quot;</span><span class="p">,</span>
        <span class="ne">FutureWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reset_peak_memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="memory_allocated"><a class="viewcode-back" href="../../../generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated">[docs]</a><span class="k">def</span> <span class="nf">memory_allocated</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current GPU memory occupied by tensors in bytes for a given</span>
<span class="sd">    device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        This is likely less than the amount shown in `nvidia-smi` since some</span>
<span class="sd">        unused memory can be held by the caching allocator and some context</span>
<span class="sd">        needs to be created on GPU. See :ref:`cuda-memory-management` for more</span>
<span class="sd">        details about GPU memory management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;allocated_bytes.all.current&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_memory_allocated"><a class="viewcode-back" href="../../../generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated">[docs]</a><span class="k">def</span> <span class="nf">max_memory_allocated</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum GPU memory occupied by tensors in bytes for a given</span>
<span class="sd">    device.</span>

<span class="sd">    By default, this returns the peak allocated memory since the beginning of</span>
<span class="sd">    this program. :func:`~torch.cuda.reset_peak_memory_stats` can be used to</span>
<span class="sd">    reset the starting point in tracking this metric. For example, these two</span>
<span class="sd">    functions can measure the peak allocated memory usage of each iteration in a</span>
<span class="sd">    training loop.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;allocated_bytes.all.peak&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="memory_reserved"><a class="viewcode-back" href="../../../generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved">[docs]</a><span class="k">def</span> <span class="nf">memory_reserved</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current GPU memory managed by the caching allocator in bytes</span>
<span class="sd">    for a given device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reserved_bytes.all.current&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_memory_reserved"><a class="viewcode-back" href="../../../generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved">[docs]</a><span class="k">def</span> <span class="nf">max_memory_reserved</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum GPU memory managed by the caching allocator in bytes</span>
<span class="sd">    for a given device.</span>

<span class="sd">    By default, this returns the peak cached memory since the beginning of this</span>
<span class="sd">    program. :func:`~torch.cuda.reset_peak_memory_stats` can be used to reset</span>
<span class="sd">    the starting point in tracking this metric. For example, these two functions</span>
<span class="sd">    can measure the peak cached memory amount of each iteration in a training</span>
<span class="sd">    loop.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reserved_bytes.all.peak&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="memory_cached"><a class="viewcode-back" href="../../../generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached">[docs]</a><span class="k">def</span> <span class="nf">memory_cached</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Deprecated; see :func:`~torch.cuda.memory_reserved`.&quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved&quot;</span><span class="p">,</span>
        <span class="ne">FutureWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">memory_reserved</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_memory_cached"><a class="viewcode-back" href="../../../generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached">[docs]</a><span class="k">def</span> <span class="nf">max_memory_cached</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Deprecated; see :func:`~torch.cuda.max_memory_reserved`.&quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.cuda.max_memory_cached has been renamed to torch.cuda.max_memory_reserved&quot;</span><span class="p">,</span>
        <span class="ne">FutureWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">max_memory_reserved</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="memory_snapshot"><a class="viewcode-back" href="../../../generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot">[docs]</a><span class="k">def</span> <span class="nf">memory_snapshot</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a snapshot of the CUDA memory allocator state across all devices.</span>

<span class="sd">    Interpreting the output of this function requires familiarity with the</span>
<span class="sd">    memory allocator internals.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_memorySnapshot</span><span class="p">()[</span><span class="s1">&#39;segments&#39;</span><span class="p">]</span></div>


<div class="viewcode-block" id="memory_summary"><a class="viewcode-back" href="../../../generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary">[docs]</a><span class="k">def</span> <span class="nf">memory_summary</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">abbreviated</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a human-readable printout of the current memory allocator</span>
<span class="sd">    statistics for a given device.</span>

<span class="sd">    This can be useful to display periodically during training, or when</span>
<span class="sd">    handling out-of-memory exceptions.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            printout for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>
<span class="sd">        abbreviated (bool, optional): whether to return an abbreviated summary</span>
<span class="sd">            (default: False).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_size</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">pref_sz</span><span class="p">):</span>
        <span class="n">prefixes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;B  &quot;</span><span class="p">,</span> <span class="s2">&quot;KiB&quot;</span><span class="p">,</span> <span class="s2">&quot;MiB&quot;</span><span class="p">,</span> <span class="s2">&quot;GiB&quot;</span><span class="p">,</span> <span class="s2">&quot;TiB&quot;</span><span class="p">,</span> <span class="s2">&quot;PiB&quot;</span><span class="p">]</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">prefixes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">new_prefix</span> <span class="ow">in</span> <span class="n">prefixes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">if</span> <span class="n">pref_sz</span> <span class="o">&lt;</span> <span class="mi">768</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">prefix</span> <span class="o">=</span> <span class="n">new_prefix</span>
            <span class="n">sz</span> <span class="o">//=</span> <span class="mi">1024</span>
            <span class="n">pref_sz</span> <span class="o">/=</span> <span class="mi">1024</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{:6d}</span><span class="s2"> </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_count</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span> <span class="n">pref_cnt</span><span class="p">):</span>
        <span class="n">prefixes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="s2">&quot;M&quot;</span><span class="p">]</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">prefixes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">new_prefix</span> <span class="ow">in</span> <span class="n">prefixes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">if</span> <span class="n">pref_cnt</span> <span class="o">&lt;</span> <span class="mi">750</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">prefix</span> <span class="o">=</span> <span class="n">new_prefix</span>
            <span class="n">cnt</span> <span class="o">//=</span> <span class="mi">1000</span>
            <span class="n">pref_cnt</span> <span class="o">/=</span> <span class="mi">1000</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{:7d}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>

    <span class="n">metrics_to_display</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;allocated_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;Allocated memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;active_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;Active memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;requested_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;Requested memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;reserved_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU reserved memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;inactive_split_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;Non-releasable memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;allocation&quot;</span><span class="p">,</span> <span class="s2">&quot;Allocations&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;active&quot;</span><span class="p">,</span> <span class="s2">&quot;Active allocs&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;segment&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU reserved segments&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;inactive_split&quot;</span><span class="p">,</span> <span class="s2">&quot;Non-releasable allocs&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; </span><span class="si">{_:16}</span><span class="s2"> PyTorch CUDA memory summary, device ID </span><span class="si">{device:&lt;17d}</span><span class="s2"> &quot;</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;  </span><span class="si">{_:9}</span><span class="s2"> CUDA OOMs: </span><span class="si">{num_ooms:&lt;12d}</span><span class="s2"> | </span><span class="si">{_:6}</span><span class="s2"> cudaMalloc retries: </span><span class="si">{num_alloc_retries:&lt;8d}</span><span class="s2">  &quot;</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  &quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">metric_key</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">formatter</span> <span class="ow">in</span> <span class="n">metrics_to_display</span><span class="p">:</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
        <span class="n">submetrics</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">)]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">abbreviated</span><span class="p">:</span>
            <span class="n">submetrics</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;large_pool&quot;</span><span class="p">,</span> <span class="s2">&quot;      from large pool&quot;</span><span class="p">))</span>
            <span class="n">submetrics</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;small_pool&quot;</span><span class="p">,</span> <span class="s2">&quot;      from small pool&quot;</span><span class="p">))</span>

        <span class="n">current_prefval</span><span class="p">,</span> <span class="n">peak_prefval</span><span class="p">,</span> <span class="n">allocated_prefval</span><span class="p">,</span> <span class="n">freed_prefval</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">submetric_key</span><span class="p">,</span> <span class="n">submetric_name</span> <span class="ow">in</span> <span class="n">submetrics</span><span class="p">:</span>
            <span class="n">prefix</span> <span class="o">=</span> <span class="n">metric_key</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">submetric_key</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>

            <span class="n">current</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;current&quot;</span><span class="p">]</span>
            <span class="n">peak</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;peak&quot;</span><span class="p">]</span>
            <span class="n">allocated</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;allocated&quot;</span><span class="p">]</span>
            <span class="n">freed</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;freed&quot;</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">current_prefval</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">current_prefval</span> <span class="o">=</span> <span class="n">current</span>
                <span class="n">peak_prefval</span> <span class="o">=</span> <span class="n">peak</span>
                <span class="n">allocated_prefval</span> <span class="o">=</span> <span class="n">allocated</span>
                <span class="n">freed_prefval</span> <span class="o">=</span> <span class="n">freed</span>

            <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; </span><span class="si">{:&lt;21}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">submetric_name</span><span class="p">,</span>
                <span class="n">formatter</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">current_prefval</span><span class="p">),</span>
                <span class="n">formatter</span><span class="p">(</span><span class="n">peak</span><span class="p">,</span> <span class="n">peak_prefval</span><span class="p">),</span>
                <span class="n">formatter</span><span class="p">(</span><span class="n">allocated</span><span class="p">,</span> <span class="n">allocated_prefval</span><span class="p">),</span>
                <span class="n">formatter</span><span class="p">(</span><span class="n">freed</span><span class="p">,</span> <span class="n">freed_prefval</span><span class="p">)),</span>
            <span class="p">)</span>

    <span class="n">metrics_to_display</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;oversize_allocations&quot;</span><span class="p">,</span> <span class="s2">&quot;Oversize allocations&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;oversize_segments&quot;</span><span class="p">,</span> <span class="s2">&quot;Oversize GPU segments&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">for</span> <span class="n">metric_key</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">formatter</span> <span class="ow">in</span> <span class="n">metrics_to_display</span><span class="p">:</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>

        <span class="n">prefix</span> <span class="o">=</span> <span class="n">metric_key</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>

        <span class="n">current</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;current&quot;</span><span class="p">]</span>
        <span class="n">peak</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;peak&quot;</span><span class="p">]</span>
        <span class="n">allocated</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;allocated&quot;</span><span class="p">]</span>
        <span class="n">freed</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;freed&quot;</span><span class="p">]</span>

        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; </span><span class="si">{:&lt;21}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">metric_name</span><span class="p">,</span>
            <span class="n">formatter</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">current</span><span class="p">),</span>
            <span class="n">formatter</span><span class="p">(</span><span class="n">peak</span><span class="p">,</span> <span class="n">peak</span><span class="p">),</span>
            <span class="n">formatter</span><span class="p">(</span><span class="n">allocated</span><span class="p">,</span> <span class="n">allocated</span><span class="p">),</span>
            <span class="n">formatter</span><span class="p">(</span><span class="n">freed</span><span class="p">,</span> <span class="n">freed</span><span class="p">)),</span>
        <span class="p">)</span>

    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>

    <span class="n">fmt_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;_&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">stats</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">fmt_dict</span><span class="p">[</span><span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">return</span> <span class="s2">&quot;|&quot;</span> <span class="o">+</span> <span class="s2">&quot;|</span><span class="se">\n</span><span class="s2">|&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">fmt_dict</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;|</span><span class="se">\n</span><span class="s2">&quot;</span></div>


<div class="viewcode-block" id="list_gpu_processes"><a class="viewcode-back" href="../../../generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes">[docs]</a><span class="k">def</span> <span class="nf">list_gpu_processes</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a human-readable printout of the running processes</span>
<span class="sd">    and their GPU memory use for a given device.</span>

<span class="sd">    This can be useful to display periodically during training, or when</span>
<span class="sd">    handling out-of-memory exceptions.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            printout for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">pynvml</span>  <span class="c1"># type: ignore[import]</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">return</span><span class="p">(</span><span class="s2">&quot;pynvml module not found, please install pynvml&quot;</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">pynvml</span> <span class="kn">import</span> <span class="n">NVMLError_DriverNotLoaded</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlInit</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">NVMLError_DriverNotLoaded</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;cuda driver can&#39;t be loaded, is cuda enabled?&quot;</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_nvml_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">procs</span> <span class="o">=</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetComputeRunningProcesses</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU:</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">procs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;no processes are running&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">procs</span><span class="p">:</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">usedGpuMemory</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;process </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">pid</span><span class="si">:</span><span class="s2">&gt;10d</span><span class="si">}</span><span class="s2"> uses </span><span class="si">{</span><span class="n">mem</span><span class="si">:</span><span class="s2">&gt;12.3f</span><span class="si">}</span><span class="s2"> MB GPU memory&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span></div>

<div class="viewcode-block" id="mem_get_info"><a class="viewcode-back" href="../../../generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info">[docs]</a><span class="k">def</span> <span class="nf">mem_get_info</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the global free and total GPU memory for a given</span>
<span class="sd">    device using cudaMemGetInfo.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more</span>
<span class="sd">        details about GPU memory management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">cudart</span><span class="p">()</span><span class="o">.</span><span class="n">cudaMemGetInfo</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">_record_memory_history_legacy</span><span class="p">(</span><span class="n">enabled</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">record_context</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">trace_alloc_max_entries</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                  <span class="n">trace_alloc_record_context</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                  <span class="n">record_context_cpp</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_cuda_recordMemoryHistory</span><span class="p">(</span><span class="n">enabled</span><span class="p">,</span> <span class="n">record_context</span><span class="p">,</span> <span class="n">record_context_cpp</span><span class="p">,</span>
                                     <span class="n">trace_alloc_max_entries</span><span class="p">,</span> <span class="n">trace_alloc_record_context</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_record_memory_history</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Enables recording of stack traces associated with memory</span>
<span class="sd">    allocations, so you can tell what allocated any piece of memory in</span>
<span class="sd">    :func:`torch.cuda.memory._snapshot()`.</span>

<span class="sd">    In addition too keeping stack traces with each current allocation and free,</span>
<span class="sd">    this will also enable recording of a history of all alloc/free events.</span>

<span class="sd">    Use :func:`torch.cuda.memory._snapshot()` to retrieve this information,</span>
<span class="sd">    and the tools in `_memory_viz.py` to visualize snapshots.</span>

<span class="sd">    The Python trace collection is fast (2us per trace), so you may consider</span>
<span class="sd">    enabling this on production jobs if you anticipate ever having to debug</span>
<span class="sd">    memory issues.</span>

<span class="sd">    C++ trace collection is also fast (~50ns/frame), which for many typical programs</span>
<span class="sd">    works out to ~2us per trace, but can vary depending on stack depth.</span>

<span class="sd">    Args:</span>
<span class="sd">        enabled (Optional[str], optional):</span>
<span class="sd">            None - disable recording memory history.</span>
<span class="sd">            &quot;state&quot; - keep information for currenly allocated memory.</span>
<span class="sd">            &quot;all&quot; - additionally keep a history of all alloc/free calls</span>
<span class="sd">            Defaults to &quot;all&quot;.</span>
<span class="sd">        context (Optional[str], optional):</span>
<span class="sd">            None - Do not record any tracebacks.</span>
<span class="sd">            &quot;state&quot; - Record tracebacks for currently allocated memory.</span>
<span class="sd">            &quot;all&quot; - additionally keep tracebacks for alloc/free calls</span>
<span class="sd">             Defaults to &quot;all&quot;.</span>
<span class="sd">        stacks (str, optional):</span>
<span class="sd">            &quot;python&quot; - include Python, TorchScript, and inductor frames in tracebacks</span>
<span class="sd">            &quot;all&quot; - additionally include C++ frames</span>
<span class="sd">            Defaults to &quot;all&quot;.</span>
<span class="sd">        max_entries (int, optional): Keep a maximum of `max_entries`</span>
<span class="sd">            alloc/free events in the recorded history recorded.</span>
<span class="sd">            Defaults to sys.maxsize.</span>

<span class="sd">        device (Union[Device, int], optional): Which CUDA device to enable recording.</span>
<span class="sd">            Defaults to the current device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">enabled</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_record_memory_history_legacy</span><span class="p">(</span><span class="n">enabled</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_record_memory_history_impl</span><span class="p">(</span><span class="n">enabled</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_record_memory_history_impl</span><span class="p">(</span><span class="n">enabled</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
                                <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
                                <span class="n">stacks</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
                                <span class="n">max_entries</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">,</span>
                                <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">enabled</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;expected state to be &#39;state&#39;, &#39;all&#39;, or None&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">context</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;expected context to be &#39;state&#39;, &#39;all&#39;, or None&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stacks</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;expected stacks to be &#39;python&#39;, or &#39;all&#39;&quot;</span><span class="p">)</span>

    <span class="n">enabled_</span> <span class="o">=</span> <span class="n">enabled</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">record_context</span> <span class="o">=</span> <span class="n">context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">trace_alloc_max_entries</span> <span class="o">=</span> <span class="n">max_entries</span> <span class="k">if</span> <span class="n">enabled</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">trace_alloc_record_context</span> <span class="o">=</span> <span class="n">context</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span>
    <span class="n">record_context_cpp</span> <span class="o">=</span> <span class="n">stacks</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_cuda_recordMemoryHistory</span><span class="p">(</span><span class="n">enabled_</span><span class="p">,</span> <span class="n">record_context</span><span class="p">,</span> <span class="n">record_context_cpp</span><span class="p">,</span>
                                     <span class="n">trace_alloc_max_entries</span><span class="p">,</span> <span class="n">trace_alloc_record_context</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_snapshot</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_cuda_memorySnapshot</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">_dump_snapshot</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;snapshot_dump&#39;</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">_snapshot</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s1">/snapshot.pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s1">/trace_plot.html&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">trace_plot</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s1">/segment_plot.html&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">segment_plot</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_save_segment_usage</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;output.svg&#39;</span><span class="p">,</span> <span class="n">snapshot</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">snapshot</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">snapshot</span> <span class="o">=</span> <span class="n">_snapshot</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">_segments</span><span class="p">(</span><span class="n">snapshot</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">_save_memory_usage</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;output.svg&#39;</span><span class="p">,</span> <span class="n">snapshot</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">snapshot</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">snapshot</span> <span class="o">=</span> <span class="n">_snapshot</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">_memory</span><span class="p">(</span><span class="n">snapshot</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">_set_allocator_settings</span><span class="p">(</span><span class="n">env</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_cudaCachingAllocator_set_allocator_settings</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<div class="viewcode-block" id="get_allocator_backend"><a class="viewcode-back" href="../../../generated/torch.cuda.get_allocator_backend.html#torch.cuda.get_allocator_backend">[docs]</a><span class="k">def</span> <span class="nf">get_allocator_backend</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a string describing the active allocator backend as set by</span>
<span class="sd">    ``PYTORCH_CUDA_ALLOC_CONF``. Currently available backends are</span>
<span class="sd">    ``native`` (PyTorch&#39;s native caching allocator) and `cudaMallocAsync``</span>
<span class="sd">    (CUDA&#39;s built-in asynchronous allocator).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for details on choosing the allocator backend.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getAllocatorBackend</span><span class="p">()</span></div>

<span class="k">class</span> <span class="nc">_CUDAAllocator</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wrapper over internal CUDA memory allocators.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">allocator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_CUDAAllocator</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_allocator</span> <span class="o">=</span> <span class="n">allocator</span>

    <span class="k">def</span> <span class="nf">allocator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_allocator</span>


<div class="viewcode-block" id="CUDAPluggableAllocator"><a class="viewcode-back" href="../../../generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator">[docs]</a><span class="k">class</span> <span class="nc">CUDAPluggableAllocator</span><span class="p">(</span><span class="n">_CUDAAllocator</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;CUDA memory allocator loaded from a so file.</span>

<span class="sd">    Memory allocators are compiled in .so files and loaded dynamically using ctypes.</span>
<span class="sd">    To change the active allocator use the :func:`torch.memory.cuda.change_current_allocator`</span>
<span class="sd">    function.</span>

<span class="sd">    Args:</span>
<span class="sd">        path_to_so_file(str): Path in the filesystem to the `.so` file containing</span>
<span class="sd">            the allocator functions</span>
<span class="sd">        alloc_fn_name(str): Name of the function to perform the memory allocation</span>
<span class="sd">            in the so file. The signature must be:</span>
<span class="sd">            void* alloc_fn_name(ssize_t size, int device, cudaStream_t stream);</span>
<span class="sd">        free_fn_name(str): Name of the function to perform the memory release</span>
<span class="sd">            in the so file. The signature must be:</span>
<span class="sd">            void free_fn_name(void* ptr, size_t size, cudaStream_t stream);</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is currently supported only in unix OSs</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for details on creating and using a custom allocator</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path_to_so_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">alloc_fn_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">free_fn_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">allocator</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">path_to_so_file</span><span class="p">)</span>
        <span class="n">alloc_fn</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">allocator</span><span class="p">,</span> <span class="n">alloc_fn_name</span><span class="p">),</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">)</span><span class="o">.</span><span class="n">value</span>
        <span class="n">free_fn</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">allocator</span><span class="p">,</span> <span class="n">free_fn_name</span><span class="p">),</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">)</span><span class="o">.</span><span class="n">value</span>
        <span class="k">assert</span> <span class="n">alloc_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="n">free_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_allocator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_customAllocator</span><span class="p">(</span><span class="n">alloc_fn</span><span class="p">,</span> <span class="n">free_fn</span><span class="p">)</span></div>


<div class="viewcode-block" id="change_current_allocator"><a class="viewcode-back" href="../../../generated/torch.cuda.change_current_allocator.html#torch.cuda.change_current_allocator">[docs]</a><span class="k">def</span> <span class="nf">change_current_allocator</span><span class="p">(</span><span class="n">allocator</span><span class="p">:</span> <span class="n">_CUDAAllocator</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Changes the currently used memory allocator to be the one provided.</span>
<span class="sd">    If the current allocator has already been used/initialized, this function will error.</span>


<span class="sd">    Args:</span>
<span class="sd">        allocator (torch.cuda.memory._CUDAAllocator): allocator to be set as the active one.</span>
<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for details on creating and using a custom allocator</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_changeCurrentAllocator</span><span class="p">(</span><span class="n">allocator</span><span class="o">.</span><span class="n">allocator</span><span class="p">())</span></div>


<span class="k">def</span> <span class="nf">_get_current_allocator</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">_CUDAAllocator</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the allocator being currently used.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for details on creating and using a custom allocator</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_CUDAAllocator</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_getAllocator</span><span class="p">())</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
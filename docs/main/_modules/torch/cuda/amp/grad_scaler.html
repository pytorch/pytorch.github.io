


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.cuda.amp.grad_scaler &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/cuda/amp/grad_scaler.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/cuda/amp/grad_scaler.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../../cuda.html">torch.cuda</a> &gt;</li>
        
      <li>torch.cuda.amp.grad_scaler</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.cuda.amp.grad_scaler</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">abc</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">cast</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">.common</span> <span class="kn">import</span> <span class="n">amp_definitely_not_available</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;OptState&quot;</span><span class="p">,</span> <span class="s2">&quot;GradScaler&quot;</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">_MultiDeviceReplicator</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Lazily serves copies of a tensor to requested devices.  Copies are cached per-device.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">master_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">master_tensor</span><span class="o">.</span><span class="n">is_cuda</span> <span class="ow">or</span> <span class="n">master_tensor</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;xla&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">master</span> <span class="o">=</span> <span class="n">master_tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_per_device_tensors</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">retval</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_device_tensors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">retval</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">retval</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">master</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_per_device_tensors</span><span class="p">[</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="n">retval</span>
        <span class="k">return</span> <span class="n">retval</span>


<span class="c1"># Defines default_factory for GradScaler&#39;s _per_optimizer_states defaultdict,</span>
<span class="c1"># as well as associated &quot;enum&quot; values.  Prefers defining these at top level because</span>
<span class="c1"># - Lambdas can&#39;t be pickled, so we don&#39;t want to supply a lambda as the factory.</span>
<span class="c1"># - Defining READY, UNSCALED, STEPPED and _refresh_per_optimizer_state within GradScaler</span>
<span class="c1">#   causes a circular reference, which we&#39;d rather avoid.</span>
<span class="k">class</span> <span class="nc">OptState</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">READY</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">UNSCALED</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">STEPPED</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">def</span> <span class="nf">_refresh_per_optimizer_state</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;stage&quot;</span><span class="p">:</span> <span class="n">OptState</span><span class="o">.</span><span class="n">READY</span><span class="p">,</span> <span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">:</span> <span class="p">{}}</span>


<div class="viewcode-block" id="GradScaler"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler">[docs]</a><span class="k">class</span> <span class="nc">GradScaler</span><span class="p">:</span>
    <span class="n">_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">_grows_tracker</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">_per_optimizer_states</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An instance ``scaler`` of :class:`GradScaler` helps perform the steps of gradient scaling</span>
<span class="sd">    conveniently.</span>

<span class="sd">    * ``scaler.scale(loss)`` multiplies a given loss by ``scaler``&#39;s current scale factor.</span>
<span class="sd">    * ``scaler.step(optimizer)`` safely unscales gradients and calls ``optimizer.step()``.</span>
<span class="sd">    * ``scaler.update()`` updates ``scaler``&#39;s scale factor.</span>

<span class="sd">    Example::</span>

<span class="sd">        # Creates a GradScaler once at the beginning of training.</span>
<span class="sd">        scaler = GradScaler()</span>

<span class="sd">        for epoch in epochs:</span>
<span class="sd">            for input, target in data:</span>
<span class="sd">                optimizer.zero_grad()</span>
<span class="sd">                output = model(input)</span>
<span class="sd">                loss = loss_fn(output, target)</span>

<span class="sd">                # Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span>
<span class="sd">                scaler.scale(loss).backward()</span>

<span class="sd">                # scaler.step() first unscales gradients of the optimizer&#39;s params.</span>
<span class="sd">                # If gradients don&#39;t contain infs/NaNs, optimizer.step() is then called,</span>
<span class="sd">                # otherwise, optimizer.step() is skipped.</span>
<span class="sd">                scaler.step(optimizer)</span>

<span class="sd">                # Updates the scale for next iteration.</span>
<span class="sd">                scaler.update()</span>

<span class="sd">    See the :ref:`Automatic Mixed Precision examples&lt;amp-examples&gt;` for usage</span>
<span class="sd">    (along with autocasting) in more complex cases like gradient clipping, gradient accumulation, gradient penalty,</span>
<span class="sd">    and multiple losses/optimizers.</span>

<span class="sd">    ``scaler`` dynamically estimates the scale factor each iteration.  To minimize gradient underflow,</span>
<span class="sd">    a large scale factor should be used.  However, ``float16`` values can &quot;overflow&quot; (become inf or NaN) if</span>
<span class="sd">    the scale factor is too large.  Therefore, the optimal scale factor is the largest factor that can be used</span>
<span class="sd">    without incurring inf or NaN gradient values.</span>
<span class="sd">    ``scaler`` approximates the optimal scale factor over time by checking the gradients for infs and NaNs during every</span>
<span class="sd">    ``scaler.step(optimizer)`` (or optional separate ``scaler.unscale_(optimizer)``, see :meth:`unscale_`).</span>

<span class="sd">    * If infs/NaNs are found, ``scaler.step(optimizer)`` skips the underlying ``optimizer.step()`` (so the params</span>
<span class="sd">      themselves remain uncorrupted) and ``update()`` multiplies the scale by ``backoff_factor``.</span>

<span class="sd">    * If no infs/NaNs are found, ``scaler.step(optimizer)`` runs the underlying ``optimizer.step()`` as usual.</span>
<span class="sd">      If ``growth_interval`` unskipped iterations occur consecutively, ``update()`` multiplies the scale by</span>
<span class="sd">      ``growth_factor``.</span>

<span class="sd">    The scale factor often causes infs/NaNs to appear in gradients for the first few iterations as its</span>
<span class="sd">    value calibrates.  ``scaler.step`` will skip the underlying ``optimizer.step()`` for these</span>
<span class="sd">    iterations.  After that, step skipping should occur rarely (once every few hundred or thousand iterations).</span>

<span class="sd">    Args:</span>
<span class="sd">        init_scale (float, optional, default=2.**16):  Initial scale factor.</span>
<span class="sd">        growth_factor (float, optional, default=2.0):  Factor by which the scale is multiplied during</span>
<span class="sd">            :meth:`update` if no inf/NaN gradients occur for ``growth_interval`` consecutive iterations.</span>
<span class="sd">        backoff_factor (float, optional, default=0.5):  Factor by which the scale is multiplied during</span>
<span class="sd">            :meth:`update` if inf/NaN gradients occur in an iteration.</span>
<span class="sd">        growth_interval (int, optional, default=2000):  Number of consecutive iterations without inf/NaN gradients</span>
<span class="sd">            that must occur for the scale to be multiplied by ``growth_factor``.</span>
<span class="sd">        enabled (bool, optional):  If ``False``, disables gradient scaling. :meth:`step` simply</span>
<span class="sd">            invokes the underlying ``optimizer.step()``, and other methods become no-ops.</span>
<span class="sd">            Default: ``True``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">init_scale</span><span class="o">=</span><span class="mf">2.</span><span class="o">**</span><span class="mi">16</span><span class="p">,</span>
                 <span class="n">growth_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                 <span class="n">backoff_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">growth_interval</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                 <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">enabled</span> <span class="ow">and</span> <span class="n">amp_definitely_not_available</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span> <span class="o">=</span> <span class="n">enabled</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">growth_factor</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;The growth factor must be &gt; 1.0.&quot;</span>
            <span class="k">assert</span> <span class="n">backoff_factor</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;The backoff factor must be &lt; 1.0.&quot;</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale</span> <span class="o">=</span> <span class="n">init_scale</span>
            <span class="c1"># self._scale will be lazily initialized during the first call to scale()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_growth_factor</span> <span class="o">=</span> <span class="n">growth_factor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backoff_factor</span> <span class="o">=</span> <span class="n">backoff_factor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_growth_interval</span> <span class="o">=</span> <span class="n">growth_interval</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_growth_tracker</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># self._growth_tracker will be lazily initialized during the first call to scale()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">_refresh_per_optimizer_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_scale_growth_tracker</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">funcname</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">fix</span> <span class="o">=</span> <span class="s2">&quot;This may indicate your script did not use scaler.scale(loss or outputs) earlier in the iteration.&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Attempted </span><span class="si">{}</span><span class="s2"> but _scale is None.  &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">funcname</span><span class="p">)</span> <span class="o">+</span> <span class="n">fix</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Attempted </span><span class="si">{}</span><span class="s2"> but _growth_tracker is None.  &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">funcname</span><span class="p">)</span> <span class="o">+</span> <span class="n">fix</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_lazy_init_scale_growth_tracker</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dev</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;_growth_tracker initialized before _scale&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">dev</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_growth_tracker</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">dev</span><span class="p">)</span>

<div class="viewcode-block" id="GradScaler.scale"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.scale">[docs]</a>    <span class="k">def</span> <span class="nf">scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Multiplies (&#39;scales&#39;) a tensor or list of tensors by the scale factor.</span>

<span class="sd">        Returns scaled outputs.  If this instance of :class:`GradScaler` is not enabled, outputs are returned</span>
<span class="sd">        unmodified.</span>

<span class="sd">        Args:</span>
<span class="sd">            outputs (Tensor or iterable of Tensors):  Outputs to scale.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">outputs</span>

        <span class="c1"># Short-circuit for the common case.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">outputs</span><span class="o">.</span><span class="n">is_cuda</span> <span class="ow">or</span> <span class="n">outputs</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;xla&#39;</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init_scale_growth_tracker</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="n">outputs</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Invoke the more complex machinery only if we&#39;re treating multiple outputs.</span>
        <span class="n">stash</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">_MultiDeviceReplicator</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># holds a reference that can be overwritten by apply_scale</span>

        <span class="k">def</span> <span class="nf">apply_scale</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">assert</span> <span class="n">val</span><span class="o">.</span><span class="n">is_cuda</span> <span class="ow">or</span> <span class="n">val</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;xla&#39;</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">stash</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_init_scale_growth_tracker</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="n">stash</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_MultiDeviceReplicator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="p">))</span>
                <span class="k">return</span> <span class="n">val</span> <span class="o">*</span> <span class="n">stash</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">abc</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
                <span class="n">iterable</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">apply_scale</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                    <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)(</span><span class="n">iterable</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">iterable</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;outputs must be a Tensor or an iterable of Tensors&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">apply_scale</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_unscale_grads_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">inv_scale</span><span class="p">,</span> <span class="n">found_inf</span><span class="p">,</span> <span class="n">allow_fp16</span><span class="p">):</span>
        <span class="n">per_device_inv_scale</span> <span class="o">=</span> <span class="n">_MultiDeviceReplicator</span><span class="p">(</span><span class="n">inv_scale</span><span class="p">)</span>
        <span class="n">per_device_found_inf</span> <span class="o">=</span> <span class="n">_MultiDeviceReplicator</span><span class="p">(</span><span class="n">found_inf</span><span class="p">)</span>

        <span class="c1"># To set up _amp_foreach_non_finite_check_and_unscale_, split grads by device and dtype.</span>
        <span class="c1"># There could be hundreds of grads, so we&#39;d like to iterate through them just once.</span>
        <span class="c1"># However, we don&#39;t know their devices or dtypes in advance.</span>

        <span class="c1"># https://stackoverflow.com/questions/5029934/defaultdict-of-defaultdict</span>
        <span class="c1"># Google says mypy struggles with defaultdicts type annotations.</span>
        <span class="n">per_device_and_dtype_grads</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">))</span>  <span class="c1"># type: ignore[var-annotated]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">allow_fp16</span><span class="p">)</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Attempting to unscale FP16 gradients.&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                        <span class="c1"># is_coalesced() == False means the sparse grad has values with duplicate indices.</span>
                        <span class="c1"># coalesce() deduplicates indices and adds all values that have the same index.</span>
                        <span class="c1"># For scaled fp16 values, there&#39;s a good chance coalescing will cause overflow,</span>
                        <span class="c1"># so we should check the coalesced _values().</span>
                        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                            <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
                        <span class="n">to_unscale</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">_values</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">to_unscale</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span>

                    <span class="c1"># TODO: is there a way to split by device and dtype without appending in the inner loop?</span>
                    <span class="n">per_device_and_dtype_grads</span><span class="p">[</span><span class="n">to_unscale</span><span class="o">.</span><span class="n">device</span><span class="p">][</span><span class="n">to_unscale</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">to_unscale</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">device</span><span class="p">,</span> <span class="n">per_dtype_grads</span> <span class="ow">in</span> <span class="n">per_device_and_dtype_grads</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">grads</span> <span class="ow">in</span> <span class="n">per_dtype_grads</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">_amp_foreach_non_finite_check_and_unscale_</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span>
                                                                     <span class="n">per_device_found_inf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                                                     <span class="n">per_device_inv_scale</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">per_device_found_inf</span><span class="o">.</span><span class="n">_per_device_tensors</span>

<div class="viewcode-block" id="GradScaler.unscale_"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.unscale_">[docs]</a>    <span class="k">def</span> <span class="nf">unscale_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Divides (&quot;unscales&quot;) the optimizer&#39;s gradient tensors by the scale factor.</span>

<span class="sd">        :meth:`unscale_` is optional, serving cases where you need to</span>
<span class="sd">        :ref:`modify or inspect gradients&lt;working-with-unscaled-gradients&gt;`</span>
<span class="sd">        between the backward pass(es) and :meth:`step`.</span>
<span class="sd">        If :meth:`unscale_` is not called explicitly,  gradients will be unscaled  automatically during :meth:`step`.</span>

<span class="sd">        Simple example, using :meth:`unscale_` to enable clipping of unscaled gradients::</span>

<span class="sd">            ...</span>
<span class="sd">            scaler.scale(loss).backward()</span>
<span class="sd">            scaler.unscale_(optimizer)</span>
<span class="sd">            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)</span>
<span class="sd">            scaler.step(optimizer)</span>
<span class="sd">            scaler.update()</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer (torch.optim.Optimizer):  Optimizer that owns the gradients to be unscaled.</span>

<span class="sd">        .. note::</span>
<span class="sd">            :meth:`unscale_` does not incur a CPU-GPU sync.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            :meth:`unscale_` should only be called once per optimizer per :meth:`step` call,</span>
<span class="sd">            and only after all gradients for that optimizer&#39;s assigned parameters have been accumulated.</span>
<span class="sd">            Calling :meth:`unscale_` twice for a given optimizer between each :meth:`step` triggers a RuntimeError.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            :meth:`unscale_` may unscale sparse gradients out of place, replacing the ``.grad`` attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_scale_growth_tracker</span><span class="p">(</span><span class="s2">&quot;unscale_&quot;</span><span class="p">)</span>

        <span class="n">optimizer_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)]</span>

        <span class="k">if</span> <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="n">OptState</span><span class="o">.</span><span class="n">UNSCALED</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;unscale_() has already been called on this optimizer since the last update().&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="n">OptState</span><span class="o">.</span><span class="n">STEPPED</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;unscale_() is being called after step().&quot;</span><span class="p">)</span>

        <span class="c1"># FP32 division can be imprecise for certain compile options, so we carry out the reciprocal in FP64.</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">inv_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">found_inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unscale_grads_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">inv_scale</span><span class="p">,</span> <span class="n">found_inf</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">OptState</span><span class="o">.</span><span class="n">UNSCALED</span></div>

    <span class="k">def</span> <span class="nf">_maybe_opt_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_state</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">retval</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">sum</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="n">retval</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">retval</span>

<div class="viewcode-block" id="GradScaler.step"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.step">[docs]</a>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :meth:`step` carries out the following two operations:</span>

<span class="sd">        1.  Internally invokes ``unscale_(optimizer)`` (unless :meth:`unscale_` was explicitly called for ``optimizer``</span>
<span class="sd">            earlier in the iteration).  As part of the :meth:`unscale_`, gradients are checked for infs/NaNs.</span>
<span class="sd">        2.  If no inf/NaN gradients are found, invokes ``optimizer.step()`` using the unscaled</span>
<span class="sd">            gradients.  Otherwise, ``optimizer.step()`` is skipped to avoid corrupting the params.</span>

<span class="sd">        ``*args`` and ``**kwargs`` are forwarded to ``optimizer.step()``.</span>

<span class="sd">        Returns the return value of ``optimizer.step(*args, **kwargs)``.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimizer (torch.optim.Optimizer):  Optimizer that applies the gradients.</span>
<span class="sd">            args:  Any arguments.</span>
<span class="sd">            kwargs:  Any keyword arguments.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            Closure use is not currently supported.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;closure&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Closure use is not currently supported if GradScaler is enabled.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_scale_growth_tracker</span><span class="p">(</span><span class="s2">&quot;step&quot;</span><span class="p">)</span>

        <span class="n">optimizer_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)]</span>

        <span class="k">if</span> <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="n">OptState</span><span class="o">.</span><span class="n">STEPPED</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;step() has already been called since the last update().&quot;</span><span class="p">)</span>

        <span class="n">retval</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;_step_supports_amp_scaling&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">_step_supports_amp_scaling</span><span class="p">):</span>
            <span class="c1"># This optimizer has customized scale-handling logic, so we can call optimizer.step() directly.</span>
            <span class="c1"># The contract with custom optimizers is that their step() should accept an additional,</span>
            <span class="c1"># optional grad_scaler kwarg.  We append self to the kwargs so the custom optimizer has full information:</span>
            <span class="c1"># it can query its own state, invoke unscale_ on itself, etc</span>
            <span class="c1"># The contract above is being deprecated to avoid introducing `grad_scaler: GradScaler` argument</span>
            <span class="c1"># to `Optimizer.step`. The new behavior is going to add two Tensor attributes of `grad_scale`</span>
            <span class="c1"># and `found_inf` to the passed optimizer so that the optimizer can utilize those</span>
            <span class="c1"># to skip the parameter updates or unscale gradients before updating parameters in</span>
            <span class="c1"># the fused kernel, e.g. `FusedAdamMathFunctor`.</span>
            <span class="c1"># In this behavior, `GradScaler._check_inf_per_device` is called if `OptState.READY`,</span>
            <span class="c1"># while the method is expected to be called by users side, i.e. their optimizers.</span>
            <span class="n">kwargs_</span> <span class="o">=</span> <span class="n">kwargs</span>
            <span class="n">has_grad_scaler_kwarg</span> <span class="o">=</span> <span class="s2">&quot;grad_scaler&quot;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
            <span class="k">if</span> <span class="n">has_grad_scaler_kwarg</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;GradScaler is going to stop passing itself as a keyword argument to the passed &quot;</span>
                    <span class="s2">&quot;optimizer. In the near future GradScaler registers `grad_scale: Tensor` and &quot;</span>
                    <span class="s2">&quot;`found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.&quot;</span><span class="p">,</span>
                    <span class="ne">FutureWarning</span><span class="p">)</span>
                <span class="n">kwargs_</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;grad_scaler&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="p">})</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="n">OptState</span><span class="o">.</span><span class="n">READY</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_check_inf_per_device</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
                <span class="n">scaler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_scale_async</span><span class="p">()</span>
                <span class="n">found_inf</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                    <span class="nb">sum</span><span class="p">([</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                    <span class="p">])</span>
                <span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">grad_scale</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">OptState</span><span class="o">.</span><span class="n">UNSCALED</span> <span class="k">else</span> <span class="n">scaler</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">found_inf</span> <span class="o">=</span> <span class="n">found_inf</span>
            <span class="n">retval</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs_</span><span class="p">)</span>
            <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">OptState</span><span class="o">.</span><span class="n">STEPPED</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">has_grad_scaler_kwarg</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">grad_scale</span>
                <span class="k">del</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">found_inf</span>
            <span class="k">return</span> <span class="n">retval</span>

        <span class="k">if</span> <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="n">OptState</span><span class="o">.</span><span class="n">READY</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;No inf checks were recorded for this optimizer.&quot;</span>

        <span class="n">retval</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_opt_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_state</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">optimizer_state</span><span class="p">[</span><span class="s2">&quot;stage&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">OptState</span><span class="o">.</span><span class="n">STEPPED</span>

        <span class="k">return</span> <span class="n">retval</span></div>

<div class="viewcode-block" id="GradScaler.update"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates the scale factor.</span>

<span class="sd">        If any optimizer steps were skipped the scale is multiplied by ``backoff_factor``</span>
<span class="sd">        to reduce it. If ``growth_interval`` unskipped iterations occurred consecutively,</span>
<span class="sd">        the scale is multiplied by ``growth_factor`` to increase it.</span>

<span class="sd">        Passing ``new_scale`` sets the new scale value manually. (``new_scale`` is not</span>
<span class="sd">        used directly, it&#39;s used to fill GradScaler&#39;s internal scale tensor. So if</span>
<span class="sd">        ``new_scale`` was a tensor, later in-place changes to that tensor will not further</span>
<span class="sd">        affect the scale GradScaler uses internally.)</span>

<span class="sd">        Args:</span>
<span class="sd">            new_scale (float or :class:`torch.cuda.FloatTensor`, optional, default=None):  New scale factor.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            :meth:`update` should only be called at the end of the iteration, after ``scaler.step(optimizer)`` has</span>
<span class="sd">            been invoked for all optimizers used this iteration.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            For performance reasons, we do not check the scale factor value to avoid synchronizations,</span>
<span class="sd">            so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or</span>
<span class="sd">            you are seeing NaNs in your gradients or loss, something is likely wrong. For example,</span>
<span class="sd">            bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">_scale</span><span class="p">,</span> <span class="n">_growth_tracker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_scale_growth_tracker</span><span class="p">(</span><span class="s2">&quot;update&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">new_scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Accept a new user-defined scale.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_scale</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">new_scale</span><span class="p">)</span>  <span class="c1"># type: ignore[union-attr]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">reason</span> <span class="o">=</span> <span class="s2">&quot;new_scale should be a float or a 1-element torch.cuda.FloatTensor with requires_grad=False.&quot;</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_scale</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">),</span> <span class="n">reason</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">assert</span> <span class="n">new_scale</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">reason</span>
                <span class="k">assert</span> <span class="n">new_scale</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="n">reason</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">new_scale</span><span class="p">)</span>  <span class="c1"># type: ignore[union-attr]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Consume shared inf/nan data collected from optimizers to update the scale.</span>
            <span class="c1"># If all found_inf tensors are on the same device as self._scale, this operation is asynchronous.</span>
            <span class="n">found_infs</span> <span class="o">=</span> <span class="p">[</span><span class="n">found_inf</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">_scale</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                          <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                          <span class="k">for</span> <span class="n">found_inf</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">found_infs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;No inf checks were recorded prior to update.&quot;</span>

            <span class="n">found_inf_combined</span> <span class="o">=</span> <span class="n">found_infs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">found_infs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">found_infs</span><span class="p">)):</span>
                    <span class="n">found_inf_combined</span> <span class="o">+=</span> <span class="n">found_infs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

            <span class="n">torch</span><span class="o">.</span><span class="n">_amp_update_scale_</span><span class="p">(</span><span class="n">_scale</span><span class="p">,</span>
                                     <span class="n">_growth_tracker</span><span class="p">,</span>
                                     <span class="n">found_inf_combined</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_growth_factor</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_backoff_factor</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_growth_interval</span><span class="p">)</span>

        <span class="c1"># To prepare for next iteration, clear the data collected from optimizers this iteration.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">_refresh_per_optimizer_state</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_get_scale_async</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span>

<div class="viewcode-block" id="GradScaler.get_scale"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.get_scale">[docs]</a>    <span class="k">def</span> <span class="nf">get_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a Python float containing the current scale, or 1.0 if scaling is disabled.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            :meth:`get_scale` incurs a CPU-GPU sync.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_scale_async</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">1.0</span></div>

<div class="viewcode-block" id="GradScaler.get_growth_factor"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.get_growth_factor">[docs]</a>    <span class="k">def</span> <span class="nf">get_growth_factor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a Python float containing the scale growth factor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_factor</span></div>

<div class="viewcode-block" id="GradScaler.set_growth_factor"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.set_growth_factor">[docs]</a>    <span class="k">def</span> <span class="nf">set_growth_factor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_factor</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            new_scale (float):  Value to use as the new scale growth factor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_growth_factor</span> <span class="o">=</span> <span class="n">new_factor</span></div>

<div class="viewcode-block" id="GradScaler.get_backoff_factor"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.get_backoff_factor">[docs]</a>    <span class="k">def</span> <span class="nf">get_backoff_factor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a Python float containing the scale backoff factor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backoff_factor</span></div>

<div class="viewcode-block" id="GradScaler.set_backoff_factor"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.set_backoff_factor">[docs]</a>    <span class="k">def</span> <span class="nf">set_backoff_factor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_factor</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            new_scale (float):  Value to use as the new scale backoff factor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backoff_factor</span> <span class="o">=</span> <span class="n">new_factor</span></div>

<div class="viewcode-block" id="GradScaler.get_growth_interval"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.get_growth_interval">[docs]</a>    <span class="k">def</span> <span class="nf">get_growth_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a Python int containing the growth interval.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_interval</span></div>

<div class="viewcode-block" id="GradScaler.set_growth_interval"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.set_growth_interval">[docs]</a>    <span class="k">def</span> <span class="nf">set_growth_interval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_interval</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            new_interval (int):  Value to use as the new growth interval.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_growth_interval</span> <span class="o">=</span> <span class="n">new_interval</span></div>

    <span class="k">def</span> <span class="nf">_get_growth_tracker</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_growth_tracker</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

<div class="viewcode-block" id="GradScaler.is_enabled"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.is_enabled">[docs]</a>    <span class="k">def</span> <span class="nf">is_enabled</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a bool indicating whether this instance is enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span></div>

<div class="viewcode-block" id="GradScaler.state_dict"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the state of the scaler as a :class:`dict`.  It contains five entries:</span>

<span class="sd">        * ``&quot;scale&quot;`` - a Python float containing the current scale</span>
<span class="sd">        * ``&quot;growth_factor&quot;`` - a Python float containing the current growth factor</span>
<span class="sd">        * ``&quot;backoff_factor&quot;`` - a Python float containing the current backoff factor</span>
<span class="sd">        * ``&quot;growth_interval&quot;`` - a Python int containing the current growth interval</span>
<span class="sd">        * ``&quot;_growth_tracker&quot;`` - a Python int containing the number of recent consecutive unskipped steps.</span>

<span class="sd">        If this instance is not enabled, returns an empty dict.</span>

<span class="sd">        .. note::</span>
<span class="sd">           If you wish to checkpoint the scaler&#39;s state after a particular iteration, :meth:`state_dict`</span>
<span class="sd">           should be called after :meth:`update`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_scale</span><span class="p">(),</span>
                <span class="s2">&quot;growth_factor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_factor</span><span class="p">,</span>
                <span class="s2">&quot;backoff_factor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backoff_factor</span><span class="p">,</span>
                <span class="s2">&quot;growth_interval&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_interval</span><span class="p">,</span>
                <span class="s2">&quot;_growth_tracker&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_growth_tracker</span><span class="p">()}</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span> <span class="k">else</span> <span class="p">{}</span></div>

<div class="viewcode-block" id="GradScaler.load_state_dict"><a class="viewcode-back" href="../../../../amp.html#torch.cuda.amp.GradScaler.load_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the scaler state.  If this instance is disabled, :meth:`load_state_dict` is a no-op.</span>

<span class="sd">        Args:</span>
<span class="sd">           state_dict(dict): scaler state.  Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The source state dict is empty, possibly because it was saved &quot;</span>
                               <span class="s2">&quot;from a disabled instance of GradScaler.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_scale</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_growth_factor</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;growth_factor&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backoff_factor</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;backoff_factor&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_growth_interval</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;growth_interval&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_growth_tracker</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_growth_tracker&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_growth_tracker</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_growth_tracker&quot;</span><span class="p">])</span></div>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enabled</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;A GradScaler instance may only be pickled at the beginning &quot;</span>\
                                                         <span class="s2">&quot;of an iteration, or at the end after scaler.update().&quot;</span>
            <span class="c1"># Pickling _scale and _growth_tracker Tensors directly triggers</span>
            <span class="c1"># &quot;warnings.warn(&quot;pickle support for Storage will be removed in 1.5...&quot;</span>
            <span class="c1"># so instead, we set the unpickled instance up to reinitialize them lazily.</span>
            <span class="n">state</span><span class="p">[</span><span class="s1">&#39;_init_scale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_scale</span><span class="p">()</span>
            <span class="n">state</span><span class="p">[</span><span class="s1">&#39;_init_growth_tracker&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_growth_tracker</span><span class="p">()</span>
            <span class="n">state</span><span class="p">[</span><span class="s1">&#39;_scale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">state</span><span class="p">[</span><span class="s1">&#39;_growth_tracker&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_inf_per_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="n">_scale</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_scale_growth_tracker</span><span class="p">(</span><span class="s2">&quot;_check_inf_per_device&quot;</span><span class="p">)</span>

        <span class="n">dummy_inv_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">_scale</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">found_inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">_scale</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)][</span><span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">]</span> <span class="o">=</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">_unscale_grads_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">dummy_inv_scale</span><span class="p">,</span> <span class="n">found_inf</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)][</span><span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_found_inf_per_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_per_optimizer_states</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)][</span><span class="s2">&quot;found_inf_per_device&quot;</span><span class="p">]</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/sphinx_highlight.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
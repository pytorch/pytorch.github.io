


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">Module code</a> &gt;</li>
        
      <li>torch</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch</h1><div class="highlight"><pre>
<span></span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The torch package contains data structures for multi-dimensional</span>
<span class="sd">tensors and defines mathematical operations over these tensors.</span>
<span class="sd">Additionally, it provides many utilities for efficient serialization of</span>
<span class="sd">Tensors and arbitrary types, and other useful utilities.</span>

<span class="sd">It has a CUDA counterpart, that enables you to run your tensor computations</span>
<span class="sd">on an NVIDIA GPU with compute capability &gt;= 3.0.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="kn">import</span> <span class="nn">ctypes</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">3</span><span class="p">,):</span>
    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Python 2 has reached end-of-life and is no longer supported by PyTorch.&quot;</span><span class="p">)</span>

<span class="c1"># multipy/deploy is setting this import before importing torch, this is the most</span>
<span class="c1"># reliable way we have to detect if we&#39;re running within deploy.</span>
<span class="c1"># https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137</span>
<span class="k">def</span> <span class="nf">_running_with_deploy</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;torch._meta_registrations&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">object</span>

<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_import_dotted_name</span><span class="p">,</span> <span class="n">classproperty</span>
<span class="kn">from</span> <span class="nn">._utils_internal</span> <span class="kn">import</span> <span class="n">get_file_path</span><span class="p">,</span> <span class="n">prepare_multiprocessing_environment</span><span class="p">,</span> \
    <span class="n">USE_RTLD_GLOBAL_WITH_LIBTORCH</span><span class="p">,</span> <span class="n">USE_GLOBAL_DEPS</span>

<span class="c1"># TODO(torch_deploy) figure out how to freeze version.py in fbcode build</span>
<span class="k">if</span> <span class="n">_running_with_deploy</span><span class="p">():</span>
    <span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;torch-deploy-1.8&quot;</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">.torch_version</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">__version__</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">builtins</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;typename&#39;</span><span class="p">,</span> <span class="s1">&#39;is_tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;is_storage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_default_tensor_type&#39;</span><span class="p">,</span> <span class="s1">&#39;set_default_device&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_rng_state&#39;</span><span class="p">,</span> <span class="s1">&#39;get_rng_state&#39;</span><span class="p">,</span> <span class="s1">&#39;manual_seed&#39;</span><span class="p">,</span> <span class="s1">&#39;initial_seed&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">,</span>
    <span class="s1">&#39;save&#39;</span><span class="p">,</span> <span class="s1">&#39;load&#39;</span><span class="p">,</span> <span class="s1">&#39;set_printoptions&#39;</span><span class="p">,</span> <span class="s1">&#39;chunk&#39;</span><span class="p">,</span> <span class="s1">&#39;split&#39;</span><span class="p">,</span> <span class="s1">&#39;stack&#39;</span><span class="p">,</span> <span class="s1">&#39;matmul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;no_grad&#39;</span><span class="p">,</span> <span class="s1">&#39;enable_grad&#39;</span><span class="p">,</span> <span class="s1">&#39;rand&#39;</span><span class="p">,</span> <span class="s1">&#39;randn&#39;</span><span class="p">,</span> <span class="s1">&#39;inference_mode&#39;</span><span class="p">,</span>
    <span class="s1">&#39;DoubleStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;FloatStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;LongStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;IntStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ShortStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;CharStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;ByteStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;BoolStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;TypedStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;UntypedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;DoubleTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;FloatTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;LongTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;IntTensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ShortTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;CharTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;ByteTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;BoolTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lobpcg&#39;</span><span class="p">,</span> <span class="s1">&#39;use_deterministic_algorithms&#39;</span><span class="p">,</span>
    <span class="s1">&#39;are_deterministic_algorithms_enabled&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_deterministic_algorithms_warn_only_enabled&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_deterministic_debug_mode&#39;</span><span class="p">,</span> <span class="s1">&#39;get_deterministic_debug_mode&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_float32_matmul_precision&#39;</span><span class="p">,</span> <span class="s1">&#39;get_float32_matmul_precision&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_warn_always&#39;</span><span class="p">,</span> <span class="s1">&#39;is_warn_always_enabled&#39;</span><span class="p">,</span> <span class="s1">&#39;SymInt&#39;</span><span class="p">,</span> <span class="s1">&#39;SymFloat&#39;</span><span class="p">,</span>
    <span class="s1">&#39;SymBool&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_not&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sym_int&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_float&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_max&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_min&#39;</span><span class="p">,</span> <span class="s1">&#39;compile&#39;</span><span class="p">,</span> <span class="s1">&#39;vmap&#39;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1">################################################################################</span>
<span class="c1"># Load the extension module</span>
<span class="c1">################################################################################</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;win32&#39;</span><span class="p">:</span>
    <span class="n">pfiles_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;ProgramFiles&#39;</span><span class="p">,</span> <span class="s1">&#39;C:</span><span class="se">\\</span><span class="s1">Program Files&#39;</span><span class="p">)</span>
    <span class="n">py_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">exec_prefix</span><span class="p">,</span> <span class="s1">&#39;Library&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="n">th_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;lib&#39;</span><span class="p">)</span>

    <span class="c1"># When users create a virtualenv that inherits the base environment,</span>
    <span class="c1"># we will need to add the corresponding library directory into</span>
    <span class="c1"># DLL search directories. Otherwise, it will rely on `PATH` which</span>
    <span class="c1"># is dependent on user settings.</span>
    <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">exec_prefix</span> <span class="o">!=</span> <span class="n">sys</span><span class="o">.</span><span class="n">base_exec_prefix</span><span class="p">:</span>
        <span class="n">base_py_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">base_exec_prefix</span><span class="p">,</span> <span class="s1">&#39;Library&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_py_dll_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="n">dll_paths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">,</span> <span class="p">[</span><span class="n">th_dll_path</span><span class="p">,</span> <span class="n">py_dll_path</span><span class="p">,</span> <span class="n">base_py_dll_path</span><span class="p">]))</span>

    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;nvToolsExt64_1.dll&#39;</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">):</span>
        <span class="n">nvtoolsext_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;NVTOOLSEXT_PATH&#39;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pfiles_path</span><span class="p">,</span> <span class="s1">&#39;NVIDIA Corporation&#39;</span><span class="p">,</span> <span class="s1">&#39;NvToolsExt&#39;</span><span class="p">)),</span> <span class="s1">&#39;bin&#39;</span><span class="p">,</span> <span class="s1">&#39;x64&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">nvtoolsext_dll_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="kn">from</span> <span class="nn">.version</span> <span class="kn">import</span> <span class="n">cuda</span> <span class="k">as</span> <span class="n">cuda_version</span>
    <span class="kn">import</span> <span class="nn">glob</span>
    <span class="k">if</span> <span class="n">cuda_version</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="ow">not</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;cudart64*.dll&#39;</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">):</span>
        <span class="n">cuda_version_1</span> <span class="o">=</span> <span class="n">cuda_version</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
        <span class="n">cuda_path_var</span> <span class="o">=</span> <span class="s1">&#39;CUDA_PATH_V&#39;</span> <span class="o">+</span> <span class="n">cuda_version_1</span>
        <span class="n">default_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pfiles_path</span><span class="p">,</span> <span class="s1">&#39;NVIDIA GPU Computing Toolkit&#39;</span><span class="p">,</span> <span class="s1">&#39;CUDA&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span> <span class="o">+</span> <span class="n">cuda_version</span><span class="p">)</span>
        <span class="n">cuda_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="n">cuda_path_var</span><span class="p">,</span> <span class="n">default_path</span><span class="p">),</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cuda_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="n">dll_paths</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">,</span> <span class="p">[</span><span class="n">nvtoolsext_dll_path</span><span class="p">,</span> <span class="n">cuda_path</span><span class="p">]))</span>

    <span class="n">kernel32</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinDLL</span><span class="p">(</span><span class="s1">&#39;kernel32.dll&#39;</span><span class="p">,</span> <span class="n">use_last_error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">with_load_library_flags</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">kernel32</span><span class="p">,</span> <span class="s1">&#39;AddDllDirectory&#39;</span><span class="p">)</span>
    <span class="n">prev_error_mode</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">SetErrorMode</span><span class="p">(</span><span class="mh">0x0001</span><span class="p">)</span>

    <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryW</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>
    <span class="k">if</span> <span class="n">with_load_library_flags</span><span class="p">:</span>
        <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryExW</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>

    <span class="k">for</span> <span class="n">dll_path</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">add_dll_directory</span><span class="p">(</span><span class="n">dll_path</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;vcruntime140.dll&#39;</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;msvcp140.dll&#39;</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;vcruntime140_1.dll&#39;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.</span>
<span class="s1">                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe&#39;&#39;&#39;</span><span class="p">)</span>

    <span class="n">dlls</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">th_dll_path</span><span class="p">,</span> <span class="s1">&#39;*.dll&#39;</span><span class="p">))</span>
    <span class="n">path_patched</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">dll</span> <span class="ow">in</span> <span class="n">dlls</span><span class="p">:</span>
        <span class="n">is_loaded</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">with_load_library_flags</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryExW</span><span class="p">(</span><span class="n">dll</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mh">0x00001100</span><span class="p">)</span>
            <span class="n">last_error</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">get_last_error</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_error</span> <span class="o">!=</span> <span class="mi">126</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinError</span><span class="p">(</span><span class="n">last_error</span><span class="p">)</span>
                <span class="n">err</span><span class="o">.</span><span class="n">strerror</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39; Error loading &quot;</span><span class="si">{</span><span class="n">dll</span><span class="si">}</span><span class="s1">&quot; or one of its dependencies.&#39;</span>
                <span class="k">raise</span> <span class="n">err</span>
            <span class="k">elif</span> <span class="n">res</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">is_loaded</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_loaded</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">path_patched</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dll_paths</span> <span class="o">+</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]])</span>
                <span class="n">path_patched</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryW</span><span class="p">(</span><span class="n">dll</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinError</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">get_last_error</span><span class="p">())</span>
                <span class="n">err</span><span class="o">.</span><span class="n">strerror</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39; Error loading &quot;</span><span class="si">{</span><span class="n">dll</span><span class="si">}</span><span class="s1">&quot; or one of its dependencies.&#39;</span>
                <span class="k">raise</span> <span class="n">err</span>

    <span class="n">kernel32</span><span class="o">.</span><span class="n">SetErrorMode</span><span class="p">(</span><span class="n">prev_error_mode</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_preload_cuda_deps</span><span class="p">(</span><span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Preloads cuda deps if they could not be found otherwise.&quot;&quot;&quot;</span>
    <span class="c1"># Should only be called on Linux if default path resolution have failed</span>
    <span class="k">assert</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Linux&#39;</span><span class="p">,</span> <span class="s1">&#39;Should only be called on Linux&#39;</span>
    <span class="kn">import</span> <span class="nn">glob</span>
    <span class="n">lib_path</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
        <span class="n">nvidia_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;nvidia&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">nvidia_path</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">candidate_lib_paths</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nvidia_path</span><span class="p">,</span> <span class="n">lib_folder</span><span class="p">,</span> <span class="s1">&#39;lib&#39;</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">candidate_lib_paths</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">lib_path</span><span class="p">:</span>
            <span class="n">lib_path</span> <span class="o">=</span> <span class="n">candidate_lib_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">lib_path</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">lib_path</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lib_name</span><span class="si">}</span><span class="s2"> not found in the system path </span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">)</span>


<span class="c1"># See Note [Global dependencies]</span>
<span class="k">def</span> <span class="nf">_load_global_deps</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">_running_with_deploy</span><span class="p">()</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">lib_name</span> <span class="o">=</span> <span class="s1">&#39;libtorch_global_deps&#39;</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.dylib&#39;</span> <span class="k">if</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span> <span class="k">else</span> <span class="s1">&#39;.so&#39;</span><span class="p">)</span>
    <span class="n">here</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>
    <span class="n">lib_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">here</span><span class="p">),</span> <span class="s1">&#39;lib&#39;</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">ctypes</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="c1"># Can only happen for wheel with cuda libs as PYPI deps</span>
        <span class="c1"># As PyTorch is not purelib, but nvidia-*-cu11 is</span>
        <span class="n">cuda_libs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;cublas&#39;</span><span class="p">:</span> <span class="s1">&#39;libcublas.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cudnn&#39;</span><span class="p">:</span> <span class="s1">&#39;libcudnn.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_nvrtc&#39;</span><span class="p">:</span> <span class="s1">&#39;libnvrtc.so.*[0-9].*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_runtime&#39;</span><span class="p">:</span> <span class="s1">&#39;libcudart.so.*[0-9].*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_cupti&#39;</span><span class="p">:</span> <span class="s1">&#39;libcupti.so.*[0-9].*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cufft&#39;</span><span class="p">:</span> <span class="s1">&#39;libcufft.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;curand&#39;</span><span class="p">:</span> <span class="s1">&#39;libcurand.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cusolver&#39;</span><span class="p">:</span> <span class="s1">&#39;libcusolver.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cusparse&#39;</span><span class="p">:</span> <span class="s1">&#39;libcusparse.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;nccl&#39;</span><span class="p">:</span> <span class="s1">&#39;libnccl.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;nvtx&#39;</span><span class="p">:</span> <span class="s1">&#39;libnvToolsExt.so.*[0-9]&#39;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">is_cuda_lib_err</span> <span class="o">=</span> <span class="p">[</span><span class="n">lib</span> <span class="k">for</span> <span class="n">lib</span> <span class="ow">in</span> <span class="n">cuda_libs</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span><span class="p">(</span><span class="n">lib</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">err</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cuda_lib_err</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">err</span>
        <span class="k">for</span> <span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span> <span class="ow">in</span> <span class="n">cuda_libs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">_preload_cuda_deps</span><span class="p">(</span><span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">ctypes</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span><span class="p">)</span>


<span class="k">if</span> <span class="p">(</span><span class="n">USE_RTLD_GLOBAL_WITH_LIBTORCH</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;TORCH_USE_RTLD_GLOBAL&#39;</span><span class="p">))</span> <span class="ow">and</span> \
        <span class="p">(</span><span class="n">_running_with_deploy</span><span class="p">()</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">!=</span> <span class="s1">&#39;Windows&#39;</span><span class="p">):</span>
    <span class="c1"># Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a</span>
    <span class="c1"># few circumstances:</span>
    <span class="c1">#</span>
    <span class="c1">#   1. You&#39;re in a build environment (e.g., fbcode) where</span>
    <span class="c1">#      libtorch_global_deps is not available, but you still need</span>
    <span class="c1">#      to get mkl to link in with RTLD_GLOBAL or it will just</span>
    <span class="c1">#      not work.</span>
    <span class="c1">#</span>
    <span class="c1">#   2. You&#39;re trying to run PyTorch under UBSAN and you need</span>
    <span class="c1">#      to ensure that only one copy of libtorch is loaded, so</span>
    <span class="c1">#      vptr checks work properly</span>
    <span class="c1">#</span>
    <span class="c1"># If you&#39;re using this setting, you must verify that all the libraries</span>
    <span class="c1"># you load consistently use the same libstdc++, or you may have</span>
    <span class="c1"># mysterious segfaults.</span>
    <span class="c1">#</span>
    <span class="n">old_flags</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">getdlopenflags</span><span class="p">()</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">setdlopenflags</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">RTLD_LAZY</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">setdlopenflags</span><span class="p">(</span><span class="n">old_flags</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">old_flags</span>

<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Easy way.  You want this most of the time, because it will prevent</span>
    <span class="c1"># C++ symbols from libtorch clobbering C++ symbols from other</span>
    <span class="c1"># libraries, leading to mysterious segfaults.</span>
    <span class="c1">#</span>
    <span class="c1"># If building in an environment where libtorch_global_deps isn&#39;t available</span>
    <span class="c1"># like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will</span>
    <span class="c1"># want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False</span>
    <span class="c1">#</span>
    <span class="c1"># See Note [Global dependencies]</span>
    <span class="k">if</span> <span class="n">USE_GLOBAL_DEPS</span><span class="p">:</span>
        <span class="n">_load_global_deps</span><span class="p">()</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>

<span class="c1"># Appease the type checker; ordinarily this binding is inserted by the</span>
<span class="c1"># torch._C module initialization code in C</span>
<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C</span>

<div class="viewcode-block" id="SymInt"><a class="viewcode-back" href="../torch.html#torch.SymInt">[docs]</a><span class="k">class</span> <span class="nc">SymInt</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an int (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="bp">self</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">int_</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__index__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">int_</span><span class="p">()</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.symbolic_shapes</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_max__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_min__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_float__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="p">)</span></div>

<div class="viewcode-block" id="SymFloat"><a class="viewcode-back" href="../torch.html#torch.SymFloat">[docs]</a><span class="k">class</span> <span class="nc">SymFloat</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an float (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">()</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.symbolic_shapes</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_max__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_min__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">str</span><span class="p">()</span></div>

<div class="viewcode-block" id="SymBool"><a class="viewcode-back" href="../torch.html#torch.SymBool">[docs]</a><span class="k">class</span> <span class="nc">SymBool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an bool (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>

<span class="sd">    Unlike regular bools, regular boolean operators will force extra guards instead</span>
<span class="sd">    of symbolically evaluate.  Use the bitwise operators instead to handle this.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">())</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.symbolic_shapes</span>
    <span class="k">def</span> <span class="fm">__and__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__or__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="c1"># We very carefully define __sym_not__, and not a number of other</span>
    <span class="c1"># plausible alternatives:</span>
    <span class="c1">#</span>
    <span class="c1">#   - We do not override __not__ because this is not a real magic</span>
    <span class="c1">#     method; you cannot override the meaning of the not builtin in</span>
    <span class="c1">#     Python.  We use the name &#39;sym_not&#39; to clarify that in user code you</span>
    <span class="c1">#     cannot use the builtin not or operator.not_ or operator.__not__ and</span>
    <span class="c1">#     hit this magic method; you must use our custom sym_not operator.</span>
    <span class="c1">#</span>
    <span class="c1">#   - We do not override the __invert__ method because SymBool is</span>
    <span class="c1">#     meant to be usable in situations where bool is expected.  However,</span>
    <span class="c1">#     bitwise negation ~a does the wrong thing with booleans (because</span>
    <span class="c1">#     bool is a subclass of int, so ~1 = -2 which is not falseish.)</span>
    <span class="c1">#     This would be a giant footgun, so we get around it by defining</span>
    <span class="c1">#     our own operator.  Note that bitwise and/or do the right thing,</span>
    <span class="c1">#     so we reuse the conventional operators there for readability.</span>
    <span class="c1">#</span>
    <span class="k">def</span> <span class="nf">__sym_not__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">str</span><span class="p">()</span></div>

<div class="viewcode-block" id="sym_not"><a class="viewcode-back" href="../generated/torch.sym_not.html#torch.sym_not">[docs]</a><span class="k">def</span> <span class="nf">sym_not</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for logical negation.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymBool or bool): Object to negate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="s1">&#39;__sym_not__&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_not__</span><span class="p">()</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="n">a</span></div>

<div class="viewcode-block" id="sym_float"><a class="viewcode-back" href="../generated/torch.sym_float.html#torch.sym_float">[docs]</a><span class="k">def</span> <span class="nf">sym_float</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for float casting.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymInt, SymFloat, or object): Object to cast</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="s1">&#39;__sym_float__&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_float__</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">py_float</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span></div>


<div class="viewcode-block" id="sym_int"><a class="viewcode-back" href="../generated/torch.sym_int.html#torch.sym_int">[docs]</a><span class="k">def</span> <span class="nf">sym_int</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for int casting.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymInt, SymFloat, or object): Object to cast</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymInt</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
    <span class="k">return</span> <span class="n">py_int</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span></div>

<div class="viewcode-block" id="sym_max"><a class="viewcode-back" href="../generated/torch.sym_max.html#torch.sym_max">[docs]</a><span class="k">def</span> <span class="nf">sym_max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; SymInt-aware utility for max().&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_max__</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="c1"># NB: If you actually care about preserving output type exactly</span>
        <span class="c1"># if you do something like max(0, 0.0), it is NOT sound to treat</span>
        <span class="c1"># min/max as commutative</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">.</span><span class="n">__sym_max__</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span></div>

<div class="viewcode-block" id="sym_min"><a class="viewcode-back" href="../generated/torch.sym_min.html#torch.sym_min">[docs]</a><span class="k">def</span> <span class="nf">sym_min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; SymInt-aware utility for max().&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_min__</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">.</span><span class="n">__sym_min__</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span></div>

<span class="c1"># Check to see if we can load C extensions, and if not provide some guidance</span>
<span class="c1"># on what the problem might be.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="c1"># _initExtension is chosen (arbitrarily) as a sentinel.</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_initExtension</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C_for_compiled_check</span>

    <span class="c1"># The __file__ check only works for Python 3.7 and above.</span>
    <span class="k">if</span> <span class="n">_C_for_compiled_check</span><span class="o">.</span><span class="vm">__file__</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">            Failed to load PyTorch C extensions:</span>
<span class="s1">                It appears that PyTorch has loaded the `torch/_C` folder</span>
<span class="s1">                of the PyTorch repository rather than the C extensions which</span>
<span class="s1">                are expected in the `torch._C` namespace. This can occur when</span>
<span class="s1">                using the `install` workflow. e.g.</span>
<span class="s1">                    $ python setup.py install &amp;&amp; python -c &quot;import torch&quot;</span>

<span class="s1">                This error can generally be solved using the `develop` workflow</span>
<span class="s1">                    $ python setup.py develop &amp;&amp; python -c &quot;import torch&quot;  # This should succeed</span>
<span class="s1">                or by running Python from a different directory.</span>
<span class="s1">            &#39;&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="kn">from</span> <span class="bp">None</span>
    <span class="k">raise</span>  <span class="c1"># If __file__ is not None the cause is unknown, so just re-raise.</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;Base&#39;</span><span class="p">):</span>
        <span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Callable</span><span class="p">)</span> <span class="ow">or</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">obj</span><span class="p">)):</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;torch&#39;</span><span class="p">):</span>
                <span class="c1"># TODO: fix their module from C++ side</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;DisableTorchFunctionSubclass&#39;</span><span class="p">,</span> <span class="s1">&#39;DisableTorchFunction&#39;</span><span class="p">,</span> <span class="s1">&#39;Generator&#39;</span><span class="p">]:</span>
                    <span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s1">&#39;torch&#39;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="c1"># issue 38137 and python issue 43367. Submodules of a C extension are</span>
    <span class="c1"># non-standard, and attributes of those submodules cannot be pickled since</span>
    <span class="c1"># pickle expect to be able to import them as &quot;from _C.sub import attr&quot;</span>
    <span class="c1"># which fails with &quot;_C is not a package</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">type</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
            <span class="c1"># submodule</span>
            <span class="k">if</span> <span class="sa">f</span><span class="s1">&#39;torch._C.</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s1">&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
                <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;torch._C.</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">candidate</span>


<span class="c1">################################################################################</span>
<span class="c1"># Define basic utilities</span>
<span class="c1">################################################################################</span>


<span class="k">def</span> <span class="nf">typename</span><span class="p">(</span><span class="n">o</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">o</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>

    <span class="n">module</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__module__&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;builtins&#39;</span> \
            <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;__builtin__&#39;</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__qualname__&#39;</span><span class="p">):</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__qualname__</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">):</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">return</span> <span class="n">module</span> <span class="o">+</span> <span class="n">class_name</span>


<div class="viewcode-block" id="is_tensor"><a class="viewcode-back" href="../generated/torch.is_tensor.html#torch.is_tensor">[docs]</a><span class="k">def</span> <span class="nf">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if `obj` is a PyTorch tensor.</span>

<span class="sd">    Note that this function is simply doing ``isinstance(obj, Tensor)``.</span>
<span class="sd">    Using that ``isinstance`` check is better for typechecking with mypy,</span>
<span class="sd">    and more explicit - so it&#39;s recommended to use that instead of</span>
<span class="sd">    ``is_tensor``.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Object): Object to test</span>
<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; x = torch.tensor([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; torch.is_tensor(x)</span>
<span class="sd">        True</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span></div>


<div class="viewcode-block" id="is_storage"><a class="viewcode-back" href="../generated/torch.is_storage.html#torch.is_storage">[docs]</a><span class="k">def</span> <span class="nf">is_storage</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if `obj` is a PyTorch storage object.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Object): Object to test</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">in</span> <span class="n">_storage_classes</span></div>


<span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="set_default_device"><a class="viewcode-back" href="../generated/torch.set_default_device.html#torch.set_default_device">[docs]</a><span class="k">def</span> <span class="nf">set_default_device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets the default ``torch.Tensor`` to be allocated on ``device``.  This</span>
<span class="sd">    does not affect factory function calls which are called with an explicit</span>
<span class="sd">    ``device`` argument.  Factory calls will be performed as if they</span>
<span class="sd">    were passed ``device`` as an argument.</span>

<span class="sd">    To only temporarily change the default device instead of setting it</span>
<span class="sd">    globally, use ``with torch.device(device):`` instead.</span>

<span class="sd">    The default device is initially ``cpu``.  If you set the default tensor</span>
<span class="sd">    device to another device (e.g., ``cuda``) without a device index, tensors</span>
<span class="sd">    will be allocated on whatever the current device for the device type,</span>
<span class="sd">    even after :func:`torch.cuda.set_device` is called.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        This function imposes a slight performance cost on every Python</span>
<span class="sd">        call to the torch API (not just factory functions).  If this</span>
<span class="sd">        is causing problems for you, please comment on</span>
<span class="sd">        https://github.com/pytorch/pytorch/issues/92701</span>

<span class="sd">    Args:</span>
<span class="sd">        device (device or string): the device to set as default</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;requires cuda, changes global state&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).device</span>
<span class="sd">        device(type=&#39;cpu&#39;)</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_device(&#39;cuda&#39;)  # current device is 0</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).device</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=0)</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_device(&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).device</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=1)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_GLOBAL_DEVICE_CONTEXT</span>
    <span class="k">if</span> <span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span>
    <span class="kn">from</span> <span class="nn">torch.utils._device</span> <span class="kn">import</span> <span class="n">DeviceContext</span>
    <span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="o">=</span> <span class="n">DeviceContext</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span></div>


<div class="viewcode-block" id="set_default_tensor_type"><a class="viewcode-back" href="../generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type">[docs]</a><span class="k">def</span> <span class="nf">set_default_tensor_type</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the default ``torch.Tensor`` type to floating point tensor type</span>
<span class="sd">    ``t``. This type will also be used as default floating point type for</span>
<span class="sd">    type inference in :func:`torch.tensor`.</span>

<span class="sd">    The default floating point tensor type is initially ``torch.FloatTensor``.</span>

<span class="sd">    Args:</span>
<span class="sd">        t (type or string): the floating point tensor type or its name</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Other tests may have changed the default type. Can we reset it?&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32</span>
<span class="sd">        torch.float32</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor</span>
<span class="sd">        torch.float64</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">_import_dotted_name</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_default_tensor_type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span></div>


<div class="viewcode-block" id="set_default_dtype"><a class="viewcode-back" href="../generated/torch.set_default_dtype.html#torch.set_default_dtype">[docs]</a><span class="k">def</span> <span class="nf">set_default_dtype</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Sets the default floating point dtype to :attr:`d`. Supports torch.float32</span>
<span class="sd">    and torch.float64 as inputs. Other dtypes may be accepted without complaint</span>
<span class="sd">    but are not supported and are unlikely to work as expected.</span>

<span class="sd">    When PyTorch is initialized its default floating point dtype is torch.float32,</span>
<span class="sd">    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like</span>
<span class="sd">    type inference. The default floating point dtype is used to:</span>

<span class="sd">    1. Implicitly determine the default complex dtype. When the default floating point</span>
<span class="sd">       type is float32 the default complex dtype is complex64, and when the default</span>
<span class="sd">       floating point type is float64 the default complex type is complex128.</span>
<span class="sd">    2. Infer the dtype for tensors constructed using Python floats or complex Python</span>
<span class="sd">       numbers. See examples below.</span>
<span class="sd">    3. Determine the result of type promotion between bool and integer tensors and</span>
<span class="sd">       Python floats and complex Python numbers.</span>

<span class="sd">    Args:</span>
<span class="sd">        d (:class:`torch.dtype`): the floating point dtype to make the default.</span>
<span class="sd">                                  Either torch.float32 or torch.float64.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Other tests may have changed the default type. Can we reset it?&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # initial default for floating point is torch.float32</span>
<span class="sd">        &gt;&gt;&gt; # Python floats are interpreted as float32</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype</span>
<span class="sd">        torch.float32</span>
<span class="sd">        &gt;&gt;&gt; # initial default for floating point is torch.complex64</span>
<span class="sd">        &gt;&gt;&gt; # Complex Python numbers are interpreted as complex64</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3j]).dtype</span>
<span class="sd">        torch.complex64</span>

<span class="sd">        &gt;&gt;&gt; torch.set_default_dtype(torch.float64)</span>

<span class="sd">        &gt;&gt;&gt; # Python floats are now interpreted as float64</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor</span>
<span class="sd">        torch.float64</span>
<span class="sd">        &gt;&gt;&gt; # Complex Python numbers are now interpreted as complex128</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3j]).dtype   # a new complex tensor</span>
<span class="sd">        torch.complex128</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_default_dtype</span><span class="p">(</span><span class="n">d</span><span class="p">)</span></div>

<div class="viewcode-block" id="use_deterministic_algorithms"><a class="viewcode-back" href="../generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms">[docs]</a><span class="k">def</span> <span class="nf">use_deterministic_algorithms</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Sets whether PyTorch operations must use &quot;deterministic&quot;</span>
<span class="sd">    algorithms. That is, algorithms which, given the same input, and when</span>
<span class="sd">    run on the same software and hardware, always produce the same output.</span>
<span class="sd">    When enabled, operations will use deterministic algorithms when available,</span>
<span class="sd">    and if only nondeterministic algorithms are available they will throw a</span>
<span class="sd">    :class:`RuntimeError` when called.</span>

<span class="sd">    .. note:: This setting alone is not always enough to make an application</span>
<span class="sd">        reproducible. Refer to :ref:`reproducibility` for more information.</span>

<span class="sd">    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative</span>
<span class="sd">        interface for this feature.</span>

<span class="sd">    The following normally-nondeterministic operations will act</span>
<span class="sd">    deterministically when ``mode=True``:</span>

<span class="sd">        * :class:`torch.nn.Conv1d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.Conv2d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.Conv3d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor</span>
<span class="sd">        * :func:`torch.bmm` when called on sparse-dense CUDA tensors</span>
<span class="sd">        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor</span>
<span class="sd">          and the index is a list of tensors</span>
<span class="sd">        * :func:`torch.Tensor.index_put` with ``accumulate=False``</span>
<span class="sd">        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU</span>
<span class="sd">          tensor</span>
<span class="sd">        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU</span>
<span class="sd">          tensor</span>
<span class="sd">        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.gather` when called on a CUDA tensor that requires grad</span>
<span class="sd">        * :func:`torch.index_add` when called on CUDA tensor</span>
<span class="sd">        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor</span>
<span class="sd">        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor</span>
<span class="sd">        * :func:`torch.Tensor.scatter_reduce` when ``reduce=&#39;sum&#39;`` or ``reduce=&#39;mean&#39;`` and called on CUDA tensor</span>

<span class="sd">    The following normally-nondeterministic operations will throw a</span>
<span class="sd">    :class:`RuntimeError` when ``mode=True``:</span>

<span class="sd">        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool1d`</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool2d`</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool3d`</span>
<span class="sd">        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor</span>
<span class="sd">          and one of the following modes is used:</span>

<span class="sd">          - ``linear``</span>
<span class="sd">          - ``bilinear``</span>
<span class="sd">          - ``bicubic``</span>
<span class="sd">          - ``trilinear``</span>

<span class="sd">        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when</span>
<span class="sd">          ``mode=&#39;max&#39;``</span>
<span class="sd">        * :func:`torch.Tensor.put_` when ``accumulate=False``</span>
<span class="sd">        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.histc` when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.bincount` when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.kthvalue` with called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.median` with indices output when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex</span>
<span class="sd">        * :func:`torch.Tensor.scatter_reduce` when ``reduce=&#39;prod&#39;`` and called on CUDA tensor</span>

<span class="sd">    A handful of CUDA operations are nondeterministic if the CUDA version is</span>
<span class="sd">    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``</span>
<span class="sd">    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more</span>
<span class="sd">    details: `&lt;https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility&gt;`_</span>
<span class="sd">    If one of these environment variable configurations is not set, a :class:`RuntimeError`</span>
<span class="sd">    will be raised from these operations when called with CUDA tensors:</span>

<span class="sd">        * :func:`torch.mm`</span>
<span class="sd">        * :func:`torch.mv`</span>
<span class="sd">        * :func:`torch.bmm`</span>

<span class="sd">    Note that deterministic operations tend to have worse performance than</span>
<span class="sd">    nondeterministic operations.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This flag does not detect or prevent nondeterministic behavior caused</span>
<span class="sd">        by calling an inplace operation on a tensor with an internal memory</span>
<span class="sd">        overlap or by giving such a tensor as the :attr:`out` argument for an</span>
<span class="sd">        operation. In these cases, multiple writes of different data may target</span>
<span class="sd">        a single memory location, and the order of writes is not guaranteed.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (:class:`bool`): If True, makes potentially nondeterministic</span>
<span class="sd">            operations switch to a deterministic algorithm or throw a runtime</span>
<span class="sd">            error. If False, allows nondeterministic operations.</span>

<span class="sd">    Keyword args:</span>
<span class="sd">        warn_only (:class:`bool`, optional): If True, operations that do not</span>
<span class="sd">            have a deterministic implementation will throw a warning instead of</span>
<span class="sd">            an error. Default: ``False``</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; torch.use_deterministic_algorithms(True)</span>

<span class="sd">        # Forward mode nondeterministic error</span>
<span class="sd">        &gt;&gt;&gt; torch.randn(10, device=&#39;cuda&#39;).kthvalue(0)</span>
<span class="sd">        ...</span>
<span class="sd">        RuntimeError: kthvalue CUDA does not have a deterministic implementation...</span>

<span class="sd">        # Backward mode nondeterministic error</span>
<span class="sd">        &gt;&gt;&gt; torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()</span>
<span class="sd">        ...</span>
<span class="sd">        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="n">warn_only</span><span class="p">)</span></div>

<div class="viewcode-block" id="are_deterministic_algorithms_enabled"><a class="viewcode-back" href="../generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled">[docs]</a><span class="k">def</span> <span class="nf">are_deterministic_algorithms_enabled</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global deterministic flag is turned on. Refer to</span>
<span class="sd">    :func:`torch.use_deterministic_algorithms` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms</span><span class="p">()</span></div>

<div class="viewcode-block" id="is_deterministic_algorithms_warn_only_enabled"><a class="viewcode-back" href="../generated/torch.is_deterministic_algorithms_warn_only_enabled.html#torch.is_deterministic_algorithms_warn_only_enabled">[docs]</a><span class="k">def</span> <span class="nf">is_deterministic_algorithms_warn_only_enabled</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global deterministic flag is set to warn only.</span>
<span class="sd">    Refer to :func:`torch.use_deterministic_algorithms` documentation for more</span>
<span class="sd">    details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms_warn_only</span><span class="p">()</span></div>

<div class="viewcode-block" id="set_deterministic_debug_mode"><a class="viewcode-back" href="../generated/torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode">[docs]</a><span class="k">def</span> <span class="nf">set_deterministic_debug_mode</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the debug mode for deterministic operations.</span>

<span class="sd">    .. note:: This is an alternative interface for</span>
<span class="sd">        :func:`torch.use_deterministic_algorithms`. Refer to that function&#39;s</span>
<span class="sd">        documentation for details about affected operations.</span>

<span class="sd">    Args:</span>
<span class="sd">        debug_mode(str or int): If &quot;default&quot; or 0, don&#39;t error or warn on</span>
<span class="sd">            nondeterministic operations. If &quot;warn&quot; or 1, warn on</span>
<span class="sd">            nondeterministic operations. If &quot;error&quot; or 2, error on</span>
<span class="sd">            nondeterministic operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># NOTE: builtins.int is used here because int in this scope resolves</span>
    <span class="c1"># to torch.int</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">,</span> <span class="p">(</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;debug_mode must be str or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;default&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;warn&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;error&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s1">&#39;invalid value of debug_mode, expected one of `default`, &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;`warn`, `error`, but got </span><span class="si">{</span><span class="n">debug_mode</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s1">&#39;invalid value of debug_mode, expected 0, 1, or 2, &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;but got </span><span class="si">{</span><span class="n">debug_mode</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="get_deterministic_debug_mode"><a class="viewcode-back" href="../generated/torch.get_deterministic_debug_mode.html#torch.get_deterministic_debug_mode">[docs]</a><span class="k">def</span> <span class="nf">get_deterministic_debug_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current value of the debug mode for deterministic</span>
<span class="sd">    operations. Refer to :func:`torch.set_deterministic_debug_mode`</span>
<span class="sd">    documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms_warn_only</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span></div>

<div class="viewcode-block" id="get_float32_matmul_precision"><a class="viewcode-back" href="../generated/torch.get_float32_matmul_precision.html#torch.get_float32_matmul_precision">[docs]</a><span class="k">def</span> <span class="nf">get_float32_matmul_precision</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current value of float32 matrix multiplication precision. Refer to</span>
<span class="sd">    :func:`torch.set_float32_matmul_precision` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_float32_matmul_precision</span><span class="p">()</span></div>

<div class="viewcode-block" id="set_float32_matmul_precision"><a class="viewcode-back" href="../generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision">[docs]</a><span class="k">def</span> <span class="nf">set_float32_matmul_precision</span><span class="p">(</span><span class="n">precision</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the internal precision of float32 matrix multiplications.</span>

<span class="sd">    Running float32 matrix multiplications in lower precision may significantly increase</span>
<span class="sd">    performance, and in some programs the loss of precision has a negligible impact.</span>

<span class="sd">    Supports three settings:</span>

<span class="sd">        * &quot;highest&quot;, float32 matrix multiplications use the float32 datatype for</span>
<span class="sd">          internal computations.</span>
<span class="sd">        * &quot;high&quot;, float32 matrix multiplications use the TensorFloat32 or bfloat16_3x</span>
<span class="sd">          datatypes for internal computations, if fast matrix multiplication algorithms</span>
<span class="sd">          using those datatypes internally are available. Otherwise float32</span>
<span class="sd">          matrix multiplications are computed as if the precision is &quot;highest&quot;.</span>
<span class="sd">        * &quot;medium&quot;, float32 matrix multiplications use the bfloat16 datatype for</span>
<span class="sd">          internal computations, if a fast matrix multiplication algorithm</span>
<span class="sd">          using that datatype internally is available. Otherwise float32</span>
<span class="sd">          matrix multiplications are computed as if the precision is &quot;high&quot;.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This does not change the output dtype of float32 matrix multiplications,</span>
<span class="sd">        it controls how the internal computation of the matrix multiplication is performed.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This does not change the precision of convolution operations. Other flags,</span>
<span class="sd">        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution</span>
<span class="sd">        operations.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This flag currently only affects one native device type: CUDA.</span>
<span class="sd">        If &quot;high&quot; or &quot;medium&quot; are set then the TensorFloat32 datatype will be used</span>
<span class="sd">        when computing float32 matrix multiplications, equivalent to setting</span>
<span class="sd">        `torch.backends.cuda.matmul.allow_tf32 = True`. When &quot;highest&quot; (the default)</span>
<span class="sd">        is set then the float32 datatype is used for internal computations, equivalent</span>
<span class="sd">        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.</span>

<span class="sd">    Args:</span>
<span class="sd">        precision(str): can be set to &quot;highest&quot; (default), &quot;high&quot;, or &quot;medium&quot; (see above).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_float32_matmul_precision</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span></div>

<div class="viewcode-block" id="set_warn_always"><a class="viewcode-back" href="../generated/torch.set_warn_always.html#torch.set_warn_always">[docs]</a><span class="k">def</span> <span class="nf">set_warn_always</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;When this flag is False (default) then some PyTorch warnings may only</span>
<span class="sd">    appear once per process. This helps avoid excessive warning information.</span>
<span class="sd">    Setting it to True causes these warnings to always appear, which may be</span>
<span class="sd">    helpful when debugging.</span>

<span class="sd">    Args:</span>
<span class="sd">        b (:class:`bool`): If True, force warnings to always be emitted</span>
<span class="sd">                           If False, set to the default behaviour</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_warnAlways</span><span class="p">(</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="is_warn_always_enabled"><a class="viewcode-back" href="../generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled">[docs]</a><span class="k">def</span> <span class="nf">is_warn_always_enabled</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global warn_always flag is turned on. Refer to</span>
<span class="sd">    :func:`torch.set_warn_always` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_warnAlways</span><span class="p">()</span></div>

<span class="c1">################################################################################</span>
<span class="c1"># Define error checking functions</span>
<span class="c1">################################################################################</span>

<span class="c1"># These error checking functions must be kept consistent with their C++</span>
<span class="c1"># equivalents. Their C++ equivalents are mentioned where applicable.</span>

<span class="k">def</span> <span class="nf">_check_with</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="n">cond</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">SymBool</span><span class="p">],</span> <span class="n">message</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="nb">str</span><span class="p">]):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="p">(</span><span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymBool</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cond must be a bool, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cond</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># error_type must be a subclass of Exception and not subclass of Warning</span>
    <span class="k">assert</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="ne">Exception</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="ne">Warning</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">message_evaluated</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s1">&#39;Expected cond to be True, but got False. (Could this error &#39;</span>
            <span class="s1">&#39;message be improved? If so, please report an enhancement request &#39;</span>
            <span class="s1">&#39;to PyTorch.)&#39;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;message must be a callable&#39;</span><span class="p">)</span>

        <span class="n">message_evaluated</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">message</span><span class="p">())</span>

    <span class="k">raise</span> <span class="n">error_type</span><span class="p">(</span><span class="n">message_evaluated</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``RuntimeError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">RuntimeError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_index</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``IndexError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_INDEX``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">IndexError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_value</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``ValueError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_VALUE``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_type</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``TypeError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_TYPE``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_not_implemented</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``NotImplementedError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_NOT_IMPLEMENTED``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">NotImplementedError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_tensor_all_with</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">cond</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cond must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">cond</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;cond tensor must have dtype torch.bool, but got </span><span class="si">{</span><span class="n">cond</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">_check_with</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="n">cond</span><span class="o">.</span><span class="n">_is_all_true</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">message</span><span class="p">)</span>

<span class="c1"># C++ equivalent: `TORCH_CHECK_TENSOR_ALL`</span>
<span class="k">def</span> <span class="nf">_check_tensor_all</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``RuntimeError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_TENSOR_ALL``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`torch.Tensor`): Tensor of dtype ``torch.bool``. If any</span>
<span class="sd">            element is ``False``, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_tensor_all_with</span><span class="p">(</span><span class="ne">RuntimeError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define numeric constants</span>
<span class="c1">################################################################################</span>

<span class="c1"># For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and</span>
<span class="c1"># NumPy consistency (https://numpy.org/devdocs/reference/constants.html)</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">e</span> <span class="p">,</span> <span class="n">nan</span> <span class="p">,</span> <span class="n">inf</span> <span class="p">,</span> <span class="n">pi</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;pi&#39;</span><span class="p">,</span> <span class="s1">&#39;nan&#39;</span><span class="p">,</span> <span class="s1">&#39;inf&#39;</span><span class="p">])</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define Storage and Tensor classes</span>
<span class="c1">################################################################################</span>

<span class="kn">from</span> <span class="nn">._tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">.storage</span> <span class="kn">import</span> <span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">,</span> <span class="n">_LegacyStorage</span><span class="p">,</span> <span class="n">UntypedStorage</span><span class="p">,</span> <span class="n">_warn_typed_storage_removal</span>

<span class="c1"># NOTE: New &lt;type&gt;Storage classes should never be added. When adding a new</span>
<span class="c1"># dtype, use torch.storage.TypedStorage directly.</span>

<div class="viewcode-block" id="ByteStorage"><a class="viewcode-back" href="../storage.html#torch.ByteStorage">[docs]</a><span class="k">class</span> <span class="nc">ByteStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="ByteStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.ByteStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span></div>

<div class="viewcode-block" id="DoubleStorage"><a class="viewcode-back" href="../storage.html#torch.DoubleStorage">[docs]</a><span class="k">class</span> <span class="nc">DoubleStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="DoubleStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.DoubleStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span></div>

<div class="viewcode-block" id="FloatStorage"><a class="viewcode-back" href="../storage.html#torch.FloatStorage">[docs]</a><span class="k">class</span> <span class="nc">FloatStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="FloatStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.FloatStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span></div>

<div class="viewcode-block" id="HalfStorage"><a class="viewcode-back" href="../storage.html#torch.HalfStorage">[docs]</a><span class="k">class</span> <span class="nc">HalfStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="HalfStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.HalfStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span></div>

<div class="viewcode-block" id="LongStorage"><a class="viewcode-back" href="../storage.html#torch.LongStorage">[docs]</a><span class="k">class</span> <span class="nc">LongStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="LongStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.LongStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span></div>

<div class="viewcode-block" id="IntStorage"><a class="viewcode-back" href="../storage.html#torch.IntStorage">[docs]</a><span class="k">class</span> <span class="nc">IntStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="IntStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.IntStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">int</span></div>

<div class="viewcode-block" id="ShortStorage"><a class="viewcode-back" href="../storage.html#torch.ShortStorage">[docs]</a><span class="k">class</span> <span class="nc">ShortStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="ShortStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.ShortStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">short</span></div>

<div class="viewcode-block" id="CharStorage"><a class="viewcode-back" href="../storage.html#torch.CharStorage">[docs]</a><span class="k">class</span> <span class="nc">CharStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="CharStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.CharStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span></div>

<div class="viewcode-block" id="BoolStorage"><a class="viewcode-back" href="../storage.html#torch.BoolStorage">[docs]</a><span class="k">class</span> <span class="nc">BoolStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="BoolStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.BoolStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span></div>

<div class="viewcode-block" id="BFloat16Storage"><a class="viewcode-back" href="../storage.html#torch.BFloat16Storage">[docs]</a><span class="k">class</span> <span class="nc">BFloat16Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="BFloat16Storage.dtype"><a class="viewcode-back" href="../storage.html#torch.BFloat16Storage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span></div>

<div class="viewcode-block" id="ComplexDoubleStorage"><a class="viewcode-back" href="../storage.html#torch.ComplexDoubleStorage">[docs]</a><span class="k">class</span> <span class="nc">ComplexDoubleStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="ComplexDoubleStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.ComplexDoubleStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span></div>

<div class="viewcode-block" id="ComplexFloatStorage"><a class="viewcode-back" href="../storage.html#torch.ComplexFloatStorage">[docs]</a><span class="k">class</span> <span class="nc">ComplexFloatStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="ComplexFloatStorage.dtype"><a class="viewcode-back" href="../storage.html#torch.ComplexFloatStorage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span></div>

<div class="viewcode-block" id="QUInt8Storage"><a class="viewcode-back" href="../storage.html#torch.QUInt8Storage">[docs]</a><span class="k">class</span> <span class="nc">QUInt8Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="QUInt8Storage.dtype"><a class="viewcode-back" href="../storage.html#torch.QUInt8Storage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span></div>

<div class="viewcode-block" id="QInt8Storage"><a class="viewcode-back" href="../storage.html#torch.QInt8Storage">[docs]</a><span class="k">class</span> <span class="nc">QInt8Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="QInt8Storage.dtype"><a class="viewcode-back" href="../storage.html#torch.QInt8Storage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span></div>

<div class="viewcode-block" id="QInt32Storage"><a class="viewcode-back" href="../storage.html#torch.QInt32Storage">[docs]</a><span class="k">class</span> <span class="nc">QInt32Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="QInt32Storage.dtype"><a class="viewcode-back" href="../storage.html#torch.QInt32Storage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span></div>

<div class="viewcode-block" id="QUInt4x2Storage"><a class="viewcode-back" href="../storage.html#torch.QUInt4x2Storage">[docs]</a><span class="k">class</span> <span class="nc">QUInt4x2Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="QUInt4x2Storage.dtype"><a class="viewcode-back" href="../storage.html#torch.QUInt4x2Storage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span></div>

<div class="viewcode-block" id="QUInt2x4Storage"><a class="viewcode-back" href="../storage.html#torch.QUInt2x4Storage">[docs]</a><span class="k">class</span> <span class="nc">QUInt2x4Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
<div class="viewcode-block" id="QUInt2x4Storage.dtype"><a class="viewcode-back" href="../storage.html#torch.QUInt2x4Storage.dtype">[docs]</a>    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span></div>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span></div>

<span class="n">_storage_classes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">UntypedStorage</span><span class="p">,</span> <span class="n">DoubleStorage</span><span class="p">,</span> <span class="n">FloatStorage</span><span class="p">,</span> <span class="n">LongStorage</span><span class="p">,</span> <span class="n">IntStorage</span><span class="p">,</span>
    <span class="n">ShortStorage</span><span class="p">,</span> <span class="n">CharStorage</span><span class="p">,</span> <span class="n">ByteStorage</span><span class="p">,</span> <span class="n">HalfStorage</span><span class="p">,</span> <span class="n">BoolStorage</span><span class="p">,</span>
    <span class="n">QUInt8Storage</span><span class="p">,</span> <span class="n">QInt8Storage</span><span class="p">,</span> <span class="n">QInt32Storage</span><span class="p">,</span> <span class="n">BFloat16Storage</span><span class="p">,</span>
    <span class="n">ComplexFloatStorage</span><span class="p">,</span> <span class="n">ComplexDoubleStorage</span><span class="p">,</span> <span class="n">QUInt4x2Storage</span><span class="p">,</span> <span class="n">QUInt2x4Storage</span><span class="p">,</span>
    <span class="n">TypedStorage</span>
<span class="p">}</span>

<span class="c1"># The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()</span>
<span class="n">_tensor_classes</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Type</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="c1"># If you edit these imports, please update torch/__init__.py.in as well</span>
<span class="kn">from</span> <span class="nn">.random</span> <span class="kn">import</span> <span class="n">set_rng_state</span><span class="p">,</span> <span class="n">get_rng_state</span><span class="p">,</span> <span class="n">manual_seed</span><span class="p">,</span> <span class="n">initial_seed</span><span class="p">,</span> <span class="n">seed</span>
<span class="kn">from</span> <span class="nn">.serialization</span> <span class="kn">import</span> <span class="n">save</span><span class="p">,</span> <span class="n">load</span>
<span class="kn">from</span> <span class="nn">._tensor_str</span> <span class="kn">import</span> <span class="n">set_printoptions</span>

<span class="c1">################################################################################</span>
<span class="c1"># Initialize extension</span>
<span class="c1">################################################################################</span>

<span class="k">def</span> <span class="nf">manager_path</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">_running_with_deploy</span><span class="p">()</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">get_file_path</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">,</span> <span class="s1">&#39;torch_shm_manager&#39;</span><span class="p">)</span>
    <span class="n">prepare_multiprocessing_environment</span><span class="p">(</span><span class="n">get_file_path</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Unable to find torch_shm_manager at &quot;</span> <span class="o">+</span> <span class="n">path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">path</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch.amp</span> <span class="kn">import</span> <span class="n">autocast</span>

<span class="c1"># Initializing the extension shadows the built-in python float / int classes;</span>
<span class="c1"># store them for later use by SymInt / SymFloat.</span>
<span class="n">py_float</span> <span class="o">=</span> <span class="nb">float</span>
<span class="n">py_int</span> <span class="o">=</span> <span class="nb">int</span>

<span class="c1"># Shared memory manager needs to know the exact location of manager executable</span>
<span class="n">_C</span><span class="o">.</span><span class="n">_initExtension</span><span class="p">(</span><span class="n">manager_path</span><span class="p">())</span>
<span class="k">del</span> <span class="n">manager_path</span>

<span class="c1"># Appease the type checker: it can&#39;t deal with direct setting of globals().</span>
<span class="c1"># Note that we will see &quot;too many&quot; functions when reexporting this way; there</span>
<span class="c1"># is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions</span>
<span class="c1"># so that this import is good enough</span>
<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="c1"># Some type signatures pulled in from _VariableFunctions here clash with</span>
    <span class="c1"># signatures already imported. For now these clashes are ignored; see</span>
    <span class="c1"># PR #43339 for details.</span>
    <span class="kn">from</span> <span class="nn">torch._C._VariableFunctions</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># type: ignore[misc] # noqa: F403</span>
    <span class="c1"># Fixup segment_reduce visibility</span>
    <span class="n">_segment_reduce</span> <span class="o">=</span> <span class="n">segment_reduce</span>
    <span class="k">del</span> <span class="n">segment_reduce</span>

<span class="c1"># Ops not to be exposed in `torch` namespace,</span>
<span class="c1"># mostly helper ops.</span>
<span class="n">PRIVATE_OPS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s1">&#39;unique_dim&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">PRIVATE_OPS</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s1">&#39;torch&#39;</span>
    <span class="c1"># Hide some APIs that should not be public</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;segment_reduce&quot;</span><span class="p">:</span>
        <span class="c1"># TODO: Once the undocumented FC window is passed, remove the line bellow</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">name</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">):</span>
        <span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>



<span class="c1">################################################################################</span>
<span class="c1"># Import TorchDynamo&#39;s lazy APIs to avoid circular dependenices</span>
<span class="c1">################################################################################</span>

<span class="c1"># needs to be before from .functional import * to avoid circular dependencies</span>
<span class="kn">from</span> <span class="nn">._compile</span> <span class="kn">import</span> <span class="n">_disable_dynamo</span>

<span class="c1">################################################################################</span>
<span class="c1"># Import interface functions defined in Python</span>
<span class="c1">################################################################################</span>

<span class="c1"># needs to be after the above ATen bindings so we can overwrite from Python side</span>
<span class="kn">from</span> <span class="nn">.functional</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>


<span class="c1">################################################################################</span>
<span class="c1"># Remove unnecessary members</span>
<span class="c1">################################################################################</span>

<span class="k">del</span> <span class="n">_StorageBase</span>
<span class="k">del</span> <span class="n">_LegacyStorage</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define _assert</span>
<span class="c1">################################################################################</span>

<span class="c1"># needs to be before the submodule imports to avoid circular dependencies</span>
<div class="viewcode-block" id="_assert"><a class="viewcode-back" href="../generated/torch._assert.html#torch._assert">[docs]</a><span class="k">def</span> <span class="nf">_assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A wrapper around Python&#39;s assert which is symbolically traceable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">condition</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="n">condition</span><span class="p">,)):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">_assert</span><span class="p">,</span> <span class="p">(</span><span class="n">condition</span><span class="p">,),</span> <span class="n">condition</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">condition</span><span class="p">,</span> <span class="n">message</span></div>

<span class="c1">################################################################################</span>
<span class="c1"># Import most common subpackages</span>
<span class="c1">################################################################################</span>

<span class="c1"># Use the redundant form so that type checkers know that these are a part of</span>
<span class="c1"># the public API. The &quot;regular&quot; import lines are there solely for the runtime</span>
<span class="c1"># side effect of adding to the imported module&#39;s members for other users.</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">cuda</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">cpu</span> <span class="k">as</span> <span class="n">cpu</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">mps</span> <span class="k">as</span> <span class="n">mps</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">autograd</span> <span class="k">as</span> <span class="n">autograd</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">no_grad</span> <span class="k">as</span> <span class="n">no_grad</span><span class="p">,</span>
    <span class="n">enable_grad</span> <span class="k">as</span> <span class="n">enable_grad</span><span class="p">,</span>
    <span class="n">set_grad_enabled</span> <span class="k">as</span> <span class="n">set_grad_enabled</span><span class="p">,</span>
    <span class="n">inference_mode</span> <span class="k">as</span> <span class="n">inference_mode</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">fft</span> <span class="k">as</span> <span class="n">fft</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">futures</span> <span class="k">as</span> <span class="n">futures</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">_awaits</span> <span class="k">as</span> <span class="n">_awaits</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nested</span> <span class="k">as</span> <span class="n">nested</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.signal</span> <span class="kn">import</span> <span class="n">windows</span> <span class="k">as</span> <span class="n">windows</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.optim._multi_tensor</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">multiprocessing</span> <span class="k">as</span> <span class="n">multiprocessing</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">sparse</span> <span class="k">as</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">special</span> <span class="k">as</span> <span class="n">special</span>
<span class="kn">import</span> <span class="nn">torch.utils.backcompat</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">jit</span> <span class="k">as</span> <span class="n">jit</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">linalg</span> <span class="k">as</span> <span class="n">linalg</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">hub</span> <span class="k">as</span> <span class="n">hub</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">random</span> <span class="k">as</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">distributions</span> <span class="k">as</span> <span class="n">distributions</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">testing</span> <span class="k">as</span> <span class="n">testing</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">backends</span> <span class="k">as</span> <span class="n">backends</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">__config__</span> <span class="k">as</span> <span class="n">__config__</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">__future__</span> <span class="k">as</span> <span class="n">__future__</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">profiler</span> <span class="k">as</span> <span class="n">profiler</span>

<span class="c1"># Quantized, sparse, AO, etc. should be last to get imported, as nothing</span>
<span class="c1"># is expected to depend on them.</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">ao</span> <span class="k">as</span> <span class="n">ao</span>
<span class="c1"># nn.quant* depends on ao -- so should be after those.</span>
<span class="kn">import</span> <span class="nn">torch.nn.quantizable</span>
<span class="kn">import</span> <span class="nn">torch.nn.quantized</span>
<span class="kn">import</span> <span class="nn">torch.nn.qat</span>
<span class="kn">import</span> <span class="nn">torch.nn.intrinsic</span>

<span class="n">_C</span><span class="o">.</span><span class="n">_init_names</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="p">))</span>

<span class="c1"># attach docstrings to torch and tensor functions</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_torch_docs</span><span class="p">,</span> <span class="n">_tensor_docs</span><span class="p">,</span> <span class="n">_storage_docs</span>
<span class="k">del</span> <span class="n">_torch_docs</span><span class="p">,</span> <span class="n">_tensor_docs</span><span class="p">,</span> <span class="n">_storage_docs</span>


<div class="viewcode-block" id="compiled_with_cxx11_abi"><a class="viewcode-back" href="../generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi">[docs]</a><span class="k">def</span> <span class="nf">compiled_with_cxx11_abi</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_GLIBCXX_USE_CXX11_ABI</span></div>


<span class="c1"># Import the ops &quot;namespace&quot;</span>
<span class="kn">from</span> <span class="nn">torch._ops</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">torch._classes</span> <span class="kn">import</span> <span class="n">classes</span>

<span class="c1"># quantization depends on torch.fx</span>
<span class="c1"># Import quantization</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">quantization</span> <span class="k">as</span> <span class="n">quantization</span>

<span class="c1"># Import the quasi random sampler</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">quasirandom</span> <span class="k">as</span> <span class="n">quasirandom</span>

<span class="c1"># If you are seeing this, it means that this call site was not checked if</span>
<span class="c1"># the memory format could be preserved, and it was switched to old default</span>
<span class="c1"># behaviour of contiguous</span>
<span class="n">legacy_contiguous_format</span> <span class="o">=</span> <span class="n">contiguous_format</span>

<span class="c1"># Register fork handler to initialize OpenMP in child processes (see gh-28389)</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing._atfork</span> <span class="kn">import</span> <span class="n">register_after_fork</span>
<span class="n">register_after_fork</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">get_num_threads</span><span class="p">)</span>
<span class="k">del</span> <span class="n">register_after_fork</span>

<span class="c1"># Import tools that require fully imported torch (for applying</span>
<span class="c1"># torch.jit.script as a decorator, for instance):</span>
<span class="kn">from</span> <span class="nn">._lobpcg</span> <span class="kn">import</span> <span class="n">lobpcg</span> <span class="k">as</span> <span class="n">lobpcg</span>

<span class="c1"># These were previously defined in native_functions.yaml and appeared on the</span>
<span class="c1"># `torch` namespace, but we moved them to c10 dispatch to facilitate custom</span>
<span class="c1"># class usage. We add these lines here to preserve backward compatibility.</span>
<span class="n">quantized_lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">quantized_lstm</span>
<span class="n">quantized_gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">quantized_gru</span>

<span class="kn">from</span> <span class="nn">torch.utils.dlpack</span> <span class="kn">import</span> <span class="n">from_dlpack</span><span class="p">,</span> <span class="n">to_dlpack</span>

<span class="c1"># Import experimental masked operations support. See</span>
<span class="c1"># [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more</span>
<span class="c1"># information.</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">masked</span>

<span class="c1"># Import removed ops with error message about removal</span>
<span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="n">matrix_rank</span><span class="p">,</span>
    <span class="n">eig</span><span class="p">,</span>
    <span class="n">solve</span><span class="p">,</span>
    <span class="n">lstsq</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="n">_symeig</span> <span class="k">as</span> <span class="n">symeig</span>  <span class="c1"># type: ignore[misc]</span>

<span class="k">class</span> <span class="nc">_TorchCompileInductorWrapper</span><span class="p">:</span>
    <span class="n">compiler_name</span> <span class="o">=</span> <span class="s2">&quot;inductor&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">=</span> <span class="n">dynamic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>

        <span class="c1"># FIXME: CUPTI Lazy Re-init and CUDA Graph crashes with CUDA 11.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;triton.cudagraphs&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DISABLE_CUPTI_LAZY_REINIT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">_TorchCompileInductorWrapper</span><span class="p">)</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">config</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">dynamic</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;reduce-overhead&quot;</span><span class="p">,</span> <span class="s2">&quot;max-autotune&quot;</span><span class="p">,</span> <span class="s2">&quot;max-autotune-no-cudagraphs&quot;</span><span class="p">):</span>
            <span class="kn">from</span> <span class="nn">torch._inductor</span> <span class="kn">import</span> <span class="n">list_mode_options</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">apply_options</span><span class="p">(</span><span class="n">list_mode_options</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unrecognized mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">, should be one of: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">options</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="kn">from</span> <span class="nn">torch._inductor</span> <span class="kn">import</span> <span class="n">config</span>
        <span class="n">current_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">options</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">attr_name</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attr_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">current_config</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unexpected optimization option </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">, known options are </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">current_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="n">current_config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">]):</span>
                <span class="n">val_type_str</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="n">expected_type_str</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">current_config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">])</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unexpected type of attr </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">val_type_str</span><span class="si">}</span><span class="s2"> should be </span><span class="si">{</span><span class="n">expected_type_str</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._inductor.compile_fx</span> <span class="kn">import</span> <span class="n">compile_fx</span>

        <span class="k">return</span> <span class="n">compile_fx</span><span class="p">(</span><span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">,</span> <span class="n">config_patches</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._inductor</span> <span class="kn">import</span> <span class="n">config</span>
        <span class="k">if</span> <span class="s2">&quot;triton.cudagraphs&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">triton</span><span class="o">.</span><span class="n">cudagraphs</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;triton.cudagraphs&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
                <span class="kn">from</span> <span class="nn">torch._inductor.cudagraph_trees</span> <span class="kn">import</span> <span class="n">reset_cudagraph_trees</span>
                <span class="n">reset_cudagraph_trees</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">_TorchCompileWrapper</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._dynamo.backends.registry</span> <span class="kn">import</span> <span class="n">lookup_backend</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compiler_name</span> <span class="o">=</span> <span class="n">backend</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="s2">&quot;__name__&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compiler_name</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compiler_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">=</span> <span class="n">dynamic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">lookup_backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># only pass the args if they non-empty</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="k">if</span> <span class="n">options</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;options&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">options</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">_TorchCompileWrapper</span><span class="p">)</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">compiler_fn</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">kwargs</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">dynamic</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span><span class="p">(</span><span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>


<div class="viewcode-block" id="compile"><a class="viewcode-back" href="../compile/generated/torch.compile.html#torch.compile">[docs]</a><span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
            <span class="n">fullgraph</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">dynamic</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">backend</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;inductor&quot;</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">disable</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizes given model/function using TorchDynamo and specified backend.</span>

<span class="sd">    Concretely, for every frame executed within the compiled region, we will attempt</span>
<span class="sd">    to compile it and cache the compiled result on the code object for future</span>
<span class="sd">    use.  A single frame may be compiled multiple times if previous compiled</span>
<span class="sd">    results are not applicable for subsequent calls (this is called a &quot;guard</span>
<span class="sd">    failure), you can use TORCH_LOGS=guards to debug these situations.</span>
<span class="sd">    Multiple compiled results can be associated with a frame up to</span>
<span class="sd">    ``torch._dynamo.config.cache_size_limit``, which defaults to 64; at which</span>
<span class="sd">    point we will fall back to eager.  Note that compile caches are per</span>
<span class="sd">    *code object*, not frame; if you dynamically create multiple copies of a</span>
<span class="sd">    function, they will all share the same code cache.</span>

<span class="sd">    Args:</span>
<span class="sd">       model (Callable): Module/function to optimize</span>
<span class="sd">       fullgraph (bool): Whether it is ok to break model into several subgraphs</span>
<span class="sd">       dynamic (bool): Use dynamic shape tracing.  When this is True, we will up-front attempt</span>
<span class="sd">        to generate a kernel that is as dynamic as possible to avoid recompilations when</span>
<span class="sd">        sizes change.  This may not always work as some operations/optimizations will</span>
<span class="sd">        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.</span>
<span class="sd">        In particular, if you use &quot;reduce-overhead&quot;, this will force sizes to be static</span>
<span class="sd">        even with dynamic=True.</span>
<span class="sd">       backend (str or Callable): backend to be used</span>
<span class="sd">        - &quot;inductor&quot; is the default backend, which is a good balance between performance and overhead</span>
<span class="sd">        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`</span>
<span class="sd">        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`</span>
<span class="sd">        - To register an out-of-tree custom backend: https://pytorch.org/docs/master/dynamo/custom-backends.html</span>
<span class="sd">       mode (str): Can be either &quot;default&quot;, &quot;reduce-overhead&quot; or &quot;max-autotune&quot;</span>
<span class="sd">        - &quot;default&quot; is the default mode, which is a good balance between performance and overhead</span>

<span class="sd">        - &quot;reduce-overhead&quot; is a mode that reduces the overhead of python with CUDA graphs,</span>
<span class="sd">          useful for small batches.  Reduction of overhead can come at the cost of more memory</span>
<span class="sd">          usage, as we will cache the workspace memory required for the invocation so that we</span>
<span class="sd">          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed</span>
<span class="sd">          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.</span>
<span class="sd">          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints</span>
<span class="sd">          to debug.</span>

<span class="sd">        - &quot;max-autotune&quot; is a mode that that leverages Triton based matrix multiplications and convolutions</span>

<span class="sd">        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`</span>

<span class="sd">       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are</span>
<span class="sd">        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set</span>
<span class="sd">        - `max_autotune` which will profile to pick the best matmul configuration</span>
<span class="sd">        - `fallback_random` which is useful when debugging accuracy issues</span>
<span class="sd">        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores</span>
<span class="sd">        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs</span>
<span class="sd">        - `trace.enabled` which is the most useful debugging flag to turn on</span>
<span class="sd">        - `trace.graph_diagram` which will show you a picture of your graph after fusion</span>
<span class="sd">        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`</span>
<span class="sd">       disable (bool): Turn torch.compile() into a no-op for testing</span>

<span class="sd">    Example::</span>

<span class="sd">        @torch.compile(options={&quot;triton.cudagraphs&quot;: True}, fullgraph=True)</span>
<span class="sd">        def foo(x):</span>
<span class="sd">            return torch.sin(x) + torch.cos(x)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.compile&quot;</span><span class="p">)</span>
    <span class="c1"># Decorator mode</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model can&#39;t be None&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                           <span class="n">fullgraph</span><span class="o">=</span><span class="n">fullgraph</span><span class="p">,</span>
                           <span class="n">dynamic</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span>
                           <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
                           <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
                           <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
                           <span class="n">disable</span><span class="o">=</span><span class="n">disable</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fn</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">options</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Either mode or options can be specified, but both can&#39;t be specified at the same time.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">options</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;inductor&quot;</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">_TorchCompileInductorWrapper</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">_TorchCompileWrapper</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">nopython</span><span class="o">=</span><span class="n">fullgraph</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="n">disable</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_register_device_module</span><span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register an external runtime module of the specific :attr:`device_type`</span>
<span class="sd">    supported by torch.</span>

<span class="sd">    After the :attr:`module` is registered correctly, the user can refer</span>
<span class="sd">    the external runtime module as part of torch with attribute torch.xxx.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make sure the device_type represent a supported device type for torch.</span>
    <span class="n">device_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span><span class="o">.</span><span class="n">type</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">device_type</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The runtime module of &#39;</span><span class="si">{}</span><span class="s2">&#39; has already &quot;</span>
                           <span class="s2">&quot;been registered with &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">device_type</span><span class="p">)))</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">device_type</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="n">torch_module_name</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">device_type</span><span class="p">])</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">torch_module_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>

<span class="c1"># expose return_types</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">return_types</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">library</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_meta_registrations</span>

<span class="c1"># Enable CUDA Sanitizer</span>
<span class="k">if</span> <span class="s1">&#39;TORCH_CUDA_SANITIZER&#39;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch.cuda._sanitizer</span> <span class="k">as</span> <span class="nn">csan</span>

    <span class="n">csan</span><span class="o">.</span><span class="n">enable_cuda_sanitizer</span><span class="p">()</span>

<span class="c1"># Populate magic methods on SymInt and SymFloat</span>
<span class="kn">import</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">func</span> <span class="k">as</span> <span class="n">func</span>
<span class="kn">from</span> <span class="nn">torch.func</span> <span class="kn">import</span> <span class="n">vmap</span>


<span class="c1"># ONNX must be imported after _dynamo, _ops, _subclasses, fx, func and jit</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">onnx</span> <span class="k">as</span> <span class="n">onnx</span>  <span class="c1"># ONNX depends on a bunch of Dynamo stuff</span>


<span class="c1"># The function _sparse_coo_tensor_unsafe is removed from PyTorch</span>
<span class="c1"># Python API (v. 1.13), here we temporarily provide its replacement</span>
<span class="c1"># with a deprecation warning.</span>
<span class="c1"># TODO: remove the function for PyTorch v 1.15.</span>
<span class="k">def</span> <span class="nf">_sparse_coo_tensor_unsafe</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;torch._sparse_coo_tensor_unsafe is deprecated, &#39;</span>
                  <span class="s1">&#39;use torch.sparse_coo_tensor(..., check_invariants=False) instead.&#39;</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;check_invariants&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1"># Register MPS specific decomps</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">_running_with_deploy</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">compiler</span> <span class="k">as</span> <span class="n">compiler</span>

    <span class="k">class</span> <span class="nc">_TritonLibrary</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
        <span class="n">lib</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">Library</span><span class="p">(</span><span class="s2">&quot;triton&quot;</span><span class="p">,</span> <span class="s2">&quot;DEF&quot;</span><span class="p">)</span>
        <span class="n">ops_table</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="nd">@classmethod</span>
        <span class="k">def</span> <span class="nf">registerOp</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">op_key</span><span class="p">,</span> <span class="n">full_schema</span><span class="p">,</span> <span class="n">op_impl</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">op_key</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">ops_table</span><span class="p">:</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">full_schema</span><span class="p">)</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="s2">&quot;triton::&quot;</span> <span class="o">+</span> <span class="n">op_key</span><span class="p">,</span> <span class="n">op_impl</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">ops_table</span><span class="p">[(</span><span class="n">op_key</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)]</span> <span class="o">=</span> <span class="n">op_impl</span>

            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">ops_table</span><span class="p">[(</span><span class="n">op_key</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)]</span>


<span class="c1"># Deprecated attributes</span>
<span class="n">_deprecated_attrs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;has_mps&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_built</span><span class="p">,</span>
    <span class="s2">&quot;has_cuda&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_built</span><span class="p">,</span>
    <span class="s2">&quot;has_cudnn&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">is_available</span><span class="p">,</span>
    <span class="s2">&quot;has_mkldnn&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mkldnn</span><span class="o">.</span><span class="n">is_available</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_lazy_modules</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;_dynamo&quot;</span><span class="p">,</span>
    <span class="s2">&quot;_inductor&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="c1"># Deprecated attrs</span>
    <span class="n">replacement</span> <span class="o">=</span> <span class="n">_deprecated_attrs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">replacement</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">warnings</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; is deprecated, please use &#39;</span><span class="si">{</span><span class="n">replacement</span><span class="o">.</span><span class="vm">__module__</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">replacement</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">()&#39;&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">replacement</span><span class="p">()</span>

    <span class="c1"># Lazy modules</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">_lazy_modules</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">importlib</span>
        <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;.</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="vm">__name__</span><span class="p">)</span>

    <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;module &#39;</span><span class="si">{</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&#39; has no attribute &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_logging</span>
<span class="n">_logging</span><span class="o">.</span><span class="n">_init_logs</span><span class="p">()</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
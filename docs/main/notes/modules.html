


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Modules &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/notes/modules.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="MPS backend" href="mps.html" />
    <link rel="prev" title="Features for large-scale deployments" href="large_scale_deployments.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/notes/modules.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Modules</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/notes/modules.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="modules">
<span id="id1"></span><h1>Modules<a class="headerlink" href="#modules" title="Permalink to this heading">¶</a></h1>
<p>PyTorch uses modules to represent neural networks. Modules are:</p>
<ul class="simple">
<li><p><strong>Building blocks of stateful computation.</strong>
PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for
easy construction of elaborate, multi-layer neural networks.</p></li>
<li><p><strong>Tightly integrated with PyTorch’s</strong>
<a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">autograd</a>
<strong>system.</strong> Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.</p></li>
<li><p><strong>Easy to work with and transform.</strong> Modules are straightforward to save and restore, transfer between
CPU / GPU / TPU devices, prune, quantize, and more.</p></li>
</ul>
<p>This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,
many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents
are provided here as well.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#a-simple-custom-module" id="id4">A Simple Custom Module</a></p></li>
<li><p><a class="reference internal" href="#modules-as-building-blocks" id="id5">Modules as Building Blocks</a></p></li>
<li><p><a class="reference internal" href="#neural-network-training-with-modules" id="id6">Neural Network Training with Modules</a></p></li>
<li><p><a class="reference internal" href="#module-state" id="id7">Module State</a></p></li>
<li><p><a class="reference internal" href="#module-initialization" id="id8">Module Initialization</a></p></li>
<li><p><a class="reference internal" href="#module-hooks" id="id9">Module Hooks</a></p></li>
<li><p><a class="reference internal" href="#advanced-features" id="id10">Advanced Features</a></p>
<ul>
<li><p><a class="reference internal" href="#distributed-training" id="id11">Distributed Training</a></p></li>
<li><p><a class="reference internal" href="#profiling-performance" id="id12">Profiling Performance</a></p></li>
<li><p><a class="reference internal" href="#improving-performance-with-quantization" id="id13">Improving Performance with Quantization</a></p></li>
<li><p><a class="reference internal" href="#improving-memory-usage-with-pruning" id="id14">Improving Memory Usage with Pruning</a></p></li>
<li><p><a class="reference internal" href="#parametrizations" id="id15">Parametrizations</a></p></li>
<li><p><a class="reference internal" href="#transforming-modules-with-fx" id="id16">Transforming Modules with FX</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="a-simple-custom-module">
<h2><a class="toc-backref" href="#id4">A Simple Custom Module</a><a class="headerlink" href="#a-simple-custom-module" title="Permalink to this heading">¶</a></h2>
<p>To get started, let’s look at a simpler, custom version of PyTorch’s <a class="reference internal" href="../generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code></a> module.
This module applies an affine transformation to its input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MyLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">input</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</pre></div>
</div>
<p>This simple module has the following fundamental characteristics of modules:</p>
<ul class="simple">
<li><p><strong>It inherits from the base Module class.</strong>
All modules should subclass <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> for composability with other modules.</p></li>
<li><p><strong>It defines some “state” that is used in computation.</strong>
Here, the state consists of randomly-initialized <code class="docutils literal notranslate"><span class="pre">weight</span></code> and <code class="docutils literal notranslate"><span class="pre">bias</span></code> tensors that define the affine
transformation. Because each of these is defined as a <a class="reference internal" href="../generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>, they are
<em>registered</em> for the module and will automatically be tracked and returned from calls
to <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">parameters()</span></code></a>. Parameters can be
considered the “learnable” aspects of the module’s computation (more on this later). Note that modules
are not required to have state, and can also be stateless.</p></li>
<li><p><strong>It defines a forward() function that performs the computation.</strong> For this affine transformation module, the input
is matrix-multiplied with the <code class="docutils literal notranslate"><span class="pre">weight</span></code> parameter (using the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> short-hand notation) and added to the <code class="docutils literal notranslate"><span class="pre">bias</span></code>
parameter to produce the output. More generally, the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> implementation for a module can perform arbitrary
computation involving any number of inputs and outputs.</p></li>
</ul>
<p>This simple module demonstrates how modules package state and computation together. Instances of this module can be
constructed and called:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">m</span><span class="p">(</span><span class="n">sample_input</span><span class="p">)</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.3037</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0413</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2057</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the module itself is callable, and that calling it invokes its <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function.
This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.
The “forward pass” is responsible for applying the computation represented by the module
to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of
module outputs with respect to its inputs, which can be used for “training” parameters through gradient
descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it
is not required to manually implement a <code class="docutils literal notranslate"><span class="pre">backward()</span></code> function for each module. The process of training
module parameters through successive forward / backward passes is covered in detail in
<a class="reference internal" href="#neural-network-training-with-modules"><span class="std std-ref">Neural Network Training with Modules</span></a>.</p>
<p>The full set of parameters registered by the module can be iterated through via a call to
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">parameters()</span></code></a> or <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_parameters" title="torch.nn.Module.named_parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_parameters()</span></code></a>,
where the latter includes each parameter’s name:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>
<span class="p">:</span> <span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.0597</span><span class="p">,</span>  <span class="mf">1.1796</span><span class="p">,</span>  <span class="mf">0.8247</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.5080</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2635</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1045</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0593</span><span class="p">,</span>  <span class="mf">0.2469</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4299</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5457</span><span class="p">,</span>  <span class="mf">0.4793</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.3634</span><span class="p">,</span>  <span class="mf">0.2015</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8525</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<p>In general, the parameters registered by a module are aspects of the module’s computation that should be
“learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers.
Before we get to that, however, let’s first examine how modules can be composed with one another.</p>
</div>
<div class="section" id="modules-as-building-blocks">
<h2><a class="toc-backref" href="#id5">Modules as Building Blocks</a><a class="headerlink" href="#modules-as-building-blocks" title="Permalink to this heading">¶</a></h2>
<p>Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.
The simplest way to do this is using the <a class="reference internal" href="../generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a> module. It allows us to chain together
multiple modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
  <span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
  <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
  <span class="n">MyLinear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">net</span><span class="p">(</span><span class="n">sample_input</span><span class="p">)</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.6749</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <a class="reference internal" href="../generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a> automatically feeds the output of the first <code class="docutils literal notranslate"><span class="pre">MyLinear</span></code> module as input
into the <a class="reference internal" href="../generated/torch.nn.ReLU.html#torch.nn.ReLU" title="torch.nn.ReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code></a>, and the output of that as input into the second <code class="docutils literal notranslate"><span class="pre">MyLinear</span></code> module. As
shown, it is limited to in-order chaining of modules with a single input and output.</p>
<p>In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives
full flexibility on how submodules are used for a module’s computation.</p>
<p>For example, here’s a simple neural network implemented as a custom module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l0</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>This module is composed of two “children” or “submodules” (<code class="docutils literal notranslate"><span class="pre">l0</span></code> and <code class="docutils literal notranslate"><span class="pre">l1</span></code>) that define the layers of
the neural network and are utilized for computation within the module’s <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method. Immediate
children of a module can be iterated through via a call to <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.children" title="torch.nn.Module.children"><code class="xref py py-func docutils literal notranslate"><span class="pre">children()</span></code></a> or
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_children" title="torch.nn.Module.named_children"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_children()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
<span class="p">:</span> <span class="p">(</span><span class="s1">&#39;l0&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
<span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
</pre></div>
</div>
<p>To go deeper than just the immediate children, <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.modules" title="torch.nn.Module.modules"><code class="xref py py-func docutils literal notranslate"><span class="pre">modules()</span></code></a> and
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_modules" title="torch.nn.Module.named_modules"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_modules()</span></code></a> <em>recursively</em> iterate through a module and its child modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BigNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">big_net</span> <span class="o">=</span> <span class="n">BigNet</span><span class="p">()</span>
<span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">big_net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
<span class="p">:</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">BigNet</span><span class="p">(</span>
  <span class="p">(</span><span class="n">l1</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
  <span class="p">(</span><span class="n">net</span><span class="p">):</span> <span class="n">Net</span><span class="p">(</span>
    <span class="p">(</span><span class="n">l0</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
    <span class="p">(</span><span class="n">l1</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
  <span class="p">)</span>
<span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
<span class="p">(</span><span class="s1">&#39;net&#39;</span><span class="p">,</span> <span class="n">Net</span><span class="p">(</span>
  <span class="p">(</span><span class="n">l0</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
  <span class="p">(</span><span class="n">l1</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
<span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;net.l0&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
<span class="p">(</span><span class="s1">&#39;net.l1&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
</pre></div>
</div>
<p>Sometimes, it’s necessary for a module to dynamically define submodules.
The <a class="reference internal" href="../generated/torch.nn.ModuleList.html#torch.nn.ModuleList" title="torch.nn.ModuleList"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleList</span></code></a> and <a class="reference internal" href="../generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDict</span></code></a> modules are useful here; they
register submodules from a list or dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DynamicNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
      <span class="p">[</span><span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activations</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span>
      <span class="s1">&#39;relu&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
      <span class="s1">&#39;lrelu&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">()</span>
    <span class="p">})</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">final</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">act</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">linear</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activations</span><span class="p">[</span><span class="n">act</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">dynamic_net</span> <span class="o">=</span> <span class="n">DynamicNet</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">dynamic_net</span><span class="p">(</span><span class="n">sample_input</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.
This means that calls to <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">parameters()</span></code></a> and <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_parameters" title="torch.nn.Module.named_parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_parameters()</span></code></a> will
recursively include child parameters, allowing for convenient optimization of all parameters within the network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">dynamic_net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>
<span class="p">:</span> <span class="p">(</span><span class="s1">&#39;linears.0.weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.2051</span><span class="p">,</span>  <span class="mf">0.7601</span><span class="p">,</span>  <span class="mf">1.1065</span><span class="p">,</span>  <span class="mf">0.1963</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">3.0592</span><span class="p">,</span>  <span class="mf">0.4354</span><span class="p">,</span>  <span class="mf">1.6598</span><span class="p">,</span>  <span class="mf">0.9828</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4446</span><span class="p">,</span>  <span class="mf">0.4628</span><span class="p">,</span>  <span class="mf">0.8774</span><span class="p">,</span>  <span class="mf">1.6848</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1222</span><span class="p">,</span>  <span class="mf">1.5458</span><span class="p">,</span>  <span class="mf">1.1729</span><span class="p">,</span>  <span class="mf">1.4647</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.0.bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.5310</span><span class="p">,</span>  <span class="mf">1.0609</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0940</span><span class="p">,</span>  <span class="mf">1.1266</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.1.weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.1113</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0623</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0806</span><span class="p">,</span>  <span class="mf">0.3508</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0550</span><span class="p">,</span>  <span class="mf">1.5317</span><span class="p">,</span>  <span class="mf">1.1064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5562</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4028</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6942</span><span class="p">,</span>  <span class="mf">1.5793</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0140</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0329</span><span class="p">,</span>  <span class="mf">0.1160</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7183</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0434</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.1.bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.0361</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9768</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3889</span><span class="p">,</span>  <span class="mf">1.1613</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.2.weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6340</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3887</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9979</span><span class="p">,</span>  <span class="mf">0.0767</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3526</span><span class="p">,</span>  <span class="mf">0.8756</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5847</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6016</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3269</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1608</span><span class="p">,</span>  <span class="mf">0.2897</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0829</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.6338</span><span class="p">,</span>  <span class="mf">0.9239</span><span class="p">,</span>  <span class="mf">0.6943</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5034</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.2.bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0268</span><span class="p">,</span>  <span class="mf">0.4489</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9403</span><span class="p">,</span>  <span class="mf">0.1571</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;final.weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.2509</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5052</span><span class="p">],</span> <span class="p">[</span> <span class="mf">0.3088</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.4951</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;final.bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.3381</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<p>It’s also easy to move all parameters to a different device or change their precision using
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.to" title="torch.nn.Module.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Move all parameters to a CUDA device</span>
<span class="n">dynamic_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Change precision of all parameters</span>
<span class="n">dynamic_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">dynamic_net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">6.5166</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>More generally, an arbitrary function can be applied to a module and its submodules recursively by
using the <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.apply" title="torch.nn.Module.apply"><code class="xref py py-func docutils literal notranslate"><span class="pre">apply()</span></code></a> function. For example, to apply custom initialization to parameters
of a module and its submodules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a function to initialize Linear weights.</span>
<span class="c1"># Note that no_grad() is used here to avoid tracking this computation in the autograd graph.</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

<span class="c1"># Apply the function recursively on the module and its submodules.</span>
<span class="n">dynamic_net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</pre></div>
</div>
<p>These examples show how elaborate neural networks can be formed through module composition and conveniently
manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch
provides a large library of performant modules within the <a class="reference internal" href="../nn.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code></a> namespace that perform common neural
network operations like pooling, convolutions, loss functions, etc.</p>
<p>In the next section, we give a full example of training a neural network.</p>
<p>For more information, check out:</p>
<ul class="simple">
<li><p>Library of PyTorch-provided modules: <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">torch.nn</a></p></li>
<li><p>Defining neural net modules: <a class="reference external" href="https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html">https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html</a></p></li>
</ul>
</div>
<div class="section" id="neural-network-training-with-modules">
<span id="id2"></span><h2><a class="toc-backref" href="#id6">Neural Network Training with Modules</a><a class="headerlink" href="#neural-network-training-with-modules" title="Permalink to this heading">¶</a></h2>
<p>Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s
Optimizers from <a class="reference internal" href="../optim.html#module-torch.optim" title="torch.optim"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.optim</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the network (from previous section) and optimizer</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Run a sample training loop that &quot;teaches&quot; the network</span>
<span class="c1"># to output the constant zero function</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
  <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
  <span class="n">net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># After training, switch the module to eval mode to do inference, compute performance metrics, etc.</span>
<span class="c1"># (see discussion below for a description of training and evaluation modes)</span>
<span class="o">...</span>
<span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="o">...</span>
</pre></div>
</div>
<p>In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according
to its absolute value by employing <a class="reference internal" href="../generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a> as a loss function. While this is not a very interesting task, the
key parts of training are present:</p>
<ul class="simple">
<li><p>A network is created.</p></li>
<li><p>An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s
parameters are associated with it.</p></li>
<li><dl class="simple">
<dt>A training loop…</dt><dd><ul>
<li><p>acquires an input,</p></li>
<li><p>runs the network,</p></li>
<li><p>computes a loss,</p></li>
<li><p>zeros the network’s parameters’ gradients,</p></li>
<li><p>calls loss.backward() to update the parameters’ gradients,</p></li>
<li><p>calls optimizer.step() to apply the gradients to the parameters.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the
value of <code class="docutils literal notranslate"><span class="pre">l1</span></code>‘s <code class="docutils literal notranslate"><span class="pre">weight</span></code> parameter shows that its values are now much closer to 0 (as may be expected):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">l1</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="p">:</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0013</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0030</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0008</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the above process is done entirely while the network module is in “training mode”. Modules default to
training mode and can be switched between training and evaluation modes using <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.train" title="torch.nn.Module.train"><code class="xref py py-func docutils literal notranslate"><span class="pre">train()</span></code></a> and
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.eval" title="torch.nn.Module.eval"><code class="xref py py-func docutils literal notranslate"><span class="pre">eval()</span></code></a>. They can behave differently depending on which mode they are in. For example, the
<code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code> module maintains a running mean and variance during training that are not updated
when the module is in evaluation mode. In general, modules should be in training mode during training
and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module
that behaves differently between the two modes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ModalModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
      <span class="c1"># Add a constant only in training mode.</span>
      <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">1.</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">x</span>


<span class="n">m</span> <span class="o">=</span> <span class="n">ModalModule</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;training mode output: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.6614</span><span class="p">,</span> <span class="mf">1.2669</span><span class="p">,</span> <span class="mf">1.0617</span><span class="p">,</span> <span class="mf">1.6213</span><span class="p">,</span> <span class="mf">0.5481</span><span class="p">])</span>

<span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;evaluation mode output: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.6614</span><span class="p">,</span>  <span class="mf">0.2669</span><span class="p">,</span>  <span class="mf">0.0617</span><span class="p">,</span>  <span class="mf">0.6213</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4519</span><span class="p">])</span>
</pre></div>
</div>
<p>Training neural networks can often be tricky. For more information, check out:</p>
<ul class="simple">
<li><p>Using Optimizers: <a class="reference external" href="https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html">https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html</a>.</p></li>
<li><p>Neural network training: <a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</a></p></li>
<li><p>Introduction to autograd: <a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html</a></p></li>
</ul>
</div>
<div class="section" id="module-state">
<h2><a class="toc-backref" href="#id7">Module State</a><a class="headerlink" href="#module-state" title="Permalink to this heading">¶</a></h2>
<p>In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation.
Now, if we want to save the trained model to disk, we can do so by saving its <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> (i.e. “state dictionary”):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the module</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;net.pt&#39;</span><span class="p">)</span>

<span class="o">...</span>

<span class="c1"># Load the module later on</span>
<span class="n">new_net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">new_net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;net.pt&#39;</span><span class="p">))</span>
<span class="p">:</span> <span class="o">&lt;</span><span class="n">All</span> <span class="n">keys</span> <span class="n">matched</span> <span class="n">successfully</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>A module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> contains state that affects its computation. This includes, but is not limited to, the
module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module
computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent”
and “non-persistent”. Following is an overview of the various types of state a module can have:</p>
<ul class="simple">
<li><p><strong>Parameters</strong>: learnable aspects of computation; contained within the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code></p></li>
<li><p><strong>Buffers</strong>: non-learnable aspects of computation</p>
<ul>
<li><p><strong>Persistent</strong> buffers: contained within the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> (i.e. serialized when saving &amp; loading)</p></li>
<li><p><strong>Non-persistent</strong> buffers: not contained within the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> (i.e. left out of serialization)</p></li>
</ul>
</li>
</ul>
<p>As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want
the current value of the running mean to be considered part of the module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> so that it will be
restored when loading a serialized form of the module, but we don’t want it to be learnable.
This snippet shows how to use <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_buffer" title="torch.nn.Module.register_buffer"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_buffer()</span></code></a> to accomplish this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RunningMean</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</pre></div>
</div>
<p>Now, the current value of the running mean is considered part of the module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>
and will be properly restored when loading the module from disk:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">RunningMean</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
  <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
<span class="p">:</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.1041</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1113</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0647</span><span class="p">,</span>  <span class="mf">0.1515</span><span class="p">]))]))</span>

<span class="c1"># Serialized form will contain the &#39;mean&#39; tensor</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;mean.pt&#39;</span><span class="p">)</span>

<span class="n">m_loaded</span> <span class="o">=</span> <span class="n">RunningMean</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">m_loaded</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mean.pt&#39;</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">mean</span> <span class="o">==</span> <span class="n">m_loaded</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span>
</pre></div>
</div>
<p>As mentioned previously, buffers can be left out of the module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> by marking them as non-persistent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;unserialized_thing&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.to" title="torch.nn.Module.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Moves all module parameters and buffers to the specified device / dtype</span>
<span class="n">m</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
</div>
<p>Buffers of a module can be iterated over using <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.buffers" title="torch.nn.Module.buffers"><code class="xref py py-func docutils literal notranslate"><span class="pre">buffers()</span></code></a> or
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_buffers" title="torch.nn.Module.named_buffers"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_buffers()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
<p>The following class demonstrates the various ways of registering parameters and buffers within a module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">StatefulModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># Setting a nn.Parameter as an attribute of the module automatically registers the tensor</span>
    <span class="c1"># as a parameter of the module.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">param1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="c1"># Alternative string-based way to register a parameter.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;param2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>

    <span class="c1"># Reserves the &quot;param3&quot; attribute as a parameter, preventing it from being set to anything</span>
    <span class="c1"># except a parameter. &quot;None&quot; entries like this will not be present in the module&#39;s state_dict.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;param3&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># Registers a list of parameters.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">param_list</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>

    <span class="c1"># Registers a dictionary of parameters.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">({</span>
      <span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)),</span>
      <span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="p">})</span>

    <span class="c1"># Registers a persistent buffer (one that appears in the module&#39;s state_dict).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;buffer1&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Registers a non-persistent buffer (one that does not appear in the module&#39;s state_dict).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;buffer2&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Reserves the &quot;buffer3&quot; attribute as a buffer, preventing it from being set to anything</span>
    <span class="c1"># except a buffer. &quot;None&quot; entries like this will not be present in the module&#39;s state_dict.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;buffer3&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># Adding a submodule registers its parameters as parameters of the module.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">StatefulModule</span><span class="p">()</span>

<span class="c1"># Save and load state_dict.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;state.pt&#39;</span><span class="p">)</span>
<span class="n">m_loaded</span> <span class="o">=</span> <span class="n">StatefulModule</span><span class="p">()</span>
<span class="n">m_loaded</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;state.pt&#39;</span><span class="p">))</span>

<span class="c1"># Note that non-persistent buffer &quot;buffer2&quot; and reserved attributes &quot;param3&quot; and &quot;buffer3&quot; do</span>
<span class="c1"># not appear in the state_dict.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m_loaded</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
<span class="p">:</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;param1&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.0322</span><span class="p">,</span>  <span class="mf">0.9066</span><span class="p">])),</span>
               <span class="p">(</span><span class="s1">&#39;param2&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.4472</span><span class="p">,</span>  <span class="mf">0.1409</span><span class="p">,</span>  <span class="mf">0.4852</span><span class="p">])),</span>
               <span class="p">(</span><span class="s1">&#39;buffer1&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.6949</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1944</span><span class="p">,</span>  <span class="mf">1.2911</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1044</span><span class="p">])),</span>
               <span class="p">(</span><span class="s1">&#39;param_list.0&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.4202</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1953</span><span class="p">])),</span>
               <span class="p">(</span><span class="s1">&#39;param_list.1&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span> <span class="mf">1.5299</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8747</span><span class="p">])),</span>
               <span class="p">(</span><span class="s1">&#39;param_list.2&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.6289</span><span class="p">,</span>  <span class="mf">1.4898</span><span class="p">])),</span>
               <span class="p">(</span><span class="s1">&#39;param_dict.bar&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.6434</span><span class="p">,</span>  <span class="mf">1.5187</span><span class="p">,</span>  <span class="mf">0.0346</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4077</span><span class="p">])),</span>
               <span class="p">(</span><span class="s1">&#39;param_dict.foo&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.0845</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4324</span><span class="p">,</span>  <span class="mf">0.7022</span><span class="p">])),</span>
               <span class="p">(</span><span class="s1">&#39;linear.weight&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.3915</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6176</span><span class="p">],</span>
                                         <span class="p">[</span> <span class="mf">0.6062</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5992</span><span class="p">],</span>
                                         <span class="p">[</span> <span class="mf">0.4452</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2843</span><span class="p">]])),</span>
               <span class="p">(</span><span class="s1">&#39;linear.bias&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.3710</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0795</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3947</span><span class="p">]))])</span>
</pre></div>
</div>
<p>For more information, check out:</p>
<ul class="simple">
<li><p>Saving and loading: <a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">https://pytorch.org/tutorials/beginner/saving_loading_models.html</a></p></li>
<li><p>Serialization semantics: <a class="reference external" href="https://pytorch.org/docs/main/notes/serialization.html">https://pytorch.org/docs/main/notes/serialization.html</a></p></li>
<li><p>What is a state dict? <a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html">https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html</a></p></li>
</ul>
</div>
<div class="section" id="module-initialization">
<h2><a class="toc-backref" href="#id8">Module Initialization</a><a class="headerlink" href="#module-initialization" title="Permalink to this heading">¶</a></h2>
<p>By default, parameters and floating-point buffers for modules provided by <a class="reference internal" href="../nn.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code></a> are initialized during
module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to
perform well historically for the module type. For certain use cases, it may be desired to initialize with a different
dtype, device (e.g. GPU), or initialization technique.</p>
<p>Examples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize module directly onto GPU.</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Initialize module with 16-bit floating point parameters.</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">)</span>

<span class="c1"># Skip default parameter initialization and perform custom (e.g. orthogonal) initialization.</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">skip_init</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the device and dtype options demonstrated above also apply to any floating-point buffers registered
for the module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">running_mean</span><span class="p">)</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</pre></div>
</div>
<p>While module writers can use any device or dtype to initialize parameters in their custom modules, good practice is
to use <code class="docutils literal notranslate"><span class="pre">dtype=torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">device='cpu'</span></code> by default as well. Optionally, you can provide full flexibility
in these areas for your custom module by conforming to the convention demonstrated above that all
<a class="reference internal" href="../nn.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code></a> modules follow:</p>
<ul class="simple">
<li><p>Provide a <code class="docutils literal notranslate"><span class="pre">device</span></code> constructor kwarg that applies to any parameters / buffers registered by the module.</p></li>
<li><p>Provide a <code class="docutils literal notranslate"><span class="pre">dtype</span></code> constructor kwarg that applies to any parameters / floating-point buffers registered by
the module.</p></li>
<li><p>Only use initialization functions (i.e. functions from <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn.init</span></code>) on parameters and buffers within the
module’s constructor. Note that this is only required to use <a class="reference internal" href="../generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init" title="torch.nn.utils.skip_init"><code class="xref py py-func docutils literal notranslate"><span class="pre">skip_init()</span></code></a>; see
<a class="reference external" href="https://pytorch.org/tutorials/prototype/skip_param_init.html#updating-modules-to-support-skipping-initialization">this page</a> for an explanation.</p></li>
</ul>
<p>For more information, check out:</p>
<ul class="simple">
<li><p>Skipping module parameter initialization: <a class="reference external" href="https://pytorch.org/tutorials/prototype/skip_param_init.html">https://pytorch.org/tutorials/prototype/skip_param_init.html</a></p></li>
</ul>
</div>
<div class="section" id="module-hooks">
<h2><a class="toc-backref" href="#id9">Module Hooks</a><a class="headerlink" href="#module-hooks" title="Permalink to this heading">¶</a></h2>
<p>In <a class="reference internal" href="#neural-network-training-with-modules"><span class="std std-ref">Neural Network Training with Modules</span></a>, we demonstrated the training process for a module, which iteratively
performs forward and backward passes, updating module parameters each iteration. For more control
over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward
pass, even modifying how the pass is done if desired. Some useful examples for this functionality include
debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules
you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules.</p>
<p>PyTorch provides two types of hooks for modules:</p>
<ul class="simple">
<li><p><strong>Forward hooks</strong> are called during the forward pass. They can be installed for a given module with
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook" title="torch.nn.Module.register_forward_pre_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_forward_pre_hook()</span></code></a> and <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook" title="torch.nn.Module.register_forward_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_forward_hook()</span></code></a>.
These hooks will be called respectively just before the forward function is called and just after it is called.
Alternatively, these hooks can be installed globally for all modules with the analogous
<a class="reference internal" href="../generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook" title="torch.nn.modules.module.register_module_forward_pre_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_forward_pre_hook()</span></code></a> and
<a class="reference internal" href="../generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook" title="torch.nn.modules.module.register_module_forward_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_forward_hook()</span></code></a> functions.</p></li>
<li><p><strong>Backward hooks</strong> are called during the backward pass. They can be installed with
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook" title="torch.nn.Module.register_full_backward_pre_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_full_backward_pre_hook()</span></code></a> and <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook" title="torch.nn.Module.register_full_backward_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code></a>.
These hooks will be called when the backward for this Module has been computed.
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook" title="torch.nn.Module.register_full_backward_pre_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_full_backward_pre_hook()</span></code></a> will allow the user to access the gradients for outputs
while <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook" title="torch.nn.Module.register_full_backward_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code></a> will allow the user to access the gradients
both the inputs and outputs. Alternatively, they can be installed globally for all modules with
<a class="reference internal" href="../generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook" title="torch.nn.modules.module.register_module_full_backward_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_full_backward_hook()</span></code></a> and
<a class="reference internal" href="../generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook" title="torch.nn.modules.module.register_module_full_backward_pre_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_full_backward_pre_hook()</span></code></a>.</p></li>
</ul>
<p>All hooks allow the user to return an updated value that will be used throughout the remaining computation.
Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or
modify some inputs/outputs without having to change the module’s <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function.</p>
<p>Below is an example demonstrating usage of forward and backward hooks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_pre_hook</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
  <span class="c1"># Allows for examination and modification of the input before the forward pass.</span>
  <span class="c1"># Note that inputs are always wrapped in a tuple.</span>
  <span class="nb">input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">return</span> <span class="nb">input</span> <span class="o">+</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">forward_hook</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
  <span class="c1"># Allows for examination of inputs / outputs and modification of the outputs</span>
  <span class="c1"># after the forward pass. Note that inputs are always wrapped in a tuple while outputs</span>
  <span class="c1"># are passed as-is.</span>

  <span class="c1"># Residual computation a la ResNet.</span>
  <span class="k">return</span> <span class="n">output</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">backward_hook</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">grad_inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
  <span class="c1"># Allows for examination of grad_inputs / grad_outputs and modification of</span>
  <span class="c1"># grad_inputs used in the rest of the backwards pass. Note that grad_inputs and</span>
  <span class="c1"># grad_outputs are always wrapped in tuples.</span>
  <span class="n">new_grad_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">gi</span><span class="p">)</span> <span class="o">*</span> <span class="mf">42.</span> <span class="k">for</span> <span class="n">gi</span> <span class="ow">in</span> <span class="n">grad_inputs</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">new_grad_inputs</span>

<span class="c1"># Create sample module &amp; input.</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># ==== Demonstrate forward hooks. ====</span>
<span class="c1"># Run input through module before and after adding hooks.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output with no forward hooks: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="p">:</span> <span class="n">output</span> <span class="k">with</span> <span class="n">no</span> <span class="n">forward</span> <span class="n">hooks</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.5059</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8158</span><span class="p">,</span>  <span class="mf">0.2390</span><span class="p">],</span>
                                        <span class="p">[</span><span class="o">-</span><span class="mf">0.0043</span><span class="p">,</span>  <span class="mf">0.4724</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1714</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward</span><span class="o">&gt;</span><span class="p">)</span>

<span class="c1"># Note that the modified input results in a different output.</span>
<span class="n">forward_pre_hook_handle</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span><span class="n">forward_pre_hook</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output with forward pre hook: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="p">:</span> <span class="n">output</span> <span class="k">with</span> <span class="n">forward</span> <span class="n">pre</span> <span class="n">hook</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.5752</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7421</span><span class="p">,</span>  <span class="mf">0.4942</span><span class="p">],</span>
                                        <span class="p">[</span><span class="o">-</span><span class="mf">0.0736</span><span class="p">,</span>  <span class="mf">0.5461</span><span class="p">,</span>  <span class="mf">0.0838</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward</span><span class="o">&gt;</span><span class="p">)</span>

<span class="c1"># Note the modified output.</span>
<span class="n">forward_hook_handle</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">forward_hook</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output with both forward hooks: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="p">:</span> <span class="n">output</span> <span class="k">with</span> <span class="n">both</span> <span class="n">forward</span> <span class="n">hooks</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0980</span><span class="p">,</span>  <span class="mf">0.6396</span><span class="p">,</span>  <span class="mf">0.4666</span><span class="p">],</span>
                                          <span class="p">[</span> <span class="mf">0.3634</span><span class="p">,</span>  <span class="mf">0.6538</span><span class="p">,</span>  <span class="mf">1.0256</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>

<span class="c1"># Remove hooks; note that the output here matches the output before adding hooks.</span>
<span class="n">forward_pre_hook_handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="n">forward_hook_handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output after removing forward hooks: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="p">:</span> <span class="n">output</span> <span class="n">after</span> <span class="n">removing</span> <span class="n">forward</span> <span class="n">hooks</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.5059</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8158</span><span class="p">,</span>  <span class="mf">0.2390</span><span class="p">],</span>
                                               <span class="p">[</span><span class="o">-</span><span class="mf">0.0043</span><span class="p">,</span>  <span class="mf">0.4724</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1714</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward</span><span class="o">&gt;</span><span class="p">)</span>

<span class="c1"># ==== Demonstrate backward hooks. ====</span>
<span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x.grad with no backwards hook: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>
<span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="k">with</span> <span class="n">no</span> <span class="n">backwards</span> <span class="n">hook</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.4497</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5046</span><span class="p">,</span>  <span class="mf">0.3146</span><span class="p">],</span>
                                         <span class="p">[</span> <span class="mf">0.4497</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5046</span><span class="p">,</span>  <span class="mf">0.3146</span><span class="p">]])</span>

<span class="c1"># Clear gradients before running backward pass again.</span>
<span class="n">m</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

<span class="n">m</span><span class="o">.</span><span class="n">register_full_backward_hook</span><span class="p">(</span><span class="n">backward_hook</span><span class="p">)</span>
<span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x.grad with backwards hook: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>
<span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="k">with</span> <span class="n">backwards</span> <span class="n">hook</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="mf">42.</span><span class="p">,</span> <span class="mf">42.</span><span class="p">,</span> <span class="mf">42.</span><span class="p">],</span>
                                      <span class="p">[</span><span class="mf">42.</span><span class="p">,</span> <span class="mf">42.</span><span class="p">,</span> <span class="mf">42.</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="section" id="advanced-features">
<h2><a class="toc-backref" href="#id10">Advanced Features</a><a class="headerlink" href="#advanced-features" title="Permalink to this heading">¶</a></h2>
<p>PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities
are available for custom-written modules, with the small caveat that certain features may require modules to conform
to particular constraints in order to be supported. In-depth discussion of these features and the corresponding
requirements can be found in the links below.</p>
<div class="section" id="distributed-training">
<h3><a class="toc-backref" href="#id11">Distributed Training</a><a class="headerlink" href="#distributed-training" title="Permalink to this heading">¶</a></h3>
<p>Various methods for distributed training exist within PyTorch, both for scaling up training using multiple GPUs
as well as training across multiple machines. Check out the
<a class="reference external" href="https://pytorch.org/tutorials/beginner/dist_overview.html">distributed training overview page</a> for
detailed information on how to utilize these.</p>
</div>
<div class="section" id="profiling-performance">
<h3><a class="toc-backref" href="#id12">Profiling Performance</a><a class="headerlink" href="#profiling-performance" title="Permalink to this heading">¶</a></h3>
<p>The <a class="reference external" href="https://pytorch.org/tutorials/beginner/profiler.html">PyTorch Profiler</a> can be useful for identifying
performance bottlenecks within your models. It measures and outputs performance characteristics for
both memory usage and time spent.</p>
</div>
<div class="section" id="improving-performance-with-quantization">
<h3><a class="toc-backref" href="#id13">Improving Performance with Quantization</a><a class="headerlink" href="#improving-performance-with-quantization" title="Permalink to this heading">¶</a></h3>
<p>Applying quantization techniques to modules can improve performance and memory usage by utilizing lower
bitwidths than floating-point precision. Check out the various PyTorch-provided mechanisms for quantization
<a class="reference external" href="https://pytorch.org/docs/stable/quantization.html">here</a>.</p>
</div>
<div class="section" id="improving-memory-usage-with-pruning">
<h3><a class="toc-backref" href="#id14">Improving Memory Usage with Pruning</a><a class="headerlink" href="#improving-memory-usage-with-pruning" title="Permalink to this heading">¶</a></h3>
<p>Large deep learning models are often over-parametrized, resulting in high memory usage. To combat this, PyTorch
provides mechanisms for model pruning, which can help reduce memory usage while maintaining task accuracy. The
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">Pruning tutorial</a> describes how to utilize
the pruning techniques PyTorch provides or define custom pruning techniques as necessary.</p>
</div>
<div class="section" id="parametrizations">
<h3><a class="toc-backref" href="#id15">Parametrizations</a><a class="headerlink" href="#parametrizations" title="Permalink to this heading">¶</a></h3>
<p>For certain applications, it can be beneficial to constrain the parameter space during model training. For example,
enforcing orthogonality of the learned parameters can improve convergence for RNNs. PyTorch provides a mechanism for
applying <a class="reference external" href="https://pytorch.org/tutorials/intermediate/parametrizations.html">parametrizations</a> such as this, and
further allows for custom constraints to be defined.</p>
</div>
<div class="section" id="transforming-modules-with-fx">
<h3><a class="toc-backref" href="#id16">Transforming Modules with FX</a><a class="headerlink" href="#transforming-modules-with-fx" title="Permalink to this heading">¶</a></h3>
<p>The <a class="reference external" href="https://pytorch.org/docs/stable/fx.html">FX</a> component of PyTorch provides a flexible way to transform
modules by operating directly on module computation graphs. This can be used to programmatically generate or
manipulate modules for a broad array of use cases. To explore FX, check out these examples of using FX for
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html">convolution + batch norm fusion</a> and
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/fx_profiling_tutorial.html">CPU performance analysis</a>.</p>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mps.html" class="btn btn-neutral float-right" title="MPS backend" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="large_scale_deployments.html" class="btn btn-neutral" title="Features for large-scale deployments" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Modules</a><ul>
<li><a class="reference internal" href="#a-simple-custom-module">A Simple Custom Module</a></li>
<li><a class="reference internal" href="#modules-as-building-blocks">Modules as Building Blocks</a></li>
<li><a class="reference internal" href="#neural-network-training-with-modules">Neural Network Training with Modules</a></li>
<li><a class="reference internal" href="#module-state">Module State</a></li>
<li><a class="reference internal" href="#module-initialization">Module Initialization</a></li>
<li><a class="reference internal" href="#module-hooks">Module Hooks</a></li>
<li><a class="reference internal" href="#advanced-features">Advanced Features</a><ul>
<li><a class="reference internal" href="#distributed-training">Distributed Training</a></li>
<li><a class="reference internal" href="#profiling-performance">Profiling Performance</a></li>
<li><a class="reference internal" href="#improving-performance-with-quantization">Improving Performance with Quantization</a></li>
<li><a class="reference internal" href="#improving-memory-usage-with-pruning">Improving Memory Usage with Pruning</a></li>
<li><a class="reference internal" href="#parametrizations">Parametrizations</a></li>
<li><a class="reference internal" href="#transforming-modules-with-fx">Transforming Modules with FX</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
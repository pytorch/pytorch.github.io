


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.Storage &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/storage.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.testing" href="testing.html" />
    <link rel="prev" title="check_sparse_tensor_invariants" href="generated/torch.sparse.check_sparse_tensor_invariants.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/storage.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.Storage</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/storage.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="torch-storage">
<h1>torch.Storage<a class="headerlink" href="#torch-storage" title="Permalink to this heading">¶</a></h1>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Storage</span></code> is an alias for the storage class that corresponds with
the default data type (<a class="reference internal" href="generated/torch.get_default_dtype.html#torch.get_default_dtype" title="torch.get_default_dtype"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.get_default_dtype()</span></code></a>). For instance, if the
default data type is <code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Storage</span></code> resolves to
<a class="reference internal" href="#torch.FloatStorage" title="torch.FloatStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatStorage</span></code></a>.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.&lt;type&gt;Storage</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.&lt;type&gt;Storage</span></code> classes,
like <a class="reference internal" href="#torch.FloatStorage" title="torch.FloatStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatStorage</span></code></a>, <a class="reference internal" href="#torch.IntStorage" title="torch.IntStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.IntStorage</span></code></a>, etc., are not
actually ever instantiated. Calling their constructors creates
a <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.TypedStorage</span></code></a> with the appropriate <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>.  <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.&lt;type&gt;Storage</span></code> classes have all of the
same class methods that <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.TypedStorage</span></code></a> has.</p>
<p>A <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.TypedStorage</span></code></a> is a contiguous, one-dimensional array of
elements of a particular <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>. It can be given any
<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, and the internal data will be interpreted appropriately.
<a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.TypedStorage</span></code></a> contains a <a class="reference internal" href="#torch.UntypedStorage" title="torch.UntypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.UntypedStorage</span></code></a> which
holds the data as an untyped array of bytes.</p>
<p>Every strided <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> contains a <a class="reference internal" href="#torch.TypedStorage" title="torch.TypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.TypedStorage</span></code></a>,
which stores all of the data that the <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> views.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>All storage classes except for <a class="reference internal" href="#torch.UntypedStorage" title="torch.UntypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.UntypedStorage</span></code></a> will be removed
in the future, and <a class="reference internal" href="#torch.UntypedStorage" title="torch.UntypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.UntypedStorage</span></code></a> will be used in all cases.</p>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="torch.TypedStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">TypedStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.bfloat16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to bfloat16 type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.bool">
<span class="sig-name descname"><span class="pre">bool</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.bool"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.bool" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to bool type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.byte">
<span class="sig-name descname"><span class="pre">byte</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.byte"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.byte" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to byte type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.char">
<span class="sig-name descname"><span class="pre">char</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.char"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.char" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to char type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.clone">
<span class="sig-name descname"><span class="pre">clone</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.clone"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this storage</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.complex_double">
<span class="sig-name descname"><span class="pre">complex_double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.complex_double"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.complex_double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to complex double type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.complex_float">
<span class="sig-name descname"><span class="pre">complex_float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.complex_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.complex_float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to complex float type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.copy_">
<span class="sig-name descname"><span class="pre">copy_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.copy_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.copy_" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.cpu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a CPU copy of this storage if it’s not already on the CPU</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.cuda"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – The destination GPU id. Defaults to the current device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host. Otherwise,
the argument has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>T</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.data_ptr">
<span class="sig-name descname"><span class="pre">data_ptr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.data_ptr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.TypedStorage.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><a class="headerlink" href="#torch.TypedStorage.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.double"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to double type</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.TypedStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><a class="headerlink" href="#torch.TypedStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.element_size">
<span class="sig-name descname"><span class="pre">element_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.element_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.element_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.fill_">
<span class="sig-name descname"><span class="pre">fill_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.fill_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.fill_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to float type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.from_buffer">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.from_buffer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.from_buffer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.from_file">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Storage</span></span></span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.from_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.from_file" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>shared</cite> is <cite>True</cite>, then memory is shared between all processes.
All changes are written to the file. If <cite>shared</cite> is <cite>False</cite>, then the changes on
the storage do not affect the file.</p>
<p><cite>size</cite> is the number of elements in the storage. If <cite>shared</cite> is <cite>False</cite>,
then the file must contain at least <cite>size * sizeof(Type)</cite> bytes
(<cite>Type</cite> is the type of storage). If <cite>shared</cite> is <cite>True</cite> the file will be
created if needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a>) – file name to map</p></li>
<li><p><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – whether to share memory</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of elements in the storage</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.get_device">
<span class="sig-name descname"><span class="pre">get_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.get_device"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.get_device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.half"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to half type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.hpu">
<span class="sig-name descname"><span class="pre">hpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.hpu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.hpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in HPU memory.</p>
<p>If this object is already in HPU memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – The destination HPU id. Defaults to the current device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host. Otherwise,
the argument has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><em>T</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.int">
<span class="sig-name descname"><span class="pre">int</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.int"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.int" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to int type</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.TypedStorage.is_cuda">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_cuda</span></span><a class="headerlink" href="#torch.TypedStorage.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.TypedStorage.is_hpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_hpu</span></span><a class="headerlink" href="#torch.TypedStorage.is_hpu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.is_pinned">
<span class="sig-name descname"><span class="pre">is_pinned</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.is_pinned"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd><p>Determine whether the CPU TypedStorage is already pinned on device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a>) – The device to pin memory on. Default: <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code></p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A boolean variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.is_shared">
<span class="sig-name descname"><span class="pre">is_shared</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.is_shared"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.is_shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.TypedStorage.is_sparse">
<span class="sig-name descname"><span class="pre">is_sparse</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#torch.TypedStorage.is_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.long">
<span class="sig-name descname"><span class="pre">long</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.long"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.long" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to long type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.nbytes">
<span class="sig-name descname"><span class="pre">nbytes</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.nbytes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.nbytes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.pickle_storage_type">
<span class="sig-name descname"><span class="pre">pickle_storage_type</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.pickle_storage_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.pickle_storage_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.pin_memory">
<span class="sig-name descname"><span class="pre">pin_memory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.pin_memory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the CPU TypedStorage to pinned memory, if it’s not already pinned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a>) – The device to pin memory on. Default: <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A pinned CPU storage.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.resize_">
<span class="sig-name descname"><span class="pre">resize_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.resize_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.resize_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.share_memory_">
<span class="sig-name descname"><span class="pre">share_memory_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.share_memory_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the storage to shared memory.</p>
<p>This is a no-op for storages already in shared memory and for CUDA
storages, which do not need to be moved for sharing across processes.
Storages in shared memory cannot be resized.</p>
<p>Returns: self</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.short">
<span class="sig-name descname"><span class="pre">short</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.short"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.short" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to short type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.size">
<span class="sig-name descname"><span class="pre">size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.tolist">
<span class="sig-name descname"><span class="pre">tolist</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.tolist"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list containing the elements of this storage</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.11)"><em>type</em></a><em> or </em><em>string</em>) – The desired type</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, and the source is in pinned memory
and destination is on the GPU or vice versa, the copy is performed
asynchronously with respect to the host. Otherwise, the argument
has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">async</span></code> arg is deprecated.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.11)"><em>Union</em></a>[<em>T</em>, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.TypedStorage.untyped">
<span class="sig-name descname"><span class="pre">untyped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#TypedStorage.untyped"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.TypedStorage.untyped" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the internal <a class="reference internal" href="#torch.UntypedStorage" title="torch.UntypedStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.UntypedStorage</span></code></a></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.UntypedStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">UntypedStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#UntypedStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.UntypedStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to bfloat16 type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.bool">
<span class="sig-name descname"><span class="pre">bool</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.bool" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to bool type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.byte">
<span class="sig-name descname"><span class="pre">byte</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.byte" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to byte type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.byteswap">
<span class="sig-name descname"><span class="pre">byteswap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.byteswap" title="Permalink to this definition">¶</a></dt>
<dd><p>Swaps bytes in underlying data</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.char">
<span class="sig-name descname"><span class="pre">char</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.char" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to char type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.clone">
<span class="sig-name descname"><span class="pre">clone</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this storage</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.complex_double">
<span class="sig-name descname"><span class="pre">complex_double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.complex_double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to complex double type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.complex_float">
<span class="sig-name descname"><span class="pre">complex_float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.complex_float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to complex float type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.copy_">
<span class="sig-name descname"><span class="pre">copy_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.copy_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a CPU copy of this storage if it’s not already on the CPU</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – The destination GPU id. Defaults to the current device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host. Otherwise,
the argument has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.data_ptr">
<span class="sig-name descname"><span class="pre">data_ptr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.UntypedStorage.device">
<span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><span class="pre">device</span></a></em><a class="headerlink" href="#torch.UntypedStorage.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to double type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.element_size">
<span class="sig-name descname"><span class="pre">element_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.element_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.fill_">
<span class="sig-name descname"><span class="pre">fill_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.fill_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to float type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.from_buffer">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_buffer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.from_buffer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.from_file">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Storage</span></span></span><a class="headerlink" href="#torch.UntypedStorage.from_file" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>shared</cite> is <cite>True</cite>, then memory is shared between all processes.
All changes are written to the file. If <cite>shared</cite> is <cite>False</cite>, then the changes on
the storage do not affect the file.</p>
<p><cite>size</cite> is the number of elements in the storage. If <cite>shared</cite> is <cite>False</cite>,
then the file must contain at least <cite>size * sizeof(Type)</cite> bytes
(<cite>Type</cite> is the type of storage). If <cite>shared</cite> is <cite>True</cite> the file will be
created if needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a>) – file name to map</p></li>
<li><p><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – whether to share memory</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – number of elements in the storage</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.get_device">
<span class="sig-name descname"><span class="pre">get_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.get_device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to half type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.hpu">
<span class="sig-name descname"><span class="pre">hpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.hpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in HPU memory.</p>
<p>If this object is already in HPU memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – The destination HPU id. Defaults to the current device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host. Otherwise,
the argument has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.int">
<span class="sig-name descname"><span class="pre">int</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.int" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to int type</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.UntypedStorage.is_cuda">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_cuda</span></span><a class="headerlink" href="#torch.UntypedStorage.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.UntypedStorage.is_hpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_hpu</span></span><a class="headerlink" href="#torch.UntypedStorage.is_hpu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.is_pinned">
<span class="sig-name descname"><span class="pre">is_pinned</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd><p>Determine whether the CPU storage is already pinned on device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a>) – The device to pin memory on. Default: <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A boolean variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.is_shared">
<span class="sig-name descname"><span class="pre">is_shared</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.is_shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.UntypedStorage.is_sparse">
<span class="sig-name descname"><span class="pre">is_sparse</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#torch.UntypedStorage.is_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.UntypedStorage.is_sparse_csr">
<span class="sig-name descname"><span class="pre">is_sparse_csr</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><span class="pre">bool</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#torch.UntypedStorage.is_sparse_csr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.long">
<span class="sig-name descname"><span class="pre">long</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.long" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to long type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.mps">
<span class="sig-name descname"><span class="pre">mps</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.mps" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a MPS copy of this storage if it’s not already on the MPS</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.nbytes">
<span class="sig-name descname"><span class="pre">nbytes</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.nbytes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.new">
<span class="sig-name descname"><span class="pre">new</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.new" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.pin_memory">
<span class="sig-name descname"><span class="pre">pin_memory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the CPU storage to pinned memory, if it’s not already pinned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a>) – The device to pin memory on. Default: <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A pinned CPU storage.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.resize_">
<span class="sig-name descname"><span class="pre">resize_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.resize_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.share_memory_">
<span class="sig-name descname"><span class="pre">share_memory_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/storage.html#UntypedStorage.share_memory_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.UntypedStorage.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.short">
<span class="sig-name descname"><span class="pre">short</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.short" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to short type</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.size">
<span class="sig-name descname"><span class="pre">size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.size" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.tolist">
<span class="sig-name descname"><span class="pre">tolist</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list containing the elements of this storage</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_blocking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.11)"><em>type</em></a><em> or </em><em>string</em>) – The desired type</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, and the source is in pinned memory
and destination is on the GPU or vice versa, the copy is performed
asynchronously with respect to the host. Otherwise, the argument
has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">async</span></code> arg is deprecated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.UntypedStorage.untyped">
<span class="sig-name descname"><span class="pre">untyped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.UntypedStorage.untyped" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.DoubleStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">DoubleStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#DoubleStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.DoubleStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.DoubleStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.float64</span></em><a class="reference internal" href="_modules/torch.html#DoubleStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.DoubleStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.FloatStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">FloatStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#FloatStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.FloatStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.FloatStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.float32</span></em><a class="reference internal" href="_modules/torch.html#FloatStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.FloatStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.HalfStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">HalfStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#HalfStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.HalfStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.HalfStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.float16</span></em><a class="reference internal" href="_modules/torch.html#HalfStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.HalfStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.LongStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">LongStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#LongStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.LongStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.LongStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.int64</span></em><a class="reference internal" href="_modules/torch.html#LongStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.LongStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.IntStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">IntStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#IntStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.IntStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.IntStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.int32</span></em><a class="reference internal" href="_modules/torch.html#IntStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.IntStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.ShortStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">ShortStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#ShortStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.ShortStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.ShortStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.int16</span></em><a class="reference internal" href="_modules/torch.html#ShortStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.ShortStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.CharStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">CharStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#CharStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.CharStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.CharStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.int8</span></em><a class="reference internal" href="_modules/torch.html#CharStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.CharStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.ByteStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">ByteStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#ByteStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.ByteStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.ByteStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.uint8</span></em><a class="reference internal" href="_modules/torch.html#ByteStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.ByteStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.BoolStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">BoolStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#BoolStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.BoolStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.BoolStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.bool</span></em><a class="reference internal" href="_modules/torch.html#BoolStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.BoolStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.BFloat16Storage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">BFloat16Storage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#BFloat16Storage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.BFloat16Storage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.BFloat16Storage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.bfloat16</span></em><a class="reference internal" href="_modules/torch.html#BFloat16Storage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.BFloat16Storage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.ComplexDoubleStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">ComplexDoubleStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#ComplexDoubleStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.ComplexDoubleStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.ComplexDoubleStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.complex128</span></em><a class="reference internal" href="_modules/torch.html#ComplexDoubleStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.ComplexDoubleStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.ComplexFloatStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">ComplexFloatStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#ComplexFloatStorage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.ComplexFloatStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.ComplexFloatStorage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.complex64</span></em><a class="reference internal" href="_modules/torch.html#ComplexFloatStorage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.ComplexFloatStorage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.QUInt8Storage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">QUInt8Storage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#QUInt8Storage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QUInt8Storage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.QUInt8Storage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.quint8</span></em><a class="reference internal" href="_modules/torch.html#QUInt8Storage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QUInt8Storage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.QInt8Storage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">QInt8Storage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#QInt8Storage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QInt8Storage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.QInt8Storage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.qint8</span></em><a class="reference internal" href="_modules/torch.html#QInt8Storage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QInt8Storage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.QInt32Storage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">QInt32Storage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#QInt32Storage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QInt32Storage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.QInt32Storage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.qint32</span></em><a class="reference internal" href="_modules/torch.html#QInt32Storage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QInt32Storage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.QUInt4x2Storage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">QUInt4x2Storage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#QUInt4x2Storage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QUInt4x2Storage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.QUInt4x2Storage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.quint4x2</span></em><a class="reference internal" href="_modules/torch.html#QUInt4x2Storage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QUInt4x2Storage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.QUInt2x4Storage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">QUInt2x4Storage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_storage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_internal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#QUInt2x4Storage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QUInt2x4Storage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.QUInt2x4Storage.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><span class="pre">dtype</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">torch.quint2x4</span></em><a class="reference internal" href="_modules/torch.html#QUInt2x4Storage.dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.QUInt2x4Storage.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="testing.html" class="btn btn-neutral float-right" title="torch.testing" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="generated/torch.sparse.check_sparse_tensor_invariants.html" class="btn btn-neutral" title="check_sparse_tensor_invariants" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.Storage</a><ul>
<li><a class="reference internal" href="#torch.TypedStorage"><code class="docutils literal notranslate"><span class="pre">TypedStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.TypedStorage.bfloat16"><code class="docutils literal notranslate"><span class="pre">TypedStorage.bfloat16()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.bool"><code class="docutils literal notranslate"><span class="pre">TypedStorage.bool()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.byte"><code class="docutils literal notranslate"><span class="pre">TypedStorage.byte()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.char"><code class="docutils literal notranslate"><span class="pre">TypedStorage.char()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.clone"><code class="docutils literal notranslate"><span class="pre">TypedStorage.clone()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.complex_double"><code class="docutils literal notranslate"><span class="pre">TypedStorage.complex_double()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.complex_float"><code class="docutils literal notranslate"><span class="pre">TypedStorage.complex_float()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.copy_"><code class="docutils literal notranslate"><span class="pre">TypedStorage.copy_()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.cpu"><code class="docutils literal notranslate"><span class="pre">TypedStorage.cpu()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.cuda"><code class="docutils literal notranslate"><span class="pre">TypedStorage.cuda()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.data_ptr"><code class="docutils literal notranslate"><span class="pre">TypedStorage.data_ptr()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.device"><code class="docutils literal notranslate"><span class="pre">TypedStorage.device</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.double"><code class="docutils literal notranslate"><span class="pre">TypedStorage.double()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.dtype"><code class="docutils literal notranslate"><span class="pre">TypedStorage.dtype</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.element_size"><code class="docutils literal notranslate"><span class="pre">TypedStorage.element_size()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.fill_"><code class="docutils literal notranslate"><span class="pre">TypedStorage.fill_()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.float"><code class="docutils literal notranslate"><span class="pre">TypedStorage.float()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.from_buffer"><code class="docutils literal notranslate"><span class="pre">TypedStorage.from_buffer()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.from_file"><code class="docutils literal notranslate"><span class="pre">TypedStorage.from_file()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.get_device"><code class="docutils literal notranslate"><span class="pre">TypedStorage.get_device()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.half"><code class="docutils literal notranslate"><span class="pre">TypedStorage.half()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.hpu"><code class="docutils literal notranslate"><span class="pre">TypedStorage.hpu()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.int"><code class="docutils literal notranslate"><span class="pre">TypedStorage.int()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.is_cuda"><code class="docutils literal notranslate"><span class="pre">TypedStorage.is_cuda</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.is_hpu"><code class="docutils literal notranslate"><span class="pre">TypedStorage.is_hpu</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.is_pinned"><code class="docutils literal notranslate"><span class="pre">TypedStorage.is_pinned()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.is_shared"><code class="docutils literal notranslate"><span class="pre">TypedStorage.is_shared()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.is_sparse"><code class="docutils literal notranslate"><span class="pre">TypedStorage.is_sparse</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.long"><code class="docutils literal notranslate"><span class="pre">TypedStorage.long()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.nbytes"><code class="docutils literal notranslate"><span class="pre">TypedStorage.nbytes()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.pickle_storage_type"><code class="docutils literal notranslate"><span class="pre">TypedStorage.pickle_storage_type()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.pin_memory"><code class="docutils literal notranslate"><span class="pre">TypedStorage.pin_memory()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.resize_"><code class="docutils literal notranslate"><span class="pre">TypedStorage.resize_()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.share_memory_"><code class="docutils literal notranslate"><span class="pre">TypedStorage.share_memory_()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.short"><code class="docutils literal notranslate"><span class="pre">TypedStorage.short()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.size"><code class="docutils literal notranslate"><span class="pre">TypedStorage.size()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.tolist"><code class="docutils literal notranslate"><span class="pre">TypedStorage.tolist()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.type"><code class="docutils literal notranslate"><span class="pre">TypedStorage.type()</span></code></a></li>
<li><a class="reference internal" href="#torch.TypedStorage.untyped"><code class="docutils literal notranslate"><span class="pre">TypedStorage.untyped()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.UntypedStorage"><code class="docutils literal notranslate"><span class="pre">UntypedStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.UntypedStorage.bfloat16"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.bfloat16()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.bool"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.bool()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.byte"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.byte()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.byteswap"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.byteswap()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.char"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.char()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.clone"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.clone()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.complex_double"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.complex_double()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.complex_float"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.complex_float()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.copy_"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.copy_()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.cpu"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.cpu()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.cuda"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.cuda()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.data_ptr"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.data_ptr()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.device"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.device</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.double"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.double()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.element_size"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.element_size()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.fill_"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.fill_()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.float"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.float()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.from_buffer"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.from_buffer()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.from_file"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.from_file()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.get_device"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.get_device()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.half"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.half()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.hpu"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.hpu()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.int"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.int()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.is_cuda"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.is_cuda</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.is_hpu"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.is_hpu</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.is_pinned"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.is_pinned()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.is_shared"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.is_shared()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.is_sparse"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.is_sparse</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.is_sparse_csr"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.is_sparse_csr</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.long"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.long()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.mps"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.mps()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.nbytes"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.nbytes()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.new"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.new()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.pin_memory"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.pin_memory()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.resize_"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.resize_()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.share_memory_"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.share_memory_()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.short"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.short()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.size"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.size()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.tolist"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.tolist()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.type"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.type()</span></code></a></li>
<li><a class="reference internal" href="#torch.UntypedStorage.untyped"><code class="docutils literal notranslate"><span class="pre">UntypedStorage.untyped()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.DoubleStorage"><code class="docutils literal notranslate"><span class="pre">DoubleStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.DoubleStorage.dtype"><code class="docutils literal notranslate"><span class="pre">DoubleStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.FloatStorage"><code class="docutils literal notranslate"><span class="pre">FloatStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.FloatStorage.dtype"><code class="docutils literal notranslate"><span class="pre">FloatStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.HalfStorage"><code class="docutils literal notranslate"><span class="pre">HalfStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.HalfStorage.dtype"><code class="docutils literal notranslate"><span class="pre">HalfStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.LongStorage"><code class="docutils literal notranslate"><span class="pre">LongStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.LongStorage.dtype"><code class="docutils literal notranslate"><span class="pre">LongStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.IntStorage"><code class="docutils literal notranslate"><span class="pre">IntStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.IntStorage.dtype"><code class="docutils literal notranslate"><span class="pre">IntStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.ShortStorage"><code class="docutils literal notranslate"><span class="pre">ShortStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.ShortStorage.dtype"><code class="docutils literal notranslate"><span class="pre">ShortStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.CharStorage"><code class="docutils literal notranslate"><span class="pre">CharStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.CharStorage.dtype"><code class="docutils literal notranslate"><span class="pre">CharStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.ByteStorage"><code class="docutils literal notranslate"><span class="pre">ByteStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.ByteStorage.dtype"><code class="docutils literal notranslate"><span class="pre">ByteStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.BoolStorage"><code class="docutils literal notranslate"><span class="pre">BoolStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.BoolStorage.dtype"><code class="docutils literal notranslate"><span class="pre">BoolStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.BFloat16Storage"><code class="docutils literal notranslate"><span class="pre">BFloat16Storage</span></code></a><ul>
<li><a class="reference internal" href="#torch.BFloat16Storage.dtype"><code class="docutils literal notranslate"><span class="pre">BFloat16Storage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.ComplexDoubleStorage"><code class="docutils literal notranslate"><span class="pre">ComplexDoubleStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.ComplexDoubleStorage.dtype"><code class="docutils literal notranslate"><span class="pre">ComplexDoubleStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.ComplexFloatStorage"><code class="docutils literal notranslate"><span class="pre">ComplexFloatStorage</span></code></a><ul>
<li><a class="reference internal" href="#torch.ComplexFloatStorage.dtype"><code class="docutils literal notranslate"><span class="pre">ComplexFloatStorage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.QUInt8Storage"><code class="docutils literal notranslate"><span class="pre">QUInt8Storage</span></code></a><ul>
<li><a class="reference internal" href="#torch.QUInt8Storage.dtype"><code class="docutils literal notranslate"><span class="pre">QUInt8Storage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.QInt8Storage"><code class="docutils literal notranslate"><span class="pre">QInt8Storage</span></code></a><ul>
<li><a class="reference internal" href="#torch.QInt8Storage.dtype"><code class="docutils literal notranslate"><span class="pre">QInt8Storage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.QInt32Storage"><code class="docutils literal notranslate"><span class="pre">QInt32Storage</span></code></a><ul>
<li><a class="reference internal" href="#torch.QInt32Storage.dtype"><code class="docutils literal notranslate"><span class="pre">QInt32Storage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.QUInt4x2Storage"><code class="docutils literal notranslate"><span class="pre">QUInt4x2Storage</span></code></a><ul>
<li><a class="reference internal" href="#torch.QUInt4x2Storage.dtype"><code class="docutils literal notranslate"><span class="pre">QUInt4x2Storage.dtype</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch.QUInt2x4Storage"><code class="docutils literal notranslate"><span class="pre">QUInt2x4Storage</span></code></a><ul>
<li><a class="reference internal" href="#torch.QUInt2x4Storage.dtype"><code class="docutils literal notranslate"><span class="pre">QUInt2x4Storage.dtype</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
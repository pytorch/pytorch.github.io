


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/torch.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.is_tensor" href="generated/torch.is_tensor.html" />
    <link rel="prev" title="torch::deploy has been moved to pytorch/multipy" href="deploy.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/torch.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/torch.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="module-torch">
<span id="torch"></span><h1>torch<a class="headerlink" href="#module-torch" title="Permalink to this heading">¶</a></h1>
<p>The torch package contains data structures for multi-dimensional
tensors and defines mathematical operations over these tensors.
Additionally, it provides many utilities for efficient serialization of
Tensors and arbitrary types, and other useful utilities.</p>
<p>It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability &gt;= 3.0.</p>
<div class="section" id="tensors">
<h2>Tensors<a class="headerlink" href="#tensors" title="Permalink to this heading">¶</a></h2>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.is_tensor"/><a class="reference internal" href="generated/torch.is_tensor.html#torch.is_tensor" title="torch.is_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_tensor</span></code></a></p></td>
<td><p>Returns True if <cite>obj</cite> is a PyTorch tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_storage"/><a class="reference internal" href="generated/torch.is_storage.html#torch.is_storage" title="torch.is_storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_storage</span></code></a></p></td>
<td><p>Returns True if <cite>obj</cite> is a PyTorch storage object.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.is_complex"/><a class="reference internal" href="generated/torch.is_complex.html#torch.is_complex" title="torch.is_complex"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_complex</span></code></a></p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a complex data type i.e., one of <code class="docutils literal notranslate"><span class="pre">torch.complex64</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.complex128</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_conj"/><a class="reference internal" href="generated/torch.is_conj.html#torch.is_conj" title="torch.is_conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_conj</span></code></a></p></td>
<td><p>Returns True if the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a conjugated tensor, i.e. its conjugate bit is set to <cite>True</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.is_floating_point"/><a class="reference internal" href="generated/torch.is_floating_point.html#torch.is_floating_point" title="torch.is_floating_point"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_floating_point</span></code></a></p></td>
<td><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a floating point data type i.e., one of <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_nonzero"/><a class="reference internal" href="generated/torch.is_nonzero.html#torch.is_nonzero" title="torch.is_nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_nonzero</span></code></a></p></td>
<td><p>Returns True if the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a single element tensor which is not equal to zero after type conversions.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_default_dtype"/><a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_default_dtype</span></code></a></p></td>
<td><p>Sets the default floating point dtype to <code class="xref py py-attr docutils literal notranslate"><span class="pre">d</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.get_default_dtype"/><a class="reference internal" href="generated/torch.get_default_dtype.html#torch.get_default_dtype" title="torch.get_default_dtype"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_dtype</span></code></a></p></td>
<td><p>Get the current default floating point <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_default_device"/><a class="reference internal" href="generated/torch.set_default_device.html#torch.set_default_device" title="torch.set_default_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_default_device</span></code></a></p></td>
<td><p>Sets the default <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> to be allocated on <code class="docutils literal notranslate"><span class="pre">device</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_default_tensor_type"/><a class="reference internal" href="generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_default_tensor_type</span></code></a></p></td>
<td><p>Sets the default <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> type to floating point tensor type <code class="docutils literal notranslate"><span class="pre">t</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.numel"/><a class="reference internal" href="generated/torch.numel.html#torch.numel" title="torch.numel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numel</span></code></a></p></td>
<td><p>Returns the total number of elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_printoptions"/><a class="reference internal" href="generated/torch.set_printoptions.html#torch.set_printoptions" title="torch.set_printoptions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_printoptions</span></code></a></p></td>
<td><p>Set options for printing.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_flush_denormal"/><a class="reference internal" href="generated/torch.set_flush_denormal.html#torch.set_flush_denormal" title="torch.set_flush_denormal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_flush_denormal</span></code></a></p></td>
<td><p>Disables denormal floating numbers on CPU.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="creation-ops">
<span id="tensor-creation-ops"></span><h3>Creation Ops<a class="headerlink" href="#creation-ops" title="Permalink to this heading">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Random sampling creation ops are listed under <a class="reference internal" href="#random-sampling"><span class="std std-ref">Random sampling</span></a> and
include:
<a class="reference internal" href="generated/torch.rand.html#torch.rand" title="torch.rand"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rand()</span></code></a>
<a class="reference internal" href="generated/torch.rand_like.html#torch.rand_like" title="torch.rand_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rand_like()</span></code></a>
<a class="reference internal" href="generated/torch.randn.html#torch.randn" title="torch.randn"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randn()</span></code></a>
<a class="reference internal" href="generated/torch.randn_like.html#torch.randn_like" title="torch.randn_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randn_like()</span></code></a>
<a class="reference internal" href="generated/torch.randint.html#torch.randint" title="torch.randint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randint()</span></code></a>
<a class="reference internal" href="generated/torch.randint_like.html#torch.randint_like" title="torch.randint_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randint_like()</span></code></a>
<a class="reference internal" href="generated/torch.randperm.html#torch.randperm" title="torch.randperm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randperm()</span></code></a>
You may also use <a class="reference internal" href="generated/torch.empty.html#torch.empty" title="torch.empty"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.empty()</span></code></a> with the <a class="reference internal" href="#inplace-random-sampling"><span class="std std-ref">In-place random sampling</span></a>
methods to create <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> s with values sampled from a broader
range of distributions.</p>
</div>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.tensor"/><a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor</span></code></a></p></td>
<td><p>Constructs a tensor with no autograd history (also known as a &quot;leaf tensor&quot;, see <a class="reference internal" href="notes/autograd.html"><span class="doc">Autograd mechanics</span></a>) by copying <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sparse_coo_tensor"/><a class="reference internal" href="generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor" title="torch.sparse_coo_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_coo_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-coo-docs"><span class="std std-ref">sparse tensor in COO(rdinate) format</span></a> with specified values at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sparse_csr_tensor"/><a class="reference internal" href="generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor" title="torch.sparse_csr_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_csr_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-csr-docs"><span class="std std-ref">sparse tensor in CSR (Compressed Sparse Row)</span></a> with specified values at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">crow_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">col_indices</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sparse_csc_tensor"/><a class="reference internal" href="generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor" title="torch.sparse_csc_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_csc_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-csc-docs"><span class="std std-ref">sparse tensor in CSC (Compressed Sparse Column)</span></a> with specified values at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">ccol_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">row_indices</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sparse_bsr_tensor"/><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor" title="torch.sparse_bsr_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_bsr_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-bsr-docs"><span class="std std-ref">sparse tensor in BSR (Block Compressed Sparse Row))</span></a> with specified 2-dimensional blocks at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">crow_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">col_indices</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sparse_bsc_tensor"/><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor" title="torch.sparse_bsc_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparse_bsc_tensor</span></code></a></p></td>
<td><p>Constructs a <a class="reference internal" href="sparse.html#sparse-bsc-docs"><span class="std std-ref">sparse tensor in BSC (Block Compressed Sparse Column))</span></a> with specified 2-dimensional blocks at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">ccol_indices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">row_indices</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.asarray"/><a class="reference internal" href="generated/torch.asarray.html#torch.asarray" title="torch.asarray"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asarray</span></code></a></p></td>
<td><p>Converts <code class="xref py py-attr docutils literal notranslate"><span class="pre">obj</span></code> to a tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.as_tensor"/><a class="reference internal" href="generated/torch.as_tensor.html#torch.as_tensor" title="torch.as_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_tensor</span></code></a></p></td>
<td><p>Converts <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> into a tensor, sharing data and preserving autograd history if possible.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.as_strided"/><a class="reference internal" href="generated/torch.as_strided.html#torch.as_strided" title="torch.as_strided"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_strided</span></code></a></p></td>
<td><p>Create a view of an existing <cite>torch.Tensor</cite> <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">storage_offset</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.from_numpy"/><a class="reference internal" href="generated/torch.from_numpy.html#torch.from_numpy" title="torch.from_numpy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_numpy</span></code></a></p></td>
<td><p>Creates a <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> from a <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.25)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.from_dlpack"/><a class="reference internal" href="generated/torch.from_dlpack.html#torch.from_dlpack" title="torch.from_dlpack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_dlpack</span></code></a></p></td>
<td><p>Converts a tensor from an external library into a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.frombuffer"/><a class="reference internal" href="generated/torch.frombuffer.html#torch.frombuffer" title="torch.frombuffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frombuffer</span></code></a></p></td>
<td><p>Creates a 1-dimensional <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> from an object that implements the Python buffer protocol.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.zeros"/><a class="reference internal" href="generated/torch.zeros.html#torch.zeros" title="torch.zeros"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zeros</span></code></a></p></td>
<td><p>Returns a tensor filled with the scalar value <cite>0</cite>, with the shape defined by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.zeros_like"/><a class="reference internal" href="generated/torch.zeros_like.html#torch.zeros_like" title="torch.zeros_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zeros_like</span></code></a></p></td>
<td><p>Returns a tensor filled with the scalar value <cite>0</cite>, with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ones"/><a class="reference internal" href="generated/torch.ones.html#torch.ones" title="torch.ones"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ones</span></code></a></p></td>
<td><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the shape defined by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ones_like"/><a class="reference internal" href="generated/torch.ones_like.html#torch.ones_like" title="torch.ones_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ones_like</span></code></a></p></td>
<td><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.arange"/><a class="reference internal" href="generated/torch.arange.html#torch.arange" title="torch.arange"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arange</span></code></a></p></td>
<td><p>Returns a 1-D tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">⌈</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence="true">⌉</mo></mrow><annotation encoding="application/x-tex">\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">⌈</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">step</span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">end</span></span><span class="mbin mtight">−</span><span class="mord text mtight"><span class="mord mtight">start</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">⌉</span></span></span></span></span></span></span> with values from the interval <code class="docutils literal notranslate"><span class="pre">[start,</span> <span class="pre">end)</span></code> taken with common difference <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code> beginning from <cite>start</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.range"/><a class="reference internal" href="generated/torch.range.html#torch.range" title="torch.range"><code class="xref py py-obj docutils literal notranslate"><span class="pre">range</span></code></a></p></td>
<td><p>Returns a 1-D tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo fence="true">⌊</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">step</span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">end</span></span><span class="mbin mtight">−</span><span class="mord text mtight"><span class="mord mtight">start</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">⌋</span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span> with values from <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code> with step <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.linspace"/><a class="reference internal" href="generated/torch.linspace.html#torch.linspace" title="torch.linspace"><code class="xref py py-obj docutils literal notranslate"><span class="pre">linspace</span></code></a></p></td>
<td><p>Creates a one-dimensional tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">steps</span></code> whose values are evenly spaced from <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code>, inclusive.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logspace"/><a class="reference internal" href="generated/torch.logspace.html#torch.logspace" title="torch.logspace"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logspace</span></code></a></p></td>
<td><p>Creates a one-dimensional tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">steps</span></code> whose values are evenly spaced from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>base</mtext><mtext>start</mtext></msup></mrow><annotation encoding="application/x-tex">{{\text{{base}}}}^{{\text{{start}}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8779em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord text"><span class="mord"><span class="mord">base</span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8779em;"><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight"><span class="mord mtight">start</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> to <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>base</mtext><mtext>end</mtext></msup></mrow><annotation encoding="application/x-tex">{{\text{{base}}}}^{{\text{{end}}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9334em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord text"><span class="mord"><span class="mord">base</span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9334em;"><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight"><span class="mord mtight">end</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, inclusive, on a logarithmic scale with base <code class="xref py py-attr docutils literal notranslate"><span class="pre">base</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.eye"/><a class="reference internal" href="generated/torch.eye.html#torch.eye" title="torch.eye"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eye</span></code></a></p></td>
<td><p>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.empty"/><a class="reference internal" href="generated/torch.empty.html#torch.empty" title="torch.empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">empty</span></code></a></p></td>
<td><p>Returns a tensor filled with uninitialized data.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.empty_like"/><a class="reference internal" href="generated/torch.empty_like.html#torch.empty_like" title="torch.empty_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">empty_like</span></code></a></p></td>
<td><p>Returns an uninitialized tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.empty_strided"/><a class="reference internal" href="generated/torch.empty_strided.html#torch.empty_strided" title="torch.empty_strided"><code class="xref py py-obj docutils literal notranslate"><span class="pre">empty_strided</span></code></a></p></td>
<td><p>Creates a tensor with the specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and filled with undefined data.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.full"/><a class="reference internal" href="generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-obj docutils literal notranslate"><span class="pre">full</span></code></a></p></td>
<td><p>Creates a tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.full_like"/><a class="reference internal" href="generated/torch.full_like.html#torch.full_like" title="torch.full_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">full_like</span></code></a></p></td>
<td><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantize_per_tensor"/><a class="reference internal" href="generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor" title="torch.quantize_per_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_tensor</span></code></a></p></td>
<td><p>Converts a float tensor to a quantized tensor with given scale and zero point.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantize_per_channel"/><a class="reference internal" href="generated/torch.quantize_per_channel.html#torch.quantize_per_channel" title="torch.quantize_per_channel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_channel</span></code></a></p></td>
<td><p>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.dequantize"/><a class="reference internal" href="generated/torch.dequantize.html#torch.dequantize" title="torch.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></a></p></td>
<td><p>Returns an fp32 Tensor by dequantizing a quantized Tensor</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.complex"/><a class="reference internal" href="generated/torch.complex.html#torch.complex" title="torch.complex"><code class="xref py py-obj docutils literal notranslate"><span class="pre">complex</span></code></a></p></td>
<td><p>Constructs a complex tensor with its real part equal to <a class="reference internal" href="generated/torch.real.html#torch.real" title="torch.real"><code class="xref py py-attr docutils literal notranslate"><span class="pre">real</span></code></a> and its imaginary part equal to <a class="reference internal" href="generated/torch.imag.html#torch.imag" title="torch.imag"><code class="xref py py-attr docutils literal notranslate"><span class="pre">imag</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.polar"/><a class="reference internal" href="generated/torch.polar.html#torch.polar" title="torch.polar"><code class="xref py py-obj docutils literal notranslate"><span class="pre">polar</span></code></a></p></td>
<td><p>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">abs</span></code></a> and angle <a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-attr docutils literal notranslate"><span class="pre">angle</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.heaviside"/><a class="reference internal" href="generated/torch.heaviside.html#torch.heaviside" title="torch.heaviside"><code class="xref py py-obj docutils literal notranslate"><span class="pre">heaviside</span></code></a></p></td>
<td><p>Computes the Heaviside step function for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="indexing-slicing-joining-mutating-ops">
<span id="indexing-slicing-joining"></span><h3>Indexing, Slicing, Joining, Mutating Ops<a class="headerlink" href="#indexing-slicing-joining-mutating-ops" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.adjoint"/><a class="reference internal" href="generated/torch.adjoint.html#torch.adjoint" title="torch.adjoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjoint</span></code></a></p></td>
<td><p>Returns a view of the tensor conjugated and with the last two dimensions transposed.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.argwhere"/><a class="reference internal" href="generated/torch.argwhere.html#torch.argwhere" title="torch.argwhere"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argwhere</span></code></a></p></td>
<td><p>Returns a tensor containing the indices of all non-zero elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cat"/><a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cat</span></code></a></p></td>
<td><p>Concatenates the given sequence of <code class="xref py py-attr docutils literal notranslate"><span class="pre">seq</span></code> tensors in the given dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.concat"/><a class="reference internal" href="generated/torch.concat.html#torch.concat" title="torch.concat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">concat</span></code></a></p></td>
<td><p>Alias of <a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cat()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.concatenate"/><a class="reference internal" href="generated/torch.concatenate.html#torch.concatenate" title="torch.concatenate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">concatenate</span></code></a></p></td>
<td><p>Alias of <a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cat()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.conj"/><a class="reference internal" href="generated/torch.conj.html#torch.conj" title="torch.conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conj</span></code></a></p></td>
<td><p>Returns a view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with a flipped conjugate bit.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.chunk"/><a class="reference internal" href="generated/torch.chunk.html#torch.chunk" title="torch.chunk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">chunk</span></code></a></p></td>
<td><p>Attempts to split a tensor into the specified number of chunks.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.dsplit"/><a class="reference internal" href="generated/torch.dsplit.html#torch.dsplit" title="torch.dsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with three or more dimensions, into multiple tensors depthwise according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.column_stack"/><a class="reference internal" href="generated/torch.column_stack.html#torch.column_stack" title="torch.column_stack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">column_stack</span></code></a></p></td>
<td><p>Creates a new tensor by horizontally stacking the tensors in <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensors</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.dstack"/><a class="reference internal" href="generated/torch.dstack.html#torch.dstack" title="torch.dstack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dstack</span></code></a></p></td>
<td><p>Stack tensors in sequence depthwise (along third axis).</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.gather"/><a class="reference internal" href="generated/torch.gather.html#torch.gather" title="torch.gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gather</span></code></a></p></td>
<td><p>Gathers values along an axis specified by <cite>dim</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.hsplit"/><a class="reference internal" href="generated/torch.hsplit.html#torch.hsplit" title="torch.hsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with one or more dimensions, into multiple tensors horizontally according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.hstack"/><a class="reference internal" href="generated/torch.hstack.html#torch.hstack" title="torch.hstack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hstack</span></code></a></p></td>
<td><p>Stack tensors in sequence horizontally (column wise).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.index_add"/><a class="reference internal" href="generated/torch.index_add.html#torch.index_add" title="torch.index_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_add</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_add_()</span></code></a> for function description.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.index_copy"/><a class="reference internal" href="generated/torch.index_copy.html#torch.index_copy" title="torch.index_copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_copy</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_add_()</span></code></a> for function description.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.index_reduce"/><a class="reference internal" href="generated/torch.index_reduce.html#torch.index_reduce" title="torch.index_reduce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_reduce</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_" title="torch.Tensor.index_reduce_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">index_reduce_()</span></code></a> for function description.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.index_select"/><a class="reference internal" href="generated/torch.index_select.html#torch.index_select" title="torch.index_select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">index_select</span></code></a></p></td>
<td><p>Returns a new tensor which indexes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> using the entries in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> which is a <cite>LongTensor</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.masked_select"/><a class="reference internal" href="generated/torch.masked_select.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">masked_select</span></code></a></p></td>
<td><p>Returns a new 1-D tensor which indexes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor according to the boolean mask <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> which is a <cite>BoolTensor</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.movedim"/><a class="reference internal" href="generated/torch.movedim.html#torch.movedim" title="torch.movedim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">movedim</span></code></a></p></td>
<td><p>Moves the dimension(s) of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the position(s) in <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> to the position(s) in <code class="xref py py-attr docutils literal notranslate"><span class="pre">destination</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.moveaxis"/><a class="reference internal" href="generated/torch.moveaxis.html#torch.moveaxis" title="torch.moveaxis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">moveaxis</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.movedim.html#torch.movedim" title="torch.movedim"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.movedim()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.narrow"/><a class="reference internal" href="generated/torch.narrow.html#torch.narrow" title="torch.narrow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">narrow</span></code></a></p></td>
<td><p>Returns a new tensor that is a narrowed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.narrow_copy"/><a class="reference internal" href="generated/torch.narrow_copy.html#torch.narrow_copy" title="torch.narrow_copy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">narrow_copy</span></code></a></p></td>
<td><p>Same as <a class="reference internal" href="generated/torch.Tensor.narrow.html#torch.Tensor.narrow" title="torch.Tensor.narrow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.narrow()</span></code></a> except this returns a copy rather than shared storage.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nonzero"/><a class="reference internal" href="generated/torch.nonzero.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nonzero</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.permute"/><a class="reference internal" href="generated/torch.permute.html#torch.permute" title="torch.permute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">permute</span></code></a></p></td>
<td><p>Returns a view of the original tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with its dimensions permuted.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.reshape"/><a class="reference internal" href="generated/torch.reshape.html#torch.reshape" title="torch.reshape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reshape</span></code></a></p></td>
<td><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, but with the specified shape.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.row_stack"/><a class="reference internal" href="generated/torch.row_stack.html#torch.row_stack" title="torch.row_stack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">row_stack</span></code></a></p></td>
<td><p>Alias of <a class="reference internal" href="generated/torch.vstack.html#torch.vstack" title="torch.vstack"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vstack()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.select"/><a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select</span></code></a></p></td>
<td><p>Slices the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along the selected dimension at the given index.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.scatter"/><a class="reference internal" href="generated/torch.scatter.html#torch.scatter" title="torch.scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.diagonal_scatter"/><a class="reference internal" href="generated/torch.diagonal_scatter.html#torch.diagonal_scatter" title="torch.diagonal_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagonal_scatter</span></code></a></p></td>
<td><p>Embeds the values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor into <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along the diagonal elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, with respect to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.select_scatter"/><a class="reference internal" href="generated/torch.select_scatter.html#torch.select_scatter" title="torch.select_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select_scatter</span></code></a></p></td>
<td><p>Embeds the values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor into <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the given index.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.slice_scatter"/><a class="reference internal" href="generated/torch.slice_scatter.html#torch.slice_scatter" title="torch.slice_scatter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">slice_scatter</span></code></a></p></td>
<td><p>Embeds the values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor into <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the given dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.scatter_add"/><a class="reference internal" href="generated/torch.scatter_add.html#torch.scatter_add" title="torch.scatter_add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_add</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_add_()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.scatter_reduce"/><a class="reference internal" href="generated/torch.scatter_reduce.html#torch.scatter_reduce" title="torch.scatter_reduce"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_reduce</span></code></a></p></td>
<td><p>Out-of-place version of <a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" title="torch.Tensor.scatter_reduce_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_reduce_()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.split"/><a class="reference internal" href="generated/torch.split.html#torch.split" title="torch.split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">split</span></code></a></p></td>
<td><p>Splits the tensor into chunks.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.squeeze"/><a class="reference internal" href="generated/torch.squeeze.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">squeeze</span></code></a></p></td>
<td><p>Returns a tensor with all specified dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of size <cite>1</cite> removed.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.stack"/><a class="reference internal" href="generated/torch.stack.html#torch.stack" title="torch.stack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stack</span></code></a></p></td>
<td><p>Concatenates a sequence of tensors along a new dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.swapaxes"/><a class="reference internal" href="generated/torch.swapaxes.html#torch.swapaxes" title="torch.swapaxes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swapaxes</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.swapdims"/><a class="reference internal" href="generated/torch.swapdims.html#torch.swapdims" title="torch.swapdims"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swapdims</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.t"/><a class="reference internal" href="generated/torch.t.html#torch.t" title="torch.t"><code class="xref py py-obj docutils literal notranslate"><span class="pre">t</span></code></a></p></td>
<td><p>Expects <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to be &lt;= 2-D tensor and transposes dimensions 0 and 1.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.take"/><a class="reference internal" href="generated/torch.take.html#torch.take" title="torch.take"><code class="xref py py-obj docutils literal notranslate"><span class="pre">take</span></code></a></p></td>
<td><p>Returns a new tensor with the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the given indices.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.take_along_dim"/><a class="reference internal" href="generated/torch.take_along_dim.html#torch.take_along_dim" title="torch.take_along_dim"><code class="xref py py-obj docutils literal notranslate"><span class="pre">take_along_dim</span></code></a></p></td>
<td><p>Selects values from <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the 1-dimensional indices from <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code> along the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.tensor_split"/><a class="reference internal" href="generated/torch.tensor_split.html#torch.tensor_split" title="torch.tensor_split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor_split</span></code></a></p></td>
<td><p>Splits a tensor into multiple sub-tensors, all of which are views of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> according to the indices or number of sections specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tile"/><a class="reference internal" href="generated/torch.tile.html#torch.tile" title="torch.tile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tile</span></code></a></p></td>
<td><p>Constructs a tensor by repeating the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.transpose"/><a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transpose</span></code></a></p></td>
<td><p>Returns a tensor that is a transposed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.unbind"/><a class="reference internal" href="generated/torch.unbind.html#torch.unbind" title="torch.unbind"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unbind</span></code></a></p></td>
<td><p>Removes a tensor dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.unsqueeze"/><a class="reference internal" href="generated/torch.unsqueeze.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unsqueeze</span></code></a></p></td>
<td><p>Returns a new tensor with a dimension of size one inserted at the specified position.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.vsplit"/><a class="reference internal" href="generated/torch.vsplit.html#torch.vsplit" title="torch.vsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with two or more dimensions, into multiple tensors vertically according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.vstack"/><a class="reference internal" href="generated/torch.vstack.html#torch.vstack" title="torch.vstack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vstack</span></code></a></p></td>
<td><p>Stack tensors in sequence vertically (row wise).</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.where"/><a class="reference internal" href="generated/torch.where.html#torch.where" title="torch.where"><code class="xref py py-obj docutils literal notranslate"><span class="pre">where</span></code></a></p></td>
<td><p>Return a tensor of elements selected from either <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> or <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, depending on <code class="xref py py-attr docutils literal notranslate"><span class="pre">condition</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="generators">
<span id="id1"></span><h2>Generators<a class="headerlink" href="#generators" title="Permalink to this heading">¶</a></h2>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.Generator"/><a class="reference internal" href="generated/torch.Generator.html#torch.Generator" title="torch.Generator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Generator</span></code></a></p></td>
<td><p>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="random-sampling">
<span id="id2"></span><h2>Random sampling<a class="headerlink" href="#random-sampling" title="Permalink to this heading">¶</a></h2>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.seed"/><a class="reference internal" href="generated/torch.seed.html#torch.seed" title="torch.seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">seed</span></code></a></p></td>
<td><p>Sets the seed for generating random numbers to a non-deterministic random number.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.manual_seed"/><a class="reference internal" href="generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_seed</span></code></a></p></td>
<td><p>Sets the seed for generating random numbers.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.initial_seed"/><a class="reference internal" href="generated/torch.initial_seed.html#torch.initial_seed" title="torch.initial_seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">initial_seed</span></code></a></p></td>
<td><p>Returns the initial seed for generating random numbers as a Python <cite>long</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.get_rng_state"/><a class="reference internal" href="generated/torch.get_rng_state.html#torch.get_rng_state" title="torch.get_rng_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_rng_state</span></code></a></p></td>
<td><p>Returns the random number generator state as a <cite>torch.ByteTensor</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_rng_state"/><a class="reference internal" href="generated/torch.set_rng_state.html#torch.set_rng_state" title="torch.set_rng_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_rng_state</span></code></a></p></td>
<td><p>Sets the random number generator state.</p></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.torch.default_generator">
<span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">default_generator</span></span><em class="property"><span class="w"> </span><span class="pre">Returns</span> <span class="pre">the</span> <span class="pre">default</span> <span class="pre">CPU</span> <span class="pre">torch.Generator</span></em><a class="headerlink" href="#torch.torch.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.bernoulli"/><a class="reference internal" href="generated/torch.bernoulli.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bernoulli</span></code></a></p></td>
<td><p>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.multinomial"/><a class="reference internal" href="generated/torch.multinomial.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multinomial</span></code></a></p></td>
<td><p>Returns a tensor where each row contains <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_samples</span></code> indices sampled from the multinomial probability distribution located in the corresponding row of tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.normal"/><a class="reference internal" href="generated/torch.normal.html#torch.normal" title="torch.normal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">normal</span></code></a></p></td>
<td><p>Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.poisson"/><a class="reference internal" href="generated/torch.poisson.html#torch.poisson" title="torch.poisson"><code class="xref py py-obj docutils literal notranslate"><span class="pre">poisson</span></code></a></p></td>
<td><p>Returns a tensor of the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> i.e.,</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.rand"/><a class="reference internal" href="generated/torch.rand.html#torch.rand" title="torch.rand"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rand</span></code></a></p></td>
<td><p>Returns a tensor filled with random numbers from a uniform distribution on the interval <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.rand_like"/><a class="reference internal" href="generated/torch.rand_like.html#torch.rand_like" title="torch.rand_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rand_like</span></code></a></p></td>
<td><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> that is filled with random numbers from a uniform distribution on the interval <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[0, 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.randint"/><a class="reference internal" href="generated/torch.randint.html#torch.randint" title="torch.randint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randint</span></code></a></p></td>
<td><p>Returns a tensor filled with random integers generated uniformly between <code class="xref py py-attr docutils literal notranslate"><span class="pre">low</span></code> (inclusive) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">high</span></code> (exclusive).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.randint_like"/><a class="reference internal" href="generated/torch.randint_like.html#torch.randint_like" title="torch.randint_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randint_like</span></code></a></p></td>
<td><p>Returns a tensor with the same shape as Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> filled with random integers generated uniformly between <code class="xref py py-attr docutils literal notranslate"><span class="pre">low</span></code> (inclusive) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">high</span></code> (exclusive).</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.randn"/><a class="reference internal" href="generated/torch.randn.html#torch.randn" title="torch.randn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randn</span></code></a></p></td>
<td><p>Returns a tensor filled with random numbers from a normal distribution with mean <cite>0</cite> and variance <cite>1</cite> (also called the standard normal distribution).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.randn_like"/><a class="reference internal" href="generated/torch.randn_like.html#torch.randn_like" title="torch.randn_like"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randn_like</span></code></a></p></td>
<td><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> that is filled with random numbers from a normal distribution with mean 0 and variance 1.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.randperm"/><a class="reference internal" href="generated/torch.randperm.html#torch.randperm" title="torch.randperm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">randperm</span></code></a></p></td>
<td><p>Returns a random permutation of integers from <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">-</span> <span class="pre">1</span></code>.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="in-place-random-sampling">
<span id="inplace-random-sampling"></span><h3>In-place random sampling<a class="headerlink" href="#in-place-random-sampling" title="Permalink to this heading">¶</a></h3>
<p>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</p>
<ul class="simple">
<li><p><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.bernoulli_()</span></code></a> - in-place version of <a class="reference internal" href="generated/torch.bernoulli.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.cauchy_()</span></code></a> - numbers drawn from the Cauchy distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.exponential_()</span></code></a> - numbers drawn from the exponential distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.geometric_()</span></code></a> - elements drawn from the geometric distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.log_normal_()</span></code></a> - samples from the log-normal distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.normal_.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.normal_()</span></code></a> - in-place version of <a class="reference internal" href="generated/torch.normal.html#torch.normal" title="torch.normal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.normal()</span></code></a></p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.random_.html#torch.Tensor.random_" title="torch.Tensor.random_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.random_()</span></code></a> - numbers sampled from the discrete uniform distribution</p></li>
<li><p><a class="reference internal" href="generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.uniform_()</span></code></a> - numbers sampled from the continuous uniform distribution</p></li>
</ul>
</div>
<div class="section" id="quasi-random-sampling">
<h3>Quasi-random sampling<a class="headerlink" href="#quasi-random-sampling" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quasirandom.SobolEngine</span></code></a></p></td>
<td><p>The <a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.quasirandom.SobolEngine</span></code></a> is an engine for generating (scrambled) Sobol sequences.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="serialization">
<h2>Serialization<a class="headerlink" href="#serialization" title="Permalink to this heading">¶</a></h2>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.save"/><a class="reference internal" href="generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save</span></code></a></p></td>
<td><p>Saves an object to a disk file.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.load"/><a class="reference internal" href="generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load</span></code></a></p></td>
<td><p>Loads an object saved with <a class="reference internal" href="generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> from a file.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="parallelism">
<h2>Parallelism<a class="headerlink" href="#parallelism" title="Permalink to this heading">¶</a></h2>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.get_num_threads"/><a class="reference internal" href="generated/torch.get_num_threads.html#torch.get_num_threads" title="torch.get_num_threads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_num_threads</span></code></a></p></td>
<td><p>Returns the number of threads used for parallelizing CPU operations</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_num_threads"/><a class="reference internal" href="generated/torch.set_num_threads.html#torch.set_num_threads" title="torch.set_num_threads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_num_threads</span></code></a></p></td>
<td><p>Sets the number of threads used for intraop parallelism on CPU.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.get_num_interop_threads"/><a class="reference internal" href="generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads" title="torch.get_num_interop_threads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_num_interop_threads</span></code></a></p></td>
<td><p>Returns the number of threads used for inter-op parallelism on CPU (e.g.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_num_interop_threads"/><a class="reference internal" href="generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads" title="torch.set_num_interop_threads"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_num_interop_threads</span></code></a></p></td>
<td><p>Sets the number of threads used for interop parallelism (e.g.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="locally-disabling-gradient-computation">
<span id="torch-rst-local-disable-grad"></span><h2>Locally disabling gradient computation<a class="headerlink" href="#locally-disabling-gradient-computation" title="Permalink to this heading">¶</a></h2>
<p>The context managers <a class="reference internal" href="generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.no_grad()</span></code></a>, <a class="reference internal" href="generated/torch.enable_grad.html#torch.enable_grad" title="torch.enable_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.enable_grad()</span></code></a>, and
<a class="reference internal" href="generated/torch.set_grad_enabled.html#torch.set_grad_enabled" title="torch.set_grad_enabled"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_grad_enabled()</span></code></a> are helpful for locally disabling and enabling
gradient computation. See <a class="reference internal" href="autograd.html#locally-disable-grad"><span class="std std-ref">Locally disabling gradient computation</span></a> for more details on
their usage.  These context managers are thread local, so they won’t
work if you send work to another thread using the <code class="docutils literal notranslate"><span class="pre">threading</span></code> module, etc.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># this can also be used as a function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
</pre></div>
</div>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.no_grad"/><a class="reference internal" href="generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">no_grad</span></code></a></p></td>
<td><p>Context-manager that disabled gradient calculation.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.enable_grad"/><a class="reference internal" href="generated/torch.enable_grad.html#torch.enable_grad" title="torch.enable_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_grad</span></code></a></p></td>
<td><p>Context-manager that enables gradient calculation.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.set_grad_enabled"/><a class="reference internal" href="generated/torch.set_grad_enabled.html#torch.set_grad_enabled" title="torch.set_grad_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_grad_enabled</span></code></a></p></td>
<td><p>Context-manager that sets gradient calculation on or off.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_grad_enabled"/><a class="reference internal" href="generated/torch.is_grad_enabled.html#torch.is_grad_enabled" title="torch.is_grad_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_grad_enabled</span></code></a></p></td>
<td><p>Returns True if grad mode is currently enabled.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.inference_mode"/><a class="reference internal" href="generated/torch.inference_mode.html#torch.inference_mode" title="torch.inference_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inference_mode</span></code></a></p></td>
<td><p>Context-manager that enables or disables inference mode</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.is_inference_mode_enabled"/><a class="reference internal" href="generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled" title="torch.is_inference_mode_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_inference_mode_enabled</span></code></a></p></td>
<td><p>Returns True if inference mode is currently enabled.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="math-operations">
<h2>Math operations<a class="headerlink" href="#math-operations" title="Permalink to this heading">¶</a></h2>
<div class="section" id="pointwise-ops">
<h3>Pointwise Ops<a class="headerlink" href="#pointwise-ops" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.abs"/><a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">abs</span></code></a></p></td>
<td><p>Computes the absolute value of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.absolute"/><a class="reference internal" href="generated/torch.absolute.html#torch.absolute" title="torch.absolute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">absolute</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.acos"/><a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acos</span></code></a></p></td>
<td><p>Computes the inverse cosine of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arccos"/><a class="reference internal" href="generated/torch.arccos.html#torch.arccos" title="torch.arccos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccos</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.acosh"/><a class="reference internal" href="generated/torch.acosh.html#torch.acosh" title="torch.acosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acosh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic cosine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arccosh"/><a class="reference internal" href="generated/torch.arccosh.html#torch.arccosh" title="torch.arccosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccosh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.acosh.html#torch.acosh" title="torch.acosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acosh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.add"/><a class="reference internal" href="generated/torch.add.html#torch.add" title="torch.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add</span></code></a></p></td>
<td><p>Adds <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, scaled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>, to <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.addcdiv"/><a class="reference internal" href="generated/torch.addcdiv.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addcdiv</span></code></a></p></td>
<td><p>Performs the element-wise division of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code>, multiplies the result by the scalar <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and adds it to <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.addcmul"/><a class="reference internal" href="generated/torch.addcmul.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addcmul</span></code></a></p></td>
<td><p>Performs the element-wise multiplication of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code>, multiplies the result by the scalar <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and adds it to <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.angle"/><a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">angle</span></code></a></p></td>
<td><p>Computes the element-wise angle (in radians) of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.asin"/><a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asin</span></code></a></p></td>
<td><p>Returns a new tensor with the arcsine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arcsin"/><a class="reference internal" href="generated/torch.arcsin.html#torch.arcsin" title="torch.arcsin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsin</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.asinh"/><a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asinh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arcsinh"/><a class="reference internal" href="generated/torch.arcsinh.html#torch.arcsinh" title="torch.arcsinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsinh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asinh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.atan"/><a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan</span></code></a></p></td>
<td><p>Returns a new tensor with the arctangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arctan"/><a class="reference internal" href="generated/torch.arctan.html#torch.arctan" title="torch.arctan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.atanh"/><a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atanh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arctanh"/><a class="reference internal" href="generated/torch.arctanh.html#torch.arctanh" title="torch.arctanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctanh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atanh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.atan2"/><a class="reference internal" href="generated/torch.atan2.html#torch.atan2" title="torch.atan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan2</span></code></a></p></td>
<td><p>Element-wise arctangent of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>input</mtext><mi>i</mi></msub><mi mathvariant="normal">/</mi><msub><mtext>other</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{input}_{i} / \text{other}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord text"><span class="mord">input</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2175em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord text"><span class="mord">other</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> with consideration of the quadrant.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arctan2"/><a class="reference internal" href="generated/torch.arctan2.html#torch.arctan2" title="torch.arctan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan2</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atan2.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bitwise_not"/><a class="reference internal" href="generated/torch.bitwise_not.html#torch.bitwise_not" title="torch.bitwise_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_not</span></code></a></p></td>
<td><p>Computes the bitwise NOT of the given input tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_and"/><a class="reference internal" href="generated/torch.bitwise_and.html#torch.bitwise_and" title="torch.bitwise_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_and</span></code></a></p></td>
<td><p>Computes the bitwise AND of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bitwise_or"/><a class="reference internal" href="generated/torch.bitwise_or.html#torch.bitwise_or" title="torch.bitwise_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_or</span></code></a></p></td>
<td><p>Computes the bitwise OR of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_xor"/><a class="reference internal" href="generated/torch.bitwise_xor.html#torch.bitwise_xor" title="torch.bitwise_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_xor</span></code></a></p></td>
<td><p>Computes the bitwise XOR of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bitwise_left_shift"/><a class="reference internal" href="generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift" title="torch.bitwise_left_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_left_shift</span></code></a></p></td>
<td><p>Computes the left arithmetic shift of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> bits.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_right_shift"/><a class="reference internal" href="generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift" title="torch.bitwise_right_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_right_shift</span></code></a></p></td>
<td><p>Computes the right arithmetic shift of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> bits.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ceil"/><a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ceil</span></code></a></p></td>
<td><p>Returns a new tensor with the ceil of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the smallest integer greater than or equal to each element.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.clamp"/><a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp</span></code></a></p></td>
<td><p>Clamps all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into the range <cite>[</cite> <a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-attr docutils literal notranslate"><span class="pre">min</span></code></a>, <a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-attr docutils literal notranslate"><span class="pre">max</span></code></a> <cite>]</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.clip"/><a class="reference internal" href="generated/torch.clip.html#torch.clip" title="torch.clip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.conj_physical"/><a class="reference internal" href="generated/torch.conj_physical.html#torch.conj_physical" title="torch.conj_physical"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conj_physical</span></code></a></p></td>
<td><p>Computes the element-wise conjugate of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.copysign"/><a class="reference internal" href="generated/torch.copysign.html#torch.copysign" title="torch.copysign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copysign</span></code></a></p></td>
<td><p>Create a new floating-point tensor with the magnitude of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and the sign of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, elementwise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cos"/><a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cos</span></code></a></p></td>
<td><p>Returns a new tensor with the cosine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cosh"/><a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cosh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic cosine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.deg2rad"/><a class="reference internal" href="generated/torch.deg2rad.html#torch.deg2rad" title="torch.deg2rad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deg2rad</span></code></a></p></td>
<td><p>Returns a new tensor with each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> converted from angles in degrees to radians.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.div"/><a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-obj docutils literal notranslate"><span class="pre">div</span></code></a></p></td>
<td><p>Divides each element of the input <code class="docutils literal notranslate"><span class="pre">input</span></code> by the corresponding element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.divide"/><a class="reference internal" href="generated/torch.divide.html#torch.divide" title="torch.divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">divide</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.digamma"/><a class="reference internal" href="generated/torch.digamma.html#torch.digamma" title="torch.digamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">digamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.digamma" title="torch.special.digamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.digamma()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.erf"/><a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erf</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erf" title="torch.special.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erf()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.erfc"/><a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfc</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erfc" title="torch.special.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erfc()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.erfinv"/><a class="reference internal" href="generated/torch.erfinv.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfinv</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erfinv" title="torch.special.erfinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erfinv()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.exp"/><a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp</span></code></a></p></td>
<td><p>Returns a new tensor with the exponential of the elements of the input tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.exp2"/><a class="reference internal" href="generated/torch.exp2.html#torch.exp2" title="torch.exp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp2</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.exp2" title="torch.special.exp2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.exp2()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.expm1"/><a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expm1</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.expm1" title="torch.special.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.expm1()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fake_quantize_per_channel_affine"/><a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine" title="torch.fake_quantize_per_channel_affine"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fake_quantize_per_channel_affine</span></code></a></p></td>
<td><p>Returns a new tensor with the data in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> fake quantized per channel using <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">zero_point</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant_min</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant_max</span></code>, across the channel specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">axis</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.fake_quantize_per_tensor_affine"/><a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine" title="torch.fake_quantize_per_tensor_affine"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fake_quantize_per_tensor_affine</span></code></a></p></td>
<td><p>Returns a new tensor with the data in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> fake quantized using <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">zero_point</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant_min</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant_max</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fix"/><a class="reference internal" href="generated/torch.fix.html#torch.fix" title="torch.fix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fix</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.float_power"/><a class="reference internal" href="generated/torch.float_power.html#torch.float_power" title="torch.float_power"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float_power</span></code></a></p></td>
<td><p>Raises <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the power of <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code>, elementwise, in double precision.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.floor"/><a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor</span></code></a></p></td>
<td><p>Returns a new tensor with the floor of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the largest integer less than or equal to each element.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.floor_divide"/><a class="reference internal" href="generated/torch.floor_divide.html#torch.floor_divide" title="torch.floor_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor_divide</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fmod"/><a class="reference internal" href="generated/torch.fmod.html#torch.fmod" title="torch.fmod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmod</span></code></a></p></td>
<td><p>Applies C++'s <a class="reference external" href="https://en.cppreference.com/w/cpp/numeric/math/fmod">std::fmod</a> entrywise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.frac"/><a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frac</span></code></a></p></td>
<td><p>Computes the fractional portion of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.frexp"/><a class="reference internal" href="generated/torch.frexp.html#torch.frexp" title="torch.frexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frexp</span></code></a></p></td>
<td><p>Decomposes <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into mantissa and exponent tensors such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mtext>mantissa</mtext><mo>×</mo><msup><mn>2</mn><mtext>exponent</mtext></msup></mrow><annotation encoding="application/x-tex">\text{input} = \text{mantissa} \times 2^{\text{exponent}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7512em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">mantissa</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7936em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">exponent</span></span></span></span></span></span></span></span></span></span></span></span></span></span>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.gradient"/><a class="reference internal" href="generated/torch.gradient.html#torch.gradient" title="torch.gradient"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gradient</span></code></a></p></td>
<td><p>Estimates the gradient of a function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">g : \mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6889em;"></span><span class="mord mathbb">R</span></span></span></span></span> in one or more dimensions using the <a class="reference external" href="https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf">second-order accurate central differences method</a> and either first or second order estimates at the boundaries.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.imag"/><a class="reference internal" href="generated/torch.imag.html#torch.imag" title="torch.imag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">imag</span></code></a></p></td>
<td><p>Returns a new tensor containing imaginary values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ldexp"/><a class="reference internal" href="generated/torch.ldexp.html#torch.ldexp" title="torch.ldexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ldexp</span></code></a></p></td>
<td><p>Multiplies <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by 2 ** <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lerp"/><a class="reference internal" href="generated/torch.lerp.html#torch.lerp" title="torch.lerp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lerp</span></code></a></p></td>
<td><p>Does a linear interpolation of two tensors <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> (given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code> based on a scalar or tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> and returns the resulting <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.lgamma"/><a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lgamma</span></code></a></p></td>
<td><p>Computes the natural logarithm of the absolute value of the gamma function on <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.log"/><a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a></p></td>
<td><p>Returns a new tensor with the natural logarithm of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.log10"/><a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log10</span></code></a></p></td>
<td><p>Returns a new tensor with the logarithm to the base 10 of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.log1p"/><a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log1p</span></code></a></p></td>
<td><p>Returns a new tensor with the natural logarithm of (1 + <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>).</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.log2"/><a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log2</span></code></a></p></td>
<td><p>Returns a new tensor with the logarithm to the base 2 of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logaddexp"/><a class="reference internal" href="generated/torch.logaddexp.html#torch.logaddexp" title="torch.logaddexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp</span></code></a></p></td>
<td><p>Logarithm of the sum of exponentiations of the inputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logaddexp2"/><a class="reference internal" href="generated/torch.logaddexp2.html#torch.logaddexp2" title="torch.logaddexp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp2</span></code></a></p></td>
<td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logical_and"/><a class="reference internal" href="generated/torch.logical_and.html#torch.logical_and" title="torch.logical_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_and</span></code></a></p></td>
<td><p>Computes the element-wise logical AND of the given input tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logical_not"/><a class="reference internal" href="generated/torch.logical_not.html#torch.logical_not" title="torch.logical_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_not</span></code></a></p></td>
<td><p>Computes the element-wise logical NOT of the given input tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logical_or"/><a class="reference internal" href="generated/torch.logical_or.html#torch.logical_or" title="torch.logical_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_or</span></code></a></p></td>
<td><p>Computes the element-wise logical OR of the given input tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logical_xor"/><a class="reference internal" href="generated/torch.logical_xor.html#torch.logical_xor" title="torch.logical_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logical_xor</span></code></a></p></td>
<td><p>Computes the element-wise logical XOR of the given input tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logit"/><a class="reference internal" href="generated/torch.logit.html#torch.logit" title="torch.logit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logit</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.logit" title="torch.special.logit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.logit()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.hypot"/><a class="reference internal" href="generated/torch.hypot.html#torch.hypot" title="torch.hypot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hypot</span></code></a></p></td>
<td><p>Given the legs of a right triangle, return its hypotenuse.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.i0"/><a class="reference internal" href="generated/torch.i0.html#torch.i0" title="torch.i0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">i0</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.i0" title="torch.special.i0"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.i0()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.igamma"/><a class="reference internal" href="generated/torch.igamma.html#torch.igamma" title="torch.igamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">igamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.gammainc" title="torch.special.gammainc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.gammainc()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.igammac"/><a class="reference internal" href="generated/torch.igammac.html#torch.igammac" title="torch.igammac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">igammac</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.gammaincc" title="torch.special.gammaincc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.gammaincc()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.mul"/><a class="reference internal" href="generated/torch.mul.html#torch.mul" title="torch.mul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mul</span></code></a></p></td>
<td><p>Multiplies <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.multiply"/><a class="reference internal" href="generated/torch.multiply.html#torch.multiply" title="torch.multiply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiply</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.mul.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.mvlgamma"/><a class="reference internal" href="generated/torch.mvlgamma.html#torch.mvlgamma" title="torch.mvlgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mvlgamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.multigammaln" title="torch.special.multigammaln"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.multigammaln()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nan_to_num"/><a class="reference internal" href="generated/torch.nan_to_num.html#torch.nan_to_num" title="torch.nan_to_num"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nan_to_num</span></code></a></p></td>
<td><p>Replaces <code class="docutils literal notranslate"><span class="pre">NaN</span></code>, positive infinity, and negative infinity values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the values specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">nan</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">posinf</span></code>, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">neginf</span></code>, respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.neg"/><a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">neg</span></code></a></p></td>
<td><p>Returns a new tensor with the negative of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.negative"/><a class="reference internal" href="generated/torch.negative.html#torch.negative" title="torch.negative"><code class="xref py py-obj docutils literal notranslate"><span class="pre">negative</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nextafter"/><a class="reference internal" href="generated/torch.nextafter.html#torch.nextafter" title="torch.nextafter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nextafter</span></code></a></p></td>
<td><p>Return the next floating-point value after <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> towards <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, elementwise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.polygamma"/><a class="reference internal" href="generated/torch.polygamma.html#torch.polygamma" title="torch.polygamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">polygamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.polygamma" title="torch.special.polygamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.polygamma()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.positive"/><a class="reference internal" href="generated/torch.positive.html#torch.positive" title="torch.positive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">positive</span></code></a></p></td>
<td><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.pow"/><a class="reference internal" href="generated/torch.pow.html#torch.pow" title="torch.pow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pow</span></code></a></p></td>
<td><p>Takes the power of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> and returns a tensor with the result.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantized_batch_norm"/><a class="reference internal" href="generated/torch.quantized_batch_norm.html#torch.quantized_batch_norm" title="torch.quantized_batch_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantized_batch_norm</span></code></a></p></td>
<td><p>Applies batch normalization on a 4D (NCHW) quantized tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantized_max_pool1d"/><a class="reference internal" href="generated/torch.quantized_max_pool1d.html#torch.quantized_max_pool1d" title="torch.quantized_max_pool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantized_max_pool1d</span></code></a></p></td>
<td><p>Applies a 1D max pooling over an input quantized tensor composed of several input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.quantized_max_pool2d"/><a class="reference internal" href="generated/torch.quantized_max_pool2d.html#torch.quantized_max_pool2d" title="torch.quantized_max_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantized_max_pool2d</span></code></a></p></td>
<td><p>Applies a 2D max pooling over an input quantized tensor composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.rad2deg"/><a class="reference internal" href="generated/torch.rad2deg.html#torch.rad2deg" title="torch.rad2deg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rad2deg</span></code></a></p></td>
<td><p>Returns a new tensor with each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> converted from angles in radians to degrees.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.real"/><a class="reference internal" href="generated/torch.real.html#torch.real" title="torch.real"><code class="xref py py-obj docutils literal notranslate"><span class="pre">real</span></code></a></p></td>
<td><p>Returns a new tensor containing real values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.reciprocal"/><a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reciprocal</span></code></a></p></td>
<td><p>Returns a new tensor with the reciprocal of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.remainder"/><a class="reference internal" href="generated/torch.remainder.html#torch.remainder" title="torch.remainder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">remainder</span></code></a></p></td>
<td><p>Computes <a class="reference external" href="https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations">Python's modulus operation</a> entrywise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.round"/><a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-obj docutils literal notranslate"><span class="pre">round</span></code></a></p></td>
<td><p>Rounds elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the nearest integer.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.rsqrt"/><a class="reference internal" href="generated/torch.rsqrt.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rsqrt</span></code></a></p></td>
<td><p>Returns a new tensor with the reciprocal of the square-root of each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sigmoid"/><a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sigmoid</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.expit" title="torch.special.expit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.expit()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sign"/><a class="reference internal" href="generated/torch.sign.html#torch.sign" title="torch.sign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sign</span></code></a></p></td>
<td><p>Returns a new tensor with the signs of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sgn"/><a class="reference internal" href="generated/torch.sgn.html#torch.sgn" title="torch.sgn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sgn</span></code></a></p></td>
<td><p>This function is an extension of torch.sign() to complex tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.signbit"/><a class="reference internal" href="generated/torch.signbit.html#torch.signbit" title="torch.signbit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">signbit</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> has its sign bit set or not.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sin"/><a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sin</span></code></a></p></td>
<td><p>Returns a new tensor with the sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sinc"/><a class="reference internal" href="generated/torch.sinc.html#torch.sinc" title="torch.sinc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinc</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.sinc" title="torch.special.sinc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.sinc()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sinh"/><a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.softmax"/><a class="reference internal" href="generated/torch.softmax.html#torch.softmax" title="torch.softmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">softmax</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.softmax()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sqrt"/><a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt</span></code></a></p></td>
<td><p>Returns a new tensor with the square-root of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.square"/><a class="reference internal" href="generated/torch.square.html#torch.square" title="torch.square"><code class="xref py py-obj docutils literal notranslate"><span class="pre">square</span></code></a></p></td>
<td><p>Returns a new tensor with the square of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sub"/><a class="reference internal" href="generated/torch.sub.html#torch.sub" title="torch.sub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sub</span></code></a></p></td>
<td><p>Subtracts <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, scaled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>, from <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.subtract"/><a class="reference internal" href="generated/torch.subtract.html#torch.subtract" title="torch.subtract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">subtract</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.sub.html#torch.sub" title="torch.sub"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sub()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.tan"/><a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tan</span></code></a></p></td>
<td><p>Returns a new tensor with the tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tanh"/><a class="reference internal" href="generated/torch.tanh.html#torch.tanh" title="torch.tanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tanh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.true_divide"/><a class="reference internal" href="generated/torch.true_divide.html#torch.true_divide" title="torch.true_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">true_divide</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">rounding_mode=None</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.trunc"/><a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trunc</span></code></a></p></td>
<td><p>Returns a new tensor with the truncated integer values of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.xlogy"/><a class="reference internal" href="generated/torch.xlogy.html#torch.xlogy" title="torch.xlogy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xlogy</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.xlogy" title="torch.special.xlogy"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.xlogy()</span></code></a>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="reduction-ops">
<h3>Reduction Ops<a class="headerlink" href="#reduction-ops" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.argmax"/><a class="reference internal" href="generated/torch.argmax.html#torch.argmax" title="torch.argmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argmax</span></code></a></p></td>
<td><p>Returns the indices of the maximum value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.argmin"/><a class="reference internal" href="generated/torch.argmin.html#torch.argmin" title="torch.argmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argmin</span></code></a></p></td>
<td><p>Returns the indices of the minimum value(s) of the flattened tensor or along a dimension</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.amax"/><a class="reference internal" href="generated/torch.amax.html#torch.amax" title="torch.amax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">amax</span></code></a></p></td>
<td><p>Returns the maximum value of each slice of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.amin"/><a class="reference internal" href="generated/torch.amin.html#torch.amin" title="torch.amin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">amin</span></code></a></p></td>
<td><p>Returns the minimum value of each slice of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.aminmax"/><a class="reference internal" href="generated/torch.aminmax.html#torch.aminmax" title="torch.aminmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">aminmax</span></code></a></p></td>
<td><p>Computes the minimum and maximum values of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.all"/><a class="reference internal" href="generated/torch.all.html#torch.all" title="torch.all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all</span></code></a></p></td>
<td><p>Tests if all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> evaluate to <cite>True</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.any"/><a class="reference internal" href="generated/torch.any.html#torch.any" title="torch.any"><code class="xref py py-obj docutils literal notranslate"><span class="pre">any</span></code></a></p></td>
<td><p>Tests if any element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> evaluates to <cite>True</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.max"/><a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code></a></p></td>
<td><p>Returns the maximum value of all elements in the <code class="docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.min"/><a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">min</span></code></a></p></td>
<td><p>Returns the minimum value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.dist"/><a class="reference internal" href="generated/torch.dist.html#torch.dist" title="torch.dist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist</span></code></a></p></td>
<td><p>Returns the p-norm of (<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> - <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>)</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logsumexp"/><a class="reference internal" href="generated/torch.logsumexp.html#torch.logsumexp" title="torch.logsumexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logsumexp</span></code></a></p></td>
<td><p>Returns the log of summed exponentials of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.mean"/><a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code></a></p></td>
<td><p>Returns the mean value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nanmean"/><a class="reference internal" href="generated/torch.nanmean.html#torch.nanmean" title="torch.nanmean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanmean</span></code></a></p></td>
<td><p>Computes the mean of all <cite>non-NaN</cite> elements along the specified dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.median"/><a class="reference internal" href="generated/torch.median.html#torch.median" title="torch.median"><code class="xref py py-obj docutils literal notranslate"><span class="pre">median</span></code></a></p></td>
<td><p>Returns the median of the values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nanmedian"/><a class="reference internal" href="generated/torch.nanmedian.html#torch.nanmedian" title="torch.nanmedian"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanmedian</span></code></a></p></td>
<td><p>Returns the median of the values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, ignoring <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.mode"/><a class="reference internal" href="generated/torch.mode.html#torch.mode" title="torch.mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mode</span></code></a></p></td>
<td><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the mode value of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>, i.e. a value which appears most often in that row, and <code class="docutils literal notranslate"><span class="pre">indices</span></code> is the index location of each mode value found.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.norm"/><a class="reference internal" href="generated/torch.norm.html#torch.norm" title="torch.norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code></a></p></td>
<td><p>Returns the matrix norm or vector norm of a given tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nansum"/><a class="reference internal" href="generated/torch.nansum.html#torch.nansum" title="torch.nansum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nansum</span></code></a></p></td>
<td><p>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.prod"/><a class="reference internal" href="generated/torch.prod.html#torch.prod" title="torch.prod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prod</span></code></a></p></td>
<td><p>Returns the product of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantile"/><a class="reference internal" href="generated/torch.quantile.html#torch.quantile" title="torch.quantile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantile</span></code></a></p></td>
<td><p>Computes the q-th quantiles of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nanquantile"/><a class="reference internal" href="generated/torch.nanquantile.html#torch.nanquantile" title="torch.nanquantile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nanquantile</span></code></a></p></td>
<td><p>This is a variant of <a class="reference internal" href="generated/torch.quantile.html#torch.quantile" title="torch.quantile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantile()</span></code></a> that &quot;ignores&quot; <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values, computing the quantiles <code class="xref py py-attr docutils literal notranslate"><span class="pre">q</span></code> as if <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> did not exist.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.std"/><a class="reference internal" href="generated/torch.std.html#torch.std" title="torch.std"><code class="xref py py-obj docutils literal notranslate"><span class="pre">std</span></code></a></p></td>
<td><p>Calculates the standard deviation over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.std_mean"/><a class="reference internal" href="generated/torch.std_mean.html#torch.std_mean" title="torch.std_mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">std_mean</span></code></a></p></td>
<td><p>Calculates the standard deviation and mean over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sum"/><a class="reference internal" href="generated/torch.sum.html#torch.sum" title="torch.sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sum</span></code></a></p></td>
<td><p>Returns the sum of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.unique"/><a class="reference internal" href="generated/torch.unique.html#torch.unique" title="torch.unique"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unique</span></code></a></p></td>
<td><p>Returns the unique elements of the input tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.unique_consecutive"/><a class="reference internal" href="generated/torch.unique_consecutive.html#torch.unique_consecutive" title="torch.unique_consecutive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unique_consecutive</span></code></a></p></td>
<td><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.var"/><a class="reference internal" href="generated/torch.var.html#torch.var" title="torch.var"><code class="xref py py-obj docutils literal notranslate"><span class="pre">var</span></code></a></p></td>
<td><p>Calculates the variance over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.var_mean"/><a class="reference internal" href="generated/torch.var_mean.html#torch.var_mean" title="torch.var_mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">var_mean</span></code></a></p></td>
<td><p>Calculates the variance and mean over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.count_nonzero"/><a class="reference internal" href="generated/torch.count_nonzero.html#torch.count_nonzero" title="torch.count_nonzero"><code class="xref py py-obj docutils literal notranslate"><span class="pre">count_nonzero</span></code></a></p></td>
<td><p>Counts the number of non-zero values in the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="comparison-ops">
<h3>Comparison Ops<a class="headerlink" href="#comparison-ops" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.allclose"/><a class="reference internal" href="generated/torch.allclose.html#torch.allclose" title="torch.allclose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">allclose</span></code></a></p></td>
<td><p>This function checks if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> satisfy the condition:</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.argsort"/><a class="reference internal" href="generated/torch.argsort.html#torch.argsort" title="torch.argsort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argsort</span></code></a></p></td>
<td><p>Returns the indices that sort a tensor along a given dimension in ascending order by value.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.eq"/><a class="reference internal" href="generated/torch.eq.html#torch.eq" title="torch.eq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eq</span></code></a></p></td>
<td><p>Computes element-wise equality</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.equal"/><a class="reference internal" href="generated/torch.equal.html#torch.equal" title="torch.equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">equal</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code> if two tensors have the same size and elements, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ge"/><a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ge</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≥</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \geq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.greater_equal"/><a class="reference internal" href="generated/torch.greater_equal.html#torch.greater_equal" title="torch.greater_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.gt"/><a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gt</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&gt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &gt; \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.greater"/><a class="reference internal" href="generated/torch.greater.html#torch.greater" title="torch.greater"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isclose"/><a class="reference internal" href="generated/torch.isclose.html#torch.isclose" title="torch.isclose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isclose</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is &quot;close&quot; to the corresponding element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isfinite"/><a class="reference internal" href="generated/torch.isfinite.html#torch.isfinite" title="torch.isfinite"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isfinite</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element is <cite>finite</cite> or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isin"/><a class="reference internal" href="generated/torch.isin.html#torch.isin" title="torch.isin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isin</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">elements</span></code> is in <code class="xref py py-attr docutils literal notranslate"><span class="pre">test_elements</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isinf"/><a class="reference internal" href="generated/torch.isinf.html#torch.isinf" title="torch.isinf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isinf</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is infinite (positive or negative infinity) or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isposinf"/><a class="reference internal" href="generated/torch.isposinf.html#torch.isposinf" title="torch.isposinf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isposinf</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is positive infinity or not.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isneginf"/><a class="reference internal" href="generated/torch.isneginf.html#torch.isneginf" title="torch.isneginf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isneginf</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is negative infinity or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isnan"/><a class="reference internal" href="generated/torch.isnan.html#torch.isnan" title="torch.isnan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isnan</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is NaN or not.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isreal"/><a class="reference internal" href="generated/torch.isreal.html#torch.isreal" title="torch.isreal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isreal</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is real-valued or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.kthvalue"/><a class="reference internal" href="generated/torch.kthvalue.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kthvalue</span></code></a></p></td>
<td><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the <code class="xref py py-attr docutils literal notranslate"><span class="pre">k</span></code> th smallest element of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.le"/><a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-obj docutils literal notranslate"><span class="pre">le</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≤</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \leq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.less_equal"/><a class="reference internal" href="generated/torch.less_equal.html#torch.less_equal" title="torch.less_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lt"/><a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lt</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&lt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &lt; \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.less"/><a class="reference internal" href="generated/torch.less.html#torch.less" title="torch.less"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.maximum"/><a class="reference internal" href="generated/torch.maximum.html#torch.maximum" title="torch.maximum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maximum</span></code></a></p></td>
<td><p>Computes the element-wise maximum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.minimum"/><a class="reference internal" href="generated/torch.minimum.html#torch.minimum" title="torch.minimum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">minimum</span></code></a></p></td>
<td><p>Computes the element-wise minimum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fmax"/><a class="reference internal" href="generated/torch.fmax.html#torch.fmax" title="torch.fmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmax</span></code></a></p></td>
<td><p>Computes the element-wise maximum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.fmin"/><a class="reference internal" href="generated/torch.fmin.html#torch.fmin" title="torch.fmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmin</span></code></a></p></td>
<td><p>Computes the element-wise minimum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ne"/><a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ne</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo mathvariant="normal">≠</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \neq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.not_equal"/><a class="reference internal" href="generated/torch.not_equal.html#torch.not_equal" title="torch.not_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">not_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sort"/><a class="reference internal" href="generated/torch.sort.html#torch.sort" title="torch.sort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sort</span></code></a></p></td>
<td><p>Sorts the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along a given dimension in ascending order by value.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.topk"/><a class="reference internal" href="generated/torch.topk.html#torch.topk" title="torch.topk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">topk</span></code></a></p></td>
<td><p>Returns the <code class="xref py py-attr docutils literal notranslate"><span class="pre">k</span></code> largest elements of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along a given dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.msort"/><a class="reference internal" href="generated/torch.msort.html#torch.msort" title="torch.msort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">msort</span></code></a></p></td>
<td><p>Sorts the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along its first dimension in ascending order by value.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="spectral-ops">
<h3>Spectral Ops<a class="headerlink" href="#spectral-ops" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.stft"/><a class="reference internal" href="generated/torch.stft.html#torch.stft" title="torch.stft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stft</span></code></a></p></td>
<td><p>Short-time Fourier transform (STFT).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.istft"/><a class="reference internal" href="generated/torch.istft.html#torch.istft" title="torch.istft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">istft</span></code></a></p></td>
<td><p>Inverse short time Fourier Transform.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bartlett_window"/><a class="reference internal" href="generated/torch.bartlett_window.html#torch.bartlett_window" title="torch.bartlett_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bartlett_window</span></code></a></p></td>
<td><p>Bartlett window function.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.blackman_window"/><a class="reference internal" href="generated/torch.blackman_window.html#torch.blackman_window" title="torch.blackman_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">blackman_window</span></code></a></p></td>
<td><p>Blackman window function.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.hamming_window"/><a class="reference internal" href="generated/torch.hamming_window.html#torch.hamming_window" title="torch.hamming_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hamming_window</span></code></a></p></td>
<td><p>Hamming window function.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.hann_window"/><a class="reference internal" href="generated/torch.hann_window.html#torch.hann_window" title="torch.hann_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hann_window</span></code></a></p></td>
<td><p>Hann window function.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.kaiser_window"/><a class="reference internal" href="generated/torch.kaiser_window.html#torch.kaiser_window" title="torch.kaiser_window"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kaiser_window</span></code></a></p></td>
<td><p>Computes the Kaiser window with window length <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> and shape parameter <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="other-operations">
<h3>Other Operations<a class="headerlink" href="#other-operations" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.atleast_1d"/><a class="reference internal" href="generated/torch.atleast_1d.html#torch.atleast_1d" title="torch.atleast_1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atleast_1d</span></code></a></p></td>
<td><p>Returns a 1-dimensional view of each input tensor with zero dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.atleast_2d"/><a class="reference internal" href="generated/torch.atleast_2d.html#torch.atleast_2d" title="torch.atleast_2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atleast_2d</span></code></a></p></td>
<td><p>Returns a 2-dimensional view of each input tensor with zero dimensions.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.atleast_3d"/><a class="reference internal" href="generated/torch.atleast_3d.html#torch.atleast_3d" title="torch.atleast_3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atleast_3d</span></code></a></p></td>
<td><p>Returns a 3-dimensional view of each input tensor with zero dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bincount"/><a class="reference internal" href="generated/torch.bincount.html#torch.bincount" title="torch.bincount"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bincount</span></code></a></p></td>
<td><p>Count the frequency of each value in an array of non-negative ints.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.block_diag"/><a class="reference internal" href="generated/torch.block_diag.html#torch.block_diag" title="torch.block_diag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">block_diag</span></code></a></p></td>
<td><p>Create a block diagonal matrix from provided tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.broadcast_tensors"/><a class="reference internal" href="generated/torch.broadcast_tensors.html#torch.broadcast_tensors" title="torch.broadcast_tensors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_tensors</span></code></a></p></td>
<td><p>Broadcasts the given tensors according to <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">Broadcasting semantics</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.broadcast_to"/><a class="reference internal" href="generated/torch.broadcast_to.html#torch.broadcast_to" title="torch.broadcast_to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_to</span></code></a></p></td>
<td><p>Broadcasts <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the shape <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.broadcast_shapes"/><a class="reference internal" href="generated/torch.broadcast_shapes.html#torch.broadcast_shapes" title="torch.broadcast_shapes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_shapes</span></code></a></p></td>
<td><p>Similar to <a class="reference internal" href="generated/torch.broadcast_tensors.html#torch.broadcast_tensors" title="torch.broadcast_tensors"><code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast_tensors()</span></code></a> but for shapes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bucketize"/><a class="reference internal" href="generated/torch.bucketize.html#torch.bucketize" title="torch.bucketize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bucketize</span></code></a></p></td>
<td><p>Returns the indices of the buckets to which each value in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> belongs, where the boundaries of the buckets are set by <code class="xref py py-attr docutils literal notranslate"><span class="pre">boundaries</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cartesian_prod"/><a class="reference internal" href="generated/torch.cartesian_prod.html#torch.cartesian_prod" title="torch.cartesian_prod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cartesian_prod</span></code></a></p></td>
<td><p>Do cartesian product of the given sequence of tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cdist"/><a class="reference internal" href="generated/torch.cdist.html#torch.cdist" title="torch.cdist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cdist</span></code></a></p></td>
<td><p>Computes batched the p-norm distance between each pair of the two collections of row vectors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.clone"/><a class="reference internal" href="generated/torch.clone.html#torch.clone" title="torch.clone"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clone</span></code></a></p></td>
<td><p>Returns a copy of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.combinations"/><a class="reference internal" href="generated/torch.combinations.html#torch.combinations" title="torch.combinations"><code class="xref py py-obj docutils literal notranslate"><span class="pre">combinations</span></code></a></p></td>
<td><p>Compute combinations of length <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span></span> of the given tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.corrcoef"/><a class="reference internal" href="generated/torch.corrcoef.html#torch.corrcoef" title="torch.corrcoef"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corrcoef</span></code></a></p></td>
<td><p>Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> matrix, where rows are the variables and columns are the observations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cov"/><a class="reference internal" href="generated/torch.cov.html#torch.cov" title="torch.cov"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cov</span></code></a></p></td>
<td><p>Estimates the covariance matrix of the variables given by the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> matrix, where rows are the variables and columns are the observations.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cross"/><a class="reference internal" href="generated/torch.cross.html#torch.cross" title="torch.cross"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cross</span></code></a></p></td>
<td><p>Returns the cross product of vectors in dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cummax"/><a class="reference internal" href="generated/torch.cummax.html#torch.cummax" title="torch.cummax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cummax</span></code></a></p></td>
<td><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the cumulative maximum of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cummin"/><a class="reference internal" href="generated/torch.cummin.html#torch.cummin" title="torch.cummin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cummin</span></code></a></p></td>
<td><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the cumulative minimum of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cumprod"/><a class="reference internal" href="generated/torch.cumprod.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumprod</span></code></a></p></td>
<td><p>Returns the cumulative product of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cumsum"/><a class="reference internal" href="generated/torch.cumsum.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumsum</span></code></a></p></td>
<td><p>Returns the cumulative sum of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.diag"/><a class="reference internal" href="generated/torch.diag.html#torch.diag" title="torch.diag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diag</span></code></a></p></td>
<td><p><ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor</p></li>
</ul>
</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.diag_embed"/><a class="reference internal" href="generated/torch.diag_embed.html#torch.diag_embed" title="torch.diag_embed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diag_embed</span></code></a></p></td>
<td><p>Creates a tensor whose diagonals of certain 2D planes (specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code>) are filled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.diagflat"/><a class="reference internal" href="generated/torch.diagflat.html#torch.diagflat" title="torch.diagflat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagflat</span></code></a></p></td>
<td><p><ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor</p></li>
</ul>
</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.diagonal"/><a class="reference internal" href="generated/torch.diagonal.html#torch.diagonal" title="torch.diagonal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diagonal</span></code></a></p></td>
<td><p>Returns a partial view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the its diagonal elements with respect to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code> appended as a dimension at the end of the shape.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.diff"/><a class="reference internal" href="generated/torch.diff.html#torch.diff" title="torch.diff"><code class="xref py py-obj docutils literal notranslate"><span class="pre">diff</span></code></a></p></td>
<td><p>Computes the n-th forward difference along the given dimension.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.einsum"/><a class="reference internal" href="generated/torch.einsum.html#torch.einsum" title="torch.einsum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">einsum</span></code></a></p></td>
<td><p>Sums the product of the elements of the input <code class="xref py py-attr docutils literal notranslate"><span class="pre">operands</span></code> along dimensions specified using a notation based on the Einstein summation convention.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.flatten"/><a class="reference internal" href="generated/torch.flatten.html#torch.flatten" title="torch.flatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flatten</span></code></a></p></td>
<td><p>Flattens <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by reshaping it into a one-dimensional tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.flip"/><a class="reference internal" href="generated/torch.flip.html#torch.flip" title="torch.flip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flip</span></code></a></p></td>
<td><p>Reverse the order of an n-D tensor along given axis in dims.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.fliplr"/><a class="reference internal" href="generated/torch.fliplr.html#torch.fliplr" title="torch.fliplr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fliplr</span></code></a></p></td>
<td><p>Flip tensor in the left/right direction, returning a new tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.flipud"/><a class="reference internal" href="generated/torch.flipud.html#torch.flipud" title="torch.flipud"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flipud</span></code></a></p></td>
<td><p>Flip tensor in the up/down direction, returning a new tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.kron"/><a class="reference internal" href="generated/torch.kron.html#torch.kron" title="torch.kron"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kron</span></code></a></p></td>
<td><p>Computes the Kronecker product, denoted by <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊗</span></span></span></span></span>, of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.rot90"/><a class="reference internal" href="generated/torch.rot90.html#torch.rot90" title="torch.rot90"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rot90</span></code></a></p></td>
<td><p>Rotate an n-D tensor by 90 degrees in the plane specified by dims axis.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.gcd"/><a class="reference internal" href="generated/torch.gcd.html#torch.gcd" title="torch.gcd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gcd</span></code></a></p></td>
<td><p>Computes the element-wise greatest common divisor (GCD) of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.histc"/><a class="reference internal" href="generated/torch.histc.html#torch.histc" title="torch.histc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">histc</span></code></a></p></td>
<td><p>Computes the histogram of a tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.histogram"/><a class="reference internal" href="generated/torch.histogram.html#torch.histogram" title="torch.histogram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">histogram</span></code></a></p></td>
<td><p>Computes a histogram of the values in a tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.histogramdd"/><a class="reference internal" href="generated/torch.histogramdd.html#torch.histogramdd" title="torch.histogramdd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">histogramdd</span></code></a></p></td>
<td><p>Computes a multi-dimensional histogram of the values in a tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.meshgrid"/><a class="reference internal" href="generated/torch.meshgrid.html#torch.meshgrid" title="torch.meshgrid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">meshgrid</span></code></a></p></td>
<td><p>Creates grids of coordinates specified by the 1D inputs in <cite>attr</cite>:tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lcm"/><a class="reference internal" href="generated/torch.lcm.html#torch.lcm" title="torch.lcm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lcm</span></code></a></p></td>
<td><p>Computes the element-wise least common multiple (LCM) of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logcumsumexp"/><a class="reference internal" href="generated/torch.logcumsumexp.html#torch.logcumsumexp" title="torch.logcumsumexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logcumsumexp</span></code></a></p></td>
<td><p>Returns the logarithm of the cumulative summation of the exponentiation of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ravel"/><a class="reference internal" href="generated/torch.ravel.html#torch.ravel" title="torch.ravel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ravel</span></code></a></p></td>
<td><p>Return a contiguous flattened tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.renorm"/><a class="reference internal" href="generated/torch.renorm.html#torch.renorm" title="torch.renorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">renorm</span></code></a></p></td>
<td><p>Returns a tensor where each sub-tensor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is normalized such that the <cite>p</cite>-norm of the sub-tensor is lower than the value <code class="xref py py-attr docutils literal notranslate"><span class="pre">maxnorm</span></code></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.repeat_interleave"/><a class="reference internal" href="generated/torch.repeat_interleave.html#torch.repeat_interleave" title="torch.repeat_interleave"><code class="xref py py-obj docutils literal notranslate"><span class="pre">repeat_interleave</span></code></a></p></td>
<td><p>Repeat elements of a tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.roll"/><a class="reference internal" href="generated/torch.roll.html#torch.roll" title="torch.roll"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roll</span></code></a></p></td>
<td><p>Roll the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along the given dimension(s).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.searchsorted"/><a class="reference internal" href="generated/torch.searchsorted.html#torch.searchsorted" title="torch.searchsorted"><code class="xref py py-obj docutils literal notranslate"><span class="pre">searchsorted</span></code></a></p></td>
<td><p>Find the indices from the <em>innermost</em> dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">sorted_sequence</span></code> such that, if the corresponding values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code> were inserted before the indices, when sorted, the order of the corresponding <em>innermost</em> dimension within <code class="xref py py-attr docutils literal notranslate"><span class="pre">sorted_sequence</span></code> would be preserved.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tensordot"/><a class="reference internal" href="generated/torch.tensordot.html#torch.tensordot" title="torch.tensordot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensordot</span></code></a></p></td>
<td><p>Returns a contraction of a and b over multiple dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.trace"/><a class="reference internal" href="generated/torch.trace.html#torch.trace" title="torch.trace"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trace</span></code></a></p></td>
<td><p>Returns the sum of the elements of the diagonal of the input 2-D matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tril"/><a class="reference internal" href="generated/torch.tril.html#torch.tril" title="torch.tril"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tril</span></code></a></p></td>
<td><p>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the other elements of the result tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> are set to 0.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.tril_indices"/><a class="reference internal" href="generated/torch.tril_indices.html#torch.tril_indices" title="torch.tril_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tril_indices</span></code></a></p></td>
<td><p>Returns the indices of the lower triangular part of a <code class="xref py py-attr docutils literal notranslate"><span class="pre">row</span></code>-by- <code class="xref py py-attr docutils literal notranslate"><span class="pre">col</span></code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.triu"/><a class="reference internal" href="generated/torch.triu.html#torch.triu" title="torch.triu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triu</span></code></a></p></td>
<td><p>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the other elements of the result tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> are set to 0.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.triu_indices"/><a class="reference internal" href="generated/torch.triu_indices.html#torch.triu_indices" title="torch.triu_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triu_indices</span></code></a></p></td>
<td><p>Returns the indices of the upper triangular part of a <code class="xref py py-attr docutils literal notranslate"><span class="pre">row</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">col</span></code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.unflatten"/><a class="reference internal" href="generated/torch.unflatten.html#torch.unflatten" title="torch.unflatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unflatten</span></code></a></p></td>
<td><p>Expands a dimension of the input tensor over multiple dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.vander"/><a class="reference internal" href="generated/torch.vander.html#torch.vander" title="torch.vander"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vander</span></code></a></p></td>
<td><p>Generates a Vandermonde matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.view_as_real"/><a class="reference internal" href="generated/torch.view_as_real.html#torch.view_as_real" title="torch.view_as_real"><code class="xref py py-obj docutils literal notranslate"><span class="pre">view_as_real</span></code></a></p></td>
<td><p>Returns a view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as a real tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.view_as_complex"/><a class="reference internal" href="generated/torch.view_as_complex.html#torch.view_as_complex" title="torch.view_as_complex"><code class="xref py py-obj docutils literal notranslate"><span class="pre">view_as_complex</span></code></a></p></td>
<td><p>Returns a view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as a complex tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.resolve_conj"/><a class="reference internal" href="generated/torch.resolve_conj.html#torch.resolve_conj" title="torch.resolve_conj"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resolve_conj</span></code></a></p></td>
<td><p>Returns a new tensor with materialized conjugation if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>'s conjugate bit is set to <cite>True</cite>, else returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.resolve_neg"/><a class="reference internal" href="generated/torch.resolve_neg.html#torch.resolve_neg" title="torch.resolve_neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resolve_neg</span></code></a></p></td>
<td><p>Returns a new tensor with materialized negation if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>'s negative bit is set to <cite>True</cite>, else returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="blas-and-lapack-operations">
<h3>BLAS and LAPACK Operations<a class="headerlink" href="#blas-and-lapack-operations" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.addbmm"/><a class="reference internal" href="generated/torch.addbmm.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addbmm</span></code></a></p></td>
<td><p>Performs a batch matrix-matrix product of matrices stored in <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code>, with a reduced add step (all matrix multiplications get accumulated along the first dimension).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.addmm"/><a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addmm</span></code></a></p></td>
<td><p>Performs a matrix multiplication of the matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.addmv"/><a class="reference internal" href="generated/torch.addmv.html#torch.addmv" title="torch.addmv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addmv</span></code></a></p></td>
<td><p>Performs a matrix-vector product of the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> and the vector <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.addr"/><a class="reference internal" href="generated/torch.addr.html#torch.addr" title="torch.addr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">addr</span></code></a></p></td>
<td><p>Performs the outer-product of vectors <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code> and adds it to the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.baddbmm"/><a class="reference internal" href="generated/torch.baddbmm.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">baddbmm</span></code></a></p></td>
<td><p>Performs a batch matrix-matrix product of matrices in <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bmm"/><a class="reference internal" href="generated/torch.bmm.html#torch.bmm" title="torch.bmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bmm</span></code></a></p></td>
<td><p>Performs a batch matrix-matrix product of matrices stored in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.chain_matmul"/><a class="reference internal" href="generated/torch.chain_matmul.html#torch.chain_matmul" title="torch.chain_matmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">chain_matmul</span></code></a></p></td>
<td><p>Returns the matrix product of the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></span> 2-D tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cholesky"/><a class="reference internal" href="generated/torch.cholesky.html#torch.cholesky" title="torch.cholesky"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky</span></code></a></p></td>
<td><p>Computes the Cholesky decomposition of a symmetric positive-definite matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span></span> or for batches of symmetric positive-definite matrices.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cholesky_inverse"/><a class="reference internal" href="generated/torch.cholesky_inverse.html#torch.cholesky_inverse" title="torch.cholesky_inverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky_inverse</span></code></a></p></td>
<td><p>Computes the inverse of a symmetric positive-definite matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span></span> using its Cholesky factor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">u</span></span></span></span></span>: returns matrix <code class="docutils literal notranslate"><span class="pre">inv</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cholesky_solve"/><a class="reference internal" href="generated/torch.cholesky_solve.html#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cholesky_solve</span></code></a></p></td>
<td><p>Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">u</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.dot"/><a class="reference internal" href="generated/torch.dot.html#torch.dot" title="torch.dot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dot</span></code></a></p></td>
<td><p>Computes the dot product of two 1D tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.geqrf"/><a class="reference internal" href="generated/torch.geqrf.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">geqrf</span></code></a></p></td>
<td><p>This is a low-level function for calling LAPACK's geqrf directly.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ger"/><a class="reference internal" href="generated/torch.ger.html#torch.ger" title="torch.ger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ger</span></code></a></p></td>
<td><p>Alias of <a class="reference internal" href="generated/torch.outer.html#torch.outer" title="torch.outer"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.outer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.inner"/><a class="reference internal" href="generated/torch.inner.html#torch.inner" title="torch.inner"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inner</span></code></a></p></td>
<td><p>Computes the dot product for 1D tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.inverse"/><a class="reference internal" href="generated/torch.inverse.html#torch.inverse" title="torch.inverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inverse</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.inv.html#torch.linalg.inv" title="torch.linalg.inv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.inv()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.det"/><a class="reference internal" href="generated/torch.det.html#torch.det" title="torch.det"><code class="xref py py-obj docutils literal notranslate"><span class="pre">det</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.det.html#torch.linalg.det" title="torch.linalg.det"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.det()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logdet"/><a class="reference internal" href="generated/torch.logdet.html#torch.logdet" title="torch.logdet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logdet</span></code></a></p></td>
<td><p>Calculates log determinant of a square matrix or batches of square matrices.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.slogdet"/><a class="reference internal" href="generated/torch.slogdet.html#torch.slogdet" title="torch.slogdet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">slogdet</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.slogdet.html#torch.linalg.slogdet" title="torch.linalg.slogdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.slogdet()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.lu"/><a class="reference internal" href="generated/torch.lu.html#torch.lu" title="torch.lu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lu</span></code></a></p></td>
<td><p>Computes the LU factorization of a matrix or batches of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lu_solve"/><a class="reference internal" href="generated/torch.lu_solve.html#torch.lu_solve" title="torch.lu_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lu_solve</span></code></a></p></td>
<td><p>Returns the LU solve of the linear system <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Ax = b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span> using the partially pivoted LU factorization of A from <a class="reference internal" href="generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code class="xref py py-func docutils literal notranslate"><span class="pre">lu_factor()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.lu_unpack"/><a class="reference internal" href="generated/torch.lu_unpack.html#torch.lu_unpack" title="torch.lu_unpack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lu_unpack</span></code></a></p></td>
<td><p>Unpacks the LU decomposition returned by <a class="reference internal" href="generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code class="xref py py-func docutils literal notranslate"><span class="pre">lu_factor()</span></code></a> into the <cite>P, L, U</cite> matrices.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.matmul"/><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matmul</span></code></a></p></td>
<td><p>Matrix product of two tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.matrix_power"/><a class="reference internal" href="generated/torch.matrix_power.html#torch.matrix_power" title="torch.matrix_power"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matrix_power</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power" title="torch.linalg.matrix_power"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_power()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.matrix_exp"/><a class="reference internal" href="generated/torch.matrix_exp.html#torch.matrix_exp" title="torch.matrix_exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">matrix_exp</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.matrix_exp.html#torch.linalg.matrix_exp" title="torch.linalg.matrix_exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.matrix_exp()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.mm"/><a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mm</span></code></a></p></td>
<td><p>Performs a matrix multiplication of the matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.mv"/><a class="reference internal" href="generated/torch.mv.html#torch.mv" title="torch.mv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mv</span></code></a></p></td>
<td><p>Performs a matrix-vector product of the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and the vector <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.orgqr"/><a class="reference internal" href="generated/torch.orgqr.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orgqr</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.householder_product.html#torch.linalg.householder_product" title="torch.linalg.householder_product"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.householder_product()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ormqr"/><a class="reference internal" href="generated/torch.ormqr.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ormqr</span></code></a></p></td>
<td><p>Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.outer"/><a class="reference internal" href="generated/torch.outer.html#torch.outer" title="torch.outer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">outer</span></code></a></p></td>
<td><p>Outer product of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.pinverse"/><a class="reference internal" href="generated/torch.pinverse.html#torch.pinverse" title="torch.pinverse"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pinverse</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.linalg.pinv.html#torch.linalg.pinv" title="torch.linalg.pinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.linalg.pinv()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.qr"/><a class="reference internal" href="generated/torch.qr.html#torch.qr" title="torch.qr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">qr</span></code></a></p></td>
<td><p>Computes the QR decomposition of a matrix or a batch of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, and returns a namedtuple (Q, R) of tensors such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">\text{input} = Q R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">QR</span></span></span></span></span> with <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span> being an orthogonal matrix or batch of orthogonal matrices and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span></span> being an upper triangular matrix or batch of upper triangular matrices.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.svd"/><a class="reference internal" href="generated/torch.svd.html#torch.svd" title="torch.svd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">svd</span></code></a></p></td>
<td><p>Computes the singular value decomposition of either a matrix or batch of matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.svd_lowrank"/><a class="reference internal" href="generated/torch.svd_lowrank.html#torch.svd_lowrank" title="torch.svd_lowrank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">svd_lowrank</span></code></a></p></td>
<td><p>Return the singular value decomposition <code class="docutils literal notranslate"><span class="pre">(U,</span> <span class="pre">S,</span> <span class="pre">V)</span></code> of a matrix, batches of matrices, or a sparse matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span></span> such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A \approx U diag(S) V^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mord mathnormal">d</span><span class="mord mathnormal">ia</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.pca_lowrank"/><a class="reference internal" href="generated/torch.pca_lowrank.html#torch.pca_lowrank" title="torch.pca_lowrank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pca_lowrank</span></code></a></p></td>
<td><p>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.lobpcg"/><a class="reference internal" href="generated/torch.lobpcg.html#torch.lobpcg" title="torch.lobpcg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lobpcg</span></code></a></p></td>
<td><p>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.trapz"/><a class="reference internal" href="generated/torch.trapz.html#torch.trapz" title="torch.trapz"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trapz</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.trapezoid.html#torch.trapezoid" title="torch.trapezoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trapezoid()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.trapezoid"/><a class="reference internal" href="generated/torch.trapezoid.html#torch.trapezoid" title="torch.trapezoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trapezoid</span></code></a></p></td>
<td><p>Computes the <a class="reference external" href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a> along <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cumulative_trapezoid"/><a class="reference internal" href="generated/torch.cumulative_trapezoid.html#torch.cumulative_trapezoid" title="torch.cumulative_trapezoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cumulative_trapezoid</span></code></a></p></td>
<td><p><p>Cumulatively computes the <a class="reference external" href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a> along <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p>
</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.triangular_solve"/><a class="reference internal" href="generated/torch.triangular_solve.html#torch.triangular_solve" title="torch.triangular_solve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">triangular_solve</span></code></a></p></td>
<td><p>Solves a system of equations with a square upper or lower triangular invertible matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span></span> and multiple right-hand sides <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.vdot"/><a class="reference internal" href="generated/torch.vdot.html#torch.vdot" title="torch.vdot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vdot</span></code></a></p></td>
<td><p>Computes the dot product of two 1D vectors along a dimension.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="foreach-operations">
<h3>Foreach Operations<a class="headerlink" href="#foreach-operations" title="Permalink to this heading">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This API is in beta and subject to future changes.
Forward-mode AD is not supported.</p>
</div>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch._foreach_abs"/><a class="reference internal" href="generated/torch._foreach_abs.html#torch._foreach_abs" title="torch._foreach_abs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_abs</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_abs_"/><a class="reference internal" href="generated/torch._foreach_abs_.html#torch._foreach_abs_" title="torch._foreach_abs_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_abs_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_acos"/><a class="reference internal" href="generated/torch._foreach_acos.html#torch._foreach_acos" title="torch._foreach_acos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_acos</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_acos_"/><a class="reference internal" href="generated/torch._foreach_acos_.html#torch._foreach_acos_" title="torch._foreach_acos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_acos_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_asin"/><a class="reference internal" href="generated/torch._foreach_asin.html#torch._foreach_asin" title="torch._foreach_asin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_asin</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_asin_"/><a class="reference internal" href="generated/torch._foreach_asin_.html#torch._foreach_asin_" title="torch._foreach_asin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_asin_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_atan"/><a class="reference internal" href="generated/torch._foreach_atan.html#torch._foreach_atan" title="torch._foreach_atan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_atan</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_atan_"/><a class="reference internal" href="generated/torch._foreach_atan_.html#torch._foreach_atan_" title="torch._foreach_atan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_atan_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_ceil"/><a class="reference internal" href="generated/torch._foreach_ceil.html#torch._foreach_ceil" title="torch._foreach_ceil"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_ceil</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_ceil_"/><a class="reference internal" href="generated/torch._foreach_ceil_.html#torch._foreach_ceil_" title="torch._foreach_ceil_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_ceil_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_cos"/><a class="reference internal" href="generated/torch._foreach_cos.html#torch._foreach_cos" title="torch._foreach_cos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_cos</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_cos_"/><a class="reference internal" href="generated/torch._foreach_cos_.html#torch._foreach_cos_" title="torch._foreach_cos_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_cos_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_cosh"/><a class="reference internal" href="generated/torch._foreach_cosh.html#torch._foreach_cosh" title="torch._foreach_cosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_cosh</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_cosh_"/><a class="reference internal" href="generated/torch._foreach_cosh_.html#torch._foreach_cosh_" title="torch._foreach_cosh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_cosh_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_erf"/><a class="reference internal" href="generated/torch._foreach_erf.html#torch._foreach_erf" title="torch._foreach_erf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_erf</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_erf_"/><a class="reference internal" href="generated/torch._foreach_erf_.html#torch._foreach_erf_" title="torch._foreach_erf_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_erf_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_erfc"/><a class="reference internal" href="generated/torch._foreach_erfc.html#torch._foreach_erfc" title="torch._foreach_erfc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_erfc</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_erfc_"/><a class="reference internal" href="generated/torch._foreach_erfc_.html#torch._foreach_erfc_" title="torch._foreach_erfc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_erfc_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_exp"/><a class="reference internal" href="generated/torch._foreach_exp.html#torch._foreach_exp" title="torch._foreach_exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_exp</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_exp_"/><a class="reference internal" href="generated/torch._foreach_exp_.html#torch._foreach_exp_" title="torch._foreach_exp_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_exp_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_expm1"/><a class="reference internal" href="generated/torch._foreach_expm1.html#torch._foreach_expm1" title="torch._foreach_expm1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_expm1</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_expm1_"/><a class="reference internal" href="generated/torch._foreach_expm1_.html#torch._foreach_expm1_" title="torch._foreach_expm1_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_expm1_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_floor"/><a class="reference internal" href="generated/torch._foreach_floor.html#torch._foreach_floor" title="torch._foreach_floor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_floor</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_floor_"/><a class="reference internal" href="generated/torch._foreach_floor_.html#torch._foreach_floor_" title="torch._foreach_floor_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_floor_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_log"/><a class="reference internal" href="generated/torch._foreach_log.html#torch._foreach_log" title="torch._foreach_log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_log_"/><a class="reference internal" href="generated/torch._foreach_log_.html#torch._foreach_log_" title="torch._foreach_log_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_log10"/><a class="reference internal" href="generated/torch._foreach_log10.html#torch._foreach_log10" title="torch._foreach_log10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log10</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_log10_"/><a class="reference internal" href="generated/torch._foreach_log10_.html#torch._foreach_log10_" title="torch._foreach_log10_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log10_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_log1p"/><a class="reference internal" href="generated/torch._foreach_log1p.html#torch._foreach_log1p" title="torch._foreach_log1p"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log1p</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_log1p_"/><a class="reference internal" href="generated/torch._foreach_log1p_.html#torch._foreach_log1p_" title="torch._foreach_log1p_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log1p_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_log2"/><a class="reference internal" href="generated/torch._foreach_log2.html#torch._foreach_log2" title="torch._foreach_log2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log2</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_log2_"/><a class="reference internal" href="generated/torch._foreach_log2_.html#torch._foreach_log2_" title="torch._foreach_log2_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_log2_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_neg"/><a class="reference internal" href="generated/torch._foreach_neg.html#torch._foreach_neg" title="torch._foreach_neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_neg</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_neg_"/><a class="reference internal" href="generated/torch._foreach_neg_.html#torch._foreach_neg_" title="torch._foreach_neg_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_neg_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_tan"/><a class="reference internal" href="generated/torch._foreach_tan.html#torch._foreach_tan" title="torch._foreach_tan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_tan</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_tan_"/><a class="reference internal" href="generated/torch._foreach_tan_.html#torch._foreach_tan_" title="torch._foreach_tan_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_tan_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tan()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_sin"/><a class="reference internal" href="generated/torch._foreach_sin.html#torch._foreach_sin" title="torch._foreach_sin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sin</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_sin_"/><a class="reference internal" href="generated/torch._foreach_sin_.html#torch._foreach_sin_" title="torch._foreach_sin_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sin_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_sinh"/><a class="reference internal" href="generated/torch._foreach_sinh.html#torch._foreach_sinh" title="torch._foreach_sinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sinh</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_sinh_"/><a class="reference internal" href="generated/torch._foreach_sinh_.html#torch._foreach_sinh_" title="torch._foreach_sinh_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sinh_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_round"/><a class="reference internal" href="generated/torch._foreach_round.html#torch._foreach_round" title="torch._foreach_round"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_round</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_round_"/><a class="reference internal" href="generated/torch._foreach_round_.html#torch._foreach_round_" title="torch._foreach_round_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_round_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_sqrt"/><a class="reference internal" href="generated/torch._foreach_sqrt.html#torch._foreach_sqrt" title="torch._foreach_sqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sqrt</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_sqrt_"/><a class="reference internal" href="generated/torch._foreach_sqrt_.html#torch._foreach_sqrt_" title="torch._foreach_sqrt_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sqrt_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_lgamma"/><a class="reference internal" href="generated/torch._foreach_lgamma.html#torch._foreach_lgamma" title="torch._foreach_lgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_lgamma</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_lgamma_"/><a class="reference internal" href="generated/torch._foreach_lgamma_.html#torch._foreach_lgamma_" title="torch._foreach_lgamma_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_lgamma_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lgamma()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_frac"/><a class="reference internal" href="generated/torch._foreach_frac.html#torch._foreach_frac" title="torch._foreach_frac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_frac</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_frac_"/><a class="reference internal" href="generated/torch._foreach_frac_.html#torch._foreach_frac_" title="torch._foreach_frac_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_frac_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_reciprocal"/><a class="reference internal" href="generated/torch._foreach_reciprocal.html#torch._foreach_reciprocal" title="torch._foreach_reciprocal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_reciprocal</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_reciprocal_"/><a class="reference internal" href="generated/torch._foreach_reciprocal_.html#torch._foreach_reciprocal_" title="torch._foreach_reciprocal_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_reciprocal_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_sigmoid"/><a class="reference internal" href="generated/torch._foreach_sigmoid.html#torch._foreach_sigmoid" title="torch._foreach_sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sigmoid</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_sigmoid_"/><a class="reference internal" href="generated/torch._foreach_sigmoid_.html#torch._foreach_sigmoid_" title="torch._foreach_sigmoid_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_sigmoid_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_trunc"/><a class="reference internal" href="generated/torch._foreach_trunc.html#torch._foreach_trunc" title="torch._foreach_trunc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_trunc</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch._foreach_trunc_"/><a class="reference internal" href="generated/torch._foreach_trunc_.html#torch._foreach_trunc_" title="torch._foreach_trunc_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_trunc_</span></code></a></p></td>
<td><p>Apply <a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a> to each Tensor of the input list.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._foreach_zero_"/><a class="reference internal" href="generated/torch._foreach_zero_.html#torch._foreach_zero_" title="torch._foreach_zero_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_foreach_zero_</span></code></a></p></td>
<td><p>Apply <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.zero()</span></code> to each Tensor of the input list.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="utilities">
<h2>Utilities<a class="headerlink" href="#utilities" title="Permalink to this heading">¶</a></h2>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.compiled_with_cxx11_abi"/><a class="reference internal" href="generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi" title="torch.compiled_with_cxx11_abi"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compiled_with_cxx11_abi</span></code></a></p></td>
<td><p>Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.result_type"/><a class="reference internal" href="generated/torch.result_type.html#torch.result_type" title="torch.result_type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">result_type</span></code></a></p></td>
<td><p>Returns the <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> that would result from performing an arithmetic operation on the provided input tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.can_cast"/><a class="reference internal" href="generated/torch.can_cast.html#torch.can_cast" title="torch.can_cast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">can_cast</span></code></a></p></td>
<td><p>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion <a class="reference internal" href="tensor_attributes.html#type-promotion-doc"><span class="std std-ref">documentation</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.promote_types"/><a class="reference internal" href="generated/torch.promote_types.html#torch.promote_types" title="torch.promote_types"><code class="xref py py-obj docutils literal notranslate"><span class="pre">promote_types</span></code></a></p></td>
<td><p>Returns the <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> with the smallest size and scalar kind that is not smaller nor of lower kind than either <cite>type1</cite> or <cite>type2</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.use_deterministic_algorithms"/><a class="reference internal" href="generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms" title="torch.use_deterministic_algorithms"><code class="xref py py-obj docutils literal notranslate"><span class="pre">use_deterministic_algorithms</span></code></a></p></td>
<td><p>Sets whether PyTorch operations must use &quot;deterministic&quot; algorithms.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.are_deterministic_algorithms_enabled"/><a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled" title="torch.are_deterministic_algorithms_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">are_deterministic_algorithms_enabled</span></code></a></p></td>
<td><p>Returns True if the global deterministic flag is turned on.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.is_deterministic_algorithms_warn_only_enabled"/><a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html#torch.is_deterministic_algorithms_warn_only_enabled" title="torch.is_deterministic_algorithms_warn_only_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_deterministic_algorithms_warn_only_enabled</span></code></a></p></td>
<td><p>Returns True if the global deterministic flag is set to warn only.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_deterministic_debug_mode"/><a class="reference internal" href="generated/torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode" title="torch.set_deterministic_debug_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_deterministic_debug_mode</span></code></a></p></td>
<td><p>Sets the debug mode for deterministic operations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.get_deterministic_debug_mode"/><a class="reference internal" href="generated/torch.get_deterministic_debug_mode.html#torch.get_deterministic_debug_mode" title="torch.get_deterministic_debug_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_deterministic_debug_mode</span></code></a></p></td>
<td><p>Returns the current value of the debug mode for deterministic operations.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_float32_matmul_precision"/><a class="reference internal" href="generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision" title="torch.set_float32_matmul_precision"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_float32_matmul_precision</span></code></a></p></td>
<td><p>Sets the internal precision of float32 matrix multiplications.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.get_float32_matmul_precision"/><a class="reference internal" href="generated/torch.get_float32_matmul_precision.html#torch.get_float32_matmul_precision" title="torch.get_float32_matmul_precision"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_float32_matmul_precision</span></code></a></p></td>
<td><p>Returns the current value of float32 matrix multiplication precision.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.set_warn_always"/><a class="reference internal" href="generated/torch.set_warn_always.html#torch.set_warn_always" title="torch.set_warn_always"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_warn_always</span></code></a></p></td>
<td><p>When this flag is False (default) then some PyTorch warnings may only appear once per process.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.is_warn_always_enabled"/><a class="reference internal" href="generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled" title="torch.is_warn_always_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">is_warn_always_enabled</span></code></a></p></td>
<td><p>Returns True if the global warn_always flag is turned on.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.vmap"/><a class="reference internal" href="generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vmap</span></code></a></p></td>
<td><p>vmap is the vectorizing map; <code class="docutils literal notranslate"><span class="pre">vmap(func)</span></code> returns a new function that maps <code class="docutils literal notranslate"><span class="pre">func</span></code> over some dimension of the inputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch._assert"/><a class="reference internal" href="generated/torch._assert.html#torch._assert" title="torch._assert"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_assert</span></code></a></p></td>
<td><p>A wrapper around Python's assert which is symbolically traceable.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="symbolic-numbers">
<h2>Symbolic Numbers<a class="headerlink" href="#symbolic-numbers" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.SymInt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">SymInt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#SymInt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymInt" title="Permalink to this definition">¶</a></dt>
<dd><p>Like an int (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.SymFloat">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">SymFloat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#SymFloat"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymFloat" title="Permalink to this definition">¶</a></dt>
<dd><p>Like an float (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.SymBool">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">SymBool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#SymBool"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymBool" title="Permalink to this definition">¶</a></dt>
<dd><p>Like an bool (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.</p>
<p>Unlike regular bools, regular boolean operators will force extra guards instead
of symbolically evaluate.  Use the bitwise operators instead to handle this.</p>
</dd></dl>

<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.sym_float"/><a class="reference internal" href="generated/torch.sym_float.html#torch.sym_float" title="torch.sym_float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_float</span></code></a></p></td>
<td><p>SymInt-aware utility for float casting.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sym_int"/><a class="reference internal" href="generated/torch.sym_int.html#torch.sym_int" title="torch.sym_int"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_int</span></code></a></p></td>
<td><p>SymInt-aware utility for int casting.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sym_max"/><a class="reference internal" href="generated/torch.sym_max.html#torch.sym_max" title="torch.sym_max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_max</span></code></a></p></td>
<td><p>SymInt-aware utility for max().</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sym_min"/><a class="reference internal" href="generated/torch.sym_min.html#torch.sym_min" title="torch.sym_min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_min</span></code></a></p></td>
<td><p>SymInt-aware utility for max().</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sym_not"/><a class="reference internal" href="generated/torch.sym_not.html#torch.sym_not" title="torch.sym_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sym_not</span></code></a></p></td>
<td><p>SymInt-aware utility for logical negation.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="optimizations">
<h2>Optimizations<a class="headerlink" href="#optimizations" title="Permalink to this heading">¶</a></h2>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.compile"/><a class="reference internal" href="generated/torch.compile.html#torch.compile" title="torch.compile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compile</span></code></a></p></td>
<td><p>Optimizes given model/function using TorchDynamo and specified backend.</p></td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="https://pytorch.org/docs/main/compile/index.html">torch.compile documentation</a></p>
</div>
<div class="section" id="operator-tags">
<h2>Operator Tags<a class="headerlink" href="#operator-tags" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.Tag">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">Tag</span></span><a class="headerlink" href="#torch.Tag" title="Permalink to this definition">¶</a></dt>
<dd><p>Members:</p>
<p>core</p>
<p>data_dependent_output</p>
<p>dynamic_output_shape</p>
<p>generated</p>
<p>inplace_view</p>
<p>nondeterministic_bitwise</p>
<p>nondeterministic_seeded</p>
<p>pointwise</p>
<p>view_copy</p>
<dl class="py property">
<dt class="sig sig-object py" id="torch.Tag.name">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#torch.Tag.name" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<span class="target" id="module-torch.contrib"></span><span class="target" id="module-torch.utils.backcompat"></span><span class="target" id="module-torch.utils"></span><span class="target" id="module-torch.utils.hipify"></span><span class="target" id="module-torch.utils.model_dump"></span><span class="target" id="module-torch.utils.viz"></span><span class="target" id="module-torch.autograd"></span><p><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code> provides classes and functions implementing automatic
differentiation of arbitrary scalar valued functions. It requires minimal
changes to the existing code - you only need to declare <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> s
for which gradients should be computed with the <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> keyword.
As of now, we only support autograd for floating point <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> types (
half, float, double and bfloat16) and complex <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> types (cfloat, cdouble).</p>
</div>
<div class="section" id="engine-configuration">
<h2>Engine Configuration<a class="headerlink" href="#engine-configuration" title="Permalink to this heading">¶</a></h2>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.autograd.set_multithreading_enabled"/><a class="reference internal" href="generated/torch.autograd.set_multithreading_enabled.html#torch.autograd.set_multithreading_enabled" title="torch.autograd.set_multithreading_enabled"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_multithreading_enabled</span></code></a></p></td>
<td><p>Context-manager that sets multithreaded backwards on or off.</p></td>
</tr>
</tbody>
</table>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/torch.is_tensor.html" class="btn btn-neutral float-right" title="torch.is_tensor" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="deploy.html" class="btn btn-neutral" title="torch::deploy has been moved to pytorch/multipy" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch</a><ul>
<li><a class="reference internal" href="#tensors">Tensors</a><ul>
<li><a class="reference internal" href="#creation-ops">Creation Ops</a></li>
<li><a class="reference internal" href="#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li><a class="reference internal" href="#generators">Generators</a></li>
<li><a class="reference internal" href="#random-sampling">Random sampling</a><ul>
<li><a class="reference internal" href="#torch.torch.default_generator"><code class="docutils literal notranslate"><span class="pre">torch.default_generator</span></code></a></li>
<li><a class="reference internal" href="#in-place-random-sampling">In-place random sampling</a></li>
<li><a class="reference internal" href="#quasi-random-sampling">Quasi-random sampling</a></li>
</ul>
</li>
<li><a class="reference internal" href="#serialization">Serialization</a></li>
<li><a class="reference internal" href="#parallelism">Parallelism</a></li>
<li><a class="reference internal" href="#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li><a class="reference internal" href="#math-operations">Math operations</a><ul>
<li><a class="reference internal" href="#pointwise-ops">Pointwise Ops</a></li>
<li><a class="reference internal" href="#reduction-ops">Reduction Ops</a></li>
<li><a class="reference internal" href="#comparison-ops">Comparison Ops</a></li>
<li><a class="reference internal" href="#spectral-ops">Spectral Ops</a></li>
<li><a class="reference internal" href="#other-operations">Other Operations</a></li>
<li><a class="reference internal" href="#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
<li><a class="reference internal" href="#foreach-operations">Foreach Operations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#utilities">Utilities</a></li>
<li><a class="reference internal" href="#symbolic-numbers">Symbolic Numbers</a><ul>
<li><a class="reference internal" href="#torch.SymInt"><code class="docutils literal notranslate"><span class="pre">SymInt</span></code></a></li>
<li><a class="reference internal" href="#torch.SymFloat"><code class="docutils literal notranslate"><span class="pre">SymFloat</span></code></a></li>
<li><a class="reference internal" href="#torch.SymBool"><code class="docutils literal notranslate"><span class="pre">SymBool</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#optimizations">Optimizations</a></li>
<li><a class="reference internal" href="#operator-tags">Operator Tags</a><ul>
<li><a class="reference internal" href="#torch.Tag"><code class="docutils literal notranslate"><span class="pre">Tag</span></code></a><ul>
<li><a class="reference internal" href="#torch.Tag.name"><code class="docutils literal notranslate"><span class="pre">Tag.name</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#engine-configuration">Engine Configuration</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
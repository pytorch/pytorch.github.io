


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Dynamo Deep-Dive &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="PyTorch 2.0 NNModule Support" href="torch.compiler_nn_module.html" />
    <link rel="prev" title="PyTorch 2.0 Performance Dashboard" href="torch.compiler_performance_dashboard.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch">
                  <span class="dropdown-title">ExecuTorch</span>
                </a>
              </div>
            </div>  
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.4.0a0+git064a650 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">Meta device</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">Torch Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="torch.compiler.html">torch.compiler</a> &gt;</li>
        
      <li>Dynamo Deep-Dive</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/torch.compiler_dynamo_deepdive.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="dynamo-deep-dive">
<h1>Dynamo Deep-Dive<a class="headerlink" href="#dynamo-deep-dive" title="Permalink to this heading">¶</a></h1>
<p>TorchDynamo (or simply Dynamo) is the tracer within <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
and it is, more often than not, the one to blame for those insane
backtraces. However, we cannot blindly blame Dynamo for these errors. In
order to provide the user with the flexibility it does, Dynamo is given
the arduous task of understanding any Python program. In particular,
Dynamo has to implement a good part of the Python programming language
internally!</p>
<p>In this post, we will go over the internal design of Dynamo from the
ground up. We will discuss the functionality it provides, and how it is
implemented. By the end of this post, you will have a better
understanding of what went wrong when you <code class="docutils literal notranslate"><span class="pre">torch.compiled</span></code> a PyTorch
program and the compilation errored out, or succeeded but the speed-up
was not what you expected. <a class="footnote-reference brackets" href="#id9" id="id1">1</a></p>
<div class="section" id="a-gentle-introduction-to-dynamo">
<h2>A Gentle Introduction to Dynamo<a class="headerlink" href="#a-gentle-introduction-to-dynamo" title="Permalink to this heading">¶</a></h2>
<p>Before getting our hands dirty with all the implementation details,
let’s start by discussing what it is that Dynamo does.</p>
<p>Dynamo is a tracer. This means, given and function and inputs to it, it
executes the function and records a linear sequence of instructions
(without control flow) into a graph. For example, consider the following
program:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">mse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>If we save this program into the file <code class="docutils literal notranslate"><span class="pre">example.py</span></code> and we run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">TORCH_LOGS</span><span class="o">=</span>graph_code<span class="w"> </span>python<span class="w"> </span>example.py
</pre></div>
</div>
<p>we see the output that Dynamo traced</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">l_x_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_y_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="c1"># File: example.py:5, code: z = (x - y) ** 2</span>
    <span class="n">sub</span> <span class="o">=</span> <span class="n">l_x_</span> <span class="o">-</span> <span class="n">l_y_</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">sub</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="c1"># File: example.py:6, code: return z.sum()</span>
    <span class="n">sum_1</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">sum_1</span><span class="p">,)</span>
</pre></div>
</div>
<p>We call this a <strong>graph (or trace) of the function for the given
inputs</strong>. This is represented via an <a class="reference external" href="https://pytorch.org/docs/stable/fx.html">FX
graph</a>. We will simply think
of an FX graph as a container that stores a list of function calls.</p>
<p>The first thing we should notice is that the graph is a linear sequence
of PyTorch operations. <a class="footnote-reference brackets" href="#id10" id="id2">2</a> Dynamo records all the PyTorch operations
and stores them sequentially. For example, it split <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">y)</span> <span class="pre">**</span> <span class="pre">2</span></code>
into its two constituting operations, <code class="docutils literal notranslate"><span class="pre">sub</span> <span class="pre">=</span> <span class="pre">l_x_</span> <span class="pre">-</span> <span class="pre">l_y_</span></code> and
<code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">sub</span> <span class="pre">**</span> <span class="pre">2</span></code>.</p>
<p>When we say that the trace is linear, we mean that there is no branching
or any control flow. To see this, consider</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">/</span> <span class="n">n</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>which, when executed with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_code</span></code>, returns</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">l_x_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="c1"># File: example.py:5, code: y = x ** 2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">l_x_</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="c1"># File: example.py:7, code: return (n + 1) * y</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul</span><span class="p">,)</span>
</pre></div>
</div>
<p>We see that Dynamo completely removed the <code class="docutils literal notranslate"><span class="pre">if</span></code> statement from the
trace and just recorded the operations that were executed with the
inputs.</p>
<p>As such, it should be clear that <strong>the trace of a function depends on
the inputs</strong>. In particular, this means that the trace is not generated
when we write <code class="docutils literal notranslate"><span class="pre">&#64;torch.compile</span></code>, but when we execute the function
<code class="docutils literal notranslate"><span class="pre">fn(x,</span> <span class="pre">2)</span></code> with the actual arguments.</p>
<p>The other interesting thing to note here is that Dynamo removed the
second argument to the function. Instead, it treated it as a constant
and recorded the result of the operation <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">+</span> <span class="pre">1</span></code> in the graph. This is
another feature of Dynamo: Dynamo will treat as constant any non-tensor
value… other than ints. Let’s see now how are ints special.</p>
<p>The last defining property of Dynamo is that it knows how to handle
dynamic shapes. Symbolic shapes refer to Dynamo’s ability of tracing
shapes, and more generally, integers, rather than leaving them as
constants. This allows for avoiding recompilations and deploying generic
models that work for any size in production. The main examples of places
where dynamic shapes appear are the batch size, where we might train a
model with a fixed batch size but then perform inference for an
arbitrary batch size, or the variable sequence length that one
encounters when processing text or audio.</p>
<p>We can see this by executing a few more times the example above</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">/</span> <span class="n">n</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_code</span></code> generates two more graphs</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Graph for n==2 omitted</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l_x_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_n_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">):</span>
    <span class="c1"># File: a.py:5, code: y = x ** 2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">l_x_</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="c1"># File: a.py:7, code: return (n + 1) * y</span>
    <span class="n">add</span> <span class="o">=</span> <span class="n">l_n_</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="n">add</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul</span><span class="p">,)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l_x_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_n_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">):</span>
    <span class="c1"># File: a.py:5, code: y = x ** 2</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">l_x_</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="c1"># File: a.py:9, code: return y / n</span>
    <span class="n">truediv</span> <span class="o">=</span> <span class="n">y</span> <span class="o">/</span> <span class="n">l_n_</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">truediv</span><span class="p">,)</span>
</pre></div>
</div>
<p>Dynamo detected that one integer changed its value after the first call
and started tracing it. We see that these graphs are generic, and trace
the variable <code class="docutils literal notranslate"><span class="pre">n</span></code> symbolically via an object of type <code class="docutils literal notranslate"><span class="pre">SymInt</span></code>.</p>
<p>If after these calls we call <code class="docutils literal notranslate"><span class="pre">fn(x,</span> <span class="pre">4)</span></code>, Dynamo would not recompile,
but rather reuse the graph that was already traced.</p>
<p>To summarize: 1. Dynamo is a Python tracer 2. Given some inputs, it
returns an FX graph with the PyTorch functions that were executed 3. It
can also trace integers if it detects that they changed between calls 4.
It specializes any other value that is not a tensor or a scalar</p>
<p>Of course, Dynamo does many more things, like figuring out when it needs
to retrace, rewriting the bytecode of the function, implementing graph
breaks… To keep the introduction short, we will incrementally discuss
all these in the sequel.</p>
</div>
<div class="section" id="pep-523-adding-a-frame-evaluation-api-to-cpython">
<h2>PEP 523: Adding a frame evaluation API to CPython<a class="headerlink" href="#pep-523-adding-a-frame-evaluation-api-to-cpython" title="Permalink to this heading">¶</a></h2>
<p>Imagine now that we are given the task to implement Dynamo. Where would
we even start? Rather conveniently for us, <a class="reference external" href="https://peps.python.org/pep-0523/">PEP
523</a> was released with Python 3.6.
This PEP <a class="reference external" href="https://peps.python.org/pep-0523/#a-jit-for-cpython">was
designed</a> to
allow third parties to create JIT compilers for Python. Let’s see how.</p>
<p><strong>A note on CPython</strong>: CPython is internally implemented as a <a class="reference external" href="https://en.wikipedia.org/wiki/Stack_machine">stack
machine</a>. A Python
program is compiled into
<a class="reference external" href="https://en.wikipedia.org/wiki/Bytecode">bytecodes</a> that then are
executed by this interpreter. To learn more about these bytecodes, see
the <a class="reference external" href="https://docs.python.org/3/library/dis.html">dis module</a> from the
standard library. See also <a class="reference external" href="https://devguide.python.org/internals/interpreter/">the developer
docs</a> for an
introduction to CPython’s interpreter. We will assume that the reader is
familiar with the notion of a stack machine.</p>
<p>PEP 523 exposes an API where a user can add a custom per-function
interpreter. Then, CPython will use this interpreter rather than its own
to execute the function. In order to be able to execute the function, on
entry, CPython provides the custom interpreter with things like - The
bytecode of the function - The value of the arguments of the function
(i.e., the local variables) and their names - The value of the global
variables and their names - The builtin functions like <code class="docutils literal notranslate"><span class="pre">abs</span></code> or
<code class="docutils literal notranslate"><span class="pre">print</span></code></p>
<p>You can see all the fields
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L50-L59">here</a>. <a class="footnote-reference brackets" href="#id11" id="id3">3</a></p>
<p>In summary, CPython provides the user’s interpreter with all the
information necessary to execute the function. <a class="footnote-reference brackets" href="#id12" id="id4">4</a></p>
<p>With this API, we can implement a tracer by implementing an interpreter
that runs the code and records in a graph all the PyTorch operations
that occur during this execution. This is exactly what Dynamo does.</p>
<p>Dynamo uses this CPython API to parse all these objects and packs them
into <a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L93-L108">a Python
structure</a>.
After it has done so… it goes back from C to python. Other than for this
piece of code that communicates with CPython, Dynamo is fully
implemented in Python.</p>
<p>It should be clear that it is the decorator <code class="docutils literal notranslate"><span class="pre">&#64;torch.compile</span></code>’s job
to install the necessary scaffolding that will pass the bytecode, the
args, global variables and so on to Dynamo when the function is called.
Again, <code class="docutils literal notranslate"><span class="pre">&#64;torch.compile</span></code> does not actually compile anything.</p>
</div>
<div class="section" id="implementing-cpython-in-python">
<h2>Implementing CPython in Python<a class="headerlink" href="#implementing-cpython-in-python" title="Permalink to this heading">¶</a></h2>
<p>So, we are back in the Python world. We have the bytecode of a function,
and all the context necessary to execute it. In particular, we have
landed at
<code class="docutils literal notranslate"><span class="pre">`_convert_frame_assert</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/b6df8414601e1e086e830ca9e919e7fdc8874e71/torch/_dynamo/convert_frame.py#L272-L274">https://github.com/pytorch/pytorch/blob/b6df8414601e1e086e830ca9e919e7fdc8874e71/torch/_dynamo/convert_frame.py#L272-L274</a>&gt;`__.
This is the function that the decorator <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> returns! We
get to this function from
<code class="docutils literal notranslate"><span class="pre">`_dynamo.optimize</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/b6df8414601e1e086e830ca9e919e7fdc8874e71/torch/_dynamo/eval_frame.py#L715-L727">https://github.com/pytorch/pytorch/blob/b6df8414601e1e086e830ca9e919e7fdc8874e71/torch/_dynamo/eval_frame.py#L715-L727</a>&gt;`__.
The decorator <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> is just a nice API around
<code class="docutils literal notranslate"><span class="pre">_dynamo.optimize</span></code>.</p>
<p>Before getting into implementing a Python interpreter, we want to define
an <a class="reference external" href="https://en.wikipedia.org/wiki/Intermediate_representation">IR</a>.
In particular, we want to wrap all the local and global variables in our
own internal classes. This allows us to better track these objects and
group together objects that can be treated in the same way to the eyes
of Dynamo.</p>
<p>The parent class of the internal class structure is <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code>
and represents the different objects that Dynamo understands. For
example, <code class="docutils literal notranslate"><span class="pre">ListVariable</span></code>, represents a <code class="docutils literal notranslate"><span class="pre">list</span></code> object, and keeps
internally a <cite>list of
``VariableTracker`</cite>s &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e38a3a6079a3861b4bc9f256120ec661f34e726d/torch/_dynamo/variables/lists.py#L48-L56">https://github.com/pytorch/pytorch/blob/e38a3a6079a3861b4bc9f256120ec661f34e726d/torch/_dynamo/variables/lists.py#L48-L56</a>&gt;`__.
Another example of <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> is
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/constant.py#L30">ConstantVariable</a>.
ConstantVariable wraps all the <a class="reference external" href="https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/constant.py#L98-L107">objects considered constant by
Dynamo</a>.
We also have special subclasses for objects that require special
attention, like
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/tensor.py#L68-L69">TensorVariable</a>.
All these internal classes are defined in the
<code class="docutils literal notranslate"><span class="pre">`torch/_dynamo/variables</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/tree/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables">https://github.com/pytorch/pytorch/tree/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables</a>&gt;`__
folder.</p>
<p>Python objects are wrapped into their corresponding <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code>
class in
<code class="docutils literal notranslate"><span class="pre">`VariableBuilder._wrap</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/builder.py#L365">https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/builder.py#L365</a>&gt;`__.
This function is just a very long chain of <code class="docutils literal notranslate"><span class="pre">elif</span></code>s that tries to
recursively pattern-match the Python inputs into the appropriate type of
<code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code>.</p>
<p><strong>Debugging tip</strong>. When we get unexpected results from dynamo, it is
sometimes caused by the builder. If the logic of the builder is wrong,
sometimes Dynamo may wrap a variable in the incorrect
<code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> type, and this may cause issues later on. It is
rather useful to have a look at the <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> types that
appear in the errors, and the <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> method that throws the
exception when you encounter a Dynamo error. In particular, sometimes we
find that an object is tracked as a <code class="docutils literal notranslate"><span class="pre">UserDefinedObjectVariable</span></code> (this
is Dynamo’s catch-all class), when it should have been tracked as
something more specific. In these cases, the <code class="docutils literal notranslate"><span class="pre">SourceBuilder.__call__</span></code>
logic is often to blame.</p>
<p><strong>Debugging tip</strong>. When running a program with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=dynamo</span></code>,
one of the artifacts that are printed out is lines of the form</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TRACE</span> <span class="n">LOAD_GLOBAL</span> <span class="n">y</span> <span class="p">[</span><span class="n">TorchInGraphFunctionVariable</span><span class="p">(</span><span class="o">&lt;</span><span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">method</span> <span class="nb">any</span><span class="o">&gt;</span><span class="p">),</span> <span class="n">TensorVariable</span><span class="p">()]</span>
</pre></div>
</div>
<p>This is the bytecode for the original program and the state of the stack
at that point. This is very useful to find where an object was not
traced into the right <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code>.</p>
<p>Ok, so we have an IR for our tracer, now we <em>just</em> need to reimplement
CPython’s stack machine. This is implemented by
<code class="docutils literal notranslate"><span class="pre">`InstructorTranslatorBase</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py#L576-L594">https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py#L576-L594</a>&gt;`__
in
<code class="docutils literal notranslate"><span class="pre">`symbolic_convert.py</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py">https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py</a>&gt;`__.</p>
<p><code class="docutils literal notranslate"><span class="pre">InstructionTranslatorBase</span></code> has about 200 methods, implementing almost
all of Python bytecodes. As an example, we can see the implementation of
<code class="docutils literal notranslate"><span class="pre">BUILD_LIST</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">BUILD_LIST</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="n">items</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">popn</span><span class="p">(</span><span class="n">inst</span><span class="o">.</span><span class="n">argval</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">ListVariable</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">mutable_local</span><span class="o">=</span><span class="n">MutableLocal</span><span class="p">()))</span>
</pre></div>
</div>
<p>This is the bytecode generated by constructions like <code class="docutils literal notranslate"><span class="pre">l</span> <span class="pre">=</span> <span class="pre">[2,</span> <span class="pre">3,</span> <span class="pre">4]</span></code>.
In this case, since there are three elements, the generated bytecode is
<code class="docutils literal notranslate"><span class="pre">BUILD_LIST</span> <span class="pre">3</span></code>. This means that we pop the top <code class="docutils literal notranslate"><span class="pre">3</span></code> elements of the
stack and push a new list object to the top of the stack formed by these
three elements.</p>
</div>
<div class="section" id="generating-the-output-graph">
<h2>Generating the Output Graph<a class="headerlink" href="#generating-the-output-graph" title="Permalink to this heading">¶</a></h2>
<p>With a way to symbolically execute Python code, we are set to extract
the PyTorch operations that happen during the symbolic execution of a
program given some inputs. This is implemented in Dynamo via the
<code class="docutils literal notranslate"><span class="pre">`OutputGraph</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/output_graph.py#L221-L230">https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/output_graph.py#L221-L230</a>&gt;`__
object. The <code class="docutils literal notranslate"><span class="pre">OutputGraph</span></code> object is <cite>bound to an
``InstructionTranslator`</cite>
object &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py#L2060-L2071">https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py#L2060-L2071</a>&gt;`__
and it tracks all the data necessary to create the FX graph which will
be returned by Dynamo.</p>
<p>All the inputs and intermediary elements of the FX graph are
<code class="docutils literal notranslate"><span class="pre">fx.Node</span></code>s. In Dynamo, <code class="docutils literal notranslate"><span class="pre">fx.Node</span></code>s are wrapped in
<code class="docutils literal notranslate"><span class="pre">fx.Proxy</span></code>s. <code class="docutils literal notranslate"><span class="pre">fx.Proxy</span></code>s are used to build the FX graph.
In particular, they record every PyTorch operation performed on them
into the graph. You can can create a new operation to be added to
the graph by calling <code class="docutils literal notranslate"><span class="pre">`create_proxy</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/_dynamo/output_graph.py#L430-L431">https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/_dynamo/output_graph.py#L430-L431</a>&gt;`__.
Then, we can add it to the graph through the function
<code class="docutils literal notranslate"><span class="pre">`wrap_fx_proxy</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/_dynamo/variables/builder.py#L1311">https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/_dynamo/variables/builder.py#L1311</a>&gt;`__.</p>
<p>A graph stores operations on tensors… and operations on symbolic
integers. We will discuss symbolic integers later on, but first we will
discuss how Dynamo addresses a rather important correctness issue.</p>
</div>
<div class="section" id="making-dynamo-sound-guards">
<span id="id5"></span><h2>Making Dynamo Sound: Guards<a class="headerlink" href="#making-dynamo-sound-guards" title="Permalink to this heading">¶</a></h2>
<p>At this point, we have a way to trace programs completely disregarding control flow.
And for that, we have reimplemented all of CPython… If this sounds like a bit of an
overkill, that is because it is.
<code class="docutils literal notranslate"><span class="pre">`torch.jit.trace</span></code> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html">https://pytorch.org/docs/stable/generated/torch.jit.trace.html</a>&gt;`__
already implements this without all this machinery, so what gives?</p>
<p>The issue with <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code>, as it is warned in its docs, is that
it just works if the traced program is not data dependent. In other
words, it will just work if the program itself is linear. This means
writing our program without using if-elses, for-while loops, exceptions.
Even more, none of the libraries that we use can use any control flow!
All in all, not using control flow in a language as dynamic as Python
is, in fact, a huge constraint.</p>
<p>JAX solves this problem by always retracing and caching the graph after
retracing. Dynamo, on the other hand, uses guards to avoid retracing the
whole program every time.</p>
<p>A <strong>guard</strong> is an assumption (a boolean expression on an input) made in
order to specialize a frame for one set of example inputs. Reusing the
graph is only valid if these assumptions hold on the new inputs.</p>
<p>For example, any constant input to a function, like a string, installs a
guard stating that that input should be of type <code class="docutils literal notranslate"><span class="pre">str</span></code> and equal to the
string we passed. Running</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="s2">&quot;Hello&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=guards</span></code> prints (among other guards)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">___check_type_id</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="mi">94334122025024</span><span class="p">)</span>
<span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Hello&#39;</span>
</pre></div>
</div>
<p>This reads as “the local variable <code class="docutils literal notranslate"><span class="pre">b</span></code> should have a specific type
(<code class="docutils literal notranslate"><span class="pre">str</span></code> in this case, represented by the constant <cite>9433…</cite>) and
its value should be <code class="docutils literal notranslate"><span class="pre">'Hello'</span></code>”. If we then execute the function
again passing a different argument</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="s2">&quot;Hello&quot;</span><span class="p">)</span>
<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="s2">&quot;Hi&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>we can see the guard that failed by running <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=recompiles</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Recompiling</span> <span class="n">function</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">3</span>
<span class="n">triggered</span> <span class="n">by</span> <span class="n">the</span> <span class="n">following</span> <span class="n">guard</span> <span class="n">failure</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
     <span class="o">-</span> <span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Hello&#39;</span>
</pre></div>
</div>
<p>Guards are accumulated while <a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/variables/builder.py#L808-L810">the inputs to the function are wrapped in
the
builder</a>
and <a class="reference external" href="https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/variables/dicts.py#L763-L769">during the execution of the
program</a>.
We will show many more examples of guards in the next section, but first
let us discuss sources.</p>
<p>A <strong>source</strong> tracks how to reconstruct a variable from the original
local or global variables present when entering the current frame. In
particular, it tracks the original local and global objects and any of
the objects they contain. In</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> have
<code class="docutils literal notranslate"><span class="pre">`LocalSource</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/40dc0580a69565b06ec5263efe5d87cecc8200f7/torch/_dynamo/source.py#L80-L92">https://github.com/pytorch/pytorch/blob/40dc0580a69565b06ec5263efe5d87cecc8200f7/torch/_dynamo/source.py#L80-L92</a>&gt;`__
as their source, and <code class="docutils literal notranslate"><span class="pre">y[0]</span></code> has
<code class="docutils literal notranslate"><span class="pre">`GetItemSource</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/40dc0580a69565b06ec5263efe5d87cecc8200f7/torch/_dynamo/source.py#L302">https://github.com/pytorch/pytorch/blob/40dc0580a69565b06ec5263efe5d87cecc8200f7/torch/_dynamo/source.py#L302</a>&gt;`__,
which stores a <code class="docutils literal notranslate"><span class="pre">LocalSource</span></code> inside. On the other hand, <code class="docutils literal notranslate"><span class="pre">a</span></code> will not
have a source as it is an intermediate variable that only exists within
the fx graph.</p>
<p>All these are defined in
<code class="docutils literal notranslate"><span class="pre">`torch/_dynamo/source.py</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/source.py">https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/source.py</a>&gt;`__.
We can see the guard generated by <code class="docutils literal notranslate"><span class="pre">GetItemSource</span></code> in the following
example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span> <span class="p">[</span><span class="s2">&quot;Hi&quot;</span><span class="p">,</span> <span class="s2">&quot;Hello&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>generates the following guards</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">___check_type_id</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">],</span> <span class="mi">94439025877664</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span>
<span class="n">___check_type_id</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="mi">94439025840192</span><span class="p">)</span>
<span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Hi&#39;</span>
<span class="n">___check_type_id</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="mi">94439025840192</span><span class="p">)</span>
<span class="n">L</span><span class="p">[</span><span class="s1">&#39;l&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Hello&#39;</span>
</pre></div>
</div>
<p>Here, we see the code generated by <code class="docutils literal notranslate"><span class="pre">GetItemSource</span></code> (<code class="docutils literal notranslate"><span class="pre">[0]</span></code> and
<code class="docutils literal notranslate"><span class="pre">[1]</span></code>) wrapping a <code class="docutils literal notranslate"><span class="pre">LocalSource</span></code> (<code class="docutils literal notranslate"><span class="pre">L['l']</span></code>).</p>
<p>At this point, with sources and guards, we are able to implement a
caching system to avoid recompilation without having to retrace every
time. We will discuss a bit more in detail this caching system in the
sequel.</p>
<p>The attentive reader will have noticed that this does not explain yet
why we need to have such fine control over the Python interpreter as to
having to reimplement it. The examples of guards that we have shown
depend on the input objects, so we could still compute these before
executing the function. In other words, we could implement this guard
system on top of <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> and get the same functionality with
much less effort… Enter symbolic shapes.</p>
</div>
<div class="section" id="symbolic-shapes">
<h2>Symbolic Shapes<a class="headerlink" href="#symbolic-shapes" title="Permalink to this heading">¶</a></h2>
<p>Another point we discussed in the introduction is that Dynamo knows how
to trace integers. In order to implement this, we use a symbolic class
<code class="docutils literal notranslate"><span class="pre">`torch.SymInt</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/__init__.py#L244-L249">https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/__init__.py#L244-L249</a>&gt;`__ <a class="footnote-reference brackets" href="#id13" id="id6">5</a>
that acts like an <code class="docutils literal notranslate"><span class="pre">int</span></code> but it records all the operations performed on
it in the output FX graph. We already saw this class in the introduction
when introducing symbolic integer tracing.</p>
<p>Let us now discuss the three properties that define symbolic shape
tracing in Dynamo, and how to implement them.</p>
<div class="section" id="static-by-default">
<h3>Static by default<a class="headerlink" href="#static-by-default" title="Permalink to this heading">¶</a></h3>
<p>Dynamo assumes that every integer, let that be an input or the shape of
a tensor, is static by default. In other words, no integers will be
traced on the first execution of a function. Then, only if it detects
that an integer or a shape changed value during the execution, it will
trace it and generate a graph generic on that variable.</p>
<p>We already saw this behavior in the introduction using integers. Let us
now look at an example using shapes of tensors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>Running this program with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_code</span></code> we see that these
two calls are traced as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l_a_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_b_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">l_a_</span>
    <span class="n">mul_1</span> <span class="o">=</span> <span class="n">mul</span> <span class="o">*</span> <span class="n">l_b_</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul_1</span><span class="p">,)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s0</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">l_a_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">l_b_</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">l_a_</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">getitem</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="n">getitem</span> <span class="o">*</span> <span class="n">l_a_</span>
    <span class="n">mul_1</span> <span class="o">=</span> <span class="n">mul</span> <span class="o">*</span> <span class="n">l_b_</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mul_1</span><span class="p">,)</span>
</pre></div>
</div>
<p>In the first graph the shape is traced as a constant, but once it
changes, it traces it symbolically using a <code class="docutils literal notranslate"><span class="pre">SymInt</span></code>s. In general, a
simpler way to see the shapes of the intermediary values is by running
the program with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_sizes</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TRACED</span> <span class="n">GRAPH</span> <span class="n">TENSOR</span> <span class="n">SIZES</span>
<span class="o">=====</span> <span class="n">__compiled_fn_1</span> <span class="o">=====</span>
<span class="n">l_a_</span><span class="p">:</span> <span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">l_a_</span> <span class="p">(</span><span class="n">concrete</span><span class="p">):</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">l_b_</span><span class="p">:</span> <span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">l_b_</span> <span class="p">(</span><span class="n">concrete</span><span class="p">):</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">mul</span><span class="p">:</span> <span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">mul</span> <span class="p">(</span><span class="n">concrete</span><span class="p">):</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">mul_1</span><span class="p">:</span> <span class="p">(</span><span class="n">s0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">mul_1</span> <span class="p">(</span><span class="n">concrete</span><span class="p">):</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>where we can see that the first dimension of the two tensor args is
dynamic, given that it is represented by the <code class="docutils literal notranslate"><span class="pre">s0</span></code> variable.</p>
<p>We can find how Dynamo implements this by running <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=guards</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Guards first call</span>
<span class="n">check_tensor</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">check_tensor</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Guards second call</span>
<span class="n">check_tensor</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">check_tensor</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">L</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">L</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="mi">2</span> <span class="o">&lt;=</span> <span class="n">L</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>We see that on the first call, the guards check that the tensors have
some fixed sizes and strides. These guards fail in the second execution,
so it retraces. Since it was an <code class="docutils literal notranslate"><span class="pre">int</span></code> guard that failed, in this
second iteration it traces this <code class="docutils literal notranslate"><span class="pre">int</span></code> symbolically and it installs
more general guards on this more generic kernel.</p>
<p><strong>Compilation performance tip</strong>. If you know that a dimension will vary
in size, you can mark it as dynamic by calling
<code class="docutils literal notranslate"><span class="pre">`torch._dynamo.mark_dynamic</span></code> &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/66a76516bfc341b2b55bb2056d2faa9c2de46d69/torch/_dynamo/decorators.py#L176">https://github.com/pytorch/pytorch/blob/66a76516bfc341b2b55bb2056d2faa9c2de46d69/torch/_dynamo/decorators.py#L176</a>&gt;`__
before calling <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>. This will avoid the first compilation
with a static shape. There are other useful utility functions like
<code class="docutils literal notranslate"><span class="pre">maybe_mark_dynamic</span></code> or <code class="docutils literal notranslate"><span class="pre">mark_static</span></code>. You can also have all
integers and shapes traced by calling <code class="docutils literal notranslate"><span class="pre">torch.compile(dynamic=True)</span></code>.
This is mostly useful for debugging purposes.</p>
</div>
<div class="section" id="are-always-specialized">
<h3>0, 1 are always specialized<a class="headerlink" href="#are-always-specialized" title="Permalink to this heading">¶</a></h3>
<p>Regardless of whether we mark a dimension as dynamic, or we have traced
an integer as dynamic, if we pass an input where that dimension is 0 or
1, Dynamo will trace it as non-dynamic and it will generate a specific
graph for it. This is the reason why in the example above we find guards
of the form <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">&lt;=</span> <span class="pre">L['a'].size()[0]</span></code>.</p>
<p>There are several reasons for this choice. There are two particularly
important - A tensor is empty if and only if any of its dimensions is
zero - A tensor can only be contiguous if one of the strides is one</p>
</div>
<div class="section" id="duck-shaping">
<h3>Duck shaping<a class="headerlink" href="#duck-shaping" title="Permalink to this heading">¶</a></h3>
<p>Dynamo performs what we call “duck shaping”. If two dynamic integers
have the same value at trace time, we will assume that they are equal
and guard on it. Effectively, this means that rather than having two
symbols <code class="docutils literal notranslate"><span class="pre">s0</span></code>, <code class="docutils literal notranslate"><span class="pre">s1</span></code> in the example above, we just unified them to
<code class="docutils literal notranslate"><span class="pre">s0</span></code> and had the guard <code class="docutils literal notranslate"><span class="pre">L['b'].size()[0]</span> <span class="pre">==</span> <span class="pre">L['a'].size()[0]</span></code>. This
enables performing fusions within the compiler while being able to
generate kernels that are generic enough.</p>
</div>
<div class="section" id="guards-on-symbolic-ints">
<h3>Guards on symbolic ints<a class="headerlink" href="#guards-on-symbolic-ints" title="Permalink to this heading">¶</a></h3>
<p>We now understand how symbolic shapes are implemented at a high level
and the properties they have. Now, why is that symbolic shapes forced us
through the tricky route of getting control of the CPython interpreter?
Consider the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">dynamic</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">&lt;</span> <span class="mi">16</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
</div>
<p>This code has a guard of the form <code class="docutils literal notranslate"><span class="pre">2*L['a'].size()[0]</span> <span class="pre">&gt;=</span> <span class="pre">16</span></code>. This is
a non-trivial guard in terms of the inputs of the function, but it is
registered in the middle of the execution of the program. Even more so,
we cannot know this guard is needed until we see the <code class="docutils literal notranslate"><span class="pre">if</span></code> statement
conditional on a <code class="docutils literal notranslate"><span class="pre">SymNodeVariable</span></code> argument. Such conditions are
invisible to <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> and require deep analysis of the python
code.</p>
<p><strong>Debugging tip</strong> Running this code with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=dynamo</span></code> tells us
where this guard was added</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">eval</span> <span class="mi">2</span><span class="o">*</span><span class="n">s0</span> <span class="o">&gt;=</span> <span class="mi">16</span> <span class="p">[</span><span class="n">guard</span> <span class="n">added</span><span class="p">]</span> <span class="n">at</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">5</span> <span class="ow">in</span> <span class="n">fn</span> <span class="p">(</span><span class="n">_dynamo</span><span class="o">/</span><span class="n">variables</span><span class="o">/</span><span class="n">tensor</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">812</span> <span class="ow">in</span> <span class="n">evaluate_expr</span><span class="p">)</span>
</pre></div>
</div>
<p>Placing a breakpoint there and looking at the backtrace is rather useful
to understand where a guard came from.</p>
</div>
</div>
<div class="section" id="making-dynamo-complete-graph-breaks">
<h2>Making Dynamo Complete: Graph Breaks<a class="headerlink" href="#making-dynamo-complete-graph-breaks" title="Permalink to this heading">¶</a></h2>
<p>With all the tools we have discussed, we have a tracer that can trace
PyTorch operations on tensors and integers and has a caching system that
knows when it can reuse a previously traced graph and when it needs to
retrace. All this executing arbitrary Python code!</p>
<p>There is just one small issue with this. The statement “executing
arbitrary Python code” is perhaps a bit too general. Dynamo implements a
good part of Python, but does it implement the more complex parts, like
coroutines or async? Does it implement the whole Python standard
library? NumPy also has a Python API. Does <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> also
understand NumPy? and Django? <a class="footnote-reference brackets" href="#id14" id="id7">6</a></p>
<p>Python’s ecosystem is massive, and a good part of it is written in other
more performant languages like C++ or Rust, and it just exposes Python
bindings. There is no hope in Dynamo tracing through Python objects that
are implemented in C++. What can a tracer do when it finds an operation
that it does not understand?</p>
<p>The usual way machine learning tracers handle this issue is by informing
the user that the operation they choked on and giving up tracing
altogether. This would pose a real usability issue in the case of
PyTorch, where its users are used to the flexibility it gives them. As a
real-world example the <code class="docutils literal notranslate"><span class="pre">`doctr_det_predictor</span></code> model uses NumPy and the
<code class="docutils literal notranslate"><span class="pre">cv2</span></code> library to postprocess the model’s
result &lt;<a class="reference external" href="https://github.com/mindee/doctr/blob/f2114758d529ed8d3d0030581638f0520b6b98d8/doctr/models/detection/core.py#L86">https://github.com/mindee/doctr/blob/f2114758d529ed8d3d0030581638f0520b6b98d8/doctr/models/detection/core.py#L86</a>&gt;`__.</p>
<p>Here is another place where having access to CPython is interesting.
Rather than erroring out, Dynamo can let CPython run that problematic
code! To do this, Dynamo generates at trace time one graph with all the
operations before the problematic code, and one with all the operations
after. <a class="footnote-reference brackets" href="#id15" id="id8">7</a> Then, at runtime, it will delegate to CPython to execute the
first graph, then the problematic code, and then the second graph. This
process of stopping the tracing and generating multiple graphs is called
a <strong>graph break</strong>.</p>
<p>A small confession: I lied all throughout the introduction and the first
sections. Dynamo does not generate one graph, but <strong>multiple graphs</strong>!
For all practical purposes, starting retracing after a second graph can
be thought of as starting tracing a new function. The new graph after
the graph break will have its own guards, its new set of local
variables, and so on.</p>
<p>To discuss how to implement graph breaks, we need to first revisit how
Dynamo interacts with CPython. Using PEP 523, CPython allows a user to
use their own frame evaluation mechanism. What we had not discussed is
that CPython also exposes its own frame evaluation for others to use.
Dynamo leverages this to let the fast CPython interpreter run the
compiled code. For a function without graph breaks, the whole tracing /
execution process of a program that calls the function 2 times with the
same arguments looks like this:</p>
<ol class="arabic simple">
<li><p>In the first call to the function</p>
<ol class="arabic simple">
<li><p>Dynamo traces the function into an FX graph</p>
<ol class="arabic simple">
<li><p>The FX graph is compiled by the compiler (Inductor) into
efficient low-level code… but that’s a story for another day</p></li>
</ol>
</li>
<li><p>It rewrites the bytecode of the function so that it simply calls
the compiled function</p></li>
<li><p>It gives CPython this new bytecode and asks it to run it
[<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L1006">here</a>]</p></li>
</ol>
</li>
<li><p>In the second call to the function</p>
<ol class="arabic simple">
<li><p>It checks the guards from the first call against the new arguments
[<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L658">here</a>].
Since they are the same arguments as before, they pass</p></li>
<li><p>It asks CPython to run the bytecode associated to those guards
[<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L972-L975">here</a>]</p></li>
</ol>
</li>
</ol>
<p>This process on its own looks overly complicated. Why generate new
bytecode and ask CPython to run it rather than simply creating a C++
binding to the compiled function and executing it? Well, this pattern
allows us to implement graph breaks! The bytecode generated by a graph
break has the following structure:</p>
<ol class="arabic simple">
<li><p>Bytecode that executes the first graph</p></li>
<li><p>Bytecode that leaves the stack as it would be if CPython would have
executed the first graph. It also replays any modifications to local
or global variables that would be visible at this point</p></li>
<li><p>The bytecode that made Dynamo graph break</p></li>
<li><p>Bytecode that executes the second graph</p></li>
</ol>
<p>Let us see this in a simple example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hi&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="n">a</span>

<span class="n">fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
<p>Running this with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=bytecode</span></code> shows us the initial bytecode
and the modified bytecode</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MODIFIED</span> <span class="n">BYTECODE</span> <span class="n">fn</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span> <span class="n">line</span> <span class="mi">3</span>
 <span class="mi">0</span> <span class="n">LOAD_GLOBAL</span>              <span class="mi">1</span> <span class="p">(</span><span class="n">__compiled_fn_0</span><span class="p">)</span>
 <span class="mi">2</span> <span class="n">LOAD_FAST</span>                <span class="mi">0</span> <span class="p">(</span><span class="n">a</span><span class="p">)</span>
 <span class="mi">4</span> <span class="n">CALL_FUNCTION</span>            <span class="mi">1</span>
 <span class="mi">6</span> <span class="n">STORE_FAST</span>               <span class="mi">3</span> <span class="p">(</span><span class="n">graph_out_0</span><span class="p">)</span>
 <span class="mi">8</span> <span class="n">LOAD_GLOBAL</span>              <span class="mi">0</span> <span class="p">(</span><span class="nb">print</span><span class="p">)</span>
<span class="mi">10</span> <span class="n">LOAD_CONST</span>               <span class="mi">2</span> <span class="p">(</span><span class="s1">&#39;Hi&#39;</span><span class="p">)</span>
<span class="mi">12</span> <span class="n">LOAD_FAST</span>                <span class="mi">3</span> <span class="p">(</span><span class="n">graph_out_0</span><span class="p">)</span>
<span class="mi">14</span> <span class="n">LOAD_CONST</span>               <span class="mi">3</span> <span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="mi">16</span> <span class="n">BINARY_SUBSCR</span>
<span class="mi">18</span> <span class="n">STORE_FAST</span>               <span class="mi">1</span> <span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="mi">20</span> <span class="n">CALL_FUNCTION</span>            <span class="mi">1</span>
<span class="mi">22</span> <span class="n">LOAD_GLOBAL</span>              <span class="mi">2</span> <span class="p">(</span><span class="n">__resume_at_14_1</span><span class="p">)</span>
<span class="mi">24</span> <span class="n">ROT_TWO</span>
<span class="mi">26</span> <span class="n">LOAD_FAST</span>                <span class="mi">0</span> <span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="mi">28</span> <span class="n">LOAD_FAST</span>                <span class="mi">1</span> <span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="mi">30</span> <span class="n">CALL_FUNCTION</span>            <span class="mi">3</span>
<span class="mi">32</span> <span class="n">RETURN_VALUE</span>

<span class="n">MODIFIED</span> <span class="n">BYTECODE</span> <span class="n">resume_in_fn</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span> <span class="n">line</span> <span class="mi">6</span>
 <span class="mi">0</span> <span class="n">LOAD_GLOBAL</span>              <span class="mi">1</span> <span class="p">(</span><span class="n">__compiled_fn_2</span><span class="p">)</span>
 <span class="mi">2</span> <span class="n">LOAD_FAST</span>                <span class="mi">2</span> <span class="p">(</span><span class="n">b</span><span class="p">)</span>
 <span class="mi">4</span> <span class="n">LOAD_FAST</span>                <span class="mi">1</span> <span class="p">(</span><span class="n">a</span><span class="p">)</span>
 <span class="mi">6</span> <span class="n">CALL_FUNCTION</span>            <span class="mi">2</span>
 <span class="mi">8</span> <span class="n">UNPACK_SEQUENCE</span>          <span class="mi">1</span>
<span class="mi">10</span> <span class="n">RETURN_VALUE</span>
</pre></div>
</div>
<p>We can see that the modified bytecode is split into two functions,
<code class="docutils literal notranslate"><span class="pre">fn</span></code>, the original function, and a function called <code class="docutils literal notranslate"><span class="pre">resume_in_fn</span></code>.
This second function is a function created by Dynamo to implement the
execution of the program starting at the graph break. This is often
called a <a class="reference external" href="https://en.wikipedia.org/wiki/Continuation">continuation
function</a>. This
continuation function simply calls the second compiled function with the
right arguments. The code for the initial function is rewritten
implementing the strategy that we described before</p>
<ul class="simple">
<li><p>L0-4. Call the compiled function (<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">2</span></code>).</p></li>
<li><p>L6. Store its result in a local variable called <code class="docutils literal notranslate"><span class="pre">graph_out_0</span></code>.
<code class="docutils literal notranslate"><span class="pre">graph_out_0</span></code> is a tuple</p></li>
<li><p>L8-18. Leave the stack as it would be at the point of the graph break</p></li>
<li><p>L20. Execute the code that caused the graph break</p></li>
<li><p>L22-32. Call the compiled continuation function (<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span></code>)</p></li>
</ul>
<p>The code generation of the stack in Dynamo is delegated to
<code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> subclasses. Every <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> object in
Dynamo has a <code class="docutils literal notranslate"><span class="pre">`reconstruct</span></code>
method &lt;<a class="reference external" href="https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/_dynamo/variables/lists.py#L307-L309">https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/_dynamo/variables/lists.py#L307-L309</a>&gt;`__
that generates the necessary bytecode to create the python object it
represents on the stack.</p>
<p><strong>Debugging tip</strong>. Graph breaks hamper performance, and as such, it is
best to avoid them. Running a program with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=graph_breaks</span></code>
is a great way to find how many graph breaks did our program hit. The
information it returns is in terms of <code class="docutils literal notranslate"><span class="pre">VariableTracker</span></code> objects, so
the debugging tips above are sometimes also helpful to figure out what
caused that graph break.</p>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>Dynamo is a complex piece of software. Once you sign up to implement a
CPython interpreter you know you are in for a ride. That being said, we
hope that this post helps demystify it a bit.</p>
<p>Dynamo is (mostly) implemented in Python. We left plenty of links to the
pieces of the code that we discussed. We hope that reading those pieces
of code and grepping for the places that call them, or putting
breakpoints on them and looking at the call stack helps understanding
the rest of the code base.</p>
<p>Of course, the best way to learn how a piece of software works is by
extending it. In this case, the best way is to have a look at the <a class="reference external" href="https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22module%3A+dynamo%22+">open
dynamo issues on
github</a>.
Many of them require very minor changes in the code, once you find where
you need to make those changes.</p>
<dl class="footnote brackets">
<dt class="label" id="id9"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>In the same way that Dynamo takes its name from
[Dynamorio].(<a class="reference external" href="https://dynamorio.org/">https://dynamorio.org/</a>), this blog post’s name is a
small nod to <a class="reference external" href="https://www.ams.org/notices/200601/fea-chow.pdf">You Could Have Invented Spectral
Sequences</a>.</p>
</dd>
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>In the literature, this is called a Directed Acyclical Graph (DAG).</p>
</dd>
<dt class="label" id="id11"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>All this binding code lives in <code class="docutils literal notranslate"><span class="pre">torch/csrc/dynamo/eval_frame.c</span></code>.</p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>In CPython lingo, the set of all these objects are called <a class="reference external" href="https://github.com/python/cpython/blob/f26bfe4b25f7e5a4f68fcac26207b7175abad208/Include/internal/pycore_frame.h#L57-L71">a
frame</a>.</p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>There are also <code class="docutils literal notranslate"><span class="pre">SymBool</span></code> and <code class="docutils literal notranslate"><span class="pre">SymFloat</span></code> classes. The latter one
is not used all that much at the time of this writing.</p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id7">6</a></span></dt>
<dd><p>Interestingly enough, it does understand NumPy code! Have a look at
<a class="reference external" href="https://pytorch.org/blog/compiling-numpy-code/">this blogpost</a>
and <a class="reference external" href="https://pytorch.org/docs/stable/torch.compiler_faq.html#does-numpy-work-with-torch-compile">the
docs</a>.
Now, this is just possible because we reimplemented NumPy using
PyTorch. Good luck implementing Django in PyTorch though…</p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id8">7</a></span></dt>
<dd><p>Assuming there is just one piece of problematic code. If there are
more, Dynamo can split the code into as many graphs as it needs.</p>
</dd>
</dl>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="torch.compiler_nn_module.html" class="btn btn-neutral float-right" title="PyTorch 2.0 NNModule Support" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="torch.compiler_performance_dashboard.html" class="btn btn-neutral" title="PyTorch 2.0 Performance Dashboard" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Dynamo Deep-Dive</a><ul>
<li><a class="reference internal" href="#a-gentle-introduction-to-dynamo">A Gentle Introduction to Dynamo</a></li>
<li><a class="reference internal" href="#pep-523-adding-a-frame-evaluation-api-to-cpython">PEP 523: Adding a frame evaluation API to CPython</a></li>
<li><a class="reference internal" href="#implementing-cpython-in-python">Implementing CPython in Python</a></li>
<li><a class="reference internal" href="#generating-the-output-graph">Generating the Output Graph</a></li>
<li><a class="reference internal" href="#making-dynamo-sound-guards">Making Dynamo Sound: Guards</a></li>
<li><a class="reference internal" href="#symbolic-shapes">Symbolic Shapes</a><ul>
<li><a class="reference internal" href="#static-by-default">Static by default</a></li>
<li><a class="reference internal" href="#are-always-specialized">0, 1 are always specialized</a></li>
<li><a class="reference internal" href="#duck-shaping">Duck shaping</a></li>
<li><a class="reference internal" href="#guards-on-symbolic-ints">Guards on symbolic ints</a></li>
</ul>
</li>
<li><a class="reference internal" href="#making-dynamo-complete-graph-breaks">Making Dynamo Complete: Graph Breaks</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
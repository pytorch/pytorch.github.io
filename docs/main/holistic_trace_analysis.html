


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Holistic Trace Analysis &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/holistic_trace_analysis.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.hub" href="hub.html" />
    <link rel="prev" title="torch.fx" href="fx.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch">
                  <span class="dropdown-title">ExecuTorch</span>
                </a>
              </div>
            </div>  
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.3.0a0+git270ed13 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/holistic_trace_analysis.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">Understanding CUDA Memory Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">Generating a Snapshot</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">Using the visualizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">Snapshot API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Holistic Trace Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.bias.html">torch.nn.attention.bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.bias.html#causalbias">CausalBias</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Holistic Trace Analysis</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/holistic_trace_analysis.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="holistic-trace-analysis">
<span id="id1"></span><h1>Holistic Trace Analysis<a class="headerlink" href="#holistic-trace-analysis" title="Permalink to this heading">¶</a></h1>
<p>Holistic Trace Analysis (HTA) is an open source performance analysis and
visualization Python library for PyTorch users. HTA takes as input <a class="reference external" href="https://github.com/pytorch/kineto">Kineto
traces</a> collected by the <a class="reference external" href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/">PyTorch Profiler</a>
and up-levels the performance information contained in the traces.</p>
<p>ML researchers and systems engineers often struggle to computationally scale up
their models because they are not aware of the performance bottlenecks in their
workloads. The resources requested for a job (e.g. GPUs, memory) are often
misaligned with the resources actually required due to lack of visibility
“under the hood”.</p>
<p>The goal of HTA is to help engineers and researchers achieve the best
performance from the hardware stack. For this to happen it is imperative to
understand the resource utilization and bottlenecks for distributed training
and inference workloads.</p>
<div class="section" id="features-in-holistic-trace-analysis">
<h2>Features in Holistic Trace Analysis<a class="headerlink" href="#features-in-holistic-trace-analysis" title="Permalink to this heading">¶</a></h2>
<p>To aid in performance debugging HTA provides the following features</p>
<ol class="arabic simple">
<li><p>Temporal Breakdown: Breakdown of GPU time in
terms of time spent in computation, communication, memory events, and idle
time on a single node and across all ranks.</p></li>
<li><p>Idle Time Breakdown: Breakdown of GPU idle
time into waiting for the host, waiting for another kernel or attributed to
an unknown cause.</p></li>
<li><p>Kernel Breakdown: Find
kernels with the longest duration on each rank.</p></li>
<li><p>Kernel Duration Distribution: Distribution of average time
taken by longest kernels across different ranks.</p></li>
<li><p>Communication Computation Overlap:  Calculate the
percentage of time when communication overlaps computation.</p></li>
<li><p>CUDA Kernel Launch Statistics: Distributions
of GPU kernels with very small duration, large duration, and excessive
launch time.</p></li>
<li><p>Augmented Counters (Memory copy bandwidth, Queue length) &lt;source/features/augmented_counters.html&gt;`_:
Augmented trace files which provide insights into memory copy bandwidth and
number of outstanding operations on each CUDA stream.</p></li>
<li><p>Frequent CUDA Kernel Patterns: Find the CUDA
kernels most frequently launched by any given PyTorch or user defined
operator.</p></li>
<li><p>Trace Diff: A trace comparison tool to identify and
visualize the differences between traces.</p></li>
<li><p>CUPTI Counter Analysis: An
experimental API to interpret GPU performance counters. It attributes
performance measurements from kernels to PyTorch operators, and can help
with kernel optimization and roofline analysis.</p></li>
<li><p>Lightweight Critical Path Analysis: An
experimental API to compute the critical path in the trace. Critical path
can help one undertand if an application is CPU bound, GPU compute bound or
communication bound. The path can be visualized on the original trace
as well as manipulated as a directed acyclic graph object.</p></li>
</ol>
<p>A more detailed description of the these features is given below.</p>
</div>
<div class="section" id="performance-debugging-101">
<h2>Performance Debugging 101<a class="headerlink" href="#performance-debugging-101" title="Permalink to this heading">¶</a></h2>
<p>To understand the GPU performance in distributed workloads, we consider how the
model operators interact with the GPU devices and how such interactions are
reflected in certain measurable metrics. At a high level, we can break down the
GPU operations in a model execution into three broad categories, henceforth
referred to as kernel types:</p>
<ol class="arabic simple">
<li><p><strong>Computation (COMP)</strong> - Computation kernels execute compiled routines for
matrix multiplication and similar numeric calculations. They are responsible
for all of the number crunching necessary for model execution.</p></li>
<li><p><strong>Communication (COMM)</strong> - Communication kernels are routines which are
responsible for exchanging and synchronizing data between different GPU
devices in a distributed training job. The NVIDIA Collective Communication
Library (NCCL) is a widely used communication library and all its kernels
have the prefix “nccl”. Example NCCL kernels include NCCL_AllGather,
NCCL_ReduceScatter, NCCL_AllReduce, etc.</p></li>
<li><p><strong>Memory (MEM)</strong> - Memory kernels manage the memory allocations and
deallocations on the GPU devices and data movement between the memory space
on the host and the GPUs. The memory kernels include Memcpy_H2D, Memcpy_D2H,
Memcpy_D2D, Memset, etc. Here, H represents the Host and D represents the
GPU Device. Thus, H2D, D2H, D2D stands for Host to Device, Device to Host
and Device to Device respectively.</p></li>
</ol>
<p>Because a modern GPU device e.g. NVIDIA A100 is a massively parallel
device which is capable of running multiple kernels simultaneously, it is
possible to overlap the computation, communication, and memory kernels to
reduce the model execution time. One common technique to achieve the overlap is
to utilize multiple CUDA streams. A CUDA stream is a sequence of operations
that execute on a GPU device in the order in which they are issued by the host
code. Different CUDA streams can be interleaved and even run concurrently, thus
achieving the effect of kernel overlap.</p>
<p>The performance of multiple GPU training jobs is affected by multiple factors.
Among these factors, how does a model execution create and orchestrate the GPU
kernels plays a critical role. HTA provides insights on how the model execution
interacts with the GPU devices and highlights the opportunities for performance
improvement.</p>
<p>With the features built in HTA, we aim to provide users insights into “what
is happening under the hood in a distributed GPU workloads?” We describe
these features in the upcoming sections.</p>
</div>
<div class="section" id="trace-collection">
<h2>Trace Collection<a class="headerlink" href="#trace-collection" title="Permalink to this heading">¶</a></h2>
<p>Trace collection in PyTorch is enabled by wrapping the training/inference loop
in a <code class="docutils literal notranslate"><span class="pre">profile</span></code> context. A couple of useful options to know about are
<code class="docutils literal notranslate"><span class="pre">tracing</span> <span class="pre">schedule</span></code> and <code class="docutils literal notranslate"><span class="pre">trace</span> <span class="pre">handler</span></code>. The <cite>tracing schedule</cite> allows the
user to specify how many steps we can skip, wait, warmup the profiler, record
the activity and finally how many times to repeat the process. During the
warmup, the profiler is running but no events are being recorded hence there is
no profiling overhead. The <cite>trace handler</cite> allows to specify the output folder
along with the option to gzip the trace file. Given that trace files can easily
run into hundreds of MBs this is useful to have.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">profile</span></code> context also gives options to record either or both CPU and GPU
events using the activities argument. Users can also record the shapes of the
tensors with <code class="docutils literal notranslate"><span class="pre">record_shapes</span></code> argument and collect the python call stack with
the <code class="docutils literal notranslate"><span class="pre">with_stack</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">with_stack</span></code> argument is especially helpful in
connecting the trace event to the source code, which enables faster debugging.
The <code class="docutils literal notranslate"><span class="pre">profile_memory</span></code> option allows tracking tensor memory allocations and
deallocations.</p>
<p>To profile, wrap the code in the <code class="docutils literal notranslate"><span class="pre">profile</span></code> context manager as shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">from</span> <span class="nn">torch.profiler</span> <span class="kn">import</span> <span class="n">profile</span><span class="p">,</span> <span class="n">schedule</span><span class="p">,</span> <span class="n">tensorboard_trace_handler</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="n">tracing_schedule</span> <span class="o">=</span> <span class="n">schedule</span><span class="p">(</span><span class="n">skip_first</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 4</span><span class="n">trace_handler</span> <span class="o">=</span> <span class="n">tensorboard_trace_handler</span><span class="p">(</span><span class="n">dir_name</span><span class="o">=/</span><span class="n">output</span><span class="o">/</span><span class="n">folder</span><span class="p">,</span> <span class="n">use_gzip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="k">with</span> <span class="n">profile</span><span class="p">(</span>
<span class="linenos"> 7</span>  <span class="n">activities</span> <span class="o">=</span> <span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
<span class="linenos"> 8</span>  <span class="n">schedule</span> <span class="o">=</span> <span class="n">tracing_schedule</span><span class="p">,</span>
<span class="linenos"> 9</span>  <span class="n">on_trace_ready</span> <span class="o">=</span> <span class="n">trace_handler</span><span class="p">,</span>
<span class="linenos">10</span>  <span class="n">profile_memory</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="linenos">11</span>  <span class="n">record_shapes</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="linenos">12</span>  <span class="n">with_stack</span> <span class="o">=</span> <span class="kc">True</span>
<span class="linenos">13</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
<span class="linenos">14</span>
<span class="linenos">15</span>    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch_data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
<span class="linenos">16</span>        <span class="n">train</span><span class="p">(</span><span class="n">batch_data</span><span class="p">)</span>
<span class="hll"><span class="linenos">17</span>        <span class="n">prof</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></pre></div>
</div>
<p>Line 17 in the code snippet above signals to the profiler that a training
iteration has completed.</p>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">¶</a></h2>
<p>We recommend using a Conda environment to install HTA. To install Anaconda, see
<a class="reference external" href="https://docs.anaconda.com/anaconda/install/index.html">here</a>. Holistic Trace
Analysis runs on Linux and Mac with Python &gt;= 3.8.</p>
<p><strong>Setup a Conda environment</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the environment env_name</span>
<span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="n">env_name</span>

<span class="c1"># activate the environment</span>
<span class="n">conda</span> <span class="n">activate</span> <span class="n">env_name</span>

<span class="c1"># deactivate the environment</span>
<span class="n">conda</span> <span class="n">deactivate</span>
</pre></div>
</div>
<p><strong>Installing Holistic Trace Analysis</strong></p>
<p>Install using pip</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">HolisticTraceAnalysis</span>
</pre></div>
</div>
<p>Install from source</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># get the source code</span>
<span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">facebookresearch</span><span class="o">/</span><span class="n">HolisticTraceAnalysis</span><span class="o">.</span><span class="n">git</span>

<span class="c1"># execute the command below from the root of the repo</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">e</span> <span class="o">.</span>
</pre></div>
</div>
</div>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this heading">¶</a></h2>
<div class="section" id="temporal-breakdown">
<h3>Temporal Breakdown<a class="headerlink" href="#temporal-breakdown" title="Permalink to this heading">¶</a></h3>
<p>To best utilize the GPUs it is vital to understand where the GPU is spending
time for a given job. Is the GPU spending time on computation, communication,
memory events, or is it idle? The temporal
breakdown feature breaks down the time spent in three categories</p>
<ol class="arabic simple">
<li><p>Idle time - GPU is idle.</p></li>
<li><p>Compute time - GPU is being used for matrix multiplications or vector operations.</p></li>
<li><p>Non-compute time - GPU is being used for communication or memory events.</p></li>
</ol>
<p>To achieve high training efficiency the code should maximize compute time and
minimize idle time and non-compute time. This is accomplished by implementing
concurrent execution of computation kernels with communication or memory
kernels.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>During concurrent execution of computation kernels with communication/memory
kernels the time spent by communication/memory kernels is accounted for
under compute time.</p>
</div>
<p>The temporal breakdown can be calculated as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">TraceAnalysis</span><span class="p">(</span><span class="n">trace_dir</span> <span class="o">=</span> <span class="s2">&quot;/path/to/trace/folder&quot;</span><span class="p">)</span>
<span class="n">time_spent_df</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_temporal_breakdown</span><span class="p">()</span>
</pre></div>
</div>
<p>The function returns a dataframe containing the temporal breakdown for each rank.
See figure below.</p>
<img alt="_images/temporal_breakdown_df.png" src="_images/temporal_breakdown_df.png" />
<p>When the <code class="docutils literal notranslate"><span class="pre">visualize</span></code> argument is set to True, the <cite>get_temporal_breakdown</cite>
function also generates a bar graph representing the breakdown by rank.</p>
<img alt="_images/temporal_breakdown_plot.png" src="_images/temporal_breakdown_plot.png" />
</div>
<div class="section" id="idle-time-breakdown">
<h3>Idle Time Breakdown<a class="headerlink" href="#idle-time-breakdown" title="Permalink to this heading">¶</a></h3>
<p>Understanding how much time the GPU is idle and its causes can help direct
optimization strategies. A GPU is considered idle when no kernel is running on
it. We developed an algorithm to categorize the Idle time into 3 categories:</p>
<ol class="arabic simple">
<li><p>Host wait: is the idle duration on the GPU due to the CPU not enqueuing
kernels fast enough to keep the GPU busy. These kinds of inefficiencies can
be resolved by examining the CPU operators that are contributing to the slow
down, increasing the batch size and applying operator fusion.</p></li>
<li><p>Kernel wait: constitutes the short overhead to launch consecutive kernels on
the GPU. The idle time attributed to this category can be minimized by using
CUDA Graph optimizations.</p></li>
<li><p>Other wait: Lastly, this category includes idle we could not currently
attribute due to insufficient information. The likely causes include
synchronization among CUDA streams using CUDA events and delays in launching
kernels.</p></li>
</ol>
<p>The host wait time can be interpreted as the time when the GPU is stalling due
to the CPU. To attribute the idle time as kernel wait we use the following
heuristic:</p>
<blockquote>
<div><div class="line-block">
<div class="line"><strong>gap between consecutive kernels &lt; threshold</strong></div>
</div>
</div></blockquote>
<p>The default threshold value is 30 nanoseconds and can be configured using the
<code class="docutils literal notranslate"><span class="pre">consecutive_kernel_delay</span></code> argument. By default, the idle time breakdown is
computed for rank 0 only. In order to calculate the breakdown for other ranks,
use the <code class="docutils literal notranslate"><span class="pre">ranks</span></code> argument in the <cite>get_idle_time_breakdown</cite>
function. The idle time breakdown can be generated as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">TraceAnalysis</span><span class="p">(</span><span class="n">trace_dir</span> <span class="o">=</span> <span class="s2">&quot;/path/to/trace/folder&quot;</span><span class="p">)</span>
<span class="n">idle_time_df</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_idle_time_breakdown</span><span class="p">()</span>
</pre></div>
</div>
<img alt="_images/idle_time_breakdown_percentage.png" src="_images/idle_time_breakdown_percentage.png" />
<p>The function returns a tuple of dataframes. The first dataframe contains the
idle time by category on each stream for each rank.</p>
<img alt="_images/idle_time.png" class="align-center" src="_images/idle_time.png" />
<p>The second dataframe is generated when <code class="docutils literal notranslate"><span class="pre">show_idle_interval_stats</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>. It contains the summary statistics of the idle time for each stream
on each rank.</p>
<img alt="_images/idle_time_summary.png" src="_images/idle_time_summary.png" />
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By default, the idle time breakdown presents the percentage of each of the
idle time categories. Setting the <code class="docutils literal notranslate"><span class="pre">visualize_pctg</span></code> argument to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
the function renders with absolute time on the y-axis. See image below.</p>
</div>
<img alt="_images/idle_time_breakdown.png" src="_images/idle_time_breakdown.png" />
</div>
<div class="section" id="kernel-breakdown">
<h3>Kernel Breakdown<a class="headerlink" href="#kernel-breakdown" title="Permalink to this heading">¶</a></h3>
<p>The kernel breakdown feature breaks down the time spent for each kernel type
i.e. communication (COMM), computation (COMP), and memory (MEM) across all
ranks and presents the proportion of time spent in each category. The
percentage of time spent in each category as a pie chart.</p>
<img alt="_images/kernel_type_breakdown.png" class="align-center" src="_images/kernel_type_breakdown.png" />
<p>The kernel breakdown can be calculated as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">TraceAnalysis</span><span class="p">(</span><span class="n">trace_dir</span> <span class="o">=</span> <span class="s2">&quot;/path/to/trace/folder&quot;</span><span class="p">)</span>
<span class="n">kernel_type_metrics_df</span><span class="p">,</span> <span class="n">kernel_metrics_df</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_gpu_kernel_breakdown</span><span class="p">()</span>
</pre></div>
</div>
<p>The first dataframe returned by the function contains the raw values used to
generate the Pie chart.</p>
</div>
<div class="section" id="kernel-duration-distribution">
<h3>Kernel Duration Distribution<a class="headerlink" href="#kernel-duration-distribution" title="Permalink to this heading">¶</a></h3>
<p>The second dataframe returned by <cite>get_gpu_kernel_breakdown</cite>
contains duration summary statistics for each kernel. In particular, this
includes the count, min, max, average, standard deviation, sum and kernel type
for each kernel on each rank.</p>
<img alt="_images/kernel_metrics_df.png" class="align-center" src="_images/kernel_metrics_df.png" />
<p>Using this data HTA creates many visualizations to identify performance
bottlenecks.</p>
<ol class="arabic simple">
<li><p>Pie charts of the top kernels for each kernel type for each rank.</p></li>
<li><p>Bar graphs of the average duration across all ranks for each of the top
kernels and for each kernel type.</p></li>
</ol>
<img alt="_images/pie_charts.png" src="_images/pie_charts.png" />
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>All images are generated using plotly. Hovering on the graph shows the
mode bar on the top right which allows the user to zoom, pan, select and
download the graph.</p>
</div>
<p>The pie charts above shows the top 5 computation, communication and memory
kernels. Similar pie charts are generated for each rank. The pie charts can be
configured to show the top k kernels using the <code class="docutils literal notranslate"><span class="pre">num_kernels</span></code> argument passed to
the <cite>get_gpu_kernel_breakdown</cite>
function. Additionally, the <code class="docutils literal notranslate"><span class="pre">duration_ratio</span></code> argument can be used to tune the
percentage of time that needs to be analyzed. If both <code class="docutils literal notranslate"><span class="pre">num_kernels</span></code> and
<code class="docutils literal notranslate"><span class="pre">duration_ratio</span></code> are specified, then <code class="docutils literal notranslate"><span class="pre">num_kernels</span></code> takes precedence.</p>
<img alt="_images/comm_across_ranks.png" src="_images/comm_across_ranks.png" />
<p>The bar graph above shows the average duration of the NCCL AllReduce kernel
across all the ranks. The black lines indicate the minimum and maximum time
taken on each rank.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When using jupyter-lab set the “image_renderer” argument value to
“jupyterlab” otherwise the graphs will not render in the notebook.</p>
</div>
<p>For a detailed walkthrough of this feature see the <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/kernel_breakdown_demo.ipynb">gpu_kernel_breakdown
notebook</a>
in the examples folder of the repo.</p>
</div>
<div class="section" id="communication-computation-overlap">
<h3>Communication Computation Overlap<a class="headerlink" href="#communication-computation-overlap" title="Permalink to this heading">¶</a></h3>
<p>In distributed training a significant amount of time is spent in communication
and synchronization events between GPUs. To achieve high GPU efficiency (i.e.
TFLOPS/GPU) it is vital to keep the GPU oversubscribed with computation
kernels. In other words, the GPU should not be blocked due to unresolved data
dependencies. One way to measure the extent to which computation is blocked by
data dependencies is to calculate the communication computation overlap. Higher
GPU efficiency is observed if communication events overlap computation events.
Lack of communication and computation overlap will lead to the GPU being idle,
thus the efficiency would be low. To sum up, higher communication computation
overlap is desirable. To calculate the overlap percentage for each rank we
measure the following ratio:</p>
<blockquote>
<div><div class="line-block">
<div class="line"><strong>(time spent in computation while communicating) / (time spent in communication)</strong></div>
</div>
</div></blockquote>
<p>Communication computation overlap can be calculated as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">TraceAnalysis</span><span class="p">(</span><span class="n">trace_dir</span> <span class="o">=</span> <span class="s2">&quot;/path/to/trace/folder&quot;</span><span class="p">)</span>
<span class="n">overlap_df</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_comm_comp_overlap</span><span class="p">()</span>
</pre></div>
</div>
<p>The function returns a dataframe containing the overlap percentage
for each rank.</p>
<a class="reference internal image-reference" href="_images/overlap_df.png"><img alt="_images/overlap_df.png" class="align-center" src="_images/overlap_df.png" style="width: 253.0px; height: 309.0px;" /></a>
<p>When the <code class="docutils literal notranslate"><span class="pre">visualize</span></code> argument is set to True, the <cite>get_comm_comp_overlap</cite>
function also generates a bar graph representing the overlap by rank.</p>
<img alt="_images/overlap_plot.png" src="_images/overlap_plot.png" />
</div>
<div class="section" id="augmented-counters">
<h3>Augmented Counters<a class="headerlink" href="#augmented-counters" title="Permalink to this heading">¶</a></h3>
<p><strong>Memory Bandwidth &amp; Queue Length Counters</strong></p>
<p>Memory bandwidth counters measure the memory copy bandwidth used while copying
the data from H2D, D2H and D2D by memory copy (memcpy) and memory set (memset)
events. HTA also computes the number of outstanding operations on each CUDA
stream. We refer to this as <strong>queue length</strong>. When the queue length on a stream
is 1024 or larger new events cannot be scheduled on that stream and the CPU
will stall until the events on the GPU stream have processed.</p>
<p>The <cite>generate_trace_with_counters</cite>
API outputs a new trace file with the memory bandwidth and queue length
counters. The new trace file contains tracks which indicate the memory
bandwidth used by memcpy/memset operations and tracks for the queue length on
each stream. By default, these counters are generated using the rank 0
trace file and the new file contains the suffix <code class="docutils literal notranslate"><span class="pre">_with_counters</span></code> in its name.
Users have the option to generate the counters for multiple ranks by using the
<code class="docutils literal notranslate"><span class="pre">ranks</span></code> argument in the <cite>generate_trace_with_counters</cite>
API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">TraceAnalysis</span><span class="p">(</span><span class="n">trace_dir</span> <span class="o">=</span> <span class="s2">&quot;/path/to/trace/folder&quot;</span><span class="p">)</span>
<span class="n">analyzer</span><span class="o">.</span><span class="n">generate_trace_with_counters</span><span class="p">()</span>
</pre></div>
</div>
<p>A screenshot of the generated trace file with augmented counters.</p>
<img alt="_images/mem_bandwidth_queue_length.png" src="_images/mem_bandwidth_queue_length.png" />
<p>HTA also provides a summary of the memory copy bandwidth and queue length
counters as well as the time series of the counters for the profiled portion of
the code using the following API:</p>
<ol class="arabic simple">
<li><p><cite>get_memory_bw_summary</cite></p></li>
<li><p><cite>get_queue_length_summary</cite></p></li>
<li><p><cite>get_memory_bw_time_series</cite></p></li>
<li><p><cite>get_queue_length_series</cite></p></li>
</ol>
<p>To view the summary and time series use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate summary</span>
<span class="n">mem_bw_summary</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_memory_bw_summary</span><span class="p">()</span>
<span class="n">queue_len_summary</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_queue_length_summary</span><span class="p">()</span>

<span class="c1"># get time series</span>
<span class="n">mem_bw_series</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_memory_bw_time_series</span><span class="p">()</span>
<span class="n">queue_len_series</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_queue_length_series</span><span class="p">()</span>
</pre></div>
</div>
<p>The summary contains the count, min, max, mean, standard deviation, 25th, 50th,
and 75th percentile.</p>
<img alt="_images/queue_length_summary.png" class="align-center" src="_images/queue_length_summary.png" />
<p>The time series only contains the points when a value changes. Once a value is
observed the time series stays constant until the next update. The memory
bandwidth and queue length time series functions return a dictionary whose key
is the rank and the value is the time series for that rank. By default, the
time series is computed for rank 0 only.</p>
</div>
<div class="section" id="cuda-kernel-launch-statistics">
<h3>CUDA Kernel Launch Statistics<a class="headerlink" href="#cuda-kernel-launch-statistics" title="Permalink to this heading">¶</a></h3>
<img alt="_images/cuda_kernel_launch.png" src="_images/cuda_kernel_launch.png" />
<p>For each event launched on the GPU there is a corresponding scheduling event on
the CPU e.g. CudaLaunchKernel, CudaMemcpyAsync, CudaMemsetAsync. These events
are linked by a common correlation id in the trace. See figure above. This
feature computes the duration of the CPU runtime event, its corresponding GPU
kernel and the launch delay i.e. the difference between GPU kernel starting and
CPU operator ending. The kernel launch info can be generated as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">TraceAnalysis</span><span class="p">(</span><span class="n">trace_dir</span><span class="o">=</span><span class="s2">&quot;/path/to/trace/dir&quot;</span><span class="p">)</span>
<span class="n">kernel_info_df</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_cuda_kernel_launch_stats</span><span class="p">()</span>
</pre></div>
</div>
<p>A screenshot of the generated dataframe is given below.</p>
<img alt="_images/cuda_kernel_launch_stats.png" class="align-center" src="_images/cuda_kernel_launch_stats.png" />
<p>The duration of the CPU op, GPU kernel and the launch delay allows us to find:</p>
<ol class="arabic simple">
<li><p><strong>Short GPU kernels</strong> - GPU kernels with duration less than the
corresponding CPU runtime event.</p></li>
<li><p><strong>Runtime event outliers</strong> - CPU runtime events with excessive duration.</p></li>
<li><p><strong>Launch delay outliers</strong> - GPU kernels which take too long to be scheduled.</p></li>
</ol>
<p>HTA generates distribution plots for each of the aforementioned three categories.</p>
<p><strong>Short GPU kernels</strong></p>
<p>Usually, the launch time on the CPU side is between 5-20 microseconds. In some
cases the GPU execution time is lower than the launch time itself. The graph
below allows us to find how frequently such instances appear in the code.</p>
<img alt="_images/short_gpu_kernels.png" src="_images/short_gpu_kernels.png" />
<p><strong>Runtime event outliers</strong></p>
<p>The runtime outliers depend on the cutoff used to classify the outliers, hence
the <cite>get_cuda_kernel_launch_stats</cite>
API provides the <code class="docutils literal notranslate"><span class="pre">runtime_cutoff</span></code> argument to configure the value.</p>
<img alt="_images/runtime_outliers.png" src="_images/runtime_outliers.png" />
<p><strong>Launch delay outliers</strong></p>
<p>The launch delay outliers depend on the cutoff used to classify the outliers,
hence the <cite>get_cuda_kernel_launch_stats</cite>
API provides the <code class="docutils literal notranslate"><span class="pre">launch_delay_cutoff</span></code> argument to configure the value.</p>
<img alt="_images/launch_delay_outliers.png" src="_images/launch_delay_outliers.png" />
</div>
<div class="section" id="frequent-cuda-kernel-sequences">
<h3>Frequent CUDA Kernel Sequences<a class="headerlink" href="#frequent-cuda-kernel-sequences" title="Permalink to this heading">¶</a></h3>
<p>Consider a scenario where a sequence of CPU ops is called repeatedly in the
code. E.g. this behavior is commonly exhibited in a transformer architecture
with a large encoder or decoder stack. Suppose the user wants to know the most
frequent CUDA kernel sequences originating from an operator. Identifying these
frequent CUDA kernel sequences and their corresponding CPU ops provides
insights into which kernels would be ideal candidates for fusion.</p>
<p>This feature finds the sequences of most frequent CUDA kernels launched for any
specified operator. It generates a new trace file which overlays the top k
identified patterns on the original trace file. Searching for the keyword
<code class="docutils literal notranslate"><span class="pre">Patterns</span></code> in the new trace file highlights the relevant CPU and GPU ops. The
highlighted events indicate where to look for opportunities to fuse CUDA
kernels or CPU ops.</p>
<img alt="_images/overlaid_trace.png" src="_images/overlaid_trace.png" />
<p>This analysis is done on a single rank as the CPU and GPU ops are expected to
be the same across different ranks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">TraceAnalysis</span><span class="p">(</span><span class="n">trace_dir</span> <span class="o">=</span> <span class="s2">&quot;/path/to/trace_folder&quot;</span><span class="p">)</span>
<span class="n">cuda_sequences_df</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_frequent_cuda_kernel_sequences</span><span class="p">(</span>
    <span class="n">operator_name</span> <span class="o">=</span> <span class="s2">&quot;aten::linear&quot;</span><span class="p">,</span>
    <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&quot;/tmp/&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The minimum length of the CUDA kernel sequence that should be identified can be
specified using the <code class="docutils literal notranslate"><span class="pre">min_pattern_len</span></code> argument and the <code class="docutils literal notranslate"><span class="pre">top_k</span></code> argument
allows the user to specify the top k patterns in terms of frequency to be
overlaid on the new trace file.</p>
<p>The output of the <cite>get_frequent_cuda_kernel_sequences</cite>
is a dataframe containing a pipe separated string of the CUDA kernels
originating from the CPU operator along with their frequency and duration of
the CPU ops and GPU kernels.</p>
<img alt="_images/frequent_cuda_sequences_df.png" src="_images/frequent_cuda_sequences_df.png" />
<p>Adding the frequent pattern annotations in the trace file, as seen in the trace
screenshot above increases the trace file size considerably. In order to keep
the trace file size reasonable HTA creates a dictionary of all kernel names. The
keys in the dictionary are integers and the values are kernel names. The
overlaid trace file uses these keys to mark CPU ops which are not in the
operator search path. To view the dictionary click on the PyTorch Profiler
thread with thread id 0.</p>
<img alt="_images/overlaid_trace_with_dictionary.png" src="_images/overlaid_trace_with_dictionary.png" />
</div>
<div class="section" id="trace-diff">
<h3>Trace Diff<a class="headerlink" href="#trace-diff" title="Permalink to this heading">¶</a></h3>
<p>Occasionally, users need to identify the changes in PyTorch operators and CUDA
kernels resulting from a code change. To support such a requirement, HTA
provides a trace comparison feature. This feature allows the user to input two
sets of trace files where the first can be thought of as the <em>control group</em>
and the second as the <em>test group</em> as in an A/B test. The <code class="docutils literal notranslate"><span class="pre">Trace</span> <span class="pre">Diff</span></code> class
provides functions to compare the differences between traces and functionality
to visualize these differences. In particular, users can find operators and
kernels which were added and removed from each group along with the frequency
of each operator/kernel and the cumulative time taken by the operator/kernel.
The <code class="docutils literal notranslate"><span class="pre">TraceDiff</span></code> class has 4 methods:</p>
<ol class="arabic">
<li><p><cite>compare_traces</cite>
Compare the frequency and total duration of CPU operators and GPU kernels from
two sets of traces.</p></li>
<li><p><cite>ops_diff</cite>
Get the operators and kernels which have been:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>added</strong> to the test trace and are absent in the control trace</p></li>
<li><p><strong>deleted</strong> from the test trace and are present in the control trace</p></li>
<li><p><strong>increased</strong> in frequency in the test trace and exist in the control trace</p></li>
<li><p><strong>decreased</strong> in frequency in the test trace and exist in the control trace</p></li>
<li><p><strong>unchanged</strong> between the two sets of traces</p></li>
</ol>
</div></blockquote>
</li>
<li><p><cite>visualize_counts_diff</cite></p></li>
<li><p><cite>visualize_duration_diff</cite></p></li>
</ol>
<p>The last two methods can be used to visualize various changes in counts and
durations of CPU operators and GPU kernels using the output of the
<cite>compare_traces</cite></p>
<p>E.g. The top 10 operators with increase in frequency can be computed as
follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">compare_traces_output</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;diff_counts&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">TraceDiff</span><span class="o">.</span><span class="n">visualize_counts_diff</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/counts_diff.png" src="_images/counts_diff.png" />
<p>Similarly, the top 10 ops with the largest change in duration can be computed as
follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">compare_traces_output</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;diff_duration&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># The duration differerence can be overshadowed by the &quot;ProfilerStep&quot;,</span>
<span class="c1"># so we can filter it out to show the trend of other operators.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;ProfilerStep&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">TraceDiff</span><span class="o">.</span><span class="n">visualize_duration_diff</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/duration_diff.png" src="_images/duration_diff.png" />
<p>For a detailed example of this feature see the <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/trace_diff_demo.ipynb">trace_diff_demo notebook</a>
in the examples folder of the repo.</p>
</div>
<div class="section" id="cupti-counter-analysis">
<h3>CUPTI Counter Analysis<a class="headerlink" href="#cupti-counter-analysis" title="Permalink to this heading">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an experimental feature in PyTorch and Holistic Trace Analysis.</p>
</div>
<p><strong>Motivation and context</strong></p>
<p>Performance counter measurements can provide insights on how to speed up GPU
kernels, conduct <a class="reference external" href="https://en.wikipedia.org/wiki/Roofline_model">roofline analysis</a> and other low level optimizations. The
PyTorch Profiler includes a lightweight API to program and measure detailed
performance counters from the GPU. This mode leverages <a class="reference external" href="https://docs.nvidia.com/cupti/r_main.html#r_profiler">CUPTI Range Profiler
API</a>  and supports an
extensive list of performance metrics.</p>
<p><strong>Collecting CUPTI Counter traces</strong></p>
<p>Users can collect performance counters by adding the list of metrics using the
experimental config option in PyTorch Profiler. See the code snippet below for
an example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">],</span>
    <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">on_trace_ready</span><span class="o">=</span><span class="n">trace_handler</span><span class="p">,</span>
    <span class="n">experimental_config</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">_ExperimentalConfig</span><span class="p">(</span>
        <span class="n">profiler_metrics</span><span class="o">=</span><span class="p">[</span>
            <span class="s2">&quot;kineto__tensor_core_insts&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dram__bytes_read.sum&quot;</span><span class="p">,</span>
            <span class="s2">&quot;dram__bytes_write.sum&quot;</span><span class="p">],</span>
    <span class="n">profiler_measure_per_kernel</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">train_batch</span><span class="p">(</span><span class="n">modeldef</span><span class="p">)</span>
    <span class="n">prof</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>The generated trace contains the following additional information:</p>
<ol class="arabic simple">
<li><p>Performance measurement events are logged under the <cite>cuda_profiler_range</cite> category.</p></li>
<li><p>The counter values are logged in the <em>args</em> section of the above events.</p></li>
</ol>
<p>For a complete example see <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/cupti_flops_analysis.ipynb">here</a>.</p>
<p><strong>CUPTI Counter Analyzer</strong></p>
<p>CUPTI Counter trace analyzer can investigate performance measurements per
kernel and map kernels to CPU PyTorch operators. A single kernel can map to
multiple levels of operators (as operators can be nested). This information is
provided in the <cite>op_stack</cite> column. For further convenience, we add the top and
bottom level operator columns as well.</p>
<p>The code below runs CUPTI counter analysis on the collected trace.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">analyzer</span> <span class="o">=</span> <span class="n">TraceAnalysis</span><span class="p">(</span><span class="n">trace_dir</span> <span class="o">=</span> <span class="s2">&quot;/path/to/trace/folder&quot;</span><span class="p">)</span>
<span class="n">gpu_kernels</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">get_cupti_counter_data_with_operators</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>It returns a list of dataframes, one per rank or trace file. Each dataframe
contains the kernel name, op_stack (operator stack), top and bottom level op,
and columns for individual performance counters as shown below.</p>
<img alt="_images/cupti_counter_analysis.png" src="_images/cupti_counter_analysis.png" />
<p><strong>Example Notebook</strong></p>
<p>For a detailed walkthrough of this feature see the <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/cupti_flops_analysis.ipynb">cupti_flops_analysis
notebook</a>
in the examples folder of the repo.</p>
<p>To collect the trace used in the example we ran <a class="reference external" href="https://github.com/facebookresearch/param/tree/main/train/compute/python">PARAM Benchmarks</a>.
PARAM provides a repository of communication and computation micro-benchmarks
for AI training and inference. For this example, we ran a simple convolutional
neural network model - AlexNet - as a benchmark and collected the trace.
Instructions for the same are given below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inside dir &quot;param/train/compute&quot;</span>
$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>python.pytorch.run_benchmark<span class="w"> </span>-c<span class="w"> </span>python/examples/pytorch/configs/alex_net.json<span class="w"> </span>-p<span class="w"> </span>-i<span class="w"> </span><span class="m">1</span><span class="w"> </span>-d<span class="w"> </span>cuda<span class="w"> </span>--cupti-profiler<span class="w"> </span>--cupti-profiler-measure-per-kernel
</pre></div>
</div>
<p>The notebook then uses CUPTI floating point instructions counters to compute
FLOPs. FLOPs count can be utilized for <a class="reference external" href="https://en.wikipedia.org/wiki/Roofline_model">roofline analysis</a> and performance
optimization.</p>
<img alt="_images/cupti_counter_analysis_flops.png" src="_images/cupti_counter_analysis_flops.png" />
</div>
<div class="section" id="lightweight-critical-path-analysis">
<h3>Lightweight Critical Path Analysis<a class="headerlink" href="#lightweight-critical-path-analysis" title="Permalink to this heading">¶</a></h3>
<p><strong>TLDR</strong></p>
<ul class="simple">
<li><p>This feature performs a basic single rank critical path analysis. We demonstrated a walkthrough of using the tool.</p></li>
<li><p>Additionally, we dive into assumptions made and implementation principles.</p></li>
</ul>
<p><strong>Introduction</strong></p>
<p>The key idea behind critical path analysis is to find operations in a large system that constitute the <strong>longest path</strong> between start and end.
An operation on the critical path can significantly impact the program’s overall performance.
In other words, reducing the duration of that operation will result in a measurable change in the overall timing
This is illustrated in the figure below.</p>
<p><a class="reference internal" href="_images/Lightw002.png"><img alt="Lightw002" src="_images/Lightw002.png" style="width: 6.5in; height: 2.18056in;" /></a></p>
<p>Critical paths can shift if an operator is optimized beyond a point; like the <em>mult()</em> in Figure 1 becomes shorter than <em>add1()</em>.</p>
<p><strong>Why?</strong></p>
<p>Critical path analysis is a commonly applied technique in HPC and AI/ML optimization.
It can be leveraged in two ways:</p>
<ol class="arabic simple">
<li><p><strong>Performance/Efficiency opportunities:</strong> Operations/kernels on critical path should be the target of performance analysis and optimizations.
They can provide the “<strong>best bang for the buck”</strong> for performance improvements</p>
<ol class="loweralpha simple">
<li><p>The critical path can give us a sense if the training iteration is X% CPU bound or Y% GPU bound, or Z% communication bound for distributed training.</p></li>
<li><p>The analysis is also not limited to just CPU/GPU kernels.
Delays in launching or executing CUDA kernels can constitute a sizable portion of the critical path as well.
This could be optimized by operator fusion (Pytorch2.0) and CUDA graphs etc.</p></li>
</ol>
</li>
<li><p><strong>Simulating Improvements/Gains</strong>: After identifying the critical path we can estimate improvements by simply modifying the graph and re-running the
critical path finding algorithm.</p></li>
</ol>
<p><strong>Why Lightweight?</strong></p>
<p>The space to build such kinds of analysis is vast.
We could deduce the multi-rank critical path to better understand things like stragglers, and also consider tensor input/output dependencies among
PyTorch operators.</p>
<p>To start with, we decided to simplify the dependency analysis between PyTorch operators.
Our key core assumptions are.</p>
<ul class="simple">
<li><p>All PyTorch CPU operators are <strong>dependent serially on the last operator that ran on the respective CPU</strong> thread.</p></li>
<li><p>In addition, we consider dependencies between CPU and GPU, both in terms of kernel launch, kernel-kernel delays and synchronization events.</p></li>
</ul>
<p>The motivation behind this flavor of critical path analysis is to <strong>identify the primary bottleneck in the training loop</strong> - is it the CPU, or GPU
compute or GPU communication.</p>
<p>The operator data-dependency part can be added later and further enable insights like re-ordering of operations and subgraphs.
We can leverage <a class="reference external" href="https://engineering.fb.com/2023/09/07/networking-traffic/chakra-execution-traces-benchmarking-network-performance-optimization/">Chakra Execution Traces</a> to track data dependencies
among tensors.
This version of <strong>Critical Path Analysis does not need Execution Traces.</strong></p>
<p><strong>Using Critical Path Analysis</strong></p>
<p>This <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/experimental/critical_path_analysis.ipynb">ipython notebook</a>
illustrates basic critical path analysis.</p>
<p><strong>Prerequisite</strong></p>
<p>The PyTorch profiler traces were previously missing information regarding CUDA synchronization events.
This was fixed in <a class="reference external" href="https://github.com/pytorch/pytorch/pull/105187">PR1</a> and <a class="reference external" href="https://github.com/pytorch/kineto/pull/808">PR2</a>
. Follow the documentation <a class="reference external" href="https://github.com/pytorch/pytorch/pull/105187">here</a> to enable CUDA synchronization events to get best results from this analysis.</p>
<p><strong>Analysis</strong></p>
<p>As shown in the <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/experimental/critical_path_analysis.ipynb">notebook</a>, use <code class="docutils literal notranslate"><span class="pre">analyzer.critical_path_analysis()</span></code> for trace events within a single rank.
We can further reduce the region of interest by selecting a <em>trace annotation</em> and instance id.
For example, you can use this to limit the analysis to one iteration by passing annotation ‘ProfilerStep#500’.</p>
<p><a class="reference internal" href="_images/Lightw003.png"><img alt="Lightw003" src="_images/Lightw003.png" style="width: 6.5in; height: 1.47222in;" /></a></p>
<p>The output <strong>cp_graph</strong> object is a <em>networkx.DiGraph</em> object that is used as input for further analysis.</p>
<p><strong>Visualizing Critical Path</strong></p>
<p>Now for the fun part.
Use <code class="docutils literal notranslate"><span class="pre">overlay_critical_path_analysis()</span></code> function to visualize the critical path on the original trace file.
There are two modes for the output:</p>
<ol class="arabic simple">
<li><p>When <code class="docutils literal notranslate"><span class="pre">only_show_critical_events=True</span></code> (default value) the output trace only contains CPU operators and GPU events on the critical path.
One can compare it with the original trace to contrast the critical path identified by the algorithm.</p></li>
<li><p>When <code class="docutils literal notranslate"><span class="pre">only_show_critical_events=False</span></code> in the output trace file search for “critical” to highlight events on the critical path.</p></li>
</ol>
<p><a class="reference internal" href="_images/Lightw004.png"><img alt="Lightw004" src="_images/Lightw004.png" style="width: 6.5in; height: 0.93056in;" /></a></p>
<p>Edges in the critical path graph will be shown using arrows or flow events.</p>
<p>To illustrate this here is a simple training loop example on AlexNet, using setting (2) above.
One can search for “critical” in chrome trace viewer to highlight the critical path.
Most of the critical path is on the CPU here due to large delays in running <em>cudaMalloc</em>.</p>
<p><a class="reference internal" href="_images/Lightw005.png"><img alt="Lightw005" src="_images/Lightw005.png" style="width: 6.5in; height: 2.31944in;" /></a></p>
<p>Zooming in to the right hand side, the GPU is now more busy and we can see the critical path flow from the CPU, to two different GPU streams and then up to
the CPU again.</p>
<p><a class="reference internal" href="_images/Lightw006.png"><img alt="Lightw006" src="_images/Lightw006.png" style="width: 6.5in; height: 2.25in;" /></a></p>
<p>Unfortunately, the search based highlighting doesn’t work in Perfetto.
You can use the <code class="docutils literal notranslate"><span class="pre">only_show_critical_events-True</span></code> mode to display only the critical path events.</p>
<p><strong>Large Training Job Traces</strong></p>
<p>Here is an example of running this on an actual training job trace.
In real life training jobs have pipelined stages so the we should run critical path analysis over <strong>two iterations</strong>.
We can set the algorithm to run on two different iterations as shown below.</p>
<p><a class="reference internal" href="_images/Lightw007.png"><img alt="Lightw007" src="_images/Lightw007.png" style="width: 6.10417in; height: 1.66667in;" /></a></p>
<p><a class="reference internal" href="_images/Lightw008.png"><img alt="Lightw008" src="_images/Lightw008.png" style="width: 6.5in; height: 2.30556in;" /></a></p>
<p>This analyzes the 2nd and 3rd iterations (551 and 552).</p>
<ul class="simple">
<li><p>The critical path is initially on the CPU in step 551.
Zooming in you will see many small GPU kernels, indicating that the GPU is not being kept busy.
Increasing the batch size could be one optimization.</p></li>
<li><p>The critical path then shifts to NCCL all-to-all and all-reduce in the backward and next iteration forward pass.
Thus communication imbalance is likely slowing down this workflow</p></li>
<li><p>Finally, on the tail end we see some GPU kernels launched by the optimizer on the critical path.</p></li>
</ul>
<p>This workflow in general needs to better utilize GPU and fix NCCL imbalance issues.</p>
<p><strong>Implementation Details</strong></p>
<p>We drew inspiration from the previous work in <a class="reference external" href="https://www.hzdr.de/publications/PublDoc-9225.pdf">academia</a> to come up with our approach.</p>
<p><strong>Design Overview</strong></p>
<p>In a nutshell, computing the critical path involves 1) constructing a weighted DAG connecting all the operations, 2) finding the longest path in this
DAG.
The challenging part is constructing the DAG here.</p>
<p><strong>Nodes</strong>: The Nodes in the critical path graph represent points in time.
Each operator/kernel thus has two nodes viz.
a begin and end node.
In case of nested operators we also link the nodes in the order they appear in the call stack.</p>
<p><strong>Edges</strong> in this DAG can be one of two types</p>
<ol class="arabic simple">
<li><p>Timing edges (weight = time): include durations for the operators/kernels as well as delays to launch operators between CPU and GPU.</p></li>
<li><p>Dependency edges (weight = 0): do not have a time component but show a dependency between operations themselves.
This includes data dependencies and synchronization between CPU and GPU.</p></li>
</ol>
<p><strong>CPU Operator Nesting and Dependencies</strong></p>
<p>Firstly, each operator gets a start and end node.
To enable nested operators we basically add edges between start/end nodes of nested events.
This is shown in the image below.</p>
<p><a class="reference internal" href="_images/Lightw009.png"><img alt="Lightw009" src="_images/Lightw009.png" style="width: 6.5in; height: 1.09722in;" /></a></p>
<p>Since we are simplifying operator dependencies, each PyTorch top level operator has a dependency on the previous top level operator.
More details in <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/pull/67">PR67</a></p>
<p><strong>GPU Kernel Launches</strong></p>
<p>CUDA is based on a highly asynchronous execution model for GPUs with up to 1024 outstanding GPU kernels at a time.
To correctly determine how to connect GPU kernels and CPU operators we came up with two types of delays -</p>
<p><strong>Kernel launch delays:</strong> There is a finite delay from kernel launch in the CUDA runtime to when the GPU kernel executes.
This delay could either be due to the actual launch delay by system or the time spent waiting behind other kernels.
We propose that <strong>kernel launch delay should only count if there are no outstanding kernels on a CUDA stream.</strong></p>
<p><strong>Kernel-Kernel delays:</strong> All GPU kernels on the same CUDA stream execute in order.
Thus they have an implicit dependency on the previous kernel completing.
We factor this into our DAG by adding “kernel-kernel” delay edges when there are more than 1 outstanding kernels on a CUDA stream.</p>
<p>Here is an example of kernel launch and kernel-kernel delays in profiler trace (AlexNet).
More details in <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/pull/68">PR68</a></p>
<p><a class="reference internal" href="_images/Lightw010.png"><img alt="Lightw010" src="_images/Lightw010.png" style="width: 6.5in; height: 2.11111in;" /></a></p>
<p><strong>Synchronization Dependencies</strong></p>
<p>Lastly, the CPU will wait for the work dispatched to the GPU to complete.
These are due to synchronization</p>
<p><strong>Improving Profiler Traces</strong>: We realized the Kineto/PyTorch profiler was not providing enough information on Stream and Wait synchronization.
To fix this we <a class="reference external" href="https://github.com/pytorch/pytorch/pull/105187">introduced CUDA Sync events in the trace</a>.
The new sync events can cover 3 kinds of synchronization we will describe below.</p>
<p><strong>Synchronization Edges:</strong> Here is how we modified the DAG based on each synchronization type</p>
<ol class="arabic simple">
<li><p><strong>Context / Device Synchronization</strong>: Since this is a global synchronization type we add edges from the last GPU kernel on all streams to the runtime
function on the CPU calling Context/Device Synchronize.</p></li>
<li><p><strong>Stream Synchronization</strong>: is similar to above but it synchronizes a single stream.
Thus we only add a synchronization edge between the last GPU kernel on the specific stream and the corresponding Stream synchronization call on the
CPU.</p></li>
<li><p><strong>Event Synchronization:</strong> is a lot more complex and we explain it below.
The above 1, and 2 cases lead to <code class="docutils literal notranslate"><span class="pre">GPU</span> <span class="pre">-&gt;</span> <span class="pre">CPU</span></code> synchronization.
Typically Event based synchronization is used for <code class="docutils literal notranslate"><span class="pre">GPU</span> <span class="pre">-&gt;</span> <span class="pre">GPU</span></code> synchronization.</p></li>
</ol>
<p><a class="reference internal" href="_images/Lightw011.png"><img alt="Lightw011" src="_images/Lightw011.png" style="width: 6.5in; height: 3.81944in;" /></a></p>
<p><em>An example of CUDA Stream synchronization.</em></p>
<p><strong>Handling CUDA Event Synchronization</strong></p>
<p>In CUDA Event synchronization basically we have an event recorded on one stream and a GPU kernel waiting for that event to complete on another
stream.
Our approach is to trace this dependency</p>
<ol class="arabic simple">
<li><p>The newly added synchronization events <code class="docutils literal notranslate"><span class="pre">cudaStreamWaitEvent()</span></code> informs us of when the event sync occurs, ID of the CUDA event and which
<code class="docutils literal notranslate"><span class="pre">cudaEventRecord()</span></code> is being synced on.</p></li>
<li><p>The next kernel on the destination stream is the one that will wait.</p></li>
<li><p>We backtrack to the source <code class="docutils literal notranslate"><span class="pre">cudaEventRecord()</span></code> function call on the CPU.</p></li>
<li><p>Then find the preceding kernel launch and hence the kernel that ran on GPU due to it.</p></li>
<li><p>The two kernels in step (2) and (4) are the ones that need to be connected as shown in the figure below.</p></li>
</ol>
<p>See <a class="reference external" href="https://github.com/facebookresearch/HolisticTraceAnalysis/pull/69">PR69</a> for implementation details.</p>
<p><a class="reference internal" href="_images/Lightw012.png"><img alt="Lightw012" src="_images/Lightw012.png" style="width: 6.5in; height: 2.18056in;" /></a></p>
<p><em>An example of Event synchronization aka inter GPU stream synchronization.</em></p>
<p><strong>Future Work</strong></p>
<p>Here are a few ways we can improve on this work.</p>
<ol class="arabic simple">
<li><p><strong>Integrating Chakra Execution Traces</strong> -  <a class="reference external" href="https://engineering.fb.com/2023/09/07/networking-traffic/chakra-execution-traces-benchmarking-network-performance-optimization/">Chakra Execution Traces</a> helps to add real CPU operator dependency edges and can surface opportunities with re-ordering of
subgraphs for instance.</p></li>
<li><p><strong>Summary Statistics</strong>: a natural extension of this work is to tabulate the time spent on CPU / GPU on the critical path with further details like
time spent on kernel-launch delays, kernel-kernel delays and other overheads.</p></li>
<li><p><strong>Simulating New Hardware and Optimization wins</strong>: the analyzer today does return a Networkx DiGraph object that one can modify and recompute the
critical path. Additionally, it would be great to re-draw the trace and new critical path on the simulated optimizations or changes.</p></li>
</ol>
</div>
</div>
<div class="section" id="holistic-trace-analysis-apis">
<h2>Holistic Trace Analysis APIs<a class="headerlink" href="#holistic-trace-analysis-apis" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html">TraceAnalysis API</a></p>
<p><a class="reference external" href="https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html">TraceDiff API</a></p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="hub.html" class="btn btn-neutral float-right" title="torch.hub" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="fx.html" class="btn btn-neutral" title="torch.fx" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Holistic Trace Analysis</a><ul>
<li><a class="reference internal" href="#features-in-holistic-trace-analysis">Features in Holistic Trace Analysis</a></li>
<li><a class="reference internal" href="#performance-debugging-101">Performance Debugging 101</a></li>
<li><a class="reference internal" href="#trace-collection">Trace Collection</a></li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#features">Features</a><ul>
<li><a class="reference internal" href="#temporal-breakdown">Temporal Breakdown</a></li>
<li><a class="reference internal" href="#idle-time-breakdown">Idle Time Breakdown</a></li>
<li><a class="reference internal" href="#kernel-breakdown">Kernel Breakdown</a></li>
<li><a class="reference internal" href="#kernel-duration-distribution">Kernel Duration Distribution</a></li>
<li><a class="reference internal" href="#communication-computation-overlap">Communication Computation Overlap</a></li>
<li><a class="reference internal" href="#augmented-counters">Augmented Counters</a></li>
<li><a class="reference internal" href="#cuda-kernel-launch-statistics">CUDA Kernel Launch Statistics</a></li>
<li><a class="reference internal" href="#frequent-cuda-kernel-sequences">Frequent CUDA Kernel Sequences</a></li>
<li><a class="reference internal" href="#trace-diff">Trace Diff</a></li>
<li><a class="reference internal" href="#cupti-counter-analysis">CUPTI Counter Analysis</a></li>
<li><a class="reference internal" href="#lightweight-critical-path-analysis">Lightweight Critical Path Analysis</a></li>
</ul>
</li>
<li><a class="reference internal" href="#holistic-trace-analysis-apis">Holistic Trace Analysis APIs</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
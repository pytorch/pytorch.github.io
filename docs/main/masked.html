


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.masked &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/masked.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.nested" href="nested.html" />
    <link rel="prev" title="torch.random" href="random.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/masked.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.masked</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/masked.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <span class="target" id="module-torch.masked"></span><span class="target" id="module-torch.masked.maskedtensor"></span><div class="section" id="torch-masked">
<span id="masked-docs"></span><h1>torch.masked<a class="headerlink" href="#torch-masked" title="Permalink to this heading">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<div class="section" id="motivation">
<h3>Motivation<a class="headerlink" href="#motivation" title="Permalink to this heading">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.</p>
</div>
<p>MaskedTensor serves as an extension to <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> that provides the user with the ability to:</p>
<ul class="simple">
<li><p>use any masked semantics (e.g. variable length tensors, nan* operators, etc.)</p></li>
<li><p>differentiate between 0 and NaN gradients</p></li>
<li><p>various sparse applications (see tutorial below)</p></li>
</ul>
<p>“Specified” and “unspecified” have a long history in PyTorch without formal semantics and certainly without
consistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>
class could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for
said “specified” and “unspecified” values in PyTorch where they are a first class citizen instead of an afterthought.
In turn, this should further unlock <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html">sparsity’s</a> potential,
enable safer and more consistent operators, and provide a smoother and more intuitive experience
for users and developers alike.</p>
</div>
<div class="section" id="what-is-a-maskedtensor">
<h3>What is a MaskedTensor?<a class="headerlink" href="#what-is-a-maskedtensor" title="Permalink to this heading">¶</a></h3>
<p>A MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us
which entries from the input should be included or ignored.</p>
<p>By way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray)
and take the max:</p>
<a class="reference internal image-reference" href="_images/tensor_comparison.jpg"><img alt="_images/tensor_comparison.jpg" src="_images/tensor_comparison.jpg" style="width: 762.0px; height: 540.0px;" /></a>
<p>On top is the vanilla tensor example while the bottom is MaskedTensor where all the 0’s are masked out.
This clearly yields a different result depending on whether we have the mask, but this flexible structure
allows the user to systematically ignore any elements they’d like during computation.</p>
<p>There are already a number of existing tutorials that we’ve written to help users onboard, such as:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_overview">Overview - the place to start for new users, discusses how to use MaskedTensors and why they’re useful</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_sparsity">Sparsity - MaskedTensor supports sparse COO and CSR data and mask Tensors</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_adagrad">Adagrad sparse semantics - a practical example of how MaskedTensor can simplify sparse semantics and implementations</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics">Advanced semantics - discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations),
differences with NumPy’s MaskedArray, and reduction semantics</a></p></li>
</ul>
</div>
</div>
<div class="section" id="supported-operators">
<h2>Supported Operators<a class="headerlink" href="#supported-operators" title="Permalink to this heading">¶</a></h2>
<div class="section" id="unary-operators">
<h3>Unary Operators<a class="headerlink" href="#unary-operators" title="Permalink to this heading">¶</a></h3>
<p>Unary operators are operators that only contain only a single input.
Applying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index,
we apply the operator, otherwise we’ll continue to mask out the data.</p>
<p>The available unary operators are:</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.abs"/><a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">abs</span></code></a></p></td>
<td><p>Computes the absolute value of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.absolute"/><a class="reference internal" href="generated/torch.absolute.html#torch.absolute" title="torch.absolute"><code class="xref py py-obj docutils literal notranslate"><span class="pre">absolute</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.acos"/><a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acos</span></code></a></p></td>
<td><p>Computes the inverse cosine of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arccos"/><a class="reference internal" href="generated/torch.arccos.html#torch.arccos" title="torch.arccos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccos</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.acosh"/><a class="reference internal" href="generated/torch.acosh.html#torch.acosh" title="torch.acosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">acosh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic cosine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.arccosh"/><a class="reference internal" href="generated/torch.arccosh.html#torch.arccosh" title="torch.arccosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arccosh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.acosh.html#torch.acosh" title="torch.acosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acosh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.angle"/><a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">angle</span></code></a></p></td>
<td><p>Computes the element-wise angle (in radians) of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.asin"/><a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asin</span></code></a></p></td>
<td><p>Returns a new tensor with the arcsine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.arcsin"/><a class="reference internal" href="generated/torch.arcsin.html#torch.arcsin" title="torch.arcsin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsin</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.asinh"/><a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">asinh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.arcsinh"/><a class="reference internal" href="generated/torch.arcsinh.html#torch.arcsinh" title="torch.arcsinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arcsinh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asinh()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.atan"/><a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan</span></code></a></p></td>
<td><p>Returns a new tensor with the arctangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.arctan"/><a class="reference internal" href="generated/torch.arctan.html#torch.arctan" title="torch.arctan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.atanh"/><a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atanh</span></code></a></p></td>
<td><p>Returns a new tensor with the inverse hyperbolic tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.arctanh"/><a class="reference internal" href="generated/torch.arctanh.html#torch.arctanh" title="torch.arctanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctanh</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atanh()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_not"/><a class="reference internal" href="generated/torch.bitwise_not.html#torch.bitwise_not" title="torch.bitwise_not"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_not</span></code></a></p></td>
<td><p>Computes the bitwise NOT of the given input tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ceil"/><a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ceil</span></code></a></p></td>
<td><p>Returns a new tensor with the ceil of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the smallest integer greater than or equal to each element.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.clamp"/><a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp</span></code></a></p></td>
<td><p>Clamps all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into the range <cite>[</cite> <a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-attr docutils literal notranslate"><span class="pre">min</span></code></a>, <a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-attr docutils literal notranslate"><span class="pre">max</span></code></a> <cite>]</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.clip"/><a class="reference internal" href="generated/torch.clip.html#torch.clip" title="torch.clip"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.conj_physical"/><a class="reference internal" href="generated/torch.conj_physical.html#torch.conj_physical" title="torch.conj_physical"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conj_physical</span></code></a></p></td>
<td><p>Computes the element-wise conjugate of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.cos"/><a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cos</span></code></a></p></td>
<td><p>Returns a new tensor with the cosine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cosh"/><a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cosh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic cosine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.deg2rad"/><a class="reference internal" href="generated/torch.deg2rad.html#torch.deg2rad" title="torch.deg2rad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deg2rad</span></code></a></p></td>
<td><p>Returns a new tensor with each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> converted from angles in degrees to radians.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.digamma"/><a class="reference internal" href="generated/torch.digamma.html#torch.digamma" title="torch.digamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">digamma</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.digamma" title="torch.special.digamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.digamma()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.erf"/><a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erf</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erf" title="torch.special.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erf()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.erfc"/><a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfc</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erfc" title="torch.special.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erfc()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.erfinv"/><a class="reference internal" href="generated/torch.erfinv.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">erfinv</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.erfinv" title="torch.special.erfinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.erfinv()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.exp"/><a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp</span></code></a></p></td>
<td><p>Returns a new tensor with the exponential of the elements of the input tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.exp2"/><a class="reference internal" href="generated/torch.exp2.html#torch.exp2" title="torch.exp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">exp2</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.exp2" title="torch.special.exp2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.exp2()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.expm1"/><a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expm1</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.expm1" title="torch.special.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.expm1()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.fix"/><a class="reference internal" href="generated/torch.fix.html#torch.fix" title="torch.fix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fix</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.floor"/><a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor</span></code></a></p></td>
<td><p>Returns a new tensor with the floor of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the largest integer less than or equal to each element.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.frac"/><a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-obj docutils literal notranslate"><span class="pre">frac</span></code></a></p></td>
<td><p>Computes the fractional portion of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lgamma"/><a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lgamma</span></code></a></p></td>
<td><p>Computes the natural logarithm of the absolute value of the gamma function on <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.log"/><a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a></p></td>
<td><p>Returns a new tensor with the natural logarithm of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.log10"/><a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log10</span></code></a></p></td>
<td><p>Returns a new tensor with the logarithm to the base 10 of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.log1p"/><a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log1p</span></code></a></p></td>
<td><p>Returns a new tensor with the natural logarithm of (1 + <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.log2"/><a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log2</span></code></a></p></td>
<td><p>Returns a new tensor with the logarithm to the base 2 of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logit"/><a class="reference internal" href="generated/torch.logit.html#torch.logit" title="torch.logit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logit</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.logit" title="torch.special.logit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.logit()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.i0"/><a class="reference internal" href="generated/torch.i0.html#torch.i0" title="torch.i0"><code class="xref py py-obj docutils literal notranslate"><span class="pre">i0</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.i0" title="torch.special.i0"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.i0()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.isnan"/><a class="reference internal" href="generated/torch.isnan.html#torch.isnan" title="torch.isnan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isnan</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is NaN or not.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.nan_to_num"/><a class="reference internal" href="generated/torch.nan_to_num.html#torch.nan_to_num" title="torch.nan_to_num"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nan_to_num</span></code></a></p></td>
<td><p>Replaces <code class="docutils literal notranslate"><span class="pre">NaN</span></code>, positive infinity, and negative infinity values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the values specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">nan</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">posinf</span></code>, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">neginf</span></code>, respectively.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.neg"/><a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">neg</span></code></a></p></td>
<td><p>Returns a new tensor with the negative of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.negative"/><a class="reference internal" href="generated/torch.negative.html#torch.negative" title="torch.negative"><code class="xref py py-obj docutils literal notranslate"><span class="pre">negative</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.positive"/><a class="reference internal" href="generated/torch.positive.html#torch.positive" title="torch.positive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">positive</span></code></a></p></td>
<td><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.pow"/><a class="reference internal" href="generated/torch.pow.html#torch.pow" title="torch.pow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pow</span></code></a></p></td>
<td><p>Takes the power of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> and returns a tensor with the result.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.rad2deg"/><a class="reference internal" href="generated/torch.rad2deg.html#torch.rad2deg" title="torch.rad2deg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rad2deg</span></code></a></p></td>
<td><p>Returns a new tensor with each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> converted from angles in radians to degrees.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.reciprocal"/><a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reciprocal</span></code></a></p></td>
<td><p>Returns a new tensor with the reciprocal of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.round"/><a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-obj docutils literal notranslate"><span class="pre">round</span></code></a></p></td>
<td><p>Rounds elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the nearest integer.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.rsqrt"/><a class="reference internal" href="generated/torch.rsqrt.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rsqrt</span></code></a></p></td>
<td><p>Returns a new tensor with the reciprocal of the square-root of each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sigmoid"/><a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sigmoid</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.expit" title="torch.special.expit"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.expit()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sign"/><a class="reference internal" href="generated/torch.sign.html#torch.sign" title="torch.sign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sign</span></code></a></p></td>
<td><p>Returns a new tensor with the signs of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sgn"/><a class="reference internal" href="generated/torch.sgn.html#torch.sgn" title="torch.sgn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sgn</span></code></a></p></td>
<td><p>This function is an extension of torch.sign() to complex tensors.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.signbit"/><a class="reference internal" href="generated/torch.signbit.html#torch.signbit" title="torch.signbit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">signbit</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> has its sign bit set or not.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sin"/><a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sin</span></code></a></p></td>
<td><p>Returns a new tensor with the sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sinc"/><a class="reference internal" href="generated/torch.sinc.html#torch.sinc" title="torch.sinc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinc</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="special.html#torch.special.sinc" title="torch.special.sinc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.special.sinc()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sinh"/><a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sinh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.sqrt"/><a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sqrt</span></code></a></p></td>
<td><p>Returns a new tensor with the square-root of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.square"/><a class="reference internal" href="generated/torch.square.html#torch.square" title="torch.square"><code class="xref py py-obj docutils literal notranslate"><span class="pre">square</span></code></a></p></td>
<td><p>Returns a new tensor with the square of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.tan"/><a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tan</span></code></a></p></td>
<td><p>Returns a new tensor with the tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.tanh"/><a class="reference internal" href="generated/torch.tanh.html#torch.tanh" title="torch.tanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tanh</span></code></a></p></td>
<td><p>Returns a new tensor with the hyperbolic tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.trunc"/><a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-obj docutils literal notranslate"><span class="pre">trunc</span></code></a></p></td>
<td><p>Returns a new tensor with the truncated integer values of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
</tbody>
</table>
<p>The available inplace unary operators are all of the above <strong>except</strong>:</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.angle"/><a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">angle</span></code></a></p></td>
<td><p>Computes the element-wise angle (in radians) of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.positive"/><a class="reference internal" href="generated/torch.positive.html#torch.positive" title="torch.positive"><code class="xref py py-obj docutils literal notranslate"><span class="pre">positive</span></code></a></p></td>
<td><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.signbit"/><a class="reference internal" href="generated/torch.signbit.html#torch.signbit" title="torch.signbit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">signbit</span></code></a></p></td>
<td><p>Tests if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> has its sign bit set or not.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.isnan"/><a class="reference internal" href="generated/torch.isnan.html#torch.isnan" title="torch.isnan"><code class="xref py py-obj docutils literal notranslate"><span class="pre">isnan</span></code></a></p></td>
<td><p>Returns a new tensor with boolean elements representing if each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is NaN or not.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="binary-operators">
<h3>Binary Operators<a class="headerlink" href="#binary-operators" title="Permalink to this heading">¶</a></h3>
<p>As you may have seen in the tutorial, <code class="xref py py-class docutils literal notranslate"><span class="pre">MaskedTensor</span></code> also has binary operations implemented with the caveat
that the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you
need support for a particular operator or have proposed semantics for how they should behave instead, please open
an issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users
know exactly what is going on and are being intentional about their decisions with masked semantics.</p>
<p>The available binary operators are:</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.add"/><a class="reference internal" href="generated/torch.add.html#torch.add" title="torch.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add</span></code></a></p></td>
<td><p>Adds <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, scaled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>, to <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.atan2"/><a class="reference internal" href="generated/torch.atan2.html#torch.atan2" title="torch.atan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atan2</span></code></a></p></td>
<td><p>Element-wise arctangent of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>input</mtext><mi>i</mi></msub><mi mathvariant="normal">/</mi><msub><mtext>other</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{input}_{i} / \text{other}_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord text"><span class="mord">input</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2175em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord text"><span class="mord">other</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> with consideration of the quadrant.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.arctan2"/><a class="reference internal" href="generated/torch.arctan2.html#torch.arctan2" title="torch.arctan2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">arctan2</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.atan2.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_and"/><a class="reference internal" href="generated/torch.bitwise_and.html#torch.bitwise_and" title="torch.bitwise_and"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_and</span></code></a></p></td>
<td><p>Computes the bitwise AND of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bitwise_or"/><a class="reference internal" href="generated/torch.bitwise_or.html#torch.bitwise_or" title="torch.bitwise_or"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_or</span></code></a></p></td>
<td><p>Computes the bitwise OR of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_xor"/><a class="reference internal" href="generated/torch.bitwise_xor.html#torch.bitwise_xor" title="torch.bitwise_xor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_xor</span></code></a></p></td>
<td><p>Computes the bitwise XOR of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.bitwise_left_shift"/><a class="reference internal" href="generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift" title="torch.bitwise_left_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_left_shift</span></code></a></p></td>
<td><p>Computes the left arithmetic shift of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> bits.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.bitwise_right_shift"/><a class="reference internal" href="generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift" title="torch.bitwise_right_shift"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwise_right_shift</span></code></a></p></td>
<td><p>Computes the right arithmetic shift of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> bits.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.div"/><a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-obj docutils literal notranslate"><span class="pre">div</span></code></a></p></td>
<td><p>Divides each element of the input <code class="docutils literal notranslate"><span class="pre">input</span></code> by the corresponding element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.divide"/><a class="reference internal" href="generated/torch.divide.html#torch.divide" title="torch.divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">divide</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.floor_divide"/><a class="reference internal" href="generated/torch.floor_divide.html#torch.floor_divide" title="torch.floor_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">floor_divide</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fmod"/><a class="reference internal" href="generated/torch.fmod.html#torch.fmod" title="torch.fmod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmod</span></code></a></p></td>
<td><p>Applies C++'s <a class="reference external" href="https://en.cppreference.com/w/cpp/numeric/math/fmod">std::fmod</a> entrywise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.logaddexp"/><a class="reference internal" href="generated/torch.logaddexp.html#torch.logaddexp" title="torch.logaddexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp</span></code></a></p></td>
<td><p>Logarithm of the sum of exponentiations of the inputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logaddexp2"/><a class="reference internal" href="generated/torch.logaddexp2.html#torch.logaddexp2" title="torch.logaddexp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp2</span></code></a></p></td>
<td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.mul"/><a class="reference internal" href="generated/torch.mul.html#torch.mul" title="torch.mul"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mul</span></code></a></p></td>
<td><p>Multiplies <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.multiply"/><a class="reference internal" href="generated/torch.multiply.html#torch.multiply" title="torch.multiply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiply</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.mul.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.nextafter"/><a class="reference internal" href="generated/torch.nextafter.html#torch.nextafter" title="torch.nextafter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nextafter</span></code></a></p></td>
<td><p>Return the next floating-point value after <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> towards <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, elementwise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.remainder"/><a class="reference internal" href="generated/torch.remainder.html#torch.remainder" title="torch.remainder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">remainder</span></code></a></p></td>
<td><p>Computes <a class="reference external" href="https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations">Python's modulus operation</a> entrywise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.sub"/><a class="reference internal" href="generated/torch.sub.html#torch.sub" title="torch.sub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sub</span></code></a></p></td>
<td><p>Subtracts <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, scaled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>, from <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.subtract"/><a class="reference internal" href="generated/torch.subtract.html#torch.subtract" title="torch.subtract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">subtract</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.sub.html#torch.sub" title="torch.sub"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sub()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.true_divide"/><a class="reference internal" href="generated/torch.true_divide.html#torch.true_divide" title="torch.true_divide"><code class="xref py py-obj docutils literal notranslate"><span class="pre">true_divide</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">rounding_mode=None</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.eq"/><a class="reference internal" href="generated/torch.eq.html#torch.eq" title="torch.eq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eq</span></code></a></p></td>
<td><p>Computes element-wise equality</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ne"/><a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ne</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo mathvariant="normal">≠</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \neq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.le"/><a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-obj docutils literal notranslate"><span class="pre">le</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≤</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \leq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ge"/><a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ge</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≥</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \geq \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.greater"/><a class="reference internal" href="generated/torch.greater.html#torch.greater" title="torch.greater"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.greater_equal"/><a class="reference internal" href="generated/torch.greater_equal.html#torch.greater_equal" title="torch.greater_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">greater_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.gt"/><a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gt</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&gt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &gt; \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.less_equal"/><a class="reference internal" href="generated/torch.less_equal.html#torch.less_equal" title="torch.less_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.lt"/><a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lt</span></code></a></p></td>
<td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&lt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &lt; \text{other}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8623em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">other</span></span></span></span></span></span> element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.less"/><a class="reference internal" href="generated/torch.less.html#torch.less" title="torch.less"><code class="xref py py-obj docutils literal notranslate"><span class="pre">less</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.maximum"/><a class="reference internal" href="generated/torch.maximum.html#torch.maximum" title="torch.maximum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maximum</span></code></a></p></td>
<td><p>Computes the element-wise maximum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.minimum"/><a class="reference internal" href="generated/torch.minimum.html#torch.minimum" title="torch.minimum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">minimum</span></code></a></p></td>
<td><p>Computes the element-wise minimum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fmax"/><a class="reference internal" href="generated/torch.fmax.html#torch.fmax" title="torch.fmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmax</span></code></a></p></td>
<td><p>Computes the element-wise maximum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.fmin"/><a class="reference internal" href="generated/torch.fmin.html#torch.fmin" title="torch.fmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmin</span></code></a></p></td>
<td><p>Computes the element-wise minimum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.not_equal"/><a class="reference internal" href="generated/torch.not_equal.html#torch.not_equal" title="torch.not_equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">not_equal</span></code></a></p></td>
<td><p>Alias for <a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a>.</p></td>
</tr>
</tbody>
</table>
<p>The available inplace binary operators are all of the above <strong>except</strong>:</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.logaddexp"/><a class="reference internal" href="generated/torch.logaddexp.html#torch.logaddexp" title="torch.logaddexp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp</span></code></a></p></td>
<td><p>Logarithm of the sum of exponentiations of the inputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.logaddexp2"/><a class="reference internal" href="generated/torch.logaddexp2.html#torch.logaddexp2" title="torch.logaddexp2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logaddexp2</span></code></a></p></td>
<td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.equal"/><a class="reference internal" href="generated/torch.equal.html#torch.equal" title="torch.equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">equal</span></code></a></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code> if two tensors have the same size and elements, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fmin"/><a class="reference internal" href="generated/torch.fmin.html#torch.fmin" title="torch.fmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmin</span></code></a></p></td>
<td><p>Computes the element-wise minimum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.minimum"/><a class="reference internal" href="generated/torch.minimum.html#torch.minimum" title="torch.minimum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">minimum</span></code></a></p></td>
<td><p>Computes the element-wise minimum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.fmax"/><a class="reference internal" href="generated/torch.fmax.html#torch.fmax" title="torch.fmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fmax</span></code></a></p></td>
<td><p>Computes the element-wise maximum of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="reductions">
<h3>Reductions<a class="headerlink" href="#reductions" title="Permalink to this heading">¶</a></h3>
<p>The following reductions are available (with autograd support). For more information, the
<a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_overview.html">Overview</a> tutorial
details some examples of reductions, while the
<a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html">Advanced semantics</a> tutorial
has some further in-depth discussions about how we decided on certain reduction semantics.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.sum"/><a class="reference internal" href="generated/torch.sum.html#torch.sum" title="torch.sum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sum</span></code></a></p></td>
<td><p>Returns the sum of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.mean"/><a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code></a></p></td>
<td><p>Returns the mean value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.amin"/><a class="reference internal" href="generated/torch.amin.html#torch.amin" title="torch.amin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">amin</span></code></a></p></td>
<td><p>Returns the minimum value of each slice of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.amax"/><a class="reference internal" href="generated/torch.amax.html#torch.amax" title="torch.amax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">amax</span></code></a></p></td>
<td><p>Returns the maximum value of each slice of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.argmin"/><a class="reference internal" href="generated/torch.argmin.html#torch.argmin" title="torch.argmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argmin</span></code></a></p></td>
<td><p>Returns the indices of the minimum value(s) of the flattened tensor or along a dimension</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.argmax"/><a class="reference internal" href="generated/torch.argmax.html#torch.argmax" title="torch.argmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">argmax</span></code></a></p></td>
<td><p>Returns the indices of the maximum value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.prod"/><a class="reference internal" href="generated/torch.prod.html#torch.prod" title="torch.prod"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prod</span></code></a></p></td>
<td><p>Returns the product of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.all"/><a class="reference internal" href="generated/torch.all.html#torch.all" title="torch.all"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all</span></code></a></p></td>
<td><p>Tests if all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> evaluate to <cite>True</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.norm"/><a class="reference internal" href="generated/torch.norm.html#torch.norm" title="torch.norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">norm</span></code></a></p></td>
<td><p>Returns the matrix norm or vector norm of a given tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.var"/><a class="reference internal" href="generated/torch.var.html#torch.var" title="torch.var"><code class="xref py py-obj docutils literal notranslate"><span class="pre">var</span></code></a></p></td>
<td><p>Calculates the variance over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.std"/><a class="reference internal" href="generated/torch.std.html#torch.std" title="torch.std"><code class="xref py py-obj docutils literal notranslate"><span class="pre">std</span></code></a></p></td>
<td><p>Calculates the standard deviation over the dimensions specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="view-and-select-functions">
<h3>View and select functions<a class="headerlink" href="#view-and-select-functions" title="Permalink to this heading">¶</a></h3>
<p>We’ve included a number of view and select functions as well; intuitively, these operators will apply to
both the data and the mask and then wrap the result in a <code class="xref py py-class docutils literal notranslate"><span class="pre">MaskedTensor</span></code>. For a quick example,
consider <a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-func docutils literal notranslate"><span class="pre">select()</span></code></a>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span>
<span class="go">tensor([[ 0.,  1.,  2.,  3.],</span>
<span class="go">        [ 4.,  5.,  6.,  7.],</span>
<span class="go">        [ 8.,  9., 10., 11.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mt</span> <span class="o">=</span> <span class="n">masked_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([4., 5., 6., 7.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([False,  True, False, False])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mt</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">MaskedTensor(</span>
<span class="go">  [      --,   5.0000,       --,       --]</span>
<span class="go">)</span>
</pre></div>
</div>
<p>The following ops are currently supported:</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.atleast_1d"/><a class="reference internal" href="generated/torch.atleast_1d.html#torch.atleast_1d" title="torch.atleast_1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">atleast_1d</span></code></a></p></td>
<td><p>Returns a 1-dimensional view of each input tensor with zero dimensions.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.broadcast_tensors"/><a class="reference internal" href="generated/torch.broadcast_tensors.html#torch.broadcast_tensors" title="torch.broadcast_tensors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_tensors</span></code></a></p></td>
<td><p>Broadcasts the given tensors according to <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">Broadcasting semantics</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.broadcast_to"/><a class="reference internal" href="generated/torch.broadcast_to.html#torch.broadcast_to" title="torch.broadcast_to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">broadcast_to</span></code></a></p></td>
<td><p>Broadcasts <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to the shape <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.cat"/><a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cat</span></code></a></p></td>
<td><p>Concatenates the given sequence of <code class="xref py py-attr docutils literal notranslate"><span class="pre">seq</span></code> tensors in the given dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.chunk"/><a class="reference internal" href="generated/torch.chunk.html#torch.chunk" title="torch.chunk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">chunk</span></code></a></p></td>
<td><p>Attempts to split a tensor into the specified number of chunks.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.column_stack"/><a class="reference internal" href="generated/torch.column_stack.html#torch.column_stack" title="torch.column_stack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">column_stack</span></code></a></p></td>
<td><p>Creates a new tensor by horizontally stacking the tensors in <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensors</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.dsplit"/><a class="reference internal" href="generated/torch.dsplit.html#torch.dsplit" title="torch.dsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with three or more dimensions, into multiple tensors depthwise according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.flatten"/><a class="reference internal" href="generated/torch.flatten.html#torch.flatten" title="torch.flatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flatten</span></code></a></p></td>
<td><p>Flattens <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> by reshaping it into a one-dimensional tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.hsplit"/><a class="reference internal" href="generated/torch.hsplit.html#torch.hsplit" title="torch.hsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with one or more dimensions, into multiple tensors horizontally according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.hstack"/><a class="reference internal" href="generated/torch.hstack.html#torch.hstack" title="torch.hstack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hstack</span></code></a></p></td>
<td><p>Stack tensors in sequence horizontally (column wise).</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.kron"/><a class="reference internal" href="generated/torch.kron.html#torch.kron" title="torch.kron"><code class="xref py py-obj docutils literal notranslate"><span class="pre">kron</span></code></a></p></td>
<td><p>Computes the Kronecker product, denoted by <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊗</span></span></span></span></span>, of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.meshgrid"/><a class="reference internal" href="generated/torch.meshgrid.html#torch.meshgrid" title="torch.meshgrid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">meshgrid</span></code></a></p></td>
<td><p>Creates grids of coordinates specified by the 1D inputs in <cite>attr</cite>:tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.narrow"/><a class="reference internal" href="generated/torch.narrow.html#torch.narrow" title="torch.narrow"><code class="xref py py-obj docutils literal notranslate"><span class="pre">narrow</span></code></a></p></td>
<td><p>Returns a new tensor that is a narrowed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ravel"/><a class="reference internal" href="generated/torch.ravel.html#torch.ravel" title="torch.ravel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ravel</span></code></a></p></td>
<td><p>Return a contiguous flattened tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.select"/><a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select</span></code></a></p></td>
<td><p>Slices the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along the selected dimension at the given index.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.split"/><a class="reference internal" href="generated/torch.split.html#torch.split" title="torch.split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">split</span></code></a></p></td>
<td><p>Splits the tensor into chunks.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.t"/><a class="reference internal" href="generated/torch.t.html#torch.t" title="torch.t"><code class="xref py py-obj docutils literal notranslate"><span class="pre">t</span></code></a></p></td>
<td><p>Expects <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to be &lt;= 2-D tensor and transposes dimensions 0 and 1.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.transpose"/><a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transpose</span></code></a></p></td>
<td><p>Returns a tensor that is a transposed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.vsplit"/><a class="reference internal" href="generated/torch.vsplit.html#torch.vsplit" title="torch.vsplit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vsplit</span></code></a></p></td>
<td><p>Splits <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, a tensor with two or more dimensions, into multiple tensors vertically according to <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices_or_sections</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.vstack"/><a class="reference internal" href="generated/torch.vstack.html#torch.vstack" title="torch.vstack"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vstack</span></code></a></p></td>
<td><p>Stack tensors in sequence vertically (row wise).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.expand</span></code></a></p></td>
<td><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded to a larger size.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as" title="torch.Tensor.expand_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.expand_as</span></code></a></p></td>
<td><p>Expand this tensor to the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.reshape.html#torch.Tensor.reshape" title="torch.Tensor.reshape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.reshape</span></code></a></p></td>
<td><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> but with the specified shape.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as" title="torch.Tensor.reshape_as"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.reshape_as</span></code></a></p></td>
<td><p>Returns this tensor as the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.view.html#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tensor.view</span></code></a></p></td>
<td><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a different <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nested.html" class="btn btn-neutral float-right" title="torch.nested" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="random.html" class="btn btn-neutral" title="torch.random" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.masked</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a><ul>
<li><a class="reference internal" href="#motivation">Motivation</a></li>
<li><a class="reference internal" href="#what-is-a-maskedtensor">What is a MaskedTensor?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#supported-operators">Supported Operators</a><ul>
<li><a class="reference internal" href="#unary-operators">Unary Operators</a><ul>
</ul>
</li>
<li><a class="reference internal" href="#binary-operators">Binary Operators</a><ul>
</ul>
</li>
<li><a class="reference internal" href="#reductions">Reductions</a></li>
<li><a class="reference internal" href="#view-and-select-functions">View and select functions</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quantization API Reference &mdash; PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/quantization-support.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="quantize" href="generated/torch.ao.quantization.quantize.html" />
    <link rel="prev" title="Quantization" href="quantization.html" />

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>main (2.1.0a0+git707d265 ) &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/quantization-support.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="compile/index.html">torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/troubleshooting.html">PyTorch 2.0 Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/technical-overview.html">Technical Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/fine_grained_apis.html">TorchDynamo APIs to control fine-grained tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/profiling_torch_compile.html">Profiling to understand torch.compile performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/inductor_profiling.html">TorchInductor GPU Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/cudagraph_trees.html">CUDAGraph Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/performance-dashboard.html">PyTorch 2.0 Performance Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/torchfunc-and-torchcompile.html">torch.func interaction with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="ir.html">IRs</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/dynamic-shapes.html">Dynamic shapes</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/fake-tensor.html">Fake tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="compile/transformations.html">Writing Graph Transformations on ATen IR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch._export.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="quantization.html">Quantization</a> &gt;</li>
        
      <li>Quantization API Reference</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/quantization-support.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="quantization-api-reference">
<h1>Quantization API Reference<a class="headerlink" href="#quantization-api-reference" title="Permalink to this heading">¶</a></h1>
<div class="section" id="torch-ao-quantization">
<h2>torch.ao.quantization<a class="headerlink" href="#torch-ao-quantization" title="Permalink to this heading">¶</a></h2>
<p>This module contains Eager mode quantization APIs.</p>
<div class="section" id="top-level-apis">
<h3>Top level APIs<a class="headerlink" href="#top-level-apis" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.quantize"/><a class="reference internal" href="generated/torch.ao.quantization.quantize.html#torch.ao.quantization.quantize" title="torch.ao.quantization.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize</span></code></a></p></td>
<td><p>Quantize the input float model with post training static quantization.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.quantize_dynamic"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_dynamic.html#torch.ao.quantization.quantize_dynamic" title="torch.ao.quantization.quantize_dynamic"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_dynamic</span></code></a></p></td>
<td><p>Converts a float model to dynamic (i.e.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.quantize_qat"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_qat.html#torch.ao.quantization.quantize_qat" title="torch.ao.quantization.quantize_qat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_qat</span></code></a></p></td>
<td><p>Do quantization aware training and output a quantized model</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.prepare"/><a class="reference internal" href="generated/torch.ao.quantization.prepare.html#torch.ao.quantization.prepare" title="torch.ao.quantization.prepare"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare</span></code></a></p></td>
<td><p>Prepares a copy of the model for quantization calibration or quantization-aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.prepare_qat"/><a class="reference internal" href="generated/torch.ao.quantization.prepare_qat.html#torch.ao.quantization.prepare_qat" title="torch.ao.quantization.prepare_qat"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_qat</span></code></a></p></td>
<td><p>Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.convert"/><a class="reference internal" href="generated/torch.ao.quantization.convert.html#torch.ao.quantization.convert" title="torch.ao.quantization.convert"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert</span></code></a></p></td>
<td><p>Converts submodules in input module to a different module according to <cite>mapping</cite> by calling <cite>from_float</cite> method on the target module class.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="preparing-model-for-quantization">
<h3>Preparing model for quantization<a class="headerlink" href="#preparing-model-for-quantization" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fuse_modules"/><a class="reference internal" href="generated/torch.ao.quantization.fuse_modules.html#torch.ao.quantization.fuse_modules" title="torch.ao.quantization.fuse_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fuse_modules</span></code></a></p></td>
<td><p>Fuses a list of modules into a single module</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.QuantStub"/><a class="reference internal" href="generated/torch.ao.quantization.QuantStub.html#torch.ao.quantization.QuantStub" title="torch.ao.quantization.QuantStub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantStub</span></code></a></p></td>
<td><p>Quantize stub module, before calibration, this is same as an observer, it will be swapped as <cite>nnq.Quantize</cite> in <cite>convert</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.DeQuantStub"/><a class="reference internal" href="generated/torch.ao.quantization.DeQuantStub.html#torch.ao.quantization.DeQuantStub" title="torch.ao.quantization.DeQuantStub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DeQuantStub</span></code></a></p></td>
<td><p>Dequantize stub module, before calibration, this is same as identity, this will be swapped as <cite>nnq.DeQuantize</cite> in <cite>convert</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.QuantWrapper"/><a class="reference internal" href="generated/torch.ao.quantization.QuantWrapper.html#torch.ao.quantization.QuantWrapper" title="torch.ao.quantization.QuantWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantWrapper</span></code></a></p></td>
<td><p>A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.add_quant_dequant"/><a class="reference internal" href="generated/torch.ao.quantization.add_quant_dequant.html#torch.ao.quantization.add_quant_dequant" title="torch.ao.quantization.add_quant_dequant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_quant_dequant</span></code></a></p></td>
<td><p>Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="utility-functions">
<h3>Utility functions<a class="headerlink" href="#utility-functions" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.swap_module"/><a class="reference internal" href="generated/torch.ao.quantization.swap_module.html#torch.ao.quantization.swap_module" title="torch.ao.quantization.swap_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">swap_module</span></code></a></p></td>
<td><p>Swaps the module if it has a quantized counterpart and it has an <cite>observer</cite> attached.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.propagate_qconfig_"/><a class="reference internal" href="generated/torch.ao.quantization.propagate_qconfig_.html#torch.ao.quantization.propagate_qconfig_" title="torch.ao.quantization.propagate_qconfig_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">propagate_qconfig_</span></code></a></p></td>
<td><p>Propagate qconfig through the module hierarchy and assign <cite>qconfig</cite> attribute on each leaf module</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.default_eval_fn"/><a class="reference internal" href="generated/torch.ao.quantization.default_eval_fn.html#torch.ao.quantization.default_eval_fn" title="torch.ao.quantization.default_eval_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_eval_fn</span></code></a></p></td>
<td><p>Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="torch-ao-quantization-quantize-fx">
<h2>torch.ao.quantization.quantize_fx<a class="headerlink" href="#torch-ao-quantization-quantize-fx" title="Permalink to this heading">¶</a></h2>
<p>This module contains FX graph mode quantization APIs (prototype).</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.quantize_fx.prepare_fx"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx" title="torch.ao.quantization.quantize_fx.prepare_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_fx</span></code></a></p></td>
<td><p>Prepare a model for post training static quantization</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.quantize_fx.prepare_qat_fx"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx" title="torch.ao.quantization.quantize_fx.prepare_qat_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_qat_fx</span></code></a></p></td>
<td><p>Prepare a model for quantization aware training</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.quantize_fx.convert_fx"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx" title="torch.ao.quantization.quantize_fx.convert_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert_fx</span></code></a></p></td>
<td><p>Convert a calibrated or trained model to a quantized model</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.quantize_fx.fuse_fx"/><a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx" title="torch.ao.quantization.quantize_fx.fuse_fx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fuse_fx</span></code></a></p></td>
<td><p>Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-ao-quantization-qconfig-mapping">
<h2>torch.ao.quantization.qconfig_mapping<a class="headerlink" href="#torch-ao-quantization-qconfig-mapping" title="Permalink to this heading">¶</a></h2>
<p>This module contains QConfigMapping for configuring FX graph mode quantization.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig_mapping.QConfigMapping"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping" title="torch.ao.quantization.qconfig_mapping.QConfigMapping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QConfigMapping</span></code></a></p></td>
<td><p>Mapping from model ops to <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ao.quantization.QConfig</span></code> s.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping" title="torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_qconfig_mapping</span></code></a></p></td>
<td><p>Return the default QConfigMapping for post training quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping" title="torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_default_qat_qconfig_mapping</span></code></a></p></td>
<td><p>Return the default QConfigMapping for quantization aware training.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-ao-quantization-backend-config">
<h2>torch.ao.quantization.backend_config<a class="headerlink" href="#torch-ao-quantization-backend-config" title="Permalink to this heading">¶</a></h2>
<p>This module contains BackendConfig, a config object that defines how quantization is supported
in a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode
Quantization to work with this as well.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.backend_config.BackendConfig"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig" title="torch.ao.quantization.backend_config.BackendConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BackendConfig</span></code></a></p></td>
<td><p>Config that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.backend_config.BackendPatternConfig"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.BackendPatternConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BackendPatternConfig</span></code></a></p></td>
<td><p>Config object that specifies quantization behavior for a given operator pattern.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.backend_config.DTypeConfig"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.DTypeConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DTypeConfig</span></code></a></p></td>
<td><p>Config object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.backend_config.DTypeWithConstraints"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html#torch.ao.quantization.backend_config.DTypeWithConstraints" title="torch.ao.quantization.backend_config.DTypeWithConstraints"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DTypeWithConstraints</span></code></a></p></td>
<td><p>Config for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used in <a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.DTypeConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">DTypeConfig</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.backend_config.ObservationType"/><a class="reference internal" href="generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType" title="torch.ao.quantization.backend_config.ObservationType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ObservationType</span></code></a></p></td>
<td><p>An enum that represents different ways of how an operator/operator pattern should be observed</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-ao-quantization-fx-custom-config">
<h2>torch.ao.quantization.fx.custom_config<a class="headerlink" href="#torch-ao-quantization-fx-custom-config" title="Permalink to this heading">¶</a></h2>
<p>This module contains a few CustomConfig classes that’s used in both eager mode and FX graph mode quantization</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fx.custom_config.FuseCustomConfig"/><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig" title="torch.ao.quantization.fx.custom_config.FuseCustomConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FuseCustomConfig</span></code></a></p></td>
<td><p>Custom configuration for <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx" title="torch.ao.quantization.quantize_fx.fuse_fx"><code class="xref py py-func docutils literal notranslate"><span class="pre">fuse_fx()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig"/><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PrepareCustomConfig</span></code></a></p></td>
<td><p>Custom configuration for <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx" title="torch.ao.quantization.quantize_fx.prepare_fx"><code class="xref py py-func docutils literal notranslate"><span class="pre">prepare_fx()</span></code></a> and <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx" title="torch.ao.quantization.quantize_fx.prepare_qat_fx"><code class="xref py py-func docutils literal notranslate"><span class="pre">prepare_qat_fx()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fx.custom_config.ConvertCustomConfig"/><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig" title="torch.ao.quantization.fx.custom_config.ConvertCustomConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvertCustomConfig</span></code></a></p></td>
<td><p>Custom configuration for <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx" title="torch.ao.quantization.quantize_fx.convert_fx"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_fx()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry"/><a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry" title="torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StandaloneModuleConfigEntry</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-quantization-related-functions">
<h2>torch (quantization related functions)<a class="headerlink" href="#torch-quantization-related-functions" title="Permalink to this heading">¶</a></h2>
<p>This describes the quantization related functions of the <cite>torch</cite> namespace.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.quantize_per_tensor"/><a class="reference internal" href="generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor" title="torch.quantize_per_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_tensor</span></code></a></p></td>
<td><p>Converts a float tensor to a quantized tensor with given scale and zero point.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.quantize_per_channel"/><a class="reference internal" href="generated/torch.quantize_per_channel.html#torch.quantize_per_channel" title="torch.quantize_per_channel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_per_channel</span></code></a></p></td>
<td><p>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.dequantize"/><a class="reference internal" href="generated/torch.dequantize.html#torch.dequantize" title="torch.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></a></p></td>
<td><p>Returns an fp32 Tensor by dequantizing a quantized Tensor</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-tensor-quantization-related-methods">
<h2>torch.Tensor (quantization related methods)<a class="headerlink" href="#torch-tensor-quantization-related-methods" title="Permalink to this heading">¶</a></h2>
<p>Quantized Tensors support a limited subset of data manipulation methods of the
regular full-precision tensor.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.Tensor.view"/><a class="reference internal" href="generated/torch.Tensor.view.html#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-obj docutils literal notranslate"><span class="pre">view</span></code></a></p></td>
<td><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a different <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.as_strided"/><a class="reference internal" href="generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided" title="torch.Tensor.as_strided"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_strided</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.as_strided.html#torch.as_strided" title="torch.as_strided"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_strided()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.expand"/><a class="reference internal" href="generated/torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-obj docutils literal notranslate"><span class="pre">expand</span></code></a></p></td>
<td><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded to a larger size.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.flatten"/><a class="reference internal" href="generated/torch.Tensor.flatten.html#torch.Tensor.flatten" title="torch.Tensor.flatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">flatten</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.flatten.html#torch.flatten" title="torch.flatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.select"/><a class="reference internal" href="generated/torch.Tensor.select.html#torch.Tensor.select" title="torch.Tensor.select"><code class="xref py py-obj docutils literal notranslate"><span class="pre">select</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.select()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.ne"/><a class="reference internal" href="generated/torch.Tensor.ne.html#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ne</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.eq"/><a class="reference internal" href="generated/torch.Tensor.eq.html#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eq</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.eq.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.ge"/><a class="reference internal" href="generated/torch.Tensor.ge.html#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ge</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.le"/><a class="reference internal" href="generated/torch.Tensor.le.html#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-obj docutils literal notranslate"><span class="pre">le</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.gt"/><a class="reference internal" href="generated/torch.Tensor.gt.html#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gt</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.lt"/><a class="reference internal" href="generated/torch.Tensor.lt.html#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lt</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.copy_"/><a class="reference internal" href="generated/torch.Tensor.copy_.html#torch.Tensor.copy_" title="torch.Tensor.copy_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copy_</span></code></a></p></td>
<td><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.clone"/><a class="reference internal" href="generated/torch.Tensor.clone.html#torch.Tensor.clone" title="torch.Tensor.clone"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clone</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.clone.html#torch.clone" title="torch.clone"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clone()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.dequantize"/><a class="reference internal" href="generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize" title="torch.Tensor.dequantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dequantize</span></code></a></p></td>
<td><p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.equal"/><a class="reference internal" href="generated/torch.Tensor.equal.html#torch.Tensor.equal" title="torch.Tensor.equal"><code class="xref py py-obj docutils literal notranslate"><span class="pre">equal</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.equal.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.int_repr"/><a class="reference internal" href="generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr" title="torch.Tensor.int_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int_repr</span></code></a></p></td>
<td><p>Given a quantized Tensor, <code class="docutils literal notranslate"><span class="pre">self.int_repr()</span></code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.max"/><a class="reference internal" href="generated/torch.Tensor.max.html#torch.Tensor.max" title="torch.Tensor.max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.mean"/><a class="reference internal" href="generated/torch.Tensor.mean.html#torch.Tensor.mean" title="torch.Tensor.mean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mean</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.min"/><a class="reference internal" href="generated/torch.Tensor.min.html#torch.Tensor.min" title="torch.Tensor.min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">min</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.q_scale"/><a class="reference internal" href="generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale" title="torch.Tensor.q_scale"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_scale</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.q_zero_point"/><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point" title="torch.Tensor.q_zero_point"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_zero_point</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.q_per_channel_scales"/><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales" title="torch.Tensor.q_per_channel_scales"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_scales</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.q_per_channel_zero_points"/><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points" title="torch.Tensor.q_per_channel_zero_points"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_zero_points</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.q_per_channel_axis"/><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis" title="torch.Tensor.q_per_channel_axis"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_per_channel_axis</span></code></a></p></td>
<td><p>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.resize_"/><a class="reference internal" href="generated/torch.Tensor.resize_.html#torch.Tensor.resize_" title="torch.Tensor.resize_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">resize_</span></code></a></p></td>
<td><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.Tensor.sort"/><a class="reference internal" href="generated/torch.Tensor.sort.html#torch.Tensor.sort" title="torch.Tensor.sort"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sort</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.sort.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.Tensor.topk"/><a class="reference internal" href="generated/torch.Tensor.topk.html#torch.Tensor.topk" title="torch.Tensor.topk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">topk</span></code></a></p></td>
<td><p>See <a class="reference internal" href="generated/torch.topk.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-ao-quantization-observer">
<h2>torch.ao.quantization.observer<a class="headerlink" href="#torch-ao-quantization-observer" title="Permalink to this heading">¶</a></h2>
<p>This module contains observers which are used to collect statistics about
the values observed during calibration (PTQ) or training (QAT).</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.ObserverBase"/><a class="reference internal" href="generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase" title="torch.ao.quantization.observer.ObserverBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ObserverBase</span></code></a></p></td>
<td><p>Base observer Module.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.MinMaxObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver" title="torch.ao.quantization.observer.MinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running min and max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.MovingAverageMinMaxObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html#torch.ao.quantization.observer.MovingAverageMinMaxObserver" title="torch.ao.quantization.observer.MovingAverageMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MovingAverageMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the moving average of the min and max values.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.PerChannelMinMaxObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver" title="torch.ao.quantization.observer.PerChannelMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PerChannelMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running per channel min and max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver" title="torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MovingAveragePerChannelMinMaxObserver</span></code></a></p></td>
<td><p>Observer module for computing the quantization parameters based on the running per channel min and max values.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.HistogramObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.HistogramObserver.html#torch.ao.quantization.observer.HistogramObserver" title="torch.ao.quantization.observer.HistogramObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HistogramObserver</span></code></a></p></td>
<td><p>The module records the running histogram of tensor values along with min/max values.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.PlaceholderObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.PlaceholderObserver.html#torch.ao.quantization.observer.PlaceholderObserver" title="torch.ao.quantization.observer.PlaceholderObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PlaceholderObserver</span></code></a></p></td>
<td><p>Observer that doesn't do anything and just passes its configuration to the quantized module's <code class="docutils literal notranslate"><span class="pre">.from_float()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.RecordingObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.RecordingObserver.html#torch.ao.quantization.observer.RecordingObserver" title="torch.ao.quantization.observer.RecordingObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RecordingObserver</span></code></a></p></td>
<td><p>The module is mainly for debug and records the tensor values during runtime.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.NoopObserver"/><a class="reference internal" href="generated/torch.ao.quantization.observer.NoopObserver.html#torch.ao.quantization.observer.NoopObserver" title="torch.ao.quantization.observer.NoopObserver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NoopObserver</span></code></a></p></td>
<td><p>Observer that doesn't do anything and just passes its configuration to the quantized module's <code class="docutils literal notranslate"><span class="pre">.from_float()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.get_observer_state_dict"/><a class="reference internal" href="generated/torch.ao.quantization.observer.get_observer_state_dict.html#torch.ao.quantization.observer.get_observer_state_dict" title="torch.ao.quantization.observer.get_observer_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_observer_state_dict</span></code></a></p></td>
<td><p>Returns the state dict corresponding to the observer stats.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.load_observer_state_dict"/><a class="reference internal" href="generated/torch.ao.quantization.observer.load_observer_state_dict.html#torch.ao.quantization.observer.load_observer_state_dict" title="torch.ao.quantization.observer.load_observer_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_observer_state_dict</span></code></a></p></td>
<td><p>Given input model and a state_dict containing model observer stats, load the stats back into the model.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.default_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_observer.html#torch.ao.quantization.observer.default_observer" title="torch.ao.quantization.observer.default_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_observer</span></code></a></p></td>
<td><p>Default observer for static quantization, usually used for debugging.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.default_placeholder_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_placeholder_observer.html#torch.ao.quantization.observer.default_placeholder_observer" title="torch.ao.quantization.observer.default_placeholder_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_placeholder_observer</span></code></a></p></td>
<td><p>Default placeholder observer, usually used for quantization to torch.float16.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.default_debug_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_debug_observer.html#torch.ao.quantization.observer.default_debug_observer" title="torch.ao.quantization.observer.default_debug_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_debug_observer</span></code></a></p></td>
<td><p>Default debug-only observer.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.default_weight_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_weight_observer.html#torch.ao.quantization.observer.default_weight_observer" title="torch.ao.quantization.observer.default_weight_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_observer</span></code></a></p></td>
<td><p>Default weight observer.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.default_histogram_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_histogram_observer.html#torch.ao.quantization.observer.default_histogram_observer" title="torch.ao.quantization.observer.default_histogram_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_histogram_observer</span></code></a></p></td>
<td><p>Default histogram observer, usually used for PTQ.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.default_per_channel_weight_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html#torch.ao.quantization.observer.default_per_channel_weight_observer" title="torch.ao.quantization.observer.default_per_channel_weight_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_weight_observer</span></code></a></p></td>
<td><p>Default per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such as <cite>fbgemm</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.observer.default_dynamic_quant_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html#torch.ao.quantization.observer.default_dynamic_quant_observer" title="torch.ao.quantization.observer.default_dynamic_quant_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_dynamic_quant_observer</span></code></a></p></td>
<td><p>Default observer for dynamic quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.observer.default_float_qparams_observer"/><a class="reference internal" href="generated/torch.ao.quantization.observer.default_float_qparams_observer.html#torch.ao.quantization.observer.default_float_qparams_observer" title="torch.ao.quantization.observer.default_float_qparams_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_float_qparams_observer</span></code></a></p></td>
<td><p>Default observer for a floating point zero-point.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-ao-quantization-fake-quantize">
<h2>torch.ao.quantization.fake_quantize<a class="headerlink" href="#torch-ao-quantization-fake-quantize" title="Permalink to this heading">¶</a></h2>
<p>This module implements modules which are used to perform fake quantization
during QAT.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.FakeQuantizeBase"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html#torch.ao.quantization.fake_quantize.FakeQuantizeBase" title="torch.ao.quantization.fake_quantize.FakeQuantizeBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FakeQuantizeBase</span></code></a></p></td>
<td><p>Base fake quantize module Any fake quantize implementation should derive from this class.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.FakeQuantize"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantize.html#torch.ao.quantization.fake_quantize.FakeQuantize" title="torch.ao.quantization.fake_quantize.FakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FakeQuantize</span></code></a></p></td>
<td><p>Simulate the quantize and dequantize operations in training time. The output of this module is given by::.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize" title="torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FixedQParamsFakeQuantize</span></code></a></p></td>
<td><p>Simulate quantize and dequantize with fixed quantization parameters in training time.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize" title="torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FusedMovingAvgObsFakeQuantize</span></code></a></p></td>
<td><p>Fused module that is used to observe the input tensor (compute min/max), compute scale/zero_point and fake_quantize the tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.default_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fake_quant.html#torch.ao.quantization.fake_quantize.default_fake_quant" title="torch.ao.quantization.fake_quantize.default_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fake_quant</span></code></a></p></td>
<td><p>Default fake_quant for activations.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.default_weight_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_weight_fake_quant" title="torch.ao.quantization.fake_quantize.default_weight_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_fake_quant</span></code></a></p></td>
<td><p>Default fake_quant for weights.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant" title="torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_weight_fake_quant</span></code></a></p></td>
<td><p>Default fake_quant for per-channel weights.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.default_histogram_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html#torch.ao.quantization.fake_quantize.default_histogram_fake_quant" title="torch.ao.quantization.fake_quantize.default_histogram_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_histogram_fake_quant</span></code></a></p></td>
<td><p>Fake_quant for activations using a histogram..</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.default_fused_act_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_act_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_act_fake_quant</span></code></a></p></td>
<td><p>Fused version of <cite>default_fake_quant</cite>, with improved performance.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_wt_fake_quant</span></code></a></p></td>
<td><p>Fused version of <cite>default_weight_fake_quant</cite>, with improved performance.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_fused_per_channel_wt_fake_quant</span></code></a></p></td>
<td><p>Fused version of <cite>default_per_channel_weight_fake_quant</cite>, with improved performance.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.disable_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html#torch.ao.quantization.fake_quantize.disable_fake_quant" title="torch.ao.quantization.fake_quantize.disable_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">disable_fake_quant</span></code></a></p></td>
<td><p>Disable fake quantization for this module, if applicable. Example usage::.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.enable_fake_quant"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html#torch.ao.quantization.fake_quantize.enable_fake_quant" title="torch.ao.quantization.fake_quantize.enable_fake_quant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_fake_quant</span></code></a></p></td>
<td><p>Enable fake quantization for this module, if applicable. Example usage::.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.fake_quantize.disable_observer"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_observer.html#torch.ao.quantization.fake_quantize.disable_observer" title="torch.ao.quantization.fake_quantize.disable_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">disable_observer</span></code></a></p></td>
<td><p>Disable observation for this module, if applicable. Example usage::.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.fake_quantize.enable_observer"/><a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_observer.html#torch.ao.quantization.fake_quantize.enable_observer" title="torch.ao.quantization.fake_quantize.enable_observer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_observer</span></code></a></p></td>
<td><p>Enable observation for this module, if applicable. Example usage::.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-ao-quantization-qconfig">
<h2>torch.ao.quantization.qconfig<a class="headerlink" href="#torch-ao-quantization-qconfig" title="Permalink to this heading">¶</a></h2>
<p>This module defines <cite>QConfig</cite> objects which are used
to configure quantization settings for individual ops.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.QConfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig" title="torch.ao.quantization.qconfig.QConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QConfig</span></code></a></p></td>
<td><p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.default_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qconfig.html#torch.ao.quantization.qconfig.default_qconfig" title="torch.ao.quantization.qconfig.default_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qconfig</span></code></a></p></td>
<td><p>Default qconfig configuration.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.default_debug_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_debug_qconfig.html#torch.ao.quantization.qconfig.default_debug_qconfig" title="torch.ao.quantization.qconfig.default_debug_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_debug_qconfig</span></code></a></p></td>
<td><p>Default qconfig configuration for debugging.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.default_per_channel_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html#torch.ao.quantization.qconfig.default_per_channel_qconfig" title="torch.ao.quantization.qconfig.default_per_channel_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_per_channel_qconfig</span></code></a></p></td>
<td><p>Default qconfig configuration for per channel weight quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.default_dynamic_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html#torch.ao.quantization.qconfig.default_dynamic_qconfig" title="torch.ao.quantization.qconfig.default_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_dynamic_qconfig</span></code></a></p></td>
<td><p>Default dynamic qconfig.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.float16_dynamic_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html#torch.ao.quantization.qconfig.float16_dynamic_qconfig" title="torch.ao.quantization.qconfig.float16_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float16_dynamic_qconfig</span></code></a></p></td>
<td><p>Dynamic qconfig with weights quantized to <cite>torch.float16</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.float16_static_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_static_qconfig.html#torch.ao.quantization.qconfig.float16_static_qconfig" title="torch.ao.quantization.qconfig.float16_static_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float16_static_qconfig</span></code></a></p></td>
<td><p>Dynamic qconfig with both activations and weights quantized to <cite>torch.float16</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.per_channel_dynamic_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig" title="torch.ao.quantization.qconfig.per_channel_dynamic_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">per_channel_dynamic_qconfig</span></code></a></p></td>
<td><p>Dynamic qconfig with weights quantized per channel.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig" title="torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float_qparams_weight_only_qconfig</span></code></a></p></td>
<td><p>Dynamic qconfig with weights quantized with a floating point zero_point.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.default_qat_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig.html#torch.ao.quantization.qconfig.default_qat_qconfig" title="torch.ao.quantization.qconfig.default_qat_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qat_qconfig</span></code></a></p></td>
<td><p>Default qconfig for QAT.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.default_weight_only_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html#torch.ao.quantization.qconfig.default_weight_only_qconfig" title="torch.ao.quantization.qconfig.default_weight_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_weight_only_qconfig</span></code></a></p></td>
<td><p>Default qconfig for quantizing weights only.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.quantization.qconfig.default_activation_only_qconfig"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html#torch.ao.quantization.qconfig.default_activation_only_qconfig" title="torch.ao.quantization.qconfig.default_activation_only_qconfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_activation_only_qconfig</span></code></a></p></td>
<td><p>Default qconfig for quantizing activations only.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.quantization.qconfig.default_qat_qconfig_v2"/><a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html#torch.ao.quantization.qconfig.default_qat_qconfig_v2" title="torch.ao.quantization.qconfig.default_qat_qconfig_v2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">default_qat_qconfig_v2</span></code></a></p></td>
<td><p>Fused version of <cite>default_qat_config</cite>, has performance benefits.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.intrinsic">
<span id="torch-ao-nn-intrinsic"></span><h2>torch.ao.nn.intrinsic<a class="headerlink" href="#module-torch.ao.nn.intrinsic" title="Permalink to this heading">¶</a></h2>
<span class="target" id="module-torch.ao.nn.intrinsic.modules"></span><p>This module implements the combined (fused) modules conv + relu which can
then be quantized.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvReLU1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU1d.html#torch.ao.nn.intrinsic.ConvReLU1d" title="torch.ao.nn.intrinsic.ConvReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv1d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.ConvReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU2d.html#torch.ao.nn.intrinsic.ConvReLU2d" title="torch.ao.nn.intrinsic.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv2d and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU3d.html#torch.ao.nn.intrinsic.ConvReLU3d" title="torch.ao.nn.intrinsic.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv3d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.LinearReLU"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.LinearReLU.html#torch.ao.nn.intrinsic.LinearReLU" title="torch.ao.nn.intrinsic.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>This is a sequential container which calls the Linear and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvBn1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn1d.html#torch.ao.nn.intrinsic.ConvBn1d" title="torch.ao.nn.intrinsic.ConvBn1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 1d and Batch Norm 1d modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.ConvBn2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn2d.html#torch.ao.nn.intrinsic.ConvBn2d" title="torch.ao.nn.intrinsic.ConvBn2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 2d and Batch Norm 2d modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvBn3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn3d.html#torch.ao.nn.intrinsic.ConvBn3d" title="torch.ao.nn.intrinsic.ConvBn3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 3d and Batch Norm 3d modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.ConvBnReLU1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html#torch.ao.nn.intrinsic.ConvBnReLU1d" title="torch.ao.nn.intrinsic.ConvBnReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU1d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.ConvBnReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html#torch.ao.nn.intrinsic.ConvBnReLU2d" title="torch.ao.nn.intrinsic.ConvBnReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.ConvBnReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html#torch.ao.nn.intrinsic.ConvBnReLU3d" title="torch.ao.nn.intrinsic.ConvBnReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.BNReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU2d.html#torch.ao.nn.intrinsic.BNReLU2d" title="torch.ao.nn.intrinsic.BNReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU2d</span></code></a></p></td>
<td><p>This is a sequential container which calls the BatchNorm 2d and ReLU modules.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.BNReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU3d.html#torch.ao.nn.intrinsic.BNReLU3d" title="torch.ao.nn.intrinsic.BNReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU3d</span></code></a></p></td>
<td><p>This is a sequential container which calls the BatchNorm 3d and ReLU modules.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.intrinsic.qat">
<span id="torch-ao-nn-intrinsic-qat"></span><h2>torch.ao.nn.intrinsic.qat<a class="headerlink" href="#module-torch.ao.nn.intrinsic.qat" title="Permalink to this heading">¶</a></h2>
<span class="target" id="module-torch.ao.nn.intrinsic.qat.modules"></span><p>This module implements the versions of those fused operations needed for
quantization aware training.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.LinearReLU"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.LinearReLU.html#torch.ao.nn.intrinsic.qat.LinearReLU" title="torch.ao.nn.intrinsic.qat.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBn1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html#torch.ao.nn.intrinsic.qat.ConvBn1d" title="torch.ao.nn.intrinsic.qat.ConvBn1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn1d</span></code></a></p></td>
<td><p>A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU1d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU1d</span></code></a></p></td>
<td><p>A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBn2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html#torch.ao.nn.intrinsic.qat.ConvBn2d" title="torch.ao.nn.intrinsic.qat.ConvBn2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn2d</span></code></a></p></td>
<td><p>A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU2d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU2d</span></code></a></p></td>
<td><p>A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html#torch.ao.nn.intrinsic.qat.ConvReLU2d" title="torch.ao.nn.intrinsic.qat.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBn3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html#torch.ao.nn.intrinsic.qat.ConvBn3d" title="torch.ao.nn.intrinsic.qat.ConvBn3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBn3d</span></code></a></p></td>
<td><p>A ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvBnReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU3d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvBnReLU3d</span></code></a></p></td>
<td><p>A ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.ConvReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html#torch.ao.nn.intrinsic.qat.ConvReLU3d" title="torch.ao.nn.intrinsic.qat.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.qat.update_bn_stats"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html#torch.ao.nn.intrinsic.qat.update_bn_stats" title="torch.ao.nn.intrinsic.qat.update_bn_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">update_bn_stats</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.qat.freeze_bn_stats"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html#torch.ao.nn.intrinsic.qat.freeze_bn_stats" title="torch.ao.nn.intrinsic.qat.freeze_bn_stats"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze_bn_stats</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.intrinsic.quantized">
<span id="torch-ao-nn-intrinsic-quantized"></span><h2>torch.ao.nn.intrinsic.quantized<a class="headerlink" href="#module-torch.ao.nn.intrinsic.quantized" title="Permalink to this heading">¶</a></h2>
<span class="target" id="module-torch.ao.nn.intrinsic.quantized.modules"></span><p>This module implements the quantized implementations of fused operations
like conv + relu. No BatchNorm variants as it’s usually folded into convolution
for inference.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.quantized.BNReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html#torch.ao.nn.intrinsic.quantized.BNReLU2d" title="torch.ao.nn.intrinsic.quantized.BNReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU2d</span></code></a></p></td>
<td><p>A BNReLU2d module is a fused module of BatchNorm2d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.quantized.BNReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html#torch.ao.nn.intrinsic.quantized.BNReLU3d" title="torch.ao.nn.intrinsic.quantized.BNReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BNReLU3d</span></code></a></p></td>
<td><p>A BNReLU3d module is a fused module of BatchNorm3d and ReLU</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.quantized.ConvReLU1d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html#torch.ao.nn.intrinsic.quantized.ConvReLU1d" title="torch.ao.nn.intrinsic.quantized.ConvReLU1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU1d</span></code></a></p></td>
<td><p>A ConvReLU1d module is a fused module of Conv1d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.quantized.ConvReLU2d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html#torch.ao.nn.intrinsic.quantized.ConvReLU2d" title="torch.ao.nn.intrinsic.quantized.ConvReLU2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU2d</span></code></a></p></td>
<td><p>A ConvReLU2d module is a fused module of Conv2d and ReLU</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.quantized.ConvReLU3d"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html#torch.ao.nn.intrinsic.quantized.ConvReLU3d" title="torch.ao.nn.intrinsic.quantized.ConvReLU3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvReLU3d</span></code></a></p></td>
<td><p>A ConvReLU3d module is a fused module of Conv3d and ReLU</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.intrinsic.quantized.LinearReLU"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html#torch.ao.nn.intrinsic.quantized.LinearReLU" title="torch.ao.nn.intrinsic.quantized.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.intrinsic.quantized.dynamic">
<span id="torch-ao-nn-intrinsic-quantized-dynamic"></span><h2>torch.ao.nn.intrinsic.quantized.dynamic<a class="headerlink" href="#module-torch.ao.nn.intrinsic.quantized.dynamic" title="Permalink to this heading">¶</a></h2>
<span class="target" id="module-torch.ao.nn.intrinsic.quantized.dynamic.modules"></span><p>This module implements the quantized dynamic implementations of fused operations
like linear + relu.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU"/><a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU" title="torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearReLU</span></code></a></p></td>
<td><p>A LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.qat">
<span id="torch-ao-nn-qat"></span><h2>torch.ao.nn.qat<a class="headerlink" href="#module-torch.ao.nn.qat" title="Permalink to this heading">¶</a></h2>
<span class="target" id="module-torch.ao.nn.qat.modules"></span><p>This module implements versions of the key nn modules <strong>Conv2d()</strong> and
<strong>Linear()</strong> which run in FP32 but with rounding applied to simulate the
effect of INT8 quantization.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.qat.Conv2d"/><a class="reference internal" href="generated/torch.ao.nn.qat.Conv2d.html#torch.ao.nn.qat.Conv2d" title="torch.ao.nn.qat.Conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2d</span></code></a></p></td>
<td><p>A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.qat.Conv3d"/><a class="reference internal" href="generated/torch.ao.nn.qat.Conv3d.html#torch.ao.nn.qat.Conv3d" title="torch.ao.nn.qat.Conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3d</span></code></a></p></td>
<td><p>A Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.qat.Linear"/><a class="reference internal" href="generated/torch.ao.nn.qat.Linear.html#torch.ao.nn.qat.Linear" title="torch.ao.nn.qat.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A linear module attached with FakeQuantize modules for weight, used for quantization aware training.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.qat.dynamic">
<span id="torch-ao-nn-qat-dynamic"></span><h2>torch.ao.nn.qat.dynamic<a class="headerlink" href="#module-torch.ao.nn.qat.dynamic" title="Permalink to this heading">¶</a></h2>
<span class="target" id="module-torch.ao.nn.qat.dynamic.modules"></span><p>This module implements versions of the key nn modules such as <strong>Linear()</strong>
which run in FP32 but with rounding applied to simulate the effect of INT8
quantization and will be dynamically quantized during inference.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.qat.dynamic.Linear"/><a class="reference internal" href="generated/torch.ao.nn.qat.dynamic.Linear.html#torch.ao.nn.qat.dynamic.Linear" title="torch.ao.nn.qat.dynamic.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.quantized.modules">
<span id="torch-ao-nn-quantized"></span><h2>torch.ao.nn.quantized<a class="headerlink" href="#module-torch.ao.nn.quantized.modules" title="Permalink to this heading">¶</a></h2>
<p>This module implements the quantized versions of the nn layers such as
~`torch.nn.Conv2d` and <cite>torch.nn.ReLU</cite>.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.ReLU6"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ReLU6.html#torch.ao.nn.quantized.ReLU6" title="torch.ao.nn.quantized.ReLU6"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ReLU6</span></code></a></p></td>
<td><p>Applies the element-wise function:</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.Hardswish"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Hardswish.html#torch.ao.nn.quantized.Hardswish" title="torch.ao.nn.quantized.Hardswish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Hardswish</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.Hardswish.html#torch.nn.Hardswish" title="torch.nn.Hardswish"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardswish</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.ELU"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ELU.html#torch.ao.nn.quantized.ELU" title="torch.ao.nn.quantized.ELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ELU</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.ELU.html#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ELU</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.LeakyReLU"/><a class="reference internal" href="generated/torch.ao.nn.quantized.LeakyReLU.html#torch.ao.nn.quantized.LeakyReLU" title="torch.ao.nn.quantized.LeakyReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.Sigmoid"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Sigmoid.html#torch.ao.nn.quantized.Sigmoid" title="torch.ao.nn.quantized.Sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Sigmoid</span></code></a></p></td>
<td><p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.BatchNorm2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm2d.html#torch.ao.nn.quantized.BatchNorm2d" title="torch.ao.nn.quantized.BatchNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.BatchNorm3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm3d.html#torch.ao.nn.quantized.BatchNorm3d" title="torch.ao.nn.quantized.BatchNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.Conv1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv1d.html#torch.ao.nn.quantized.Conv1d" title="torch.ao.nn.quantized.Conv1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv1d</span></code></a></p></td>
<td><p>Applies a 1D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.Conv2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv2d.html#torch.ao.nn.quantized.Conv2d" title="torch.ao.nn.quantized.Conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2d</span></code></a></p></td>
<td><p>Applies a 2D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.Conv3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Conv3d.html#torch.ao.nn.quantized.Conv3d" title="torch.ao.nn.quantized.Conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3d</span></code></a></p></td>
<td><p>Applies a 3D convolution over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.ConvTranspose1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose1d.html#torch.ao.nn.quantized.ConvTranspose1d" title="torch.ao.nn.quantized.ConvTranspose1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a></p></td>
<td><p>Applies a 1D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.ConvTranspose2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose2d.html#torch.ao.nn.quantized.ConvTranspose2d" title="torch.ao.nn.quantized.ConvTranspose2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a></p></td>
<td><p>Applies a 2D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.ConvTranspose3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose3d.html#torch.ao.nn.quantized.ConvTranspose3d" title="torch.ao.nn.quantized.ConvTranspose3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a></p></td>
<td><p>Applies a 3D transposed convolution operator over an input image composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.Embedding"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Embedding.html#torch.ao.nn.quantized.Embedding" title="torch.ao.nn.quantized.Embedding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Embedding</span></code></a></p></td>
<td><p>A quantized Embedding module with quantized packed weights as inputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.EmbeddingBag"/><a class="reference internal" href="generated/torch.ao.nn.quantized.EmbeddingBag.html#torch.ao.nn.quantized.EmbeddingBag" title="torch.ao.nn.quantized.EmbeddingBag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EmbeddingBag</span></code></a></p></td>
<td><p>A quantized EmbeddingBag module with quantized packed weights as inputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.FloatFunctional"/><a class="reference internal" href="generated/torch.ao.nn.quantized.FloatFunctional.html#torch.ao.nn.quantized.FloatFunctional" title="torch.ao.nn.quantized.FloatFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FloatFunctional</span></code></a></p></td>
<td><p>State collector class for float operations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.FXFloatFunctional"/><a class="reference internal" href="generated/torch.ao.nn.quantized.FXFloatFunctional.html#torch.ao.nn.quantized.FXFloatFunctional" title="torch.ao.nn.quantized.FXFloatFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FXFloatFunctional</span></code></a></p></td>
<td><p>module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.QFunctional"/><a class="reference internal" href="generated/torch.ao.nn.quantized.QFunctional.html#torch.ao.nn.quantized.QFunctional" title="torch.ao.nn.quantized.QFunctional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QFunctional</span></code></a></p></td>
<td><p>Wrapper class for quantized operations.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.Linear"/><a class="reference internal" href="generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear" title="torch.ao.nn.quantized.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A quantized linear module with quantized tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.LayerNorm"/><a class="reference internal" href="generated/torch.ao.nn.quantized.LayerNorm.html#torch.ao.nn.quantized.LayerNorm" title="torch.ao.nn.quantized.LayerNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LayerNorm</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.GroupNorm"/><a class="reference internal" href="generated/torch.ao.nn.quantized.GroupNorm.html#torch.ao.nn.quantized.GroupNorm" title="torch.ao.nn.quantized.GroupNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GroupNorm</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm" title="torch.nn.GroupNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">GroupNorm</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.InstanceNorm1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm1d.html#torch.ao.nn.quantized.InstanceNorm1d" title="torch.ao.nn.quantized.InstanceNorm1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.InstanceNorm2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm2d.html#torch.ao.nn.quantized.InstanceNorm2d" title="torch.ao.nn.quantized.InstanceNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.InstanceNorm3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm3d.html#torch.ao.nn.quantized.InstanceNorm3d" title="torch.ao.nn.quantized.InstanceNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a>.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.quantized.functional">
<span id="torch-ao-nn-quantized-functional"></span><h2>torch.ao.nn.quantized.functional<a class="headerlink" href="#module-torch.ao.nn.quantized.functional" title="Permalink to this heading">¶</a></h2>
<p>Functional interface (quantized).</p>
<p>This module implements the quantized versions of the functional layers such as
~`torch.nn.functional.conv2d` and <cite>torch.nn.functional.relu</cite>. Note:
<a class="reference internal" href="generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><code class="xref py py-meth docutils literal notranslate"><span class="pre">relu()</span></code></a> supports quantized inputs.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.avg_pool2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool2d.html#torch.ao.nn.quantized.functional.avg_pool2d" title="torch.ao.nn.quantized.functional.avg_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">avg_pool2d</span></code></a></p></td>
<td><p>Applies 2D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kH \times kW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">kW</span></span></span></span></span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sH \times sW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">sH</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span> steps.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.avg_pool3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool3d.html#torch.ao.nn.quantized.functional.avg_pool3d" title="torch.ao.nn.quantized.functional.avg_pool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">avg_pool3d</span></code></a></p></td>
<td><p>Applies 3D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>D</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kD \ times kH \times kW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace"> </span><span class="mord mathnormal">t</span><span class="mord mathnormal">im</span><span class="mord mathnormal">es</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">kW</span></span></span></span></span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>D</mi><mo>×</mo><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sD \times sH \times sW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">sD</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">sH</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span> steps.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.adaptive_avg_pool2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool2d" title="torch.ao.nn.quantized.functional.adaptive_avg_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adaptive_avg_pool2d</span></code></a></p></td>
<td><p>Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.adaptive_avg_pool3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool3d" title="torch.ao.nn.quantized.functional.adaptive_avg_pool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adaptive_avg_pool3d</span></code></a></p></td>
<td><p>Applies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.conv1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv1d.html#torch.ao.nn.quantized.functional.conv1d" title="torch.ao.nn.quantized.functional.conv1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv1d</span></code></a></p></td>
<td><p>Applies a 1D convolution over a quantized 1D input composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.conv2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv2d.html#torch.ao.nn.quantized.functional.conv2d" title="torch.ao.nn.quantized.functional.conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv2d</span></code></a></p></td>
<td><p>Applies a 2D convolution over a quantized 2D input composed of several input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.conv3d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv3d.html#torch.ao.nn.quantized.functional.conv3d" title="torch.ao.nn.quantized.functional.conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">conv3d</span></code></a></p></td>
<td><p>Applies a 3D convolution over a quantized 3D input composed of several input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.interpolate"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.interpolate.html#torch.ao.nn.quantized.functional.interpolate" title="torch.ao.nn.quantized.functional.interpolate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">interpolate</span></code></a></p></td>
<td><p>Down/up samples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.linear"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.linear.html#torch.ao.nn.quantized.functional.linear" title="torch.ao.nn.quantized.functional.linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">linear</span></code></a></p></td>
<td><p>Applies a linear transformation to the incoming quantized data: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9247em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.max_pool1d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool1d.html#torch.ao.nn.quantized.functional.max_pool1d" title="torch.ao.nn.quantized.functional.max_pool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_pool1d</span></code></a></p></td>
<td><p>Applies a 1D max pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.max_pool2d"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool2d.html#torch.ao.nn.quantized.functional.max_pool2d" title="torch.ao.nn.quantized.functional.max_pool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_pool2d</span></code></a></p></td>
<td><p>Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.celu"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.celu.html#torch.ao.nn.quantized.functional.celu" title="torch.ao.nn.quantized.functional.celu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">celu</span></code></a></p></td>
<td><p>Applies the quantized CELU function element-wise.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.leaky_relu"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.leaky_relu.html#torch.ao.nn.quantized.functional.leaky_relu" title="torch.ao.nn.quantized.functional.leaky_relu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">leaky_relu</span></code></a></p></td>
<td><p>Quantized version of the.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.hardtanh"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardtanh.html#torch.ao.nn.quantized.functional.hardtanh" title="torch.ao.nn.quantized.functional.hardtanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardtanh</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardtanh()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.hardswish"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardswish.html#torch.ao.nn.quantized.functional.hardswish" title="torch.ao.nn.quantized.functional.hardswish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardswish</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish" title="torch.nn.functional.hardswish"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardswish()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.threshold"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.threshold.html#torch.ao.nn.quantized.functional.threshold" title="torch.ao.nn.quantized.functional.threshold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code></a></p></td>
<td><p>Applies the quantized version of the threshold function element-wise:</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.elu"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.elu.html#torch.ao.nn.quantized.functional.elu" title="torch.ao.nn.quantized.functional.elu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">elu</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.elu.html#torch.nn.functional.elu" title="torch.nn.functional.elu"><code class="xref py py-func docutils literal notranslate"><span class="pre">elu()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.hardsigmoid"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardsigmoid.html#torch.ao.nn.quantized.functional.hardsigmoid" title="torch.ao.nn.quantized.functional.hardsigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hardsigmoid</span></code></a></p></td>
<td><p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid" title="torch.nn.functional.hardsigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardsigmoid()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.clamp"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.clamp.html#torch.ao.nn.quantized.functional.clamp" title="torch.ao.nn.quantized.functional.clamp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clamp</span></code></a></p></td>
<td><p>float(input, min_, max_) -&gt; Tensor</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.upsample"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample.html#torch.ao.nn.quantized.functional.upsample" title="torch.ao.nn.quantized.functional.upsample"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample</span></code></a></p></td>
<td><p>Upsamples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.functional.upsample_bilinear"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_bilinear.html#torch.ao.nn.quantized.functional.upsample_bilinear" title="torch.ao.nn.quantized.functional.upsample_bilinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample_bilinear</span></code></a></p></td>
<td><p>Upsamples the input, using bilinear upsampling.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.functional.upsample_nearest"/><a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_nearest.html#torch.ao.nn.quantized.functional.upsample_nearest" title="torch.ao.nn.quantized.functional.upsample_nearest"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upsample_nearest</span></code></a></p></td>
<td><p>Upsamples the input, using nearest neighbours' pixel values.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="torch-ao-nn-quantizable">
<h2>torch.ao.nn.quantizable<a class="headerlink" href="#torch-ao-nn-quantizable" title="Permalink to this heading">¶</a></h2>
<p>This module implements the quantizable versions of some of the nn layers.
These modules can be used in conjunction with the custom module mechanism,
by providing the <code class="docutils literal notranslate"><span class="pre">custom_module_config</span></code> argument to both prepare and convert.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantizable.LSTM"/><a class="reference internal" href="generated/torch.ao.nn.quantizable.LSTM.html#torch.ao.nn.quantizable.LSTM" title="torch.ao.nn.quantizable.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTM</span></code></a></p></td>
<td><p>A quantizable long short-term memory (LSTM).</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantizable.MultiheadAttention"/><a class="reference internal" href="generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention" title="torch.ao.nn.quantizable.MultiheadAttention"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiheadAttention</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="module-torch.ao.nn.quantized.dynamic">
<span id="torch-ao-nn-quantized-dynamic"></span><h2>torch.ao.nn.quantized.dynamic<a class="headerlink" href="#module-torch.ao.nn.quantized.dynamic" title="Permalink to this heading">¶</a></h2>
<span class="target" id="module-torch.ao.nn.quantized.dynamic.modules"></span><p>Dynamically quantized <a class="reference internal" href="generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code></a>, <a class="reference internal" href="generated/torch.nn.LSTM.html#torch.nn.LSTM" title="torch.nn.LSTM"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTM</span></code></a>,
<a class="reference internal" href="generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell" title="torch.nn.LSTMCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMCell</span></code></a>, <a class="reference internal" href="generated/torch.nn.GRUCell.html#torch.nn.GRUCell" title="torch.nn.GRUCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">GRUCell</span></code></a>, and
<a class="reference internal" href="generated/torch.nn.RNNCell.html#torch.nn.RNNCell" title="torch.nn.RNNCell"><code class="xref py py-class docutils literal notranslate"><span class="pre">RNNCell</span></code></a>.</p>
<table class="autosummary longtable docutils colwidths-auto align-default">
<tbody>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.dynamic.Linear"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear" title="torch.ao.nn.quantized.dynamic.Linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Linear</span></code></a></p></td>
<td><p>A dynamic quantized linear module with floating point tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.dynamic.LSTM"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTM.html#torch.ao.nn.quantized.dynamic.LSTM" title="torch.ao.nn.quantized.dynamic.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTM</span></code></a></p></td>
<td><p>A dynamic quantized LSTM module with floating point tensor as inputs and outputs.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.dynamic.GRU"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRU.html#torch.ao.nn.quantized.dynamic.GRU" title="torch.ao.nn.quantized.dynamic.GRU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRU</span></code></a></p></td>
<td><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.dynamic.RNNCell"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.RNNCell.html#torch.ao.nn.quantized.dynamic.RNNCell" title="torch.ao.nn.quantized.dynamic.RNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RNNCell</span></code></a></p></td>
<td><p>An Elman RNN cell with tanh or ReLU non-linearity.</p></td>
</tr>
<tr class="row-odd"><td><p><p id="torch.ao.nn.quantized.dynamic.LSTMCell"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTMCell.html#torch.ao.nn.quantized.dynamic.LSTMCell" title="torch.ao.nn.quantized.dynamic.LSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LSTMCell</span></code></a></p></td>
<td><p>A long short-term memory (LSTM) cell.</p></td>
</tr>
<tr class="row-even"><td><p><p id="torch.ao.nn.quantized.dynamic.GRUCell"/><a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRUCell.html#torch.ao.nn.quantized.dynamic.GRUCell" title="torch.ao.nn.quantized.dynamic.GRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRUCell</span></code></a></p></td>
<td><p>A gated recurrent unit (GRU) cell</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="quantized-dtypes-and-quantization-schemes">
<h2>Quantized dtypes and quantization schemes<a class="headerlink" href="#quantized-dtypes-and-quantization-schemes" title="Permalink to this heading">¶</a></h2>
<p>Note that operator implementations currently only
support per channel quantization for weights of the <strong>conv</strong> and <strong>linear</strong>
operators. Furthermore, the input data is
mapped linearly to the quantized data and vice versa
as follows:</p>
<blockquote>
<div><div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Quantization:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>Q</mi><mtext>out</mtext></msub><mo>=</mo><mtext>clamp</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>input</mtext></msub><mi mathvariant="normal">/</mi><mi>s</mi><mo>+</mo><mi>z</mi><mo separator="true">,</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Dequantization:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>x</mi><mtext>out</mtext></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>Q</mi><mtext>input</mtext></msub><mo>−</mo><mi>z</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>s</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \text{Quantization:}&amp;\\
    &amp;Q_\text{out} = \text{clamp}(x_\text{input}/s+z, Q_\text{min}, Q_\text{max})\\
    \text{Dequantization:}&amp;\\
    &amp;x_\text{out} = (Q_\text{input}-z)*s
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6em;vertical-align:-2.75em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">Quantization:</span></span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">Dequantization:</span></span></span></span><span style="top:-0.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.75em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.25em;"><span style="top:-5.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord text"><span class="mord">clamp</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">input</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-0.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">input</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.75em;"><span></span></span></span></span></span></span></span></span></span></span></span></div></div></blockquote>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>clamp</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">.</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{clamp}(.)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">clamp</span></span><span class="mopen">(</span><span class="mord">.</span><span class="mclose">)</span></span></span></span></span> is the same as <a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">clamp()</span></code></a> while the
scale <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> and zero point <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> are then computed
as described in <a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver" title="torch.ao.quantization.observer.MinMaxObserver"><code class="xref py py-class docutils literal notranslate"><span class="pre">MinMaxObserver</span></code></a>, specifically:</p>
<blockquote>
<div><div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>if Symmetric:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>s</mi><mo>=</mo><mn>2</mn><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mtext>min</mtext></msub><mi mathvariant="normal">∣</mi><mo separator="true">,</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mrow><mo fence="true">(</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>z</mi><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>if dtype is qint8</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>128</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Otherwise:</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>s</mi><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">/</mi><mrow><mo fence="true">(</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>z</mi><mo>=</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo>−</mo><mtext>round</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>min</mtext></msub><mi mathvariant="normal">/</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \text{if Symmetric:}&amp;\\
    &amp;s = 2 \max(|x_\text{min}|, x_\text{max}) /
        \left( Q_\text{max} - Q_\text{min} \right) \\
    &amp;z = \begin{cases}
        0 &amp; \text{if dtype is qint8} \\
        128 &amp; \text{otherwise}
    \end{cases}\\
    \text{Otherwise:}&amp;\\
        &amp;s = \left( x_\text{max} - x_\text{min}  \right ) /
            \left( Q_\text{max} - Q_\text{min} \right ) \\
        &amp;z = Q_\text{min} - \text{round}(x_\text{min} / s)
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:10.8em;vertical-align:-5.15em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:5.65em;"><span style="top:-8.56em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord text"><span class="mord">if Symmetric:</span></span></span></span><span style="top:-7.06em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:-4.65em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:-2.26em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord text"><span class="mord">Otherwise:</span></span></span></span><span style="top:-0.76em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span><span style="top:0.74em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:5.15em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:5.65em;"><span style="top:-8.56em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-7.06em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:-4.65em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">128</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if dtype is qint8</span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">otherwise</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.26em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span></span></span><span style="top:-0.76em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">/</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span><span style="top:0.74em;"><span class="pstrut" style="height:3.75em;"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord text"><span class="mord">round</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:5.15em;"><span></span></span></span></span></span></span></span></span></span></span></span></div></div></blockquote>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo separator="true">,</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_\text{min}, x_\text{max}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span> denotes the range of the input data while
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>min</mtext></msub></mrow><annotation encoding="application/x-tex">Q_\text{min}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>max</mtext></msub></mrow><annotation encoding="application/x-tex">Q_\text{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> are respectively the minimum and maximum values of the quantized dtype.</p>
<p>Note that the choice of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span></span> implies that zero is represented with no quantization error whenever zero is within
the range of the input data or symmetric quantization is being used.</p>
<p>Additional data types and quantization schemes can be implemented through
the <a class="reference external" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">custom operator mechanism</a>.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qscheme</span></code> — Type to describe the quantization scheme of a tensor.
Supported types:</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_tensor_affine</span></code> — per tensor, asymmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_channel_affine</span></code> — per channel, asymmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_tensor_symmetric</span></code> — per tensor, symmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_channel_symmetric</span></code> — per channel, symmetric</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code> — Type to describe the data. Supported types:</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.quint8</span></code> — 8-bit unsigned integer</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qint8</span></code> — 8-bit signed integer</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qint32</span></code> — 32-bit signed integer</p></li>
</ul>
</li>
</ul>
<span class="target" id="module-torch.nn.quantizable"></span><span class="target" id="module-torch.nn.qat.dynamic.modules"></span><span class="target" id="module-torch.nn.qat.modules"></span><p>QAT Modules</p>
<p>This package is in the process of being deprecated.
Please, use <cite>torch.ao.nn.qat.modules</cite> instead.</p>
<span class="target" id="module-torch.nn.qat"></span><p>QAT Dynamic Modules</p>
<p>This package is in the process of being deprecated.
Please, use <cite>torch.ao.nn.qat.dynamic</cite> instead.</p>
<span class="target" id="module-torch.nn.intrinsic.qat.modules"></span><span class="target" id="module-torch.nn.quantized.dynamic"></span><span class="target" id="module-torch.nn.intrinsic"></span><span class="target" id="module-torch.nn.intrinsic.quantized.modules"></span><span class="target" id="module-torch.quantization.fx"></span><p>This file is in the process of migration to <cite>torch/ao/quantization</cite>, and
is kept here for compatibility while the migration process is ongoing.
If you are adding a new entry/functionality, please, add it to the
appropriate files under <cite>torch/ao/quantization/fx/</cite>, while adding an import statement
here.</p>
<span class="target" id="module-torch.nn.intrinsic.quantized.dynamic"></span><span class="target" id="module-torch.nn.qat.dynamic"></span><p>QAT Dynamic Modules</p>
<p>This package is in the process of being deprecated.
Please, use <cite>torch.ao.nn.qat.dynamic</cite> instead.</p>
<span class="target" id="module-torch.nn.intrinsic.qat"></span><span class="target" id="module-torch.nn.quantized.modules"></span><p>Quantized Modules</p>
<dl class="simple">
<dt>Note::</dt><dd><p>The <cite>torch.nn.quantized</cite> namespace is in the process of being deprecated.
Please, use <cite>torch.ao.nn.quantized</cite> instead.</p>
</dd>
</dl>
<span class="target" id="module-torch.nn.intrinsic.quantized"></span><span class="target" id="module-torch.nn.quantizable.modules"></span><span class="target" id="module-torch.nn.quantized"></span><span class="target" id="module-torch.nn.intrinsic.quantized.dynamic.modules"></span><span class="target" id="module-torch.nn.quantized.dynamic.modules"></span><p>Quantized Dynamic Modules</p>
<p>This file is in the process of migration to <cite>torch/ao/nn/quantized/dynamic</cite>,
and is kept here for compatibility while the migration process is ongoing.
If you are adding a new entry/functionality, please, add it to the
appropriate file under the <cite>torch/ao/nn/quantized/dynamic</cite>,
while adding an import statement here.</p>
<span class="target" id="module-torch.quantization"></span><span class="target" id="module-torch.nn.intrinsic.modules"></span></div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/torch.ao.quantization.quantize.html" class="btn btn-neutral float-right" title="quantize" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="quantization.html" class="btn btn-neutral" title="Quantization" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Quantization API Reference</a><ul>
<li><a class="reference internal" href="#torch-ao-quantization">torch.ao.quantization</a><ul>
<li><a class="reference internal" href="#top-level-apis">Top level APIs</a></li>
<li><a class="reference internal" href="#preparing-model-for-quantization">Preparing model for quantization</a></li>
<li><a class="reference internal" href="#utility-functions">Utility functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch-ao-quantization-quantize-fx">torch.ao.quantization.quantize_fx</a></li>
<li><a class="reference internal" href="#torch-ao-quantization-qconfig-mapping">torch.ao.quantization.qconfig_mapping</a></li>
<li><a class="reference internal" href="#torch-ao-quantization-backend-config">torch.ao.quantization.backend_config</a></li>
<li><a class="reference internal" href="#torch-ao-quantization-fx-custom-config">torch.ao.quantization.fx.custom_config</a></li>
<li><a class="reference internal" href="#torch-quantization-related-functions">torch (quantization related functions)</a></li>
<li><a class="reference internal" href="#torch-tensor-quantization-related-methods">torch.Tensor (quantization related methods)</a></li>
<li><a class="reference internal" href="#torch-ao-quantization-observer">torch.ao.quantization.observer</a></li>
<li><a class="reference internal" href="#torch-ao-quantization-fake-quantize">torch.ao.quantization.fake_quantize</a></li>
<li><a class="reference internal" href="#torch-ao-quantization-qconfig">torch.ao.quantization.qconfig</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.intrinsic">torch.ao.nn.intrinsic</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.intrinsic.qat">torch.ao.nn.intrinsic.qat</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.intrinsic.quantized">torch.ao.nn.intrinsic.quantized</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.intrinsic.quantized.dynamic">torch.ao.nn.intrinsic.quantized.dynamic</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.qat">torch.ao.nn.qat</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.qat.dynamic">torch.ao.nn.qat.dynamic</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.quantized.modules">torch.ao.nn.quantized</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.quantized.functional">torch.ao.nn.quantized.functional</a></li>
<li><a class="reference internal" href="#torch-ao-nn-quantizable">torch.ao.nn.quantizable</a></li>
<li><a class="reference internal" href="#module-torch.ao.nn.quantized.dynamic">torch.ao.nn.quantized.dynamic</a></li>
<li><a class="reference internal" href="#quantized-dtypes-and-quantization-schemes">Quantized dtypes and quantization schemes</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.Tensor &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/tensors.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tensor Attributes" href="tensor_attributes.html" />
    <link rel="prev" title="torch" href="torch.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.4.1 <br/> <a href="https://pytorch.org/docs/versions.html"> version selector &#x25BC</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#requires-grad"><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-operations-with-autograd">In-place operations with autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#asynchronous-execution">Asynchronous execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#cuda-streams">CUDA streams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-nn">Extending <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/extending.html#adding-a-module">Adding a <code class="docutils literal notranslate"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#writing-custom-c-extensions">Writing custom C++ extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#id1">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory">My model reports “cuda runtime error(2): out of memory”</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-gpu-memory-isn-t-freed-properly">My GPU memory isn’t freed properly</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-data-loader-workers-return-identical-random-numbers">My data loader workers return identical random numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/faq.html#my-recurrent-network-doesn-t-work-with-data-parallelism">My recurrent network doesn’t work with data parallelism</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#building-from-source">Building from source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#include-optional-components">Include optional components</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#speeding-cuda-build-for-windows">Speeding CUDA build for Windows</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#one-key-install-script">One key install script</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#extension">Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cffi-extension">CFFI Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cpp-extension">Cpp Extension</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#package-not-found-in-win-32-channel">Package not found in win-32 channel.</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#why-are-there-no-python-2-packages-for-windows">Why are there no Python 2 packages for Windows?</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#import-error">Import error</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/windows.html#usage-multiprocessing">Usage (multiprocessing)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-without-if-clause-protection">Multiprocessing error without if-clause protection</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-broken-pipe">Multiprocessing error “Broken pipe”</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#multiprocessing-error-driver-shut-down">Multiprocessing error “driver shut down”</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/windows.html#cuda-ipc-operations">CUDA IPC operations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#spectral-ops">Spectral Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-dtype">torch.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-device">torch.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_attributes.html#torch-layout">torch.layout</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#moduledict"><span class="hidden-section">ModuleDict</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#parameterdict"><span class="hidden-section">ParameterDict</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-layers">Convolution layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#unfold"><span class="hidden-section">Unfold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#fold"><span class="hidden-section">Fold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-layers">Pooling layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lppool1d"><span class="hidden-section">LPPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#padding-layers">Padding layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#reflectionpad1d"><span class="hidden-section">ReflectionPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad1d"><span class="hidden-section">ReplicationPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad1d"><span class="hidden-section">ConstantPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#constantpad3d"><span class="hidden-section">ConstantPad3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activations-weighted-sum-nonlinearity">Non-linear activations (weighted sum, nonlinearity)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardshrink"><span class="hidden-section">Hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rrelu"><span class="hidden-section">RReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activations-other">Non-linear activations (other)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptivelogsoftmaxwithloss"><span class="hidden-section">AdaptiveLogSoftmaxWithLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#groupnorm"><span class="hidden-section">GroupNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#layernorm"><span class="hidden-section">LayerNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#localresponsenorm"><span class="hidden-section">LocalResponseNorm</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#clip-grad-value"><span class="hidden-section">clip_grad_value_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#parameters-to-vector"><span class="hidden-section">parameters_to_vector</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#vector-to-parameters"><span class="hidden-section">vector_to_parameters</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#spectral-norm"><span class="hidden-section">spectral_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#remove-spectral-norm"><span class="hidden-section">remove_spectral_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad-sequence"><span class="hidden-section">pad_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pack-sequence"><span class="hidden-section">pack_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id20"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id21"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id22"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id23"><span class="hidden-section">unfold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id24"><span class="hidden-section">fold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lp-pool1d"><span class="hidden-section">lp_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id25"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id26"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id27"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id28"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id29"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id30"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id31"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id32"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id33"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id34"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id35"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id36"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id37"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id38"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id39"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id40"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#gumbel-softmax"><span class="hidden-section">gumbel_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id41"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id42"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#instance-norm"><span class="hidden-section">instance_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#layer-norm"><span class="hidden-section">layer_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#local-response-norm"><span class="hidden-section">local_response_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id43"><span class="hidden-section">linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id44"><span class="hidden-section">bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id45"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id46"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id47"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#sparse-functions">Sparse functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id48"><span class="hidden-section">embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#embedding-bag"><span class="hidden-section">embedding_bag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id49">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#id50">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#interpolate"><span class="hidden-section">interpolate</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#id51"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dataparallel-functions-multi-gpu-distributed">DataParallel functions (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn.html#data-parallel"><span class="hidden-section">data_parallel</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step"><code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step-closure"><code class="docutils literal notranslate"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#in-place-operations-on-tensors">In-place operations on Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#variable-deprecated">Variable (deprecated)</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#tensor-autograd-functions">Tensor autograd functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#numerical-gradient-checking">Numerical gradient checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#profiler">Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#anomaly-detection">Anomaly detection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#score-function">Score function</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#pathwise-derivative">Pathwise derivative</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#exponentialfamily"><span class="hidden-section">ExponentialFamily</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#beta"><span class="hidden-section">Beta</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#binomial"><span class="hidden-section">Binomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#cauchy"><span class="hidden-section">Cauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#chi2"><span class="hidden-section">Chi2</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#dirichlet"><span class="hidden-section">Dirichlet</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#exponential"><span class="hidden-section">Exponential</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#fishersnedecor"><span class="hidden-section">FisherSnedecor</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#gamma"><span class="hidden-section">Gamma</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#geometric"><span class="hidden-section">Geometric</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#gumbel"><span class="hidden-section">Gumbel</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#halfcauchy"><span class="hidden-section">HalfCauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#halfnormal"><span class="hidden-section">HalfNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#independent"><span class="hidden-section">Independent</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#laplace"><span class="hidden-section">Laplace</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#lognormal"><span class="hidden-section">LogNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#multinomial"><span class="hidden-section">Multinomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#multivariatenormal"><span class="hidden-section">MultivariateNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#onehotcategorical"><span class="hidden-section">OneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#pareto"><span class="hidden-section">Pareto</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#poisson"><span class="hidden-section">Poisson</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#relaxedbernoulli"><span class="hidden-section">RelaxedBernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#relaxedonehotcategorical"><span class="hidden-section">RelaxedOneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#studentt"><span class="hidden-section">StudentT</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#transformeddistribution"><span class="hidden-section">TransformedDistribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#uniform"><span class="hidden-section">Uniform</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.kl"><cite>KL Divergence</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.transforms"><cite>Transforms</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.constraints"><cite>Constraints</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html#module-torch.distributions.constraint_registry"><cite>Constraint Registry</cite></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal notranslate"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-system-file-system">File system - <code class="docutils literal notranslate"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#collective-functions">Collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#multi-gpu-collective-functions">Multi-GPU collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed.html#launch-utility">Launch utility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx.html#functions">Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#emnist">EMNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#datasetfolder">DatasetFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/models.html">torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#module-torchvision.transforms.functional">Functional Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>torch.Tensor</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/tensors.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="torch-tensor">
<span id="tensor-doc"></span><h1>torch.Tensor<a class="headerlink" href="#torch-tensor" title="Permalink to this headline">¶</a></h1>
<p>A <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> is a multi-dimensional matrix containing elements of
a single data type.</p>
<p>Torch defines eight CPU tensor types and eight GPU tensor types:</p>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="34%" />
<col width="21%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Data type</th>
<th class="head">dtype</th>
<th class="head">CPU tensor</th>
<th class="head">GPU tensor</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>32-bit floating point</td>
<td><code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.FloatTensor</span></code></td>
</tr>
<tr class="row-odd"><td>64-bit floating point</td>
<td><code class="docutils literal notranslate"><span class="pre">torch.float64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.double</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.DoubleTensor</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.DoubleTensor</span></code></td>
</tr>
<tr class="row-even"><td>16-bit floating point</td>
<td><code class="docutils literal notranslate"><span class="pre">torch.float16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.half</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.HalfTensor</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.HalfTensor</span></code></td>
</tr>
<tr class="row-odd"><td>8-bit integer (unsigned)</td>
<td><code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code></td>
<td><a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code></a></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.ByteTensor</span></code></td>
</tr>
<tr class="row-even"><td>8-bit integer (signed)</td>
<td><code class="docutils literal notranslate"><span class="pre">torch.int8</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.CharTensor</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.CharTensor</span></code></td>
</tr>
<tr class="row-odd"><td>16-bit integer (signed)</td>
<td><code class="docutils literal notranslate"><span class="pre">torch.int16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.short</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ShortTensor</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.ShortTensor</span></code></td>
</tr>
<tr class="row-even"><td>32-bit integer (signed)</td>
<td><code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.int</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.IntTensor</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.IntTensor</span></code></td>
</tr>
<tr class="row-odd"><td>64-bit integer (signed)</td>
<td><code class="docutils literal notranslate"><span class="pre">torch.int64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.long</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.LongTensor</span></code></td>
<td><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.LongTensor</span></code></td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> is an alias for the default tensor type (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>).</p>
<p>A tensor can be constructed from a Python <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a> or sequence using the
<a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> constructor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]])</span>
<span class="go">tensor([[ 1.0000, -1.0000],</span>
<span class="go">        [ 1.0000, -1.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> and just want to change its <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> flag, use
<a class="reference internal" href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">requires_grad_()</span></code></a> or
<a class="reference internal" href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">detach()</span></code></a> to avoid a copy.
If you have a numpy array and want to avoid a copy, use
<a class="reference internal" href="torch.html#torch.as_tensor" title="torch.as_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_tensor()</span></code></a>.</p>
</div>
<p>An tensor of specific data type can be constructed by passing a
<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and/or a <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> to a
constructor or tensor creation op:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">tensor([[ 0,  0,  0,  0],</span>
<span class="go">        [ 0,  0,  0,  0]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],</span>
<span class="go">        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
<p>The contents of a tensor can be accessed and modified using Python’s indexing
and slicing notation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="go">tensor(6)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 1,  8,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<p>Use <a class="reference internal" href="#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.item()</span></code></a> to get a Python number from a tensor containing a
single value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor(2.5000)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">2.5</span>
</pre></div>
</div>
<p>A tensor can be created with <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad=True</span></code> so that
<a class="reference internal" href="autograd.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> records operations on them for automatic differentiation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[ 2.0000, -2.0000],</span>
<span class="go">        [ 2.0000,  2.0000]])</span>
</pre></div>
</div>
<p>Each tensor has an associated <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Storage</span></code>, which holds its data.
The tensor class provides multi-dimensional, <a class="reference external" href="https://en.wikipedia.org/wiki/Stride_of_an_array">strided</a>
view of a storage and defines numeric operations on it.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For more information on the <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, and
<a class="reference internal" href="tensor_attributes.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a> attributes of a <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, see
<a class="reference internal" href="tensor_attributes.html#tensor-attributes-doc"><span class="std std-ref">Tensor Attributes</span></a>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Methods which mutate a tensor are marked with an underscore suffix.
For example, <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.FloatTensor.abs_()</span></code> computes the absolute value
in-place and returns the modified tensor, while <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.FloatTensor.abs()</span></code>
computes the result in a new tensor.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To change an existing tensor’s <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> and/or <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, consider using
<a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code></a> method on the tensor.</p>
</div>
<dl class="class">
<dt id="torch.Tensor">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><a class="headerlink" href="#torch.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>There are a few main ways to create a tensor, depending on your use case.</p>
<ul class="simple">
<li>To create a tensor with pre-existing data, use <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a>.</li>
<li>To create a tensor with specific size, use <code class="docutils literal notranslate"><span class="pre">torch.*</span></code> tensor creation
ops (see <a class="reference internal" href="torch.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a>).</li>
<li>To create a tensor with the same size (and similar types) as another tensor,
use <code class="docutils literal notranslate"><span class="pre">torch.*_like</span></code> tensor creation ops
(see <a class="reference internal" href="torch.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a>).</li>
<li>To create a tensor with similar type but different size as another tensor,
use <code class="docutils literal notranslate"><span class="pre">tensor.new_*</span></code> creation ops.</li>
</ul>
<dl class="method">
<dt id="torch.Tensor.new_tensor">
<code class="descname">new_tensor</code><span class="sig-paren">(</span><em>data</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> as the tensor data.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><a class="reference internal" href="#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="docutils literal notranslate"><span class="pre">data</span></code> and want to avoid a copy, use <a class="reference internal" href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code></a>
or <a class="reference internal" href="autograd.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.detach()</span></code></a>.
If you have a numpy array and want to avoid a copy, use
<a class="reference internal" href="torch.html#torch.from_numpy" title="torch.from_numpy"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.from_numpy()</span></code></a>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<em>array_like</em>) – The returned Tensor copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</li>
<li><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</li>
<li><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="go">tensor([[ 0,  1],</span>
<span class="go">        [ 2,  3]], dtype=torch.int8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_full">
<code class="descname">new_full</code><span class="sig-paren">(</span><em>size</em>, <em>fill_value</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_full" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>fill_value</strong> (<em>scalar</em>) – the number to fill the output tensor with.</li>
<li><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</li>
<li><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">3.141592</span><span class="p">)</span>
<span class="go">tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_empty">
<code class="descname">new_empty</code><span class="sig-paren">(</span><em>size</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with uninitialized data.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</li>
<li><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],</span>
<span class="go">        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_ones">
<code class="descname">new_ones</code><span class="sig-paren">(</span><em>size</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">1</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<em>int...</em>) – a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the
shape of the output tensor.</li>
<li><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</li>
<li><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 1,  1,  1],</span>
<span class="go">        [ 1,  1,  1]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_zeros">
<code class="descname">new_zeros</code><span class="sig-paren">(</span><em>size</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">0</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<em>int...</em>) – a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the
shape of the output tensor.</li>
<li><strong>dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</li>
<li><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.abs">
<code class="descname">abs</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.abs_">
<code class="descname">abs_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.abs_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.abs" title="torch.Tensor.abs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos">
<code class="descname">acos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos_">
<code class="descname">acos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.acos_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.acos" title="torch.Tensor.acos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.add" title="Permalink to this definition">¶</a></dt>
<dd><p>add(value=1, other) -&gt; Tensor</p>
<p>See <a class="reference internal" href="torch.html#torch.add" title="torch.add"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add_">
<code class="descname">add_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.add_" title="Permalink to this definition">¶</a></dt>
<dd><p>add_(value=1, other) -&gt; Tensor</p>
<p>In-place version of <a class="reference internal" href="#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm">
<code class="descname">addbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm_">
<code class="descname">addbmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addbmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv">
<code class="descname">addcdiv</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv_">
<code class="descname">addcdiv_</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul">
<code class="descname">addcmul</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcmul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul_">
<code class="descname">addcmul_</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcmul_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm">
<code class="descname">addmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm_">
<code class="descname">addmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv">
<code class="descname">addmv</code><span class="sig-paren">(</span><em>beta=1</em>, <em>tensor</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addmv" title="torch.addmv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv_">
<code class="descname">addmv_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>tensor</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmv" title="torch.Tensor.addmv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr">
<code class="descname">addr</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.addr" title="torch.addr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr_">
<code class="descname">addr_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addr_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addr" title="torch.Tensor.addr"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.apply_">
<code class="descname">apply_</code><span class="sig-paren">(</span><em>callable</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.apply_" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the function <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> to each element in the tensor, replacing
each element with the value returned by <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function only works with CPU tensors and should not be used in code
sections that require high performance.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.argmax">
<code class="descname">argmax</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.argmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.argmax" title="torch.argmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmax()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.argmin">
<code class="descname">argmin</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.argmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.argmin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.argmin" title="torch.argmin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin">
<code class="descname">asin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin_">
<code class="descname">asin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.asin_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan">
<code class="descname">atan</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2">
<code class="descname">atan2</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2_">
<code class="descname">atan2_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan2_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan2" title="torch.Tensor.atan2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan_">
<code class="descname">atan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan" title="torch.Tensor.atan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm">
<code class="descname">baddbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm_">
<code class="descname">baddbmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli">
<code class="descname">bernoulli</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli_">
<code class="descname">bernoulli_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bernoulli_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.bernoulli" title="torch.Tensor.bernoulli"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bmm">
<code class="descname">bmm</code><span class="sig-paren">(</span><em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.bmm" title="torch.bmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.byte">
<code class="descname">byte</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.byte" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.byte()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.uint8)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.btrifact">
<code class="descname">btrifact</code><span class="sig-paren">(</span><em>info=None</em>, <em>pivot=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.btrifact"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.btrifact" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.btrifact" title="torch.btrifact"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.btrifact()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.btrifact_with_info">
<code class="descname">btrifact_with_info</code><span class="sig-paren">(</span><em>pivot=True) -&gt; (Tensor</em>, <em>Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.btrifact_with_info" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.btrifact_with_info" title="torch.btrifact_with_info"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.btrifact_with_info()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.btrisolve">
<code class="descname">btrisolve</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.btrisolve" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.cauchy_">
<code class="descname">cauchy_</code><span class="sig-paren">(</span><em>median=0</em>, <em>sigma=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cauchy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - median)^2 + \sigma^2}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil">
<code class="descname">ceil</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil_">
<code class="descname">ceil_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ceil_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ceil" title="torch.Tensor.ceil"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.char">
<code class="descname">char</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.char" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.char()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int8)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.chunk">
<code class="descname">chunk</code><span class="sig-paren">(</span><em>chunks</em>, <em>dim=0</em><span class="sig-paren">)</span> &#x2192; List of Tensors<a class="headerlink" href="#torch.Tensor.chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.chunk" title="torch.chunk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.chunk()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp">
<code class="descname">clamp</code><span class="sig-paren">(</span><em>min</em>, <em>max</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clamp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp_">
<code class="descname">clamp_</code><span class="sig-paren">(</span><em>min</em>, <em>max</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clamp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. The copy has the same size and data
type as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Unlike <cite>copy_()</cite>, this function is recorded in the computation graph. Gradients
propagating to the cloned tensor will propagate to the original tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.contiguous">
<code class="descname">contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a contiguous tensor containing the same data as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous, this function returns the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.copy_">
<code class="descname">copy_</code><span class="sig-paren">(</span><em>src</em>, <em>non_blocking=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor must be <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>
with the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. It may be of a different data type or reside on a
different device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>src</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source tensor to copy from</li>
<li><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> and this copy is between CPU and GPU,
the copy may occur asynchronously with respect to the host. For other
cases, this argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos">
<code class="descname">cos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos_">
<code class="descname">cos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cos_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cos" title="torch.Tensor.cos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh">
<code class="descname">cosh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh_">
<code class="descname">cosh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cosh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cosh" title="torch.Tensor.cosh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.cpu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.cross">
<code class="descname">cross</code><span class="sig-paren">(</span><em>other</em>, <em>dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cross" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cross" title="torch.cross"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cross()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em>, <em>non_blocking=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>) – The destination GPU device.
Defaults to the current CUDA device.</li>
<li><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumprod">
<code class="descname">cumprod</code><span class="sig-paren">(</span><em>dim</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cumprod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumprod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumsum">
<code class="descname">cumsum</code><span class="sig-paren">(</span><em>dim</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cumsum" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumsum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.data_ptr">
<code class="descname">data_ptr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the address of the first element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.det">
<code class="descname">det</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.det" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.det" title="torch.det"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.det()</span></code></a></p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.device">
<code class="descname">device</code><a class="headerlink" href="#torch.Tensor.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.diag">
<code class="descname">diag</code><span class="sig-paren">(</span><em>diagonal=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.diag" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.diag" title="torch.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dim">
<code class="descname">dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dist">
<code class="descname">dist</code><span class="sig-paren">(</span><em>other</em>, <em>p=2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.dist" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.dist" title="torch.dist"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dist()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div">
<code class="descname">div</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.div" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div_">
<code class="descname">div_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.div_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-meth docutils literal notranslate"><span class="pre">div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dot">
<code class="descname">dot</code><span class="sig-paren">(</span><em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.dot" title="torch.dot"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dot()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.double" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.double()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float64)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eig">
<code class="descname">eig</code><span class="sig-paren">(</span><em>eigenvectors=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.eig" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.eig" title="torch.eig"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.element_size">
<code class="descname">element_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.element_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size in bytes of an individual element.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq">
<code class="descname">eq</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.eq" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq_">
<code class="descname">eq_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.eq_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-meth docutils literal notranslate"><span class="pre">eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.equal">
<code class="descname">equal</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.equal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erf">
<code class="descname">erf</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erf_">
<code class="descname">erf_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.erf_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfc">
<code class="descname">erfc</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.erfc" title="Permalink to this definition">¶</a></dt>
<dd><p>erf() -&gt; Tensor</p>
<p>See <a class="reference internal" href="torch.html#torch.erfc" title="torch.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfc_">
<code class="descname">erfc_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.erfc_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfinv">
<code class="descname">erfinv</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfinv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfinv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfinv_">
<code class="descname">erfinv_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.erfinv_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp">
<code class="descname">exp</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp_">
<code class="descname">exp_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.exp" title="torch.Tensor.exp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expm1">
<code class="descname">expm1</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expm1" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expm1_">
<code class="descname">expm1_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expm1_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.expm1" title="torch.Tensor.expm1"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expm1()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expand" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded
to a larger size.</p>
<p>Passing -1 as the size for a dimension means not changing the size of
that dimension.</p>
<p>Tensor can be also expanded to a larger number of dimensions, and the
new ones will be appended at the front. For the new dimensions, the
size cannot be set to -1.</p>
<p>Expanding a tensor does not allocate new memory, but only creates a
new view on the existing tensor where a dimension of size one is
expanded to a larger size by setting the <code class="docutils literal notranslate"><span class="pre">stride</span></code> to 0. Any dimension
of size 1 can be expanded to an arbitrary value without allocating new
memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired expanded size</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>   <span class="c1"># -1 means not changing the size of that dimension</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand_as">
<code class="descname">expand_as</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expand_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Expand this tensor to the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.expand_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.expand(other.size())</span></code>.</p>
<p>Please see <a class="reference internal" href="#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">expand</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exponential_">
<code class="descname">exponential_</code><span class="sig-paren">(</span><em>lambd=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exponential_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the exponential distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \lambda e^{-\lambda x}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fill_">
<code class="descname">fill_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with the specified value.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.flip">
<code class="descname">flip</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.flip" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.float" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.float()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float32)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor">
<code class="descname">floor</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor_">
<code class="descname">floor_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.floor_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.floor" title="torch.Tensor.floor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod">
<code class="descname">fmod</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fmod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.fmod" title="torch.fmod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod_">
<code class="descname">fmod_</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fmod_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.fmod" title="torch.Tensor.fmod"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac">
<code class="descname">frac</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.frac" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac_">
<code class="descname">frac_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.frac_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.frac" title="torch.Tensor.frac"><code class="xref py py-meth docutils literal notranslate"><span class="pre">frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gather">
<code class="descname">gather</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gather" title="torch.gather"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gather()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge">
<code class="descname">ge</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ge" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge_">
<code class="descname">ge_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ge_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gels">
<code class="descname">gels</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gels" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gels" title="torch.gels"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gels()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geometric_">
<code class="descname">geometric_</code><span class="sig-paren">(</span><em>p</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.geometric_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the geometric distribution:</p>
<div class="math notranslate nohighlight">
\[f(X=k) = (1 - p)^{k - 1} p\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geqrf">
<code class="descname">geqrf</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.geqrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ger">
<code class="descname">ger</code><span class="sig-paren">(</span><em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ger" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ger" title="torch.ger"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ger()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gesv">
<code class="descname">gesv</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &#x2192; Tensor, Tensor<a class="headerlink" href="#torch.Tensor.gesv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gesv" title="torch.gesv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gesv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.get_device">
<code class="descname">get_device</code><span class="sig-paren">(</span><em>A) -&gt; Device ordinal (Integer</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.get_device" title="Permalink to this definition">¶</a></dt>
<dd><p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
For CPU tensors, an error is thrown.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>  <span class="c1"># RuntimeError: get_device is not implemented for type torch.FloatTensor</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt">
<code class="descname">gt</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt_">
<code class="descname">gt_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.half" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.half()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float16)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.histc">
<code class="descname">histc</code><span class="sig-paren">(</span><em>bins=100</em>, <em>min=0</em>, <em>max=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.histc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.histc" title="torch.histc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_add_">
<code class="descname">index_add_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_add_" title="Permalink to this definition">¶</a></dt>
<dd><p>Accumulate the elements of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by adding
to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>
and <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> is added to the
<code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) – indices of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> to select from</li>
<li><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to add</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[  2.,   3.,   4.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  8.,   9.,  10.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  5.,   6.,   7.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_copy_">
<code class="descname">index_copy_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by selecting
the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>
and <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> is copied to the
<code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) – indices of <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> to select from</li>
<li><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to copy</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_copy_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.,  3.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 7.,  8.,  9.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 4.,  5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_fill_">
<code class="descname">index_fill_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>val</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with value <code class="xref py py-attr docutils literal notranslate"><span class="pre">val</span></code> by
selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) – indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to fill in</li>
<li><strong>val</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the value to fill with</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Example::</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[-1.,  2., -1.],</span>
<span class="go">        [-1.,  5., -1.],</span>
<span class="go">        [-1.,  8., -1.]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_put_">
<code class="descname">index_put_</code><span class="sig-paren">(</span><em>indices</em>, <em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_put_" title="Permalink to this definition">¶</a></dt>
<dd><p>Puts values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> into the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> using
the indices specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code> (which is a tuple of Tensors). The
expression <code class="docutils literal notranslate"><span class="pre">tensor.index_put_(indices,</span> <span class="pre">value)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">tensor[indices]</span> <span class="pre">=</span> <span class="pre">value</span></code>. Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>indices</strong> (<em>tuple of LongTensor</em>) – tensors used to index into <cite>self</cite>.</li>
<li><strong>value</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor of same dtype as <cite>self</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_select">
<code class="descname">index_select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_select" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.index_select" title="torch.index_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.index_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.int">
<code class="descname">int</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.int" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.int()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int32)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.inverse">
<code class="descname">inverse</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.inverse" title="torch.inverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_contiguous">
<code class="descname">is_contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous in memory in C order.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.is_cuda">
<code class="descname">is_cuda</code><a class="headerlink" href="#torch.Tensor.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_pinned">
<code class="descname">is_pinned</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.is_pinned"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if this tensor resides in pinned memory</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_set_to">
<code class="descname">is_set_to</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_set_to" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if this object refers to the same <code class="docutils literal notranslate"><span class="pre">THTensor</span></code> object from the
Torch C API as the given tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_signed">
<code class="descname">is_signed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.is_signed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.item">
<code class="descname">item</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; number<a class="headerlink" href="#torch.Tensor.item" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of this tensor as a standard Python number. This only works
for tensors with one element. For other cases, see <a class="reference internal" href="#torch.Tensor.tolist" title="torch.Tensor.tolist"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tolist()</span></code></a>.</p>
<p>This operation is not differentiable.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.kthvalue">
<code class="descname">kthvalue</code><span class="sig-paren">(</span><em>k</em>, <em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.kthvalue" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kthvalue()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le">
<code class="descname">le</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.le" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le_">
<code class="descname">le_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.le_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-meth docutils literal notranslate"><span class="pre">le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp">
<code class="descname">lerp</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lerp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lerp" title="torch.lerp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp_">
<code class="descname">lerp_</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lerp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lerp" title="torch.Tensor.lerp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log">
<code class="descname">log</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_">
<code class="descname">log_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log" title="torch.Tensor.log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logdet">
<code class="descname">logdet</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logdet" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.logdet" title="torch.logdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logdet()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log10">
<code class="descname">log10</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log10" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log10" title="torch.log10"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log10_">
<code class="descname">log10_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log10_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log10" title="torch.Tensor.log10"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log10()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p">
<code class="descname">log1p</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p_">
<code class="descname">log1p_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log1p_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log2">
<code class="descname">log2</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log2" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.log2" title="torch.log2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log2_">
<code class="descname">log2_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log2_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log2" title="torch.Tensor.log2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_normal_">
<code class="descname">log_normal_</code><span class="sig-paren">(</span><em>mean=1</em>, <em>std=2</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.log_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers samples from the log-normal distribution
parameterized by the given mean (µ) and standard deviation (σ).
Note that <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">stdv</span></code> are the mean and standard deviation of
the underlying normal distribution, and not of the returned distribution:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\ e^{-\dfrac{(\ln x - \mu)^2}{2\sigma^2}}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logsumexp">
<code class="descname">logsumexp</code><span class="sig-paren">(</span><em>dim</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logsumexp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.logsumexp" title="torch.logsumexp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logsumexp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.long">
<code class="descname">long</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.long" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.long()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int64)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt">
<code class="descname">lt</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt_">
<code class="descname">lt_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.map_">
<code class="descname">map_</code><span class="sig-paren">(</span><em>tensor</em>, <em>callable</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.map_" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and the given
<a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> and stores the results in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and
the given <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must be <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> should have the signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">number</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_scatter_">
<code class="descname">masked_scatter_</code><span class="sig-paren">(</span><em>mask</em>, <em>source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_scatter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor at positions where
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is one.
The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be <a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a>
with the shape of the underlying tensor. The <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> should have at least
as many elements as the number of ones in <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>mask</strong> (<a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – the binary mask</li>
<li><strong>source</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to copy from</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> operates on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor, not on the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_fill_">
<code class="descname">masked_fill_</code><span class="sig-paren">(</span><em>mask</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is
one. The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be
<a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a> with the shape of the underlying
tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>mask</strong> (<a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – the binary mask</li>
<li><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the value to fill in with</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_select">
<code class="descname">masked_select</code><span class="sig-paren">(</span><em>mask</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.masked_select" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.masked_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.matmul">
<code class="descname">matmul</code><span class="sig-paren">(</span><em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.matmul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.max">
<code class="descname">max</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; Tensor or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.max" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mean">
<code class="descname">mean</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; Tensor or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.median">
<code class="descname">median</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.median" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.median" title="torch.median"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.median()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.min">
<code class="descname">min</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; Tensor or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.min" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mm">
<code class="descname">mm</code><span class="sig-paren">(</span><em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mode">
<code class="descname">mode</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mode" title="torch.mode"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mode()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul">
<code class="descname">mul</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul_">
<code class="descname">mul_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mul_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.multinomial">
<code class="descname">multinomial</code><span class="sig-paren">(</span><em>num_samples</em>, <em>replacement=False</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mv">
<code class="descname">mv</code><span class="sig-paren">(</span><em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.narrow">
<code class="descname">narrow</code><span class="sig-paren">(</span><em>dimension</em>, <em>start</em>, <em>length</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.narrow" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor that is a narrowed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. The
dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> is narrowed from <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span> <span class="pre">+</span> <span class="pre">length</span></code>. The
returned tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor share the same underlying storage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dimension</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension along which to narrow</li>
<li><strong>start</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the starting dimension</li>
<li><strong>length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the distance to the ending dimension</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 2,  3],</span>
<span class="go">        [ 5,  6],</span>
<span class="go">        [ 8,  9]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ndimension">
<code class="descname">ndimension</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.ndimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne">
<code class="descname">ne</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ne" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne_">
<code class="descname">ne_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ne_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg">
<code class="descname">neg</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.neg" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg_">
<code class="descname">neg_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.neg_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nelement">
<code class="descname">nelement</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.nelement" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nonzero">
<code class="descname">nonzero</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.Tensor.nonzero" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nonzero()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.norm">
<code class="descname">norm</code><span class="sig-paren">(</span><em>p=2</em>, <em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.norm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.norm" title="torch.norm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.norm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.normal_">
<code class="descname">normal_</code><span class="sig-paren">(</span><em>mean=0</em>, <em>std=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements samples from the normal distribution
parameterized by <a class="reference internal" href="torch.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="torch.html#torch.std" title="torch.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numel">
<code class="descname">numel</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.numel" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.numel" title="torch.numel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numpy">
<code class="descname">numpy</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="headerlink" href="#torch.Tensor.numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor as a NumPy <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>. This tensor and the
returned <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code> share the same underlying storage. Changes to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will be reflected in the <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code> and vice versa.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.orgqr">
<code class="descname">orgqr</code><span class="sig-paren">(</span><em>input2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.orgqr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.orgqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ormqr">
<code class="descname">ormqr</code><span class="sig-paren">(</span><em>input2</em>, <em>input3</em>, <em>left=True</em>, <em>transpose=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ormqr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ormqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.permute">
<code class="descname">permute</code><span class="sig-paren">(</span><em>*dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.permute" title="Permalink to this definition">¶</a></dt>
<dd><p>Permute the dimensions of this tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*dims</strong> (<em>int...</em>) – The desired ordering of dimensions</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([5, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pin_memory">
<code class="descname">pin_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.pinverse">
<code class="descname">pinverse</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pinverse" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.pinverse" title="torch.pinverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pinverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potrf">
<code class="descname">potrf</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.potrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.potrf" title="torch.potrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.potrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potri">
<code class="descname">potri</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.potri" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.potri" title="torch.potri"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.potri()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potrs">
<code class="descname">potrs</code><span class="sig-paren">(</span><em>input2</em>, <em>upper=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.potrs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.potrs" title="torch.potrs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.potrs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow">
<code class="descname">pow</code><span class="sig-paren">(</span><em>exponent</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.pow" title="torch.pow"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow_">
<code class="descname">pow_</code><span class="sig-paren">(</span><em>exponent</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pow_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.prod">
<code class="descname">prod</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.prod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.prod" title="torch.prod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.prod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pstrf">
<code class="descname">pstrf</code><span class="sig-paren">(</span><em>upper=True</em>, <em>tol=-1) -&gt; (Tensor</em>, <em>IntTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pstrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.pstrf" title="torch.pstrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pstrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.put_">
<code class="descname">put_</code><span class="sig-paren">(</span><em>indices</em>, <em>tensor</em>, <em>accumulate=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.put_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements from <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the positions specified by
indices. For the purpose of indexing, the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is treated as if
it were a 1-D tensor.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if indices
contain duplicate elements.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>indices</strong> (<em>LongTensor</em>) – the indices into self</li>
<li><strong>tensor</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to copy from</li>
<li><strong>accumulate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to accumulate into self</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="go">                        [6, 7, 8]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">put_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]))</span>
<span class="go">tensor([[  4,   9,   5],</span>
<span class="go">        [ 10,   7,   8]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.qr">
<code class="descname">qr</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.qr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.qr" title="torch.qr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.qr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.random_">
<code class="descname">random_</code><span class="sig-paren">(</span><em>from=0</em>, <em>to=None</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.random_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the discrete uniform
distribution over <code class="docutils literal notranslate"><span class="pre">[from,</span> <span class="pre">to</span> <span class="pre">-</span> <span class="pre">1]</span></code>. If not specified, the values are usually
only bounded by <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s data type. However, for floating point
types, if unspecified, range will be <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^mantissa]</span></code> to ensure that every
value is representable. For example, <cite>torch.tensor(1, dtype=torch.double).random_()</cite>
will be uniform in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^53]</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal">
<code class="descname">reciprocal</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal_">
<code class="descname">reciprocal_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder">
<code class="descname">remainder</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.remainder" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.remainder" title="torch.remainder"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder_">
<code class="descname">remainder_</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.remainder_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.remainder" title="torch.Tensor.remainder"><code class="xref py py-meth docutils literal notranslate"><span class="pre">remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm">
<code class="descname">renorm</code><span class="sig-paren">(</span><em>p</em>, <em>dim</em>, <em>maxnorm</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.renorm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.renorm" title="torch.renorm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm_">
<code class="descname">renorm_</code><span class="sig-paren">(</span><em>p</em>, <em>dim</em>, <em>maxnorm</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.renorm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.renorm" title="torch.Tensor.renorm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.repeat">
<code class="descname">repeat</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeats this tensor along the specified dimensions.</p>
<p>Unlike <a class="reference internal" href="#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a>, this function copies the tensor’s data.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat()</span></code> behaves differently from
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html">numpy.repeat</a>,
but is more similar to
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html">numpy.tile</a>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – The number of times to repeat this tensor along each
dimension</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.requires_grad_">
<code class="descname">requires_grad_</code><span class="sig-paren">(</span><em>requires_grad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on this tensor: sets this tensor’s
<code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attribute in-place. Returns this tensor.</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">require_grad_()</span></code>’s main use case is to tell autograd to begin recording
operations on a Tensor <code class="docutils literal notranslate"><span class="pre">tensor</span></code>. If <code class="docutils literal notranslate"><span class="pre">tensor</span></code> has <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>
(because it was obtained through a DataLoader, or required preprocessing or
initialization), <code class="docutils literal notranslate"><span class="pre">tensor.requires_grad_()</span></code> makes it so that autograd will
begin to record operations on <code class="docutils literal notranslate"><span class="pre">tensor</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If autograd should record operations on this tensor.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Let&#39;s say we want to preprocess some saved weights and use</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the result as new weights.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">saved_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">loaded_weights</span><span class="p">)</span>  <span class="c1"># some function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span>
<span class="go">tensor([-0.5503,  0.4926, -2.1158, -0.8303])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now, start to record operations done to weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([-1.1007,  0.9853, -4.2316, -1.6606])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reshape">
<code class="descname">reshape</code><span class="sig-paren">(</span><em>*shape</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>,
but with the specified shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>shape</strong> (<em>tuple of python:ints</em><em> or </em><em>int...</em>) – the desired shape</td>
</tr>
</tbody>
</table>
<p>See <a class="reference internal" href="torch.html#torch.reshape" title="torch.reshape"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reshape()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reshape_as">
<code class="descname">reshape_as</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reshape_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor as the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.reshape_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.reshape(other.sizes())</span></code>.</p>
<p>Please see <a class="reference internal" href="#torch.Tensor.reshape" title="torch.Tensor.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">reshape</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same shape
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_">
<code class="descname">resize_</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.resize_" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size. If the number of elements is
larger than the current storage size, then the underlying storage is resized
to fit the new number of elements. If the number of elements is smaller, the
underlying storage is not changed. Existing elements are preserved but any new
memory is uninitialized.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired size</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2],</span>
<span class="go">        [ 3,  4]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_as_">
<code class="descname">resize_as_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.resize_as_" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to be the same size as the specified
<a class="reference internal" href="torch.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a>. This is equivalent to <code class="docutils literal notranslate"><span class="pre">self.resize_(tensor.size())</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round">
<code class="descname">round</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.round" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round_">
<code class="descname">round_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.round_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.round" title="torch.Tensor.round"><code class="xref py py-meth docutils literal notranslate"><span class="pre">round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt">
<code class="descname">rsqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt_">
<code class="descname">rsqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_">
<code class="descname">scatter_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>src</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Writes all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, its output
index is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by
the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p>This is the reverse operation of the manner described in <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> (if it is a Tensor) should have same
number of dimensions. It is also required that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code>
for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all
dimensions <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.</p>
<p>Moreover, as for <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>, the values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> must be
between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">self.size(dim)</span> <span class="pre">-</span> <span class="pre">1</span></code> inclusive, and all values in a row
along the specified dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> must be unique.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter</li>
<li><strong>src</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the source element(s) to scatter</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],</span>
<span class="go">        [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],</span>
<span class="go">        [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],</span>
<span class="go">        [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span> <span class="mf">1.23</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span>
<span class="go">tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000,  1.2300]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_add_">
<code class="descname">scatter_add_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_add_" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in a similar fashion as
<a class="reference internal" href="#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_()</span></code></a>. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, it is added to
an index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>
for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> should have same number of
dimensions. It is also required that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">other.size(d)</span></code> for all
dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions
<code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.</p>
<p>Moreover, as for <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>, the values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> must be
between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">self.size(dim)</span> <span class="pre">-</span> <span class="pre">1</span></code> inclusive, and all values in a row along
the specified dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> must be unique.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter and add</li>
<li><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source elements to scatter and add</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],</span>
<span class="go">        [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],</span>
<span class="go">        [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],</span>
<span class="go">        [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.select">
<code class="descname">select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Slices the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor along the selected dimension at the given index.
This function returns a tensor with the given dimension removed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to slice</li>
<li><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the index to select with</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.Tensor.select" title="torch.Tensor.select"><code class="xref py py-meth docutils literal notranslate"><span class="pre">select()</span></code></a> is equivalent to slicing. For example,
<code class="docutils literal notranslate"><span class="pre">tensor.select(0,</span> <span class="pre">index)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">tensor[index]</span></code> and
<code class="docutils literal notranslate"><span class="pre">tensor.select(2,</span> <span class="pre">index)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">tensor[:,:,index]</span></code>.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.set_">
<code class="descname">set_</code><span class="sig-paren">(</span><em>source=None</em>, <em>storage_offset=0</em>, <em>size=None</em>, <em>stride=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.set_" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the underlying storage, size, and strides. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a tensor,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will share the same storage and have the same size and
strides as <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code>. Changes to elements in one tensor will be reflected
in the other.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Storage</span></code>, the method sets the underlying
storage, offset, size, and stride.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>source</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) – the tensor or storage to use</li>
<li><strong>storage_offset</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the offset in the storage</li>
<li><strong>size</strong> (<em>torch.Size</em><em>, </em><em>optional</em>) – the desired size. Defaults to the size of the source.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the desired stride. Defaults to C-contiguous strides.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.share_memory_">
<code class="descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.share_memory_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the underlying storage to shared memory.</p>
<p>This is a no-op if the underlying storage is already in shared memory
and for CUDA tensors. Tensors in shared memory cannot be resized.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.short">
<code class="descname">short</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.short" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.short()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int16)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid">
<code class="descname">sigmoid</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid_">
<code class="descname">sigmoid_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign">
<code class="descname">sign</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sign" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sign" title="torch.sign"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign_">
<code class="descname">sign_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sign_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sign" title="torch.Tensor.sign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin">
<code class="descname">sin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin_">
<code class="descname">sin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sin_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sin" title="torch.Tensor.sin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh">
<code class="descname">sinh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh_">
<code class="descname">sinh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sinh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sinh" title="torch.Tensor.sinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.size">
<code class="descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Size<a class="headerlink" href="#torch.Tensor.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. The returned value is a subclass of
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 4, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.slogdet">
<code class="descname">slogdet</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.slogdet" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.slogdet" title="torch.slogdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slogdet()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sort">
<code class="descname">sort</code><span class="sig-paren">(</span><em>dim=None</em>, <em>descending=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.split">
<code class="descname">split</code><span class="sig-paren">(</span><em>split_size</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.split"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.split" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.split" title="torch.split"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt">
<code class="descname">sqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt_">
<code class="descname">sqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sqrt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze">
<code class="descname">squeeze</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze_">
<code class="descname">squeeze_</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.squeeze_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.std">
<code class="descname">std</code><span class="sig-paren">(</span><em>dim=None</em>, <em>unbiased=True</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.std" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.std" title="torch.std"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.std()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage">
<code class="descname">storage</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Storage<a class="headerlink" href="#torch.Tensor.storage" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the underlying storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage_offset">
<code class="descname">storage_offset</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.storage_offset" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s offset in the underlying storage in terms of
number of storage elements (not bytes).</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage_type">
<code class="descname">storage_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.storage_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.stride">
<code class="descname">stride</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; tuple or int<a class="headerlink" href="#torch.Tensor.stride" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the stride of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
<p>Stride is the jump necessary to go from one element to the next one in the
specified dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>. A tuple of all strides is returned when no
argument is passed in. Otherwise, an integer value is returned as the stride in
the particular dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the desired dimension in which stride is required</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(5, 1)</span>
<span class="go">&gt;&gt;&gt;x.stride(0)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub">
<code class="descname">sub</code><span class="sig-paren">(</span><em>value</em>, <em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Subtracts a scalar or tensor from <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If both <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> are specified, each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is scaled by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> before being used.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is a tensor, the shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<a class="reference internal" href="notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">broadcastable</span></a> with the shape of the underlying
tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub_">
<code class="descname">sub_</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sub_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sub()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sum">
<code class="descname">sum</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.sum" title="torch.sum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.svd">
<code class="descname">svd</code><span class="sig-paren">(</span><em>some=True) -&gt; (Tensor</em>, <em>Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.svd" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.svd" title="torch.svd"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.svd()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.symeig">
<code class="descname">symeig</code><span class="sig-paren">(</span><em>eigenvectors=False</em>, <em>upper=True) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.symeig" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.symeig" title="torch.symeig"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.symeig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t">
<code class="descname">t</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.t" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.t" title="torch.t"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t_">
<code class="descname">t_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-meth docutils literal notranslate"><span class="pre">t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.to">
<code class="descname">to</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Tensor dtype and/or device conversion. A <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> are
inferred from the arguments of <code class="docutils literal notranslate"><span class="pre">self.to(*args,</span> <span class="pre">**kwargs)</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the <code class="docutils literal notranslate"><span class="pre">self</span></code> Tensor already
has the correct <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, then <code class="docutils literal notranslate"><span class="pre">self</span></code> is returned.
Otherwise, the returned tensor is a copy of <code class="docutils literal notranslate"><span class="pre">self</span></code> with the desired
<a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>.</p>
</div>
<p>Here are the ways to call <code class="docutils literal notranslate"><span class="pre">to</span></code>:</p>
<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>dtype</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with the specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></p>
</dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>device=None</em>, <em>dtype=None</em>, <em>non_blocking=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with the specified <a class="reference internal" href="#torch.Tensor.device" title="torch.Tensor.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> and (optional)
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> it is inferred to be <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert asynchronously with respect to
the host if possible, e.g., converting a CPU Tensor with pinned memory to a
CUDA Tensor.</p>
</dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>other</em>, <em>non_blocking=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with same <a class="reference internal" href="tensor_attributes.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as
the Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert
asynchronously with respect to the host if possible, e.g., converting a CPU
Tensor with pinned memory to a CUDA Tensor.</p>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Initially dtype=float32, device=cpu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">other</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.take">
<code class="descname">take</code><span class="sig-paren">(</span><em>indices</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.take" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.take" title="torch.take"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan">
<code class="descname">tan</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.tan" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan_">
<code class="descname">tan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tan_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tan" title="torch.Tensor.tan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh">
<code class="descname">tanh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.tanh" title="torch.tanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh_">
<code class="descname">tanh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tanh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tanh" title="torch.Tensor.tanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tolist">
<code class="descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>”
tolist() -&gt; list or number</p>
<p>Returns the tensor as a (nested) list. For scalars, a standard
Python number is returned, just like with <a class="reference internal" href="#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item()</span></code></a>.
Tensors are automatically moved to the CPU first if necessary.</p>
<p>This operation is not differentiable.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[0.012766935862600803, 0.5415473580360413],</span>
<span class="go"> [-0.08909505605697632, 0.7729271650314331]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">0.012766935862600803</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.topk">
<code class="descname">topk</code><span class="sig-paren">(</span><em>k</em>, <em>dim=None</em>, <em>largest=True</em>, <em>sorted=True) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.topk" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trace">
<code class="descname">trace</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trace" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.trace" title="torch.trace"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trace()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose">
<code class="descname">transpose</code><span class="sig-paren">(</span><em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose_">
<code class="descname">transpose_</code><span class="sig-paren">(</span><em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.transpose_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril">
<code class="descname">tril</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tril" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.tril" title="torch.tril"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril_">
<code class="descname">tril_</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tril_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tril" title="torch.Tensor.tril"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu">
<code class="descname">triu</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.triu" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.triu" title="torch.triu"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu_">
<code class="descname">triu_</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.triu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.triu" title="torch.Tensor.triu"><code class="xref py py-meth docutils literal notranslate"><span class="pre">triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trtrs">
<code class="descname">trtrs</code><span class="sig-paren">(</span><em>A</em>, <em>upper=True</em>, <em>transpose=False</em>, <em>unitriangular=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.trtrs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.trtrs" title="torch.trtrs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trtrs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc">
<code class="descname">trunc</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc_">
<code class="descname">trunc_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trunc_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.trunc" title="torch.Tensor.trunc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>dtype=None</em>, <em>non_blocking=False</em>, <em>**kwargs</em><span class="sig-paren">)</span> &#x2192; str or Tensor<a class="headerlink" href="#torch.Tensor.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a><em> or </em><em>string</em>) – The desired type</li>
<li><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, and the source is in pinned memory
and destination is on the GPU or vice versa, the copy is performed
asynchronously with respect to the host. Otherwise, the argument
has no effect.</li>
<li><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">async</span></code> arg is deprecated.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type_as">
<code class="descname">type_as</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.type_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor cast to the type of the given tensor.</p>
<p>This is a no-op if the tensor is already of the correct type. This is
equivalent to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">())</span>
</pre></div>
</div>
<dl class="docutils">
<dt>Params:</dt>
<dd>tensor (Tensor): the tensor which has the desired type</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unfold">
<code class="descname">unfold</code><span class="sig-paren">(</span><em>dim</em>, <em>size</em>, <em>step</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor which contains all slices of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor in the dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>.</p>
<p>Step between two slices is given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code>.</p>
<p>If <cite>sizedim</cite> is the size of dimension dim for <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, the size of
dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> in the returned tensor will be
<cite>(sizedim - size) / step + 1</cite>.</p>
<p>An additional dimension of size size is appended in the returned tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension in which unfolding happens</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of each slice that is unfolded</li>
<li><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the step between each slice</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 2.,  3.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 4.,  5.],</span>
<span class="go">        [ 5.,  6.],</span>
<span class="go">        [ 6.,  7.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.uniform_">
<code class="descname">uniform_</code><span class="sig-paren">(</span><em>from=0</em>, <em>to=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the continuous uniform
distribution:</p>
<div class="math notranslate nohighlight">
\[P(x) = \dfrac{1}{\text{to} - \text{from}}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unique">
<code class="descname">unique</code><span class="sig-paren">(</span><em>sorted=False</em>, <em>return_inverse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/tensor.html#Tensor.unique"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.Tensor.unique" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the unique scalar elements of the tensor as a 1-D tensor.</p>
<p>See <a class="reference internal" href="torch.html#torch.unique" title="torch.unique"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze">
<code class="descname">unsqueeze</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze_">
<code class="descname">unsqueeze_</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unsqueeze_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.var">
<code class="descname">var</code><span class="sig-paren">(</span><em>dim=None</em>, <em>unbiased=True</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.var" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="torch.html#torch.var" title="torch.var"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.var()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view">
<code class="descname">view</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.view" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a
different size.</p>
<p>The returned tensor shares the same data and must have the same number
of elements, but may have a different size. For a tensor to be viewed, the new
view size must be compatible with its original size and stride, i.e., each new
view dimension must either be a subspace of an original dimension, or only span
across original dimensions <span class="math notranslate nohighlight">\(d, d+1, \dots, d+k\)</span> that satisfy the following
contiguity-like condition that <span class="math notranslate nohighlight">\(\forall i = 0, \dots, k-1\)</span>,</p>
<div class="math notranslate nohighlight">
\[stride[i] = stride[i+1] \times size[i+1]\]</div>
<p>Otherwise, <a class="reference internal" href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-func docutils literal notranslate"><span class="pre">contiguous()</span></code></a> needs to be called before the tensor can be
viewed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>args</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired size</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([16])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 8])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view_as">
<code class="descname">view_as</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.view_as" title="Permalink to this definition">¶</a></dt>
<dd><p>View this tensor as the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.view_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.view(other.size())</span></code>.</p>
<p>Please see <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">view</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.zero_">
<code class="descname">zero_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.zero_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with zeros.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.ByteTensor">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">ByteTensor</code><a class="headerlink" href="#torch.ByteTensor" title="Permalink to this definition">¶</a></dt>
<dd><p>The following methods are unique to <a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code></a>.</p>
<dl class="method">
<dt id="torch.ByteTensor.all">
<code class="descname">all</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.ByteTensor.all" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descname">all</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool</dt>
<dd></dd></dl>

<p>Returns True if all elements in the tensor are non-zero, False otherwise.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1  1  0</span>
<span class="go">[torch.ByteTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="go">False</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descname">all</code><span class="sig-paren">(</span><em>dim</em>, <em>keepdim=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns True if all elements in each row of the tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> are non-zero, False otherwise.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="torch.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting
in the output tensor having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</li>
<li><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</li>
<li><strong>out</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1  1</span>
<span class="go"> 1  0</span>
<span class="go"> 0  0</span>
<span class="go"> 1  0</span>
<span class="go">[torch.ByteTensor of size 4x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 0</span>
<span class="go"> 0</span>
<span class="go"> 0</span>
<span class="go">[torch.ByteTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.ByteTensor.any">
<code class="descname">any</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.ByteTensor.any" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descname">any</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool</dt>
<dd></dd></dl>

<p>Returns True if any elements in the tensor are non-zero, False otherwise.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1  1  0</span>
<span class="go">[torch.ByteTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descname">any</code><span class="sig-paren">(</span><em>dim</em>, <em>keepdim=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns True if any elements in each row of the tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> are non-zero, False otherwise.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="torch.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting
in the output tensor having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</li>
<li><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</li>
<li><strong>out</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1  1</span>
<span class="go"> 1  0</span>
<span class="go"> 0  0</span>
<span class="go"> 1  0</span>
<span class="go">[torch.ByteTensor of size 4x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 1</span>
<span class="go"> 0</span>
<span class="go"> 1</span>
<span class="go">[torch.ByteTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tensor_attributes.html" class="btn btn-neutral float-right" title="Tensor Attributes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="torch.html" class="btn btn-neutral" title="torch" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>
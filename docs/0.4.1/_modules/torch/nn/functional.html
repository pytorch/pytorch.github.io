

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.functional &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/functional.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/pytorch_theme.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.4.1 <br/> <a href="https://pytorch.org/docs/versions.html"> version selector &#x25BC</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/autograd.html#requires-grad"><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#in-place-operations-with-autograd">In-place operations with autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/cuda.html#asynchronous-execution">Asynchronous execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#cuda-streams">CUDA streams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#extending-torch-nn">Extending <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/extending.html#adding-a-module">Adding a <code class="docutils literal notranslate"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#writing-custom-c-extensions">Writing custom C++ extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#id1">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory">My model reports “cuda runtime error(2): out of memory”</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/faq.html#my-gpu-memory-isn-t-freed-properly">My GPU memory isn’t freed properly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/faq.html#my-data-loader-workers-return-identical-random-numbers">My data loader workers return identical random numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/faq.html#my-recurrent-network-doesn-t-work-with-data-parallelism">My recurrent network doesn’t work with data parallelism</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/windows.html#building-from-source">Building from source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#include-optional-components">Include optional components</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#speeding-cuda-build-for-windows">Speeding CUDA build for Windows</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#one-key-install-script">One key install script</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/windows.html#extension">Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#cffi-extension">CFFI Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#cpp-extension">Cpp Extension</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/windows.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#package-not-found-in-win-32-channel">Package not found in win-32 channel.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#why-are-there-no-python-2-packages-for-windows">Why are there no Python 2 packages for Windows?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#import-error">Import error</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/windows.html#usage-multiprocessing">Usage (multiprocessing)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#multiprocessing-error-without-if-clause-protection">Multiprocessing error without if-clause protection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#multiprocessing-error-broken-pipe">Multiprocessing error “Broken pipe”</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#multiprocessing-error-driver-shut-down">Multiprocessing error “driver shut down”</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/windows.html#cuda-ipc-operations">CUDA IPC operations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#random-sampling">Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#spectral-ops">Spectral Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tensor_attributes.html#torch-dtype">torch.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tensor_attributes.html#torch-device">torch.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tensor_attributes.html#torch-layout">torch.layout</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#moduledict"><span class="hidden-section">ModuleDict</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#parameterdict"><span class="hidden-section">ParameterDict</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#convolution-layers">Convolution layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#unfold"><span class="hidden-section">Unfold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#fold"><span class="hidden-section">Fold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#pooling-layers">Pooling layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lppool1d"><span class="hidden-section">LPPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#padding-layers">Padding layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#reflectionpad1d"><span class="hidden-section">ReflectionPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#replicationpad1d"><span class="hidden-section">ReplicationPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#constantpad1d"><span class="hidden-section">ConstantPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#constantpad3d"><span class="hidden-section">ConstantPad3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activations-weighted-sum-nonlinearity">Non-linear activations (weighted sum, nonlinearity)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hardshrink"><span class="hidden-section">Hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rrelu"><span class="hidden-section">RReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activations-other">Non-linear activations (other)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivelogsoftmaxwithloss"><span class="hidden-section">AdaptiveLogSoftmaxWithLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#groupnorm"><span class="hidden-section">GroupNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#layernorm"><span class="hidden-section">LayerNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#localresponsenorm"><span class="hidden-section">LocalResponseNorm</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#clip-grad-value"><span class="hidden-section">clip_grad_value_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#parameters-to-vector"><span class="hidden-section">parameters_to_vector</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#vector-to-parameters"><span class="hidden-section">vector_to_parameters</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#spectral-norm"><span class="hidden-section">spectral_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#remove-spectral-norm"><span class="hidden-section">remove_spectral_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad-sequence"><span class="hidden-section">pad_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pack-sequence"><span class="hidden-section">pack_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id20"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id21"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id22"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id23"><span class="hidden-section">unfold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id24"><span class="hidden-section">fold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lp-pool1d"><span class="hidden-section">lp_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id25"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id26"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id27"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id28"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id29"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id30"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id31"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id32"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id33"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id34"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id35"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id36"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id37"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id38"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id39"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id40"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#gumbel-softmax"><span class="hidden-section">gumbel_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id41"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id42"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instance-norm"><span class="hidden-section">instance_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#layer-norm"><span class="hidden-section">layer_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#local-response-norm"><span class="hidden-section">local_response_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id43"><span class="hidden-section">linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id44"><span class="hidden-section">bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id45"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id46"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id47"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#sparse-functions">Sparse functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id48"><span class="hidden-section">embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#embedding-bag"><span class="hidden-section">embedding_bag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#id49">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#id50">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#interpolate"><span class="hidden-section">interpolate</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id51"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dataparallel-functions-multi-gpu-distributed">DataParallel functions (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#data-parallel"><span class="hidden-section">data_parallel</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../optim.html#optimizer-step"><code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../optim.html#optimizer-step-closure"><code class="docutils literal notranslate"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#in-place-operations-on-tensors">In-place operations on Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#variable-deprecated">Variable (deprecated)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#tensor-autograd-functions">Tensor autograd functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#numerical-gradient-checking">Numerical gradient checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#profiler">Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#anomaly-detection">Anomaly detection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#score-function">Score function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#pathwise-derivative">Pathwise derivative</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#exponentialfamily"><span class="hidden-section">ExponentialFamily</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#beta"><span class="hidden-section">Beta</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#binomial"><span class="hidden-section">Binomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#cauchy"><span class="hidden-section">Cauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#chi2"><span class="hidden-section">Chi2</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#dirichlet"><span class="hidden-section">Dirichlet</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#exponential"><span class="hidden-section">Exponential</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#fishersnedecor"><span class="hidden-section">FisherSnedecor</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#gamma"><span class="hidden-section">Gamma</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#geometric"><span class="hidden-section">Geometric</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#gumbel"><span class="hidden-section">Gumbel</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#halfcauchy"><span class="hidden-section">HalfCauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#halfnormal"><span class="hidden-section">HalfNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#independent"><span class="hidden-section">Independent</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#laplace"><span class="hidden-section">Laplace</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#lognormal"><span class="hidden-section">LogNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#multinomial"><span class="hidden-section">Multinomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#multivariatenormal"><span class="hidden-section">MultivariateNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#onehotcategorical"><span class="hidden-section">OneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#pareto"><span class="hidden-section">Pareto</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#poisson"><span class="hidden-section">Poisson</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#relaxedbernoulli"><span class="hidden-section">RelaxedBernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#relaxedonehotcategorical"><span class="hidden-section">RelaxedOneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#studentt"><span class="hidden-section">StudentT</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#transformeddistribution"><span class="hidden-section">TransformedDistribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#uniform"><span class="hidden-section">Uniform</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#module-torch.distributions.kl"><cite>KL Divergence</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#module-torch.distributions.transforms"><cite>Transforms</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#module-torch.distributions.constraints"><cite>Constraints</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributions.html#module-torch.distributions.constraint_registry"><cite>Constraint Registry</cite></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal notranslate"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../multiprocessing.html#file-system-file-system">File system - <code class="docutils literal notranslate"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#collective-functions">Collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#multi-gpu-collective-functions">Multi-GPU collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#launch-utility">Launch utility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx.html#functions">Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/index.html">torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#emnist">EMNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#datasetfolder">DatasetFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/models.html">torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/transforms.html#module-torchvision.transforms.functional">Functional Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PyTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
          <li><a href="../../torch.html">torch</a> &raquo;</li>
        
      <li>torch.nn.functional</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for torch.nn.functional</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Functional interface&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="k">import</span> <span class="n">mul</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="k">import</span> <span class="n">reduce</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="k">import</span> <span class="n">_infer_size</span><span class="p">,</span> <span class="n">_add_docstr</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_functions</span>
<span class="kn">from</span> <span class="nn">.modules</span> <span class="k">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">._functions.padding</span> <span class="k">import</span> <span class="n">ConstantPadNd</span>
<span class="kn">from</span> <span class="nn">._functions</span> <span class="k">import</span> <span class="n">vision</span>
<span class="kn">from</span> <span class="nn">._functions.thnn.fold</span> <span class="k">import</span> <span class="n">Col2Im</span><span class="p">,</span> <span class="n">Im2Col</span>
<span class="kn">from</span> <span class="nn">.modules.utils</span> <span class="k">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span><span class="p">,</span> <span class="n">_list_with_default</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">grad</span>


<span class="k">class</span> <span class="nc">_Reduction</span><span class="p">:</span>
    <span class="c1"># NB: Keep this class in sync with enums in THNN/Reduction.h</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;elementwise_mean&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">2</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">reduction</span> <span class="o">+</span> <span class="s2">&quot; is not a valid value for reduction&quot;</span><span class="p">)</span>

    <span class="c1"># In order to support previous versions, accept boolean size_average and reduce</span>
    <span class="c1"># and convert them into the new constants for now</span>

    <span class="c1"># We use these functions in torch/legacy as well, in which case we&#39;ll silence the warning</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">emit_warning</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">warning</span> <span class="o">=</span> <span class="s2">&quot;size_average and reduce args will be deprecated, please use reduction=&#39;</span><span class="si">{}</span><span class="s2">&#39; instead.&quot;</span>

        <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">size_average</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reduce</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">size_average</span> <span class="ow">and</span> <span class="n">reduce</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="s1">&#39;elementwise_mean&#39;</span>
        <span class="k">elif</span> <span class="n">reduce</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="s1">&#39;sum&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span>
        <span class="k">if</span> <span class="n">emit_warning</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">warning</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ret</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">emit_warning</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">emit_warning</span><span class="p">))</span>


<span class="n">conv1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 1D convolution over an input signal composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">See :class:`~torch.nn.Conv1d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`minibatch \times in\_channels \times iW`</span>
<span class="s2">    weight: filters of shape :math:`out\_channels \times \frac{in\_channels}</span><span class="si">{groups}</span><span class="s2"> \times kW`</span>
<span class="s2">    bias: optional bias of shape (:math:`out\_channels`). Default: ``None``</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or</span>
<span class="s2">      a one-element tuple `(sW,)`. Default: 1</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a one-element tuple `(padW,)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a one-element tuple `(dW,)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`in\_channels` should be divisible by</span>
<span class="s2">      the number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; filters = torch.randn(33, 16, 3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50)</span>
<span class="s2">    &gt;&gt;&gt; F.conv1d(inputs, filters)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 2D convolution over an input image composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">See :class:`~torch.nn.Conv2d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape (:math:`minibatch \times in\_channels \times iH \times iW`)</span>
<span class="s2">    weight: filters of shape (:math:`out\_channels \times \frac{in\_channels}</span><span class="si">{groups}</span><span class="s2"> \times kH \times kW`)</span>
<span class="s2">    bias: optional bias tensor of shape (:math:`out\_channels`). Default: ``None``</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple `(sH, sW)`. Default: 1</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padH, padW)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dH, dW)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`in\_channels` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">    &gt;&gt;&gt; filters = torch.randn(8,4,3,3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(1,4,5,5)</span>
<span class="s2">    &gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 3D convolution over an input image composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">See :class:`~torch.nn.Conv3d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape (:math:`minibatch \times in\_channels \times iT \times iH \times iW`)</span>
<span class="s2">    weight: filters of shape (:math:`out\_channels \times \frac{in\_channels}</span><span class="si">{groups}</span><span class="s2"> \times kT \times kH \times kW`)</span>
<span class="s2">    bias: optional bias tensor of shape (:math:`out\_channels`). Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple `(sT, sH, sW)`. Default: 1</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padT, padH, padW)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dT, dH, dW)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`in\_channels` should be divisible by</span>
<span class="s2">      the number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; filters = torch.randn(33, 16, 3, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)</span>
<span class="s2">    &gt;&gt;&gt; F.conv3d(inputs, filters)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv_transpose1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 1D transposed convolution operator over an input signal</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose1d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape (:math:`minibatch \times in\_channels \times iW`)</span>
<span class="s2">    weight: filters of shape (:math:`in\_channels \times \frac{out\_channels}</span><span class="si">{groups}</span><span class="s2"> \times kW`)</span>
<span class="s2">    bias: optional bias of shape (:math:`out\_channels`). Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sW,)``. Default: 1</span>
<span class="s2">    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padW,)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`in\_channels` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple ``(dW,)``. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(16, 33, 5)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose1d(inputs, weights)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv_transpose2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 2D transposed convolution operator over an input image</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose2d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape (:math:`minibatch \times in\_channels \times iH \times iW`)</span>
<span class="s2">    weight: filters of shape (:math:`in\_channels \times \frac{out\_channels}</span><span class="si">{groups}</span><span class="s2"> \times kH \times kW`)</span>
<span class="s2">    bias: optional bias of shape (:math:`out\_channels`). Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sH, sW)``. Default: 1</span>
<span class="s2">    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padH, padW)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.</span>
<span class="s2">      Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`in\_channels` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple ``(dH, dW)``. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(4, 8, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose2d(inputs, weights, padding=1)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv_transpose3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 3D transposed convolution operator over an input image</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose3d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape (:math:`minibatch \times in\_channels \times iT \times iH \times iW`)</span>
<span class="s2">    weight: filters of shape (:math:`in\_channels \times \frac{out\_channels}</span><span class="si">{groups}</span><span class="s2"> \times kT \times kH \times kW`)</span>
<span class="s2">    bias: optional bias of shape (:math:`out\_channels`). Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sT, sH, sW)``. Default: 1</span>
<span class="s2">    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padT, padH, padW)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple</span>
<span class="s2">      ``(out_padT, out_padH, out_padW)``. Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`in\_channels` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dT, dH, dW)`. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(16, 33, 3, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose3d(inputs, weights)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">conv_tbc</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1-dimensional sequence convolution over an input sequence.</span>
<span class="sd">    Input and output dimensions are (Time, Batch, Channels) - hence TBC.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (:math:`\text{sequence length} \times batch \times in\_channels`)</span>
<span class="sd">        weight: filter of shape (:math:`\text{kernel width} \times in\_channels \times out\_channels`)</span>
<span class="sd">        bias: bias of shape (:math:`out\_channels`)</span>
<span class="sd">        pad: number of timesteps to pad</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">conv_tbc</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>


<span class="c1"># Pooling</span>
<span class="n">avg_pool1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Tensor</span>

<span class="s2">Applies a 1D average pooling over an input signal composed of several</span>
<span class="s2">input planes.</span>

<span class="s2">See :class:`~torch.nn.AvgPool1d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape (:math:`minibatch \times in\_channels \times iW`)</span>
<span class="s2">    kernel_size: the size of the window. Can be a single number or a</span>
<span class="s2">      tuple `(kW,)`</span>
<span class="s2">    stride: the stride of the window. Can be a single number or a tuple</span>
<span class="s2">      `(sW,)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padW,)`. Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` to compute the</span>
<span class="s2">        output shape. Default: ``False``</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation. Default: ``True``</span>

<span class="s2">Example::</span>
<span class="s2">    &gt;&gt;&gt; # pool of square window of size=3, stride=2</span>
<span class="s2">    &gt;&gt;&gt; input = torch.tensor([[[1,2,3,4,5,6,7]]])</span>
<span class="s2">    &gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)</span>
<span class="s2">    tensor([[[ 2.,  4.,  6.]]])</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="n">avg_pool2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Tensor</span>

<span class="s2">Applies 2D average-pooling operation in :math:`kH \times kW` regions by step size</span>
<span class="s2">:math:`sH \times sW` steps. The number of output features is equal to the number of</span>
<span class="s2">input planes.</span>

<span class="s2">See :class:`~torch.nn.AvgPool2d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor (:math:`minibatch \times in\_channels \times iH \times iW`)</span>
<span class="s2">    kernel_size: size of the pooling region. Can be a single number or a</span>
<span class="s2">      tuple (:math:`kH \times kW`)</span>
<span class="s2">    stride: stride of the pooling operation. Can be a single number or a</span>
<span class="s2">      tuple `(sH, sW)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padH, padW)`. Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="s2">        to compute the output shape. Default: ``False``</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation. Default: ``True``</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">avg_pool3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Tensor</span>

<span class="s2">Applies 3D average-pooling operation in :math:`kT \times kH \times kW` regions by step</span>
<span class="s2">size :math:`sT \times sH \times sW` steps. The number of output features is equal to</span>
<span class="s2">:math:`\lfloor\frac{\text{input planes}}</span><span class="si">{sT}</span><span class="s2">\rfloor`.</span>

<span class="s2">See :class:`~torch.nn.AvgPool3d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor (:math:`minibatch \times in\_channels \times iT \times iH \times iW`)</span>
<span class="s2">    kernel_size: size of the pooling region. Can be a single number or a</span>
<span class="s2">      tuple (:math:`kT \times kH \times kW`)</span>
<span class="s2">    stride: stride of the pooling operation. Can be a single number or a</span>
<span class="s2">      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padT, padH, padW)`, Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="s2">        to compute the output shape</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fractional_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies 2D fractional max pooling over an input signal composed of several input planes.</span>

<span class="sd">    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham</span>

<span class="sd">    The max-pooling operation is applied in :math:`kH \times kW` regions by a stochastic</span>
<span class="sd">    step size determined by the target output size.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size: the size of the window to take a max over.</span>
<span class="sd">                     Can be a single number :math:`k` (for a square kernel of :math:`k \times k`)</span>
<span class="sd">                     or a tuple (:math:`kH \times kW`)</span>
<span class="sd">        output_size: the target output size of the image of the form :math:`oH \times oW`.</span>
<span class="sd">                     Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \times oH`</span>
<span class="sd">        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.</span>
<span class="sd">                      This has to be a number or tuple in the range (0, 1)</span>
<span class="sd">        return_indices: if ``True``, will return the indices along with the outputs.</span>
<span class="sd">                        Useful to pass to `max_unpool2d`.</span>

<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12</span>
<span class="sd">        &gt;&gt;&gt; F.fractional_max_pool2d(input, 3, output_size=(13, 12))</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window and target output size being half of input image size</span>
<span class="sd">        &gt;&gt;&gt; F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))</span>

<span class="sd">    .. _Fractional MaxPooling:</span>
<span class="sd">        http://arxiv.org/abs/1412.6071</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;fractional_max_pool2d requires specifying either &quot;</span>
                         <span class="s2">&quot;an output_size, or a output_ratio&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_ratio</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">)</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                       <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">if</span> <span class="n">_random_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_random_samples</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">fractional_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<div class="viewcode-block" id="max_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="max_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="max_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<span class="k">def</span> <span class="nf">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">default_size</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">default_size</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_size</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span>
                            <span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_size</span>

    <span class="n">output_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;output_size should be a sequence containing &quot;</span>
                         <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements, but it has a length of &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">min_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="n">max_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">min_size</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;invalid output_size &quot;</span><span class="si">{}</span><span class="s1">&quot; (dim </span><span class="si">{}</span><span class="s1"> must be between </span><span class="si">{}</span><span class="s1"> and </span><span class="si">{}</span><span class="s1">)&#39;</span>
                <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">min_size</span><span class="p">,</span> <span class="n">max_size</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">output_size</span>


<div class="viewcode-block" id="max_unpool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool1d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool1d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span> <span class="ow">or</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">output_size</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool2d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool2d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span> <span class="ow">or</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool3d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool3d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span> <span class="ow">or</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.lp_pool2d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D power-average pooling over an input signal composed of</span>
<span class="sd">    several input planes. If the sum of all inputs to the power of `p` is</span>
<span class="sd">    zero, the gradient is set to zero as well.</span>

<span class="sd">    See :class:`~torch.nn.LPPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kw</span><span class="p">,</span> <span class="n">kh</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kw</span> <span class="o">*</span> <span class="n">kh</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.lp_pool1d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D power-average pooling over an input signal composed of</span>
<span class="sd">    several input planes. If the sum of all inputs to the power of `p` is</span>
<span class="sd">    zero, the gradient is set to zero as well.</span>

<span class="sd">    See :class:`~torch.nn.LPPool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_max_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="adaptive_max_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="adaptive_max_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            triple-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<span class="n">adaptive_avg_pool1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">adaptive_avg_pool1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">adaptive_avg_pool1d(input, output_size) -&gt; Tensor</span>

<span class="s2">Applies a 1D adaptive average pooling over an input signal composed of</span>
<span class="s2">several input planes.</span>

<span class="s2">See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    output_size: the target output size (single integer)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            triple-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<span class="c1"># Activation functions</span>
<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout">[docs]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">Dropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="alpha_dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.alpha_dropout">[docs]</a><span class="k">def</span> <span class="nf">alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies alpha dropout to the input.</span>

<span class="sd">    See :class:`~torch.nn.AlphaDropout` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">AlphaDropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout2d">[docs]</a><span class="k">def</span> <span class="nf">dropout2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FeatureDropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout3d">[docs]</a><span class="k">def</span> <span class="nf">dropout3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FeatureDropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">feature_alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FeatureAlphaDropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span>


<div class="viewcode-block" id="threshold"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.threshold">[docs]</a><span class="k">def</span> <span class="nf">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Thresholds each element of the input Tensor.</span>

<span class="sd">    See :class:`~torch.nn.Threshold` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">threshold_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="n">threshold_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">threshold_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">threshold_(input, threshold, value) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~threshold`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu">[docs]</a><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;relu(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the rectified linear unit function element-wise. See</span>
<span class="sd">    :class:`~torch.nn.ReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">relu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">relu_(input) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~relu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="glu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.glu">[docs]</a><span class="k">def</span> <span class="nf">glu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    glu(input, dim=-1) -&gt; Tensor</span>

<span class="sd">    The gated linear unit. Computes:</span>

<span class="sd">    .. math ::</span>

<span class="sd">        H = A \times \sigma(B)</span>

<span class="sd">    where `input` is split in half along `dim` to form `A` and `B`.</span>

<span class="sd">    See `Language Modeling with Gated Convolutional Networks &lt;https://arxiv.org/abs/1612.08083&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input tensor</span>
<span class="sd">        dim (int): dimension on which to split the input</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;glu does not suppport scalars because halving size must be even&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardtanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hardtanh">[docs]</a><span class="k">def</span> <span class="nf">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    hardtanh(input, min_val=-1., max_val=1., inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more</span>
<span class="sd">    details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span></div>


<span class="n">hardtanh_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">hardtanh_(input, min_val=-1., max_val=1.) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~hardtanh`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu6">[docs]</a><span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;relu6(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the element-wise function :math:`\text{ReLU6}(x) = \min(\max(0,x), 6)`.</span>

<span class="sd">    See :class:`~torch.nn.ReLU6` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.elu">[docs]</a><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies element-wise,</span>
<span class="sd">    :math:`\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`.</span>

<span class="sd">    See :class:`~torch.nn.ELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span></div>


<span class="n">elu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">elu_(input, alpha=1.) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~elu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.selu">[docs]</a><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;selu(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))`,</span>
<span class="sd">    with :math:`\alpha=1.6732632423543772848170429916717` and</span>
<span class="sd">    :math:`scale=1.0507009873554804934193349852946`.</span>

<span class="sd">    See :class:`~torch.nn.SELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">selu_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>

<span class="n">selu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">selu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">selu_(input) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~selu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.leaky_relu">[docs]</a><span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    leaky_relu(input, negative_slope=0.01, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{LeakyReLU}(x) = \max(0, x) + \text{negative_slope} * \min(0, x)`</span>

<span class="sd">    See :class:`~torch.nn.LeakyReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">)</span></div>


<span class="n">leaky_relu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">leaky_relu_(input, negative_slope=0.01) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~leaky_relu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="n">prelu</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">prelu</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">prelu(input, weight) -&gt; Tensor</span>

<span class="s2">Applies element-wise the function</span>
<span class="s2">:math:`\text</span><span class="si">{PReLU}</span><span class="s2">(x) = \max(0,x) + \text</span><span class="si">{weight}</span><span class="s2"> * \min(0,x)` where weight is a</span>
<span class="s2">learnable parameter.</span>

<span class="s2">See :class:`~torch.nn.PReLU` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="rrelu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.rrelu">[docs]</a><span class="k">def</span> <span class="nf">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">8</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -&gt; Tensor</span>

<span class="sd">    Randomized leaky ReLU.</span>

<span class="sd">    See :class:`~torch.nn.RReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">rrelu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span></div>


<span class="n">rrelu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rrelu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">rrelu_(input, lower=1./8, upper=1./3, training=False) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~rrelu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">logsigmoid</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">log_sigmoid</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">logsigmoid(input) -&gt; Tensor</span>

<span class="s2">Applies element-wise :math:`\text</span><span class="si">{LogSigmoid}</span><span class="s2">(x) = \log \left(\frac</span><span class="si">{1}</span><span class="s2">{1 + \exp(-x_i)}\right)`</span>

<span class="s2">See :class:`~torch.nn.LogSigmoid` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="hardshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hardshrink">[docs]</a><span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    hardshrink(input, lambd=0.5) -&gt; Tensor</span>

<span class="sd">    Applies the hard shrinkage function element-wise</span>

<span class="sd">    See :class:`~torch.nn.Hardshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanhshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanhshrink">[docs]</a><span class="k">def</span> <span class="nf">tanhshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;tanhshrink(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise, :math:`\text{Tanhshrink}(x) = x - \text{Tanh}(x)`</span>

<span class="sd">    See :class:`~torch.nn.Tanhshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softsign">[docs]</a><span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;softsign(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise, the function :math:`\text{SoftSign}(x) = \frac{x}{1 + |x|}`</span>

<span class="sd">    See :class:`~torch.nn.Softsign` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span></div>


<span class="n">softplus</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">softplus(input, beta=1, threshold=20) -&gt; Tensor</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_softmax_dim</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">stacklevel</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Implicit dimension choice for &quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot; has been deprecated. &quot;</span>
                  <span class="s2">&quot;Change the call to include dim=X as an argument.&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="n">stacklevel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>


<div class="viewcode-block" id="softmin"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmin">[docs]</a><span class="k">def</span> <span class="nf">softmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmin function.</span>

<span class="sd">    Note that :math:`\text{Softmin}(x) = \text{Softmax}(-x)`. See softmax definition for mathematical formula.</span>

<span class="sd">    See :class:`~torch.nn.Softmin` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which softmin will be computed (so every slice</span>
<span class="sd">            along dim will sum to 1).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;softmin&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="nb">input</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmax function.</span>

<span class="sd">    Softmax is defined as:</span>

<span class="sd">    :math:`\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}`</span>

<span class="sd">    It is applied to all slices along dim, and will re-scale them so that the elements</span>
<span class="sd">    lie in the range `(0, 1)` and sum to 1.</span>

<span class="sd">    See :class:`~torch.nn.Softmax` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which softmax will be computed.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This function doesn&#39;t work directly with NLLLoss,</span>
<span class="sd">        which expects the Log to be computed between the Softmax and itself.</span>
<span class="sd">        Use log_softmax instead (it&#39;s faster and has better numerical properties).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_sample_gumbel</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from Gumbel(0, 1)</span>

<span class="sd">    based on</span>
<span class="sd">    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,</span>
<span class="sd">    (MIT license)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span> <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">eps</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_gumbel_softmax_sample</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Draw a sample from the Gumbel-Softmax distribution</span>

<span class="sd">    based on</span>
<span class="sd">    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb</span>
<span class="sd">    (MIT license)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="n">gumbel_noise</span> <span class="o">=</span> <span class="n">_sample_gumbel</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">new</span><span class="p">())</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">gumbel_noise</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">y</span> <span class="o">/</span> <span class="n">tau</span><span class="p">,</span> <span class="n">dims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>


<div class="viewcode-block" id="gumbel_softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.gumbel_softmax">[docs]</a><span class="k">def</span> <span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from the Gumbel-Softmax distribution and optionally discretize.</span>

<span class="sd">    Args:</span>
<span class="sd">      logits: `[batch_size, num_features]` unnormalized log probabilities</span>
<span class="sd">      tau: non-negative scalar temperature</span>
<span class="sd">      hard: if ``True``, the returned samples will be discretized as one-hot vectors,</span>
<span class="sd">            but will be differentiated as if it is the soft sample in autograd</span>

<span class="sd">    Returns:</span>
<span class="sd">      Sampled tensor of shape ``batch_size x num_features`` from the Gumbel-Softmax distribution.</span>
<span class="sd">      If ``hard=True``, the returned samples will be one-hot, otherwise they will</span>
<span class="sd">      be probability distributions that sum to 1 across features</span>

<span class="sd">    Constraints:</span>

<span class="sd">    - Currently only work on 2D input :attr:`logits` tensor of shape ``batch_size x num_features``</span>

<span class="sd">    Based on</span>
<span class="sd">    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,</span>
<span class="sd">    (MIT license)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="n">y_soft</span> <span class="o">=</span> <span class="n">_gumbel_softmax_sample</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hard</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">y_soft</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># this bit is based on</span>
        <span class="c1"># https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5</span>
        <span class="n">y_hard</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="c1"># this cool bit of code achieves two things:</span>
        <span class="c1"># - makes the output value exactly one-hot (since we add then</span>
        <span class="c1">#   subtract y_soft value)</span>
        <span class="c1"># - makes the gradient equal to y_soft gradient (since we strip</span>
        <span class="c1">#   all other gradients)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y_hard</span> <span class="o">-</span> <span class="n">y_soft</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">y_soft</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y_soft</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.log_softmax">[docs]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmax followed by a logarithm.</span>

<span class="sd">    While mathematically equivalent to log(softmax(x)), doing these two</span>
<span class="sd">    operations separately is slower, and numerically unstable. This function</span>
<span class="sd">    uses an alternative formulation to compute the output and gradient correctly.</span>

<span class="sd">    See :class:`~torch.nn.LogSoftmax` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which log_softmax will be computed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;log_softmax&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span></div>


<span class="n">softshrink</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softshrink</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">softshrink(input, lambd=0.5) -&gt; Tensor</span>

<span class="s2">Applies the soft shrinkage function elementwise</span>

<span class="s2">See :class:`~torch.nn.Softshrink` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanh">[docs]</a><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;tanh(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}`</span>

<span class="sd">    See :class:`~torch.nn.Tanh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.tanh is deprecated. Use torch.tanh instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span></div>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.sigmoid">[docs]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;sigmoid(input) -&gt; Tensor</span>

<span class="sd">    Applies the element-wise function :math:`\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}`</span>

<span class="sd">    See :class:`~torch.nn.Sigmoid` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span></div>


<div class="viewcode-block" id="linear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.linear">[docs]</a><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.</span>

<span class="sd">    Shape:</span>

<span class="sd">        - Input: :math:`(N, *, in\_features)` where `*` means any number of</span>
<span class="sd">          additional dimensions</span>
<span class="sd">        - Weight: :math:`(out\_features, in\_features)`</span>
<span class="sd">        - Bias: :math:`(out\_features)`</span>
<span class="sd">        - Output: :math:`(N, *, out\_features)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># fused op is marginally faster</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="bilinear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.bilinear">[docs]</a><span class="k">def</span> <span class="nf">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="embedding"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.embedding">[docs]</a><span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
              <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A simple lookup table that looks up embeddings in a fixed dictionary and size.</span>

<span class="sd">    This module is often used to retrieve word embeddings using indices.</span>
<span class="sd">    The input to the module is a list of indices, and the embedding matrix,</span>
<span class="sd">    and the output is the corresponding word embeddings.</span>

<span class="sd">    See :class:`torch.nn.Embedding` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (LongTensor): Tensor containing indices into the embedding matrix</span>
<span class="sd">        weight (Tensor): The embedding matrix</span>
<span class="sd">            Number of rows should correspond to the maximum possible index + 1,</span>
<span class="sd">            number of columns is the embedding size</span>
<span class="sd">        padding_idx (int, optional): If given, pads the output with the embedding vector at :attr:`padding_idx`</span>
<span class="sd">                                         (initialized to zeros) whenever it encounters the index.</span>
<span class="sd">        max_norm (float, optional): If given, will renormalize the embedding vectors to have a norm lesser than</span>
<span class="sd">                                    this before extracting. Note: this will modify :attr:`weight` in-place.</span>
<span class="sd">        norm_type (float, optional): The p of the p-norm to compute for the max_norm option. Default ``2``.</span>
<span class="sd">        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the inverse of frequency of</span>
<span class="sd">                                                the words in the mini-batch. Default ``False``.</span>
<span class="sd">        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under</span>
<span class="sd">                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: LongTensor of arbitrary shape containing the indices to extract</span>
<span class="sd">        - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,</span>
<span class="sd">                            where V = maximum index + 1 and embedding_dim = the embedding size</span>
<span class="sd">        - Output: `(*, embedding_dim)`, where `*` is the input shape</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([[1,2,4,5],[4,3,2,9]])</span>
<span class="sd">        &gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; F.embedding(input, embedding_matrix)</span>
<span class="sd">        tensor([[[ 0.8490,  0.9625,  0.6753],</span>
<span class="sd">                 [ 0.9666,  0.7761,  0.6108],</span>
<span class="sd">                 [ 0.6246,  0.9751,  0.3618],</span>
<span class="sd">                 [ 0.4161,  0.2419,  0.7383]],</span>

<span class="sd">                [[ 0.6246,  0.9751,  0.3618],</span>
<span class="sd">                 [ 0.0237,  0.7794,  0.0528],</span>
<span class="sd">                 [ 0.9666,  0.7761,  0.6108],</span>
<span class="sd">                 [ 0.3385,  0.8612,  0.1867]]])</span>

<span class="sd">        &gt;&gt;&gt; # example with padding_idx</span>
<span class="sd">        &gt;&gt;&gt; weights = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; weights[0, :].zero_()</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = weights</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([[0,2,0,5]])</span>
<span class="sd">        &gt;&gt;&gt; F.embedding(input, embedding_matrix, padding_idx=0)</span>
<span class="sd">        tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="sd">                 [ 0.5609,  0.5384,  0.8720],</span>
<span class="sd">                 [ 0.0000,  0.0000,  0.0000],</span>
<span class="sd">                 [ 0.6262,  0.2438,  0.7471]]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">padding_idx</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">padding_idx</span> <span class="o">&lt;</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;Padding_idx must be within num_embeddings&#39;</span>
        <span class="k">elif</span> <span class="n">padding_idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">padding_idx</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;Padding_idx must be within num_embeddings&#39;</span>
            <span class="n">padding_idx</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">padding_idx</span>
    <span class="k">elif</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">padding_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">if</span> <span class="n">max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># `embedding_renorm_` will call .contiguous() on input anyways, so we</span>
        <span class="c1"># call it here and take advantage of the improved locality in the</span>
        <span class="c1"># `embedding` call below too.</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="n">sparse</span><span class="p">)</span></div>


<div class="viewcode-block" id="embedding_bag"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.embedding_bag">[docs]</a><span class="k">def</span> <span class="nf">embedding_bag</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offsets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                  <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes sums or means of &#39;bags&#39; of embeddings, without instantiating the</span>
<span class="sd">    intermediate embeddings.</span>

<span class="sd">    See :class:`torch.nn.EmbeddingBag` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (LongTensor): Tensor containing bags of indices into the embedding matrix</span>
<span class="sd">        weight (Tensor): The embedding matrix</span>
<span class="sd">            Number of rows should correspond to the maximum possible index + 1,</span>
<span class="sd">            number of columns is the embedding size</span>
<span class="sd">        offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines</span>
<span class="sd">                             the starting index position of each bag (sequence) in :attr:`input`.</span>
<span class="sd">        max_norm (float, optional): If given, will renormalize the embedding vectors to have a norm lesser than</span>
<span class="sd">                                    this before extracting. Note: this will modify :attr:`weight` in-place.</span>
<span class="sd">        norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the max_norm option. Default ``2``.</span>
<span class="sd">        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the inverse of frequency of</span>
<span class="sd">                                                the words in the mini-batch. Default ``False``.</span>
<span class="sd">                                                Note: this option is not supported when ``mode=&quot;max&quot;``.</span>
<span class="sd">        mode (string, optional): ``&quot;sum&quot;``, ``&quot;mean&quot;`` or ``&quot;max&quot;``. Specifies the way to reduce the bag.</span>
<span class="sd">                                 Default: ``&quot;mean&quot;``</span>
<span class="sd">        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under</span>
<span class="sd">                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.</span>
<span class="sd">                                 Note: this option is not supported when ``mode=&quot;max&quot;``.</span>

<span class="sd">    Shape:</span>

<span class="sd">        - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)</span>

<span class="sd">          - If :attr:`input` is 2D of shape ``B x N``,</span>

<span class="sd">            it will be treated as ``B`` bags (sequences) each of fixed length ``N``, and</span>
<span class="sd">            this will return ``B`` values aggregated in a way depending on the :attr:`mode`.</span>
<span class="sd">            :attr:`offsets` is ignored and required to be ``None`` in this case.</span>

<span class="sd">          - If :attr:`input` is 1D of shape ``N``,</span>

<span class="sd">            it will be treated as a concatenation of multiple bags (sequences).</span>
<span class="sd">            :attr:`offsets` is required to be a 1D tensor containing the</span>
<span class="sd">            starting index positions of each bag in :attr:`input`. Therefore,</span>
<span class="sd">            for :attr:`offsets` of shape ``B``, :attr:`input` will be viewed as</span>
<span class="sd">            having ``B`` bags. Empty bags (i.e., having 0-length) will have</span>
<span class="sd">            returned vectors filled by zeros.</span>

<span class="sd">        - :attr:`weight` (Tensor): the learnable weights of the module of</span>
<span class="sd">          shape ``(num_embeddings x embedding_dim)``</span>

<span class="sd">        - :attr:`output`: aggregated embedding values of shape ``B x embedding_dim``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([1,2,4,5,4,3,2,9])</span>
<span class="sd">        &gt;&gt;&gt; offsets = torch.tensor([0,4])</span>
<span class="sd">        &gt;&gt;&gt; F.embedding_bag(embedding_matrix, input, offsets)</span>
<span class="sd">        tensor([[ 0.3397,  0.3552,  0.5545],</span>
<span class="sd">                [ 0.5893,  0.4386,  0.5882]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check for backward compatibility.</span>
    <span class="c1"># Used to be embedding_bag(weight, input, ...)</span>
    <span class="c1"># Now is     embedding_bag(input, weight, ...)</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Argument order of nn.functional.embedding_bag was changed. &quot;</span>
                      <span class="s2">&quot;Usage `embedding_bag(weight, input, ...)` is deprecated, &quot;</span>
                      <span class="s2">&quot;and should now be `embedding_bag(input, weight, ...)`.&quot;</span><span class="p">)</span>
        <span class="n">weight</span><span class="p">,</span> <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">offsets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;if input is 2D, then offsets has to be None&quot;</span>
                             <span class="s2">&quot;, as input is treated is a mini-batch of&quot;</span>
                             <span class="s2">&quot; fixed length sequences. However, found &quot;</span>
                             <span class="s2">&quot;offsets of type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">offsets</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                                   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">offsets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets has to be a 1D Tensor but got None&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">offsets</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets has to be a 1D Tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">offsets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets[0] has to be 0, i.e., the first sequence &quot;</span>
                             <span class="s2">&quot;in the mini-batch has to start from position 0. &quot;</span>
                             <span class="s2">&quot;However, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">offsets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
        <span class="k">if</span> <span class="n">offsets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&gt;</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets[-1] can not be greater than input&#39;s length&quot;</span>
                             <span class="s2">&quot; (</span><span class="si">{}</span><span class="s2">), but got offsets[-1] of </span><span class="si">{}</span><span class="s2">&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">offsets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input has to be 1D or 2D Tensor,&quot;</span>
                         <span class="s2">&quot; but got Tensor of dimension </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="n">scale_grad_by_freq</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max mode does not support scaling the gradient by the frequency&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max mode does not support sparse weights&quot;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode has to be one of sum or mean&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>

    <span class="n">ret</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">,</span>
        <span class="n">sparse</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="batch_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.batch_norm">[docs]</a><span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Batch Normalization for each channel across a batch of data.</span>

<span class="sd">    See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,</span>
<span class="sd">    :class:`~torch.nn.BatchNorm3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">reduce</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="n">size</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected more than 1 value per channel when training, got input size </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span>
        <span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="instance_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.instance_norm">[docs]</a><span class="k">def</span> <span class="nf">instance_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">running_var</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_input_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Instance Normalization for each channel in each data sample in a</span>
<span class="sd">    batch.</span>

<span class="sd">    See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,</span>
<span class="sd">    :class:`~torch.nn.InstanceNorm3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_input_stats</span> <span class="ow">and</span> <span class="p">(</span><span class="n">running_mean</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">running_var</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected running_mean and running_var to be not None when use_input_stats=False&#39;</span><span class="p">)</span>

    <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="kn">import</span> <span class="nn">torch.onnx.symbolic</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">symbolic_override_first_arg_based</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">symbolic</span><span class="o">.</span><span class="n">instance_norm</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_instance_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">running_var</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_input_stats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Repeat stored stats and affine transform params if necessary</span>
        <span class="k">if</span> <span class="n">running_mean</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">running_mean_orig</span> <span class="o">=</span> <span class="n">running_mean</span>
            <span class="n">running_mean</span> <span class="o">=</span> <span class="n">running_mean_orig</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">running_var</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">running_var_orig</span> <span class="o">=</span> <span class="n">running_var</span>
            <span class="n">running_var</span> <span class="o">=</span> <span class="n">running_var_orig</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

        <span class="c1"># Apply instance norm</span>
        <span class="n">input_reshaped</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:])</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span>
            <span class="n">input_reshaped</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="n">use_input_stats</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>

        <span class="c1"># Reshape and copy back</span>
        <span class="k">if</span> <span class="n">running_mean</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">running_mean_orig</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">running_mean</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">running_var</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">running_var_orig</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">running_var</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:])</span>
    <span class="k">return</span> <span class="n">_instance_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="o">=</span><span class="n">running_mean</span><span class="p">,</span>
                          <span class="n">running_var</span><span class="o">=</span><span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
                          <span class="n">use_input_stats</span><span class="o">=</span><span class="n">use_input_stats</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
                          <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span></div>


<div class="viewcode-block" id="layer_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.layer_norm">[docs]</a><span class="k">def</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Layer Normalization for last certain number of dimensions.</span>

<span class="sd">    See :class:`~torch.nn.LayerNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">group_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Group Normalization for last certain number of dimensions.</span>

<span class="sd">    See :class:`~torch.nn.GroupNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>


<div class="viewcode-block" id="local_response_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.local_response_norm">[docs]</a><span class="k">def</span> <span class="nf">local_response_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies local response normalization over an input signal composed of</span>
<span class="sd">    several input planes, where channels occupy the second dimension.</span>
<span class="sd">    Applies normalization across channels.</span>

<span class="sd">    See :class:`~torch.nn.LocalResponseNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 3D or higher dimensionality </span><span class="se">\</span>
<span class="s1">                         input (got </span><span class="si">{}</span><span class="s1"> dimensions)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
    <span class="n">div</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sizes</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">avg_pool3d</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
    <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="n">div</span></div>


<span class="c1"># loss</span>


<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.nll_loss">[docs]</a><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
             <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.NLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K &gt; 1`</span>
<span class="sd">            in the case of K-dimensional loss.</span>
<span class="sd">        target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`,</span>
<span class="sd">            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for</span>
<span class="sd">            K-dimensional loss.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets. Default: -100</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;elementwise_mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;elementwise_mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;elementwise_mean&#39;</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # input is of size N x C = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.tensor([1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 2 or more dimensions (got </span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected input batch_size (</span><span class="si">{}</span><span class="s1">) to match target batch_size (</span><span class="si">{}</span><span class="s1">).&#39;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected target size </span><span class="si">{}</span><span class="s1">, got </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">out_size</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="ow">not</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">ignore_index</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">ignore_index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="poisson_nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.poisson_nll_loss">[docs]</a><span class="k">def</span> <span class="nf">poisson_nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                     <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Poisson negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.PoissonNLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: expectation of underlying Poisson distribution.</span>
<span class="sd">        target: random sample :math:`target \sim \text{Poisson}(input)`.</span>
<span class="sd">        log_input: if ``True`` the loss is computed as</span>
<span class="sd">            :math:`\exp(\text{input}) - \text{target} * \text{input}`, if ``False`` then loss is</span>
<span class="sd">            :math:`\text{input} - \text{target} * \log(\text{input}+\text{eps})`. Default: ``True``</span>
<span class="sd">        full: whether to compute full loss, i. e. to add the Stirling</span>
<span class="sd">            approximation term. Default: ``False``</span>
<span class="sd">            :math:`\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when</span>
<span class="sd">            :attr:`log_input`=``False``. Default: 1e-8</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;elementwise_mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;elementwise_mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;elementwise_mean&#39;</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">log_input</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="nb">input</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">input</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">full</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="n">loss</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">target</span><span class="p">))[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="s1">&#39;elementwise_mean&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span></div>


<div class="viewcode-block" id="kl_div"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.kl_div">[docs]</a><span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The `Kullback-Leibler divergence`_ Loss.</span>

<span class="sd">    See :class:`~torch.nn.KLDivLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;elementwise_mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;elementwise_mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;elementwise_mean&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                  <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines `log_softmax` and `nll_loss` in a single</span>
<span class="sd">    function.</span>

<span class="sd">    See :class:`~torch.nn.CrossEntropyLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor) : :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K &gt; 1`</span>
<span class="sd">            in the case of K-dimensional loss.</span>
<span class="sd">        target (Tensor) : :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`,</span>
<span class="sd">            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for</span>
<span class="sd">            K-dimensional loss.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets. Default: -100</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;elementwise_mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;elementwise_mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;elementwise_mean&#39;</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(5, (3,), dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt; loss = F.cross_entropy(input, target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures the Binary Cross Entropy</span>
<span class="sd">    between the target and the output.</span>

<span class="sd">    See :class:`~torch.nn.BCELoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight</span>
<span class="sd">                if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;elementwise_mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;elementwise_mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;elementwise_mean&#39;</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = torch.randn((3, 2), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.rand((3, 2), requires_grad=False)</span>
<span class="sd">        &gt;&gt;&gt; loss = F.binary_cross_entropy(F.sigmoid(input), target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Using a target size (</span><span class="si">{}</span><span class="s2">) that is different to the input size (</span><span class="si">{}</span><span class="s2">) is deprecated. &quot;</span>
                      <span class="s2">&quot;Please ensure they have the same size.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="n">target</span><span class="o">.</span><span class="n">nelement</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target and input must have the same number of elements. target nelement (</span><span class="si">{}</span><span class="s2">) &quot;</span>
                         <span class="s2">&quot;!= input nelement (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">nelement</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">nelement</span><span class="p">()))</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_size</span> <span class="o">=</span> <span class="n">_infer_size</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">new_size</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy_with_logits"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy_with_logits">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures Binary Cross Entropy between target and output</span>
<span class="sd">    logits.</span>

<span class="sd">    See :class:`~torch.nn.BCEWithLogitsLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight</span>
<span class="sd">            if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;elementwise_mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;elementwise_mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;elementwise_mean&#39;</span>
<span class="sd">        pos_weight (Tensor, optional): a weight of positive examples.</span>
<span class="sd">                Must be a vector with length equal to the number of classes.</span>

<span class="sd">    Examples::</span>

<span class="sd">         &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)</span>
<span class="sd">         &gt;&gt;&gt; target = torch.empty(3).random_(2)</span>
<span class="sd">         &gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)</span>
<span class="sd">         &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target size (</span><span class="si">{}</span><span class="s2">) must be the same as input size (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

    <span class="n">max_val</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pos_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="n">max_val</span> <span class="o">+</span> <span class="p">((</span><span class="o">-</span><span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">log_weight</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">pos_weight</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">target</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="n">log_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_val</span> <span class="o">+</span> <span class="p">((</span><span class="o">-</span><span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span><span class="o">.</span><span class="n">log</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">weight</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;elementwise_mean&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span></div>


<span class="k">def</span> <span class="nf">_pointwise_loss</span><span class="p">(</span><span class="n">lambd</span><span class="p">,</span> <span class="n">lambd_optimized</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">lambd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">d</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;elementwise_mean&#39;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">lambd_optimized</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>


<div class="viewcode-block" id="smooth_l1_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.smooth_l1_loss">[docs]</a><span class="k">def</span> <span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below 1 and an L1 term otherwise.</span>

<span class="sd">    See :class:`~torch.nn.SmoothL1Loss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="l1_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.l1_loss">[docs]</a><span class="k">def</span> <span class="nf">l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;l1_loss(input, target, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;) -&gt; Tensor</span>

<span class="sd">    Function that takes the mean element-wise absolute value difference.</span>

<span class="sd">    See :class:`~torch.nn.L1Loss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_pointwise_loss</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">,</span>
                           <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="mse_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.mse_loss">[docs]</a><span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;mse_loss(input, target, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;) -&gt; Tensor</span>

<span class="sd">    Measures the element-wise mean squared error.</span>

<span class="sd">    See :class:`~torch.nn.MSELoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_pointwise_loss</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="margin_ranking_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.margin_ranking_loss">[docs]</a><span class="k">def</span> <span class="nf">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MarginRankingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input1</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">input2</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">target</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">((</span><span class="s2">&quot;margin_ranking_loss does not support scalars, got sizes: &quot;</span>
                            <span class="s2">&quot;input1: </span><span class="si">{}</span><span class="s2">, input2: </span><span class="si">{}</span><span class="s2">, target: </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input1</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">input2</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">())))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="hinge_embedding_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hinge_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.HingeEmbeddingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="multilabel_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multilabel_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiLabelMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="soft_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;soft_margin_loss(input, target, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.SoftMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="multilabel_soft_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multilabel_soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multilabel_soft_margin_loss(input, target, weight=None, size_average=None) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosine_embedding_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cosine_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction=&#39;elementwise_mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.CosineEmbeddingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="multi_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multi_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;elementwise_mean&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,</span>
<span class="sd">                          reduce=None, reduction=&#39;elementwise_mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;only p == 1 and p == 2 supported&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;weight must be one-dimensional&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="pixel_shuffle"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pixel_shuffle">[docs]</a><span class="k">def</span> <span class="nf">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Rearranges elements in a tensor of shape :math:`[*, C*r^2, H, W]` to a</span>
<span class="sd">    tensor of shape :math:`[C, H*r, W*r]`.</span>

<span class="sd">    See :class:`~torch.nn.PixelShuffle` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input</span>
<span class="sd">        upscale_factor (int): factor to increase spatial resolution by</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; ps = nn.PixelShuffle(3)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.empty(1, 9, 4, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = ps(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output.size())</span>
<span class="sd">        torch.Size([1, 1, 12, 12])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">channels</span> <span class="o">//=</span> <span class="n">upscale_factor</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="n">out_height</span> <span class="o">=</span> <span class="n">in_height</span> <span class="o">*</span> <span class="n">upscale_factor</span>
    <span class="n">out_width</span> <span class="o">=</span> <span class="n">in_width</span> <span class="o">*</span> <span class="n">upscale_factor</span>

    <span class="n">input_view</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span>
        <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span><span class="p">)</span>

    <span class="n">shuffle_out</span> <span class="o">=</span> <span class="n">input_view</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">shuffle_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">out_height</span><span class="p">,</span> <span class="n">out_width</span><span class="p">)</span></div>


<div class="viewcode-block" id="upsample"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample">[docs]</a><span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input to either the given :attr:`size` or the given</span>
<span class="sd">    :attr:`scale_factor`</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with ``nn.functional.interpolate(...)``.</span>


<span class="sd">    The algorithm used for upsampling is determined by :attr:`mode`.</span>

<span class="sd">    Currently temporal, spatial and volumetric upsampling are supported, i.e.</span>
<span class="sd">    expected inputs are 3-D, 4-D or 5-D in shape.</span>

<span class="sd">    The input dimensions are interpreted in the form:</span>
<span class="sd">    `mini-batch x channels x [optional depth] x [optional height] x width`.</span>

<span class="sd">    The modes available for upsampling are: `nearest`, `linear` (3D-only),</span>
<span class="sd">    `bilinear` (4D-only), `trilinear` (5D-only)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</span>
<span class="sd">            output spatial size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">        mode (string): algorithm used for upsampling:</span>
<span class="sd">            &#39;nearest&#39; | &#39;linear&#39; | &#39;bilinear&#39; | &#39;trilinear&#39;. Default: &#39;nearest&#39;</span>
<span class="sd">        align_corners (bool, optional): if True, the corner pixels of the input</span>
<span class="sd">            and output tensors are aligned, and thus preserving the values at</span>
<span class="sd">            those pixels. This only has effect when :attr:`mode` is `linear`,</span>
<span class="sd">            `bilinear`, or `trilinear`. Default: False</span>

<span class="sd">    .. warning::</span>
<span class="sd">        With ``align_corners = True``, the linearly interpolating modes</span>
<span class="sd">        (`linear`, `bilinear`, and `trilinear`) don&#39;t proportionally align the</span>
<span class="sd">        output and input pixels, and thus the output values can depend on the</span>
<span class="sd">        input size. This was the default behavior for these modes up to version</span>
<span class="sd">        0.3.1. Since then, the default behavior is ``align_corners = False``.</span>
<span class="sd">        See :class:`~torch.nn.Upsample` for concrete examples on how this</span>
<span class="sd">        affects the outputs.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span></div>


<div class="viewcode-block" id="interpolate"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.interpolate">[docs]</a><span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Down/up samples the input to either the given :attr:`size` or the given</span>
<span class="sd">    :attr:`scale_factor`</span>

<span class="sd">    The algorithm used for interpolation is determined by :attr:`mode`.</span>

<span class="sd">    Currently temporal, spatial and volumetric sampling are supported, i.e.</span>
<span class="sd">    expected inputs are 3-D, 4-D or 5-D in shape.</span>

<span class="sd">    The input dimensions are interpreted in the form:</span>
<span class="sd">    `mini-batch x channels x [optional depth] x [optional height] x width`.</span>

<span class="sd">    The modes available for resizing are: `nearest`, `linear` (3D-only),</span>
<span class="sd">    `bilinear` (4D-only), `trilinear` (5D-only), `area`</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</span>
<span class="sd">            output spatial size.</span>
<span class="sd">        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.</span>
<span class="sd">        mode (string): algorithm used for upsampling:</span>
<span class="sd">            &#39;nearest&#39; | &#39;linear&#39; | &#39;bilinear&#39; | &#39;trilinear&#39; | &#39;area&#39;. Default: &#39;nearest&#39;</span>
<span class="sd">        align_corners (bool, optional): if True, the corner pixels of the input</span>
<span class="sd">            and output tensors are aligned, and thus preserving the values at</span>
<span class="sd">            those pixels. This only has effect when :attr:`mode` is `linear`,</span>
<span class="sd">            `bilinear`, or `trilinear`. Default: False</span>

<span class="sd">    .. warning::</span>
<span class="sd">        With ``align_corners = True``, the linearly interpolating modes</span>
<span class="sd">        (`linear`, `bilinear`, and `trilinear`) don&#39;t proportionally align the</span>
<span class="sd">        output and input pixels, and thus the output values can depend on the</span>
<span class="sd">        input size. This was the default behavior for these modes up to version</span>
<span class="sd">        0.3.1. Since then, the default behavior is ``align_corners = False``.</span>
<span class="sd">        See :class:`~torch.nn.Upsample` for concrete examples on how this</span>
<span class="sd">        affects the outputs.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">numbers</span> <span class="k">import</span> <span class="n">Integral</span>
    <span class="kn">from</span> <span class="nn">.modules.utils</span> <span class="k">import</span> <span class="n">_ntuple</span>

    <span class="k">def</span> <span class="nf">_check_size_scale_factor</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;either size or scale_factor should be defined&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;only one of size or scale_factor should be defined&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>\
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;scale_factor shape must match input shape. &#39;</span>
                             <span class="s1">&#39;Input is </span><span class="si">{}</span><span class="s1">D, scale_factor size is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_output_size</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="n">_check_size_scale_factor</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">size</span>
        <span class="n">scale_factors</span> <span class="o">=</span> <span class="n">_ntuple</span><span class="p">(</span><span class="n">dim</span><span class="p">)(</span><span class="n">scale_factor</span><span class="p">)</span>
        <span class="c1"># math.floor might return float in py2.7</span>
        <span class="k">return</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="s1">&#39;area&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;align_corners option can only be set with the &quot;</span>
                             <span class="s2">&quot;interpolating modes: linear | bilinear | trilinear&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Default upsampling behavior when mode=</span><span class="si">{}</span><span class="s2"> is changed &quot;</span>
                          <span class="s2">&quot;to align_corners=False since 0.4.0. Please specify &quot;</span>
                          <span class="s2">&quot;align_corners=True if the old behavior is desired. &quot;</span>
                          <span class="s2">&quot;See the documentation of nn.Upsample for details.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
            <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_linear1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 3D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 3D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but linear mode needs 3D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_bilinear2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but linear mode needs 3D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_trilinear3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 3D, 4D and 5D input Tensors supported&quot;</span>
                                  <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">D) for the modes: nearest | linear | bilinear | trilinear&quot;</span>
                                  <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">mode</span><span class="p">))</span></div>


<div class="viewcode-block" id="upsample_nearest"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample_nearest">[docs]</a><span class="k">def</span> <span class="nf">upsample_nearest</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input, using nearest neighbours&#39; pixel values.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with ``nn.functional.interpolate(..., mode=&#39;nearest&#39;)``.</span>

<span class="sd">    Currently spatial and volumetric upsampling are supported (i.e. expected</span>
<span class="sd">    inputs are 4 or 5 dimensional).</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia</span>
<span class="sd">            size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="upsample_bilinear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample_bilinear">[docs]</a><span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input, using bilinear upsampling.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with</span>
<span class="sd">        ``nn.functional.interpolate(..., mode=&#39;bilinear&#39;, align_corners=True)``.</span>

<span class="sd">    Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo</span>
<span class="sd">    volumetric (5 dimensional) inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        size (int or Tuple[int, int]): output spatial size.</span>
<span class="sd">        scale_factor (int or Tuple[int, int]): multiplier for spatial size</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<span class="n">GRID_SAMPLE_MODE_ZEROS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">GRID_SAMPLE_MODE_BORDER</span> <span class="o">=</span> <span class="mi">1</span>


<div class="viewcode-block" id="grid_sample"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.grid_sample">[docs]</a><span class="k">def</span> <span class="nf">grid_sample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given an :attr:`input` and a flow-field :attr:`grid`, computes the</span>
<span class="sd">    `output` using input pixel locations from the grid.</span>

<span class="sd">    Uses bilinear interpolation to sample the input pixels.</span>
<span class="sd">    Currently, only spatial (4 dimensional) and volumetric (5 dimensional)</span>
<span class="sd">    inputs are supported.</span>

<span class="sd">    For each output location, :attr:`grid` has `x`, `y`</span>
<span class="sd">    input pixel locations which are used to compute output.</span>
<span class="sd">    In the case of 5D inputs, :attr:`grid` has `x`, `y`, `z` pixel locations.</span>

<span class="sd">    .. Note::</span>
<span class="sd">        To avoid confusion in notation, let&#39;s note that `x` corresponds to the `width` dimension `IW`,</span>
<span class="sd">        `y` corresponds to the height dimension `IH` and `z` corresponds to the `depth` dimension `ID`.</span>

<span class="sd">    :attr:`grid` has values in the range of `[-1, 1]`. This is because the</span>
<span class="sd">    pixel locations are normalized by the input height and width.</span>

<span class="sd">    For example, values: x: -1, y: -1 is the left-top pixel of the input, and</span>
<span class="sd">    values: x: 1, y: 1 is the right-bottom pixel of the input.</span>

<span class="sd">    If :attr:`grid` has values outside the range of `[-1, 1]`, those locations</span>
<span class="sd">    are handled as defined by `padding_mode`. Options are `zeros` or `border`,</span>
<span class="sd">    defining those locations to use 0 or image border values as contribution</span>
<span class="sd">    to the bilinear interpolation.</span>

<span class="sd">    .. Note:: This function is used in building Spatial Transformer Networks</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input batch (N x C x IH x IW) or (N x C x ID x IH x IW)</span>
<span class="sd">        grid (Tensor): flow-field of size (N x OH x OW x 2) or (N x OD x OH x OW x 3)</span>
<span class="sd">        padding_mode (str): padding mode for outside grid values</span>
<span class="sd">            &#39;zeros&#39; | &#39;border&#39;. Default: &#39;zeros&#39;</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor): output Tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;nn.functional.grid_sample got unsupported mode: &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
        <span class="n">padding_mode</span> <span class="o">=</span> <span class="n">GRID_SAMPLE_MODE_ZEROS</span>
    <span class="k">elif</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s1">&#39;border&#39;</span><span class="p">:</span>
        <span class="n">padding_mode</span> <span class="o">=</span> <span class="n">GRID_SAMPLE_MODE_BORDER</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;padding_mode needs to be &#39;zeros&#39; or &#39;border&#39;, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">padding_mode</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">grid_sampler</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="affine_grid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.affine_grid">[docs]</a><span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Generates a 2d flow field, given a batch of affine matrices :attr:`theta`</span>
<span class="sd">    Generally used in conjunction with :func:`grid_sample` to</span>
<span class="sd">    implement Spatial Transformer Networks.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta (Tensor): input batch of affine matrices (:math:`N \times 2 \times 3`)</span>
<span class="sd">        size (torch.Size): the target output image size (:math:`N \times C \times H \times W`)</span>
<span class="sd">                           Example: torch.Size((32, 3, 24, 24))</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor): output Tensor of size (:math:`N \times H \times W \times 2`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">vision</span><span class="o">.</span><span class="n">affine_grid_generator</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pad">[docs]</a><span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads tensor.</span>

<span class="sd">    `Nd` constant padding:  The number of dimensions to pad is</span>
<span class="sd">        :math:`\left\lfloor\frac{len(padding)}{2}\right\rfloor` and the dimensions that get padded begins with the</span>
<span class="sd">        last dimension and moves forward. See below for examples.</span>

<span class="sd">    `1D`, `2D` and `3D` &quot;reflect&quot; / &quot;replicate&quot; padding:</span>
<span class="sd">        for 1D:</span>
<span class="sd">                3D input tensor with padding of the form `(padLeft, padRight)`</span>
<span class="sd">        for 2D:</span>
<span class="sd">                4D input tensor with padding of the form `(padLeft, padRight, padTop, padBottom)`.</span>
<span class="sd">        for 3D:</span>
<span class="sd">                5D input tensor with padding of the form</span>
<span class="sd">                `(padLeft, padRight, padTop, padBottom, padFront, padBack)`. No &quot;reflect&quot; implementation.</span>

<span class="sd">    See :class:`torch.nn.ConstantPad2d`, :class:`torch.nn.ReflectionPad2d`, and</span>
<span class="sd">    :class:`torch.nn.ReplicationPad2d` for concrete examples on how each of the</span>
<span class="sd">    padding modes works.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): `Nd` tensor</span>
<span class="sd">        pad (tuple): m-elem tuple, where :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.</span>
<span class="sd">        mode: &#39;constant&#39;, &#39;reflect&#39; or &#39;replicate&#39;. Default: &#39;constant&#39;</span>
<span class="sd">        value: fill value for &#39;constant&#39; padding. Default: 0</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)</span>
<span class="sd">        &gt;&gt;&gt; p1d = (1, 1) # pad last dim by 1 on each side</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p1d, &quot;constant&quot;, 0)  # effectively zero padding</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 3, 4, 4])</span>
<span class="sd">        &gt;&gt;&gt; p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p2d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 3, 8, 4])</span>
<span class="sd">        &gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)</span>
<span class="sd">        &gt;&gt;&gt; p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p3d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 9, 7, 3])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Padding length must be divisible by 2&#39;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&lt;=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="s1">&#39;Padding length too large&#39;</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ConstantPadNd</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Padding mode &quot;</span><span class="si">{}</span><span class="s1">&quot;&quot; doesn</span><span class="se">\&#39;</span><span class="s1">t take in value argument&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;3D tensors expect 2 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;4D tensors expect 4 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;5D tensors expect 6 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only 3D, 4D, 5D padding with non-constant padding are supported for now&quot;</span><span class="p">)</span></div>


<span class="c1"># distance</span>

<div class="viewcode-block" id="pairwise_distance"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pairwise_distance">[docs]</a><span class="k">def</span> <span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See :class:`torch.nn.PairwiseDistance` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosine_similarity"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cosine_similarity">[docs]</a><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns cosine similarity between x1 and x2, computed along dim.</span>

<span class="sd">    .. math ::</span>
<span class="sd">        \text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): First input.</span>
<span class="sd">        x2 (Tensor): Second input (of size matching x1).</span>
<span class="sd">        dim (int, optional): Dimension of vectors. Default: 1</span>
<span class="sd">        eps (float, optional): Small value to avoid division by zero.</span>
<span class="sd">            Default: 1e-8</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(\ast_1, D, \ast_2)` where D is at position `dim`.</span>
<span class="sd">        - Output: :math:`(\ast_1, \ast_2)` where 1 is at position `dim`.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; input1 = torch.randn(100, 128)</span>
<span class="sd">        &gt;&gt;&gt; input2 = torch.randn(100, 128)</span>
<span class="sd">        &gt;&gt;&gt; output = F.cosine_similarity(input1, input2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w12</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w12</span> <span class="o">/</span> <span class="p">(</span><span class="n">w1</span> <span class="o">*</span> <span class="n">w2</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span></div>


<div class="viewcode-block" id="triplet_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.triplet_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;elementwise_mean&quot;</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See :class:`~torch.nn.TripletMarginLoss` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                                     <span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="normalize"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.normalize">[docs]</a><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Performs :math:`L_p` normalization of inputs over specified dimension.</span>

<span class="sd">    Does:</span>

<span class="sd">    .. math::</span>
<span class="sd">        v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}</span>

<span class="sd">    for each subtensor v over dimension dim of input. Each subtensor is</span>
<span class="sd">    flattened into a vector, i.e. :math:`\lVert v \rVert_p` is not a matrix</span>
<span class="sd">    norm.</span>

<span class="sd">    With default arguments normalizes over the second dimension with Euclidean</span>
<span class="sd">    norm.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of any shape</span>
<span class="sd">        p (float): the exponent value in the norm formulation. Default: 2</span>
<span class="sd">        dim (int): the dimension to reduce. Default: 1</span>
<span class="sd">        eps (float): small value to avoid division by zero. Default: 1e-12</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">assert_int_or_pair</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="n">message</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">arg_name</span><span class="p">)</span>


<div class="viewcode-block" id="unfold"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.unfold">[docs]</a><span class="k">def</span> <span class="nf">unfold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Extracts sliding local blocks from an batched input tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, only 4-D input tensors (batched image-like tensors) are</span>
<span class="sd">        supported.</span>

<span class="sd">    See :class:`torch.nn.Unfold` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> must be int or 2-tuple for 4D input&#39;</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Im2Col</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
                            <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 4D input Tensors are supported (got </span><span class="si">{}</span><span class="s2">D)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span></div>


<div class="viewcode-block" id="fold"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.fold">[docs]</a><span class="k">def</span> <span class="nf">fold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Combines an array of sliding local blocks into a large containing</span>
<span class="sd">    tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, only 4-D output tensors (batched image-like tensors) are</span>
<span class="sd">        supported.</span>

<span class="sd">    See :class:`torch.nn.Fold` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> must be int or 2-tuple for 3D input&#39;</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Col2Im</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_size</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
                            <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 3D input Tensors are supported (got </span><span class="si">{}</span><span class="s2">D)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>
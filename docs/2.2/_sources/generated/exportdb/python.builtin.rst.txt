python.builtin
==================
dynamic_shape_round
^^^^^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`python.builtin <python.builtin>`, :doc:`torch.dynamic-shape <torch.dynamic-shape>`

    Support Level: NOT_SUPPORTED_YET

Original source code:

.. code-block:: python

    import torch
    
    from torch.export import Dim
    
    x = torch.ones(3, 2)
    dim0_x = Dim("dim0_x")
    
    def dynamic_shape_round(x):
        """
        Calling round on dynamic shapes is not supported.
        """
        return x[: round(x.shape[0] / 2)]
    

Result:

.. code-block::

    Unsupported: Calling round() on symbolic value is not supported. You can use floor() to implement this functionality


tensor_setattr
^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`python.builtin <python.builtin>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    import torch
    
    
    
    def tensor_setattr(x, attr):
        """
        setattr() call onto tensors is not supported.
        """
        setattr(x, attr, torch.randn(3, 2))
        return x + 4
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, l_x_: "f32[3, 2]", arg1):
                    add: "f32[3, 2]" = torch.ops.aten.add.Tensor(l_x_, 4);  l_x_ = None
                return (add,)
                
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='l_x_'), target=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=ConstantArgument(value='attr'), target=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add'), target=None)])
    Range constraints: {}
    Equality constraints: []
    


type_reflection_method
^^^^^^^^^^^^^^^^^^^^^^

.. note::

    Tags: :doc:`python.builtin <python.builtin>`

    Support Level: SUPPORTED

Original source code:

.. code-block:: python

    import torch
    
    
    
    class A:
        @classmethod
        def func(cls, x):
            return 1 + x
    
    
    def type_reflection_method(x):
        """
        type() calls on custom objects followed by method calls are not allowed
        due to its overly dynamic nature.
        """
        a = A()
        return type(a).func(x)
    
    
    

Result:

.. code-block::

    ExportedProgram:
        class GraphModule(torch.nn.Module):
            def forward(self, l_x_: "f32[3, 4]"):
                    add: "f32[3, 4]" = torch.ops.aten.add.Tensor(l_x_, 1);  l_x_ = None
                return (add,)
                
    Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='l_x_'), target=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add'), target=None)])
    Range constraints: {}
    Equality constraints: []
    


You can rewrite the example above to something like the following:

.. code-block:: python

    def type_reflection_method_rewrite(x):
        """
        Custom object class methods will be inlined.
        """
        return A.func(x)
    


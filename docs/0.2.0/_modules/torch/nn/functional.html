

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.functional &mdash; PyTorch master documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  
    <link rel="stylesheet" href="../../../_static/css/pytorch_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../../../genindex.html"/>
        <link rel="search" title="Search" href="../../../search.html"/>
    <link rel="top" title="PyTorch master documentation" href="../../../index.html"/>
        <link rel="up" title="torch" href="../../torch.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                <a href="https://pytorch.org/docs/versions.html"> 0.2.0_1 &#x25BC</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/autograd.html#volatile"><code class="docutils literal"><span class="pre">volatile</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../notes/serialization.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#random-sampling">Random sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#convolution-layers">Convolution Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#pooling-layers">Pooling Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#padding-layers">Padding Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activations">Non-linear Activations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#normalization-layers">Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#recurrent-layers">Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#linear-layers">Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#linear"><span class="hidden-section">Linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dropout-layers">Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#sparse-layers">Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#distance-functions">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#loss-functions">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nllloss2d"><span class="hidden-section">NLLLoss2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#vision-layers">Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#utilities">Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-functional">torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#convolution-functions">Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id16"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id17"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id18"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#pooling-functions">Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id19"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id20"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id21"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id22"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id23"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id24"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id25"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#rrelu"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id26"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hardshrink"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id27"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id28"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id29"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id30"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id31"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id32"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id33"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id34"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#normalization-functions">Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#linear-functions">Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id35"><span class="hidden-section">linear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#dropout-functions">Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id36"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id37"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id38"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#id39">Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#id40">Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../nn.html#vision-functions">Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#id42"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#variable">Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#api-compatibility">API compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../autograd.html#function"><span class="hidden-section">Function</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#initialization">Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../distributed.html#collective-functions">Collective functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../legacy.html">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/torchvision.html">torchvision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/models.html">torchvision.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL.Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/utils.html">torchvision.utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PyTorch</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
          <li><a href="../../torch.html">torch</a> &raquo;</li>
        
      <li>torch.nn.functional</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for torch.nn.functional</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Functional interface&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">numbers</span> <span class="k">import</span> <span class="n">Integral</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="k">import</span> <span class="n">_infer_size</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_functions</span>
<span class="kn">from</span> <span class="nn">.modules</span> <span class="k">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">._functions.linear</span> <span class="k">import</span> <span class="n">Bilinear</span>
<span class="kn">from</span> <span class="nn">._functions.padding</span> <span class="k">import</span> <span class="n">ConstantPad2d</span>
<span class="kn">from</span> <span class="nn">._functions.vision</span> <span class="k">import</span> <span class="n">GridSampler</span><span class="p">,</span> <span class="n">AffineGridGenerator</span>
<span class="kn">from</span> <span class="nn">..autograd</span> <span class="k">import</span> <span class="n">_functions</span> <span class="k">as</span> <span class="n">_autograd_functions</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">.modules.utils</span> <span class="k">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span>

<span class="c1"># Convolutions</span>
<span class="n">ConvNd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">ConvNd</span>


<div class="viewcode-block" id="conv2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv2d">[docs]</a><span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 2D convolution over an input image composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor (minibatch x in_channels x iH x iW)</span>
<span class="sd">        weight: filters tensor (out_channels, in_channels/groups, kH, kW)</span>
<span class="sd">        bias: optional bias tensor (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or</span>
<span class="sd">          a tuple (sh x sw). Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input. Can be a single number or</span>
<span class="sd">          a tuple. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(8,4,3,3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(1,4,5,5))</span>
<span class="sd">        &gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 4D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv1d">[docs]</a><span class="k">def</span> <span class="nf">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 1D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iW)</span>
<span class="sd">        weight: filters of shape (out_channels, in_channels, kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel, default 1</span>
<span class="sd">        padding: implicit zero padding on the input. Can be a single number or</span>
<span class="sd">          a tuple. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(33, 16, 3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(20, 16, 50))</span>
<span class="sd">        &gt;&gt;&gt; F.conv1d(inputs, filters)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 3D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv3d">[docs]</a><span class="k">def</span> <span class="nf">conv3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 3D convolution over an input image composed of several input</span>
<span class="sd">        planes.</span>

<span class="sd">    See :class:`~torch.nn.Conv3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iT x iH x iW)</span>
<span class="sd">        weight: filters tensor of shape (out_channels, in_channels, kT, kH, kW)</span>
<span class="sd">        bias: optional bias tensor of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel. Can be a single number or</span>
<span class="sd">          a tuple (st x sh x sw). Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input. Can be a single number or</span>
<span class="sd">          a tuple. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; filters = autograd.Variable(torch.randn(33, 16, 3, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; inputs = autograd.Variable(torch.randn(20, 16, 50, 10, 20))</span>
<span class="sd">        &gt;&gt;&gt; F.conv3d(inputs, filters)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 5D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span>
               <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose1d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 1D transposed convolution operator over an input signal</span>
<span class="sd">    composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="sd">    See :class:`~torch.nn.ConvTranspose1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iW)</span>
<span class="sd">        weight: filters of shape (in_channels x out_channels x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel. Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input. Default: 0</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>
<span class="sd">        output_padding: A zero-padding of 0 &lt;= padding &lt; stride that should be</span>
<span class="sd">          added to the output. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 3D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
               <span class="n">_single</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span>
               <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose2d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 2D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="sd">    See :class:`~torch.nn.ConvTranspose2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iH x iW)</span>
<span class="sd">        weight: filters of shape (in_channels x out_channels x kH x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel, a single number or a</span>
<span class="sd">          tuple (sh x sw). Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input, a single number or a</span>
<span class="sd">          tuple (padh x padw). Default: 0</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>
<span class="sd">        output_padding: A zero-padding of 0 &lt;= padding &lt; stride that should be</span>
<span class="sd">          added to the output. Can be a single number or a tuple. Default: 0</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 4D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
               <span class="n">_pair</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv_transpose3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.conv_transpose3d">[docs]</a><span class="k">def</span> <span class="nf">conv_transpose3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies a 3D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes, sometimes also called &quot;deconvolution&quot;</span>

<span class="sd">    See :class:`~torch.nn.ConvTranspose3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of shape (minibatch x in_channels x iT x iH x iW)</span>
<span class="sd">        weight: filters of shape (in_channels x out_channels x kH x kW)</span>
<span class="sd">        bias: optional bias of shape (out_channels)</span>
<span class="sd">        stride: the stride of the convolving kernel, a single number or a</span>
<span class="sd">          tuple (sh x sw). Default: 1</span>
<span class="sd">        padding: implicit zero padding on the input, a single number or a</span>
<span class="sd">          tuple (padh x padw). Default: 0</span>
<span class="sd">        output_padding: A zero-padding of 0 &lt;= padding &lt; stride that should be</span>
<span class="sd">          added to the output. Can be a single number or a tuple. Default: 0</span>
<span class="sd">        groups: split input into groups, in_channels should be divisible by</span>
<span class="sd">          the number of groups</span>
<span class="sd">        dilation: the spacing between kernel elements. Default: 1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 5D tensor as input, got </span><span class="si">{}</span><span class="s2">D tensor instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">ConvNd</span><span class="p">(</span><span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span>
               <span class="n">_triple</span><span class="p">(</span><span class="n">output_padding</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="c1"># Pooling</span>
<div class="viewcode-block" id="avg_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.avg_pool1d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D average pooling over an input signal composed of several</span>
<span class="sd">    input planes.</span>

<span class="sd">    See :class:`~torch.nn.AvgPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size: the size of the window</span>
<span class="sd">        stride: the stride of the window. Default value is :attr:`kernel_size`</span>
<span class="sd">        padding: implicit zero padding to be added on both sides</span>
<span class="sd">        ceil_mode: when True, will use `ceil` instead of `floor` to compute the</span>
<span class="sd">            output shape</span>
<span class="sd">        count_include_pad: when True, will include the zero-padding in the</span>
<span class="sd">            averaging calculation</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window of size=3, stride=2</span>
<span class="sd">        &gt;&gt;&gt; input = Variable(torch.Tensor([[[1,2,3,4,5,6,7]]]))</span>
<span class="sd">        &gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)</span>
<span class="sd">        Variable containing:</span>
<span class="sd">        (0 ,.,.) =</span>
<span class="sd">          2  4  6</span>
<span class="sd">        [torch.FloatTensor of size 1x1x3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;expected 3D input (got </span><span class="si">{}</span><span class="s1"> dimensions)&#39;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                           <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="avg_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies 2D average-pooling operation in kh x kw regions by step size</span>
<span class="sd">    dh x dw steps. The number of output features is equal to the number of</span>
<span class="sd">    input planes.</span>

<span class="sd">    See :class:`~torch.nn.AvgPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor (minibatch x in_channels x iH x iW)</span>
<span class="sd">        kernel_size: size of the pooling region, a single number or a</span>
<span class="sd">          tuple (kh x kw)</span>
<span class="sd">        stride: stride of the pooling operation, a single number or a</span>
<span class="sd">          tuple (sh x sw). Default is equal to kernel size</span>
<span class="sd">        padding: implicit zero padding on the input, a single number or</span>
<span class="sd">          a tuple (padh x padw), Default: 0</span>
<span class="sd">        ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="sd">            to compute the output shape</span>
<span class="sd">        count_include_pad: when True, will include the zero-padding in th</span>
<span class="sd">            averaging calculation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                           <span class="n">ceil_mode</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">)</span></div>


<div class="viewcode-block" id="avg_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies 3D average-pooling operation in kt x kh x kw regions by step</span>
<span class="sd">    size dt x dh x dw steps. The number of output features is equal to the</span>
<span class="sd">    number of input planes / dt.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span></div>


<span class="c1"># share the same interface</span>
<div class="viewcode-block" id="max_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                          <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="max_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                          <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="max_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_pool3d">[docs]</a><span class="k">def</span> <span class="nf">max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                          <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span> <span class="k">if</span> <span class="n">return_indices</span> <span class="k">else</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<span class="k">def</span> <span class="nf">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">default_size</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">default_size</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_size</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span>
                            <span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_size</span>

    <span class="n">output_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;output_size should be a sequence containing &quot;</span>
                         <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements, but it has a length of &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">min_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="n">max_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">min_size</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;invalid output_size &quot;</span><span class="si">{}</span><span class="s1">&quot; (dim </span><span class="si">{}</span><span class="s1"> must be between </span><span class="si">{}</span><span class="s1"> and </span><span class="si">{}</span><span class="s1">)&#39;</span>
                <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">min_size</span><span class="p">,</span> <span class="n">max_size</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">output_size</span>


<div class="viewcode-block" id="max_unpool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool1d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">output_size</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool2d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool3d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.lp_pool2d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">kw</span><span class="p">,</span> <span class="n">kh</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kw</span> <span class="o">*</span> <span class="n">kh</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_max_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">        return_indices: whether to return pooling indices</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveMaxPool1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_max_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_max_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool1d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<span class="c1"># Activation functions</span>

<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout">[docs]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">Dropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="alpha_dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.alpha_dropout">[docs]</a><span class="k">def</span> <span class="nf">alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies alpha dropout to the input.</span>

<span class="sd">    See :class:`~torch.nn.AlphaDropout` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float, optional): the drop probability</span>
<span class="sd">        training (bool, optional): switch between training and evaluation mode</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">training</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.7580993408473766</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span>
    <span class="c1"># TODO avoid casting to byte after resize</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">new</span><span class="p">()</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">bernoulli_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">noise</span><span class="o">.</span><span class="n">byte</span><span class="p">())</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">keep_prob</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">keep_prob</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">keep_prob</span><span class="p">))</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">b</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout2d">[docs]</a><span class="k">def</span> <span class="nf">dropout2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FeatureDropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="dropout3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout3d">[docs]</a><span class="k">def</span> <span class="nf">dropout3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">FeatureDropout</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="threshold"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.threshold">[docs]</a><span class="k">def</span> <span class="nf">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">Threshold</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu">[docs]</a><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">Threshold</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">glu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">ndim</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="n">ndim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="s2">&quot;dim </span><span class="si">{}</span><span class="s2"> is out of range for tensor of dimension </span><span class="si">{}</span><span class="s2">&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">ndim</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">+=</span> <span class="n">ndim</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">GatedLinear</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>


<div class="viewcode-block" id="hardtanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hardtanh">[docs]</a><span class="k">def</span> <span class="nf">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Hardtanh</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu6">[docs]</a><span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Hardtanh</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.elu">[docs]</a><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">ELU</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.selu">[docs]</a><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">SELU</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.leaky_relu">[docs]</a><span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.prelu">[docs]</a><span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">PReLU</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="rrelu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.rrelu">[docs]</a><span class="k">def</span> <span class="nf">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">8</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">RReLU</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="logsigmoid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.logsigmoid">[docs]</a><span class="k">def</span> <span class="nf">logsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hardshrink">[docs]</a><span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Hardshrink</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanhshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanhshrink">[docs]</a><span class="k">def</span> <span class="nf">tanhshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">_autograd_functions</span><span class="o">.</span><span class="n">Tanh</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softsign">[docs]</a><span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">activation</span><span class="o">.</span><span class="n">Softsign</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softplus"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softplus">[docs]</a><span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Softplus</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmin"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmin">[docs]</a><span class="k">def</span> <span class="nf">softmin</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Softmax</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="softshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softshrink">[docs]</a><span class="k">def</span> <span class="nf">softshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">auto</span><span class="o">.</span><span class="n">Softshrink</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.log_softmax">[docs]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanh">[docs]</a><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_autograd_functions</span><span class="o">.</span><span class="n">Tanh</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.sigmoid">[docs]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_autograd_functions</span><span class="o">.</span><span class="n">Sigmoid</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="c1"># etc.</span>

<div class="viewcode-block" id="linear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.linear">[docs]</a><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># fused op is marginally faster</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Bilinear</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Bilinear</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">,</span>
              <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A simple lookup table that looks up embeddings in a fixed dictionary and size.</span>

<span class="sd">    This module is often used to retrieve word embeddings using indices.</span>
<span class="sd">    The input to the module is a list of indices, and the embedding matrix,</span>
<span class="sd">    and the output is the corresponding word embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: tensor, containing indices into the embedding matrix</span>
<span class="sd">        embedding_matrix:</span>
<span class="sd">                Number of rows should correspond to the maximum possible index + 1,</span>
<span class="sd">                number of columns is the embedding size</span>
<span class="sd">        max_norm (float, optional): If given, will renormalize the embeddings to always have a norm lesser than this</span>
<span class="sd">        norm_type (float, optional): The p of the p-norm to compute for the max_norm option</span>
<span class="sd">        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the frequency of</span>
<span class="sd">                                                the words in the mini-batch.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: LongTensor `(N, W)`, N = mini-batch, W = number of indices to extract per mini-batch</span>
<span class="sd">        - Embedding_matrix: FloatTensor `(V, embedding_dim)`, V = maximum index + 1, embedding_dim = embedding size</span>
<span class="sd">        - Output: `(N, W, embedding_dim)`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">        &gt;&gt;&gt; input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))</span>
<span class="sd">        &gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = Variable(torch.rand(10, 3))</span>
<span class="sd">        &gt;&gt;&gt; torch.nn.functional.embedding(input, embedding_matrix)</span>

<span class="sd">        Variable containing:</span>
<span class="sd">        (0 ,.,.) =</span>
<span class="sd">         -1.0822  1.2522  0.2434</span>
<span class="sd">          0.8393 -0.6062 -0.3348</span>
<span class="sd">          0.6597  0.0350  0.0837</span>
<span class="sd">          0.5521  0.9447  0.0498</span>

<span class="sd">        (1 ,.,.) =</span>
<span class="sd">          0.6597  0.0350  0.0837</span>
<span class="sd">         -0.1527  0.0877  0.4260</span>
<span class="sd">          0.8393 -0.6062 -0.3348</span>
<span class="sd">         -0.8738 -0.9054  0.4281</span>
<span class="sd">        [torch.FloatTensor of size 2x4x3]</span>

<span class="sd">        &gt;&gt;&gt; # example with padding_idx</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = Variable(torch.rand(10, 3))</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix[0].zero_()</span>
<span class="sd">        &gt;&gt;&gt; input = Variable(torch.LongTensor([[0,2,0,5]]))</span>
<span class="sd">        &gt;&gt;&gt; torch.nn.functional.embedding(input, embedding_matrix)</span>

<span class="sd">        Variable containing:</span>
<span class="sd">        (0 ,.,.) =</span>
<span class="sd">          0.0000  0.0000  0.0000</span>
<span class="sd">          0.3452  0.4937 -0.9361</span>
<span class="sd">          0.0000  0.0000  0.0000</span>
<span class="sd">          0.0706 -2.1962 -0.6276</span>
<span class="sd">        [torch.FloatTensor of size 1x4x3]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">,</span>
        <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="n">sparse</span>
    <span class="p">)</span>


<div class="viewcode-block" id="batch_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.batch_norm">[docs]</a><span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_functions</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="c1"># loss</span>

<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.nll_loss">[docs]</a><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.NLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: :math:`(N, C)` where `C = number of classes` or `(N, C, H, W)`</span>
<span class="sd">            in case of 2D - Loss</span>
<span class="sd">        target: :math:`(N)` where each value is `0 &lt;= targets[i] &lt;= C-1`</span>
<span class="sd">        weight (Variable, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, has to be a Variable of size &quot;nclasses&quot;</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">            over observations for each minibatch. If size_average</span>
<span class="sd">            is False, the losses are summed for each minibatch.</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When size_average is</span>
<span class="sd">            True, the loss is averaged over non-ignored targets.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # input is of size nBatch x nClasses = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = autograd.Variable(torch.randn(3, 5))</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; nclasses</span>
<span class="sd">        &gt;&gt;&gt; target = autograd.Variable(torch.LongTensor([1, 0, 4]))</span>
<span class="sd">        &gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">NLLLoss2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 2 or 4 dimensions (got </span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span></div>


<div class="viewcode-block" id="poisson_nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.poisson_nll_loss">[docs]</a><span class="k">def</span> <span class="nf">poisson_nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Poisson negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.PoissonNLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: expectation of underlying Poisson distribution.</span>
<span class="sd">        target: random sample :math:`target \sim Pois(input)`.</span>
<span class="sd">        log_input: if True the loss is computed as</span>
<span class="sd">            `exp(input) - target * input`, if False then loss is</span>
<span class="sd">            `input - target * log(input)`.</span>
<span class="sd">        full: whether to compute full loss, i. e. to add the Stirling</span>
<span class="sd">            approximation term</span>
<span class="sd">            `target * log(target) - target + 0.5 * log(2 * pi * target)`.</span>
<span class="sd">        size_average: By default, the losses are averaged over observations for</span>
<span class="sd">            each minibatch. However, if the field sizeAverage is set to False,</span>
<span class="sd">            the losses are instead summed for each minibatch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">log_input</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="nb">input</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">full</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="n">loss</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">target</span><span class="p">))[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">size_average</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span></div>


<div class="viewcode-block" id="kl_div"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.kl_div">[docs]</a><span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The `Kullback-Leibler divergence`_ Loss.</span>

<span class="sd">    See :class:`~torch.nn.KLDivLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable of arbitrary shape</span>
<span class="sd">        target: Variable of the same shape as input</span>
<span class="sd">        size_average: if True the output is divided by the number of elements</span>
<span class="sd">          in input tensor</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">                class. If given, has to be a Tensor of size &quot;nclasses&quot;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines `log_softmax` and `nll_loss` in a single</span>
<span class="sd">    function.</span>

<span class="sd">    See :class:`torch.nn.CrossEntropyLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable :math:`(N, C)` where `C = number of classes`</span>
<span class="sd">        target: Variable :math:`(N)` where each value is</span>
<span class="sd">            `0 &lt;= targets[i] &lt;= C-1`</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">                class. If given, has to be a Tensor of size &quot;nclasses&quot;</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch.</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">                and does not contribute to the input gradient. When size_average is</span>
<span class="sd">                True, the loss is averaged over non-ignored targets.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures the Binary Cross Entropy</span>
<span class="sd">    between the target and the output:</span>

<span class="sd">    See :class:`~torch.nn.BCELoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable of arbitrary shape</span>
<span class="sd">        target: Variable of the same shape as input</span>
<span class="sd">        weight (Variable, optional): a manual rescaling weight</span>
<span class="sd">                if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">target</span><span class="o">.</span><span class="n">is_same_size</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Using a target size (</span><span class="si">{}</span><span class="s2">) that is different to the input size (</span><span class="si">{}</span><span class="s2">) is deprecated. &quot;</span>
                      <span class="s2">&quot;Please ensure they have the same size.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="o">!=</span> <span class="n">target</span><span class="o">.</span><span class="n">nelement</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target and input must have the same number of elements. target nelement (</span><span class="si">{}</span><span class="s2">) &quot;</span>
                         <span class="s2">&quot;!= input nelement (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">nelement</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">nelement</span><span class="p">()))</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_size</span> <span class="o">=</span> <span class="n">_infer_size</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">new_size</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">BCELoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy_with_logits"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy_with_logits">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures Binary Cross Entropy between target and output</span>
<span class="sd">    logits:</span>

<span class="sd">    See :class:`~torch.nn.BCEWithLogitsLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Variable of arbitrary shape</span>
<span class="sd">        target: Variable of the same shape as input</span>
<span class="sd">        weight (Variable, optional): a manual rescaling weight</span>
<span class="sd">                if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): By default, the losses are averaged</span>
<span class="sd">                over observations for each minibatch. However, if the field</span>
<span class="sd">                sizeAverage is set to False, the losses are instead summed</span>
<span class="sd">                for each minibatch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">target</span><span class="o">.</span><span class="n">is_same_size</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target size (</span><span class="si">{}</span><span class="s2">) must be the same as input size (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

    <span class="n">max_val</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="n">max_val</span> <span class="o">+</span> <span class="p">((</span><span class="o">-</span><span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">weight</span>

    <span class="k">if</span> <span class="n">size_average</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span></div>


<div class="viewcode-block" id="smooth_l1_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.smooth_l1_loss">[docs]</a><span class="k">def</span> <span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="l1_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.l1_loss">[docs]</a><span class="k">def</span> <span class="nf">l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">L1Loss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="mse_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.mse_loss">[docs]</a><span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MSELoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="margin_ranking_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.margin_ranking_loss">[docs]</a><span class="k">def</span> <span class="nf">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="hinge_embedding_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hinge_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">HingeEmbeddingLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="multilabel_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multilabel_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MultiLabelMarginLoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="soft_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">SoftMarginLoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="multilabel_soft_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multilabel_soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosine_embedding_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cosine_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">CosineEmbeddingLoss</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="n">size_average</span><span class="p">)(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="multi_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multi_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;only p == 1 and p == 2 supported&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;weight must be one-dimensional&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">MultiMarginLoss</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="p">)</span></div>


<div class="viewcode-block" id="pixel_shuffle"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pixel_shuffle">[docs]</a><span class="k">def</span> <span class="nf">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Rearranges elements in a tensor of shape ``[*, C*r^2, H, W]`` to a</span>
<span class="sd">    tensor of shape ``[C, H*r, W*r]``.</span>

<span class="sd">    See :class:`~torch.nn.PixelShuffle` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): Input</span>
<span class="sd">        upscale_factor (int): factor to increase spatial resolution by</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; ps = nn.PixelShuffle(3)</span>
<span class="sd">        &gt;&gt;&gt; input = autograd.Variable(torch.Tensor(1, 9, 4, 4))</span>
<span class="sd">        &gt;&gt;&gt; output = ps(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output.size())</span>
<span class="sd">        torch.Size([1, 1, 12, 12])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">channels</span> <span class="o">//=</span> <span class="n">upscale_factor</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="n">out_height</span> <span class="o">=</span> <span class="n">in_height</span> <span class="o">*</span> <span class="n">upscale_factor</span>
    <span class="n">out_width</span> <span class="o">=</span> <span class="n">in_width</span> <span class="o">*</span> <span class="n">upscale_factor</span>

    <span class="n">input_view</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span> <span class="n">upscale_factor</span><span class="p">,</span>
        <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span><span class="p">)</span>

    <span class="n">shuffle_out</span> <span class="o">=</span> <span class="n">input_view</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">shuffle_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">out_height</span><span class="p">,</span> <span class="n">out_width</span><span class="p">)</span></div>


<div class="viewcode-block" id="upsample"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample">[docs]</a><span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Upsamples the input to either the given :attr:`size` or the given</span>
<span class="sd">    :attr:`scale_factor`</span>

<span class="sd">    The algorithm used for upsampling is determined by :attr:`mode`.</span>

<span class="sd">    Currently spatial and volumetric upsampling are supported, i.e.</span>
<span class="sd">    expected inputs are 4-D or 5-D in shape.</span>

<span class="sd">    The input dimensions are interpreted in the form:</span>
<span class="sd">    `mini-batch x channels x [depth] x height x width`</span>

<span class="sd">    The modes available for upsampling are: `nearest`, `bilinear` (4D-only),</span>
<span class="sd">    `trilinear` (5D-only)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        size (int or Tuple[int, int] or Tuple[int, int, int]):</span>
<span class="sd">            output spatial size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">        mode (string): algorithm used for upsampling:</span>
<span class="sd">            &#39;nearest&#39; | &#39;bilinear&#39; | &#39;trilinear&#39;. Default: &#39;nearest&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">_pair</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingNearest3d</span><span class="p">(</span><span class="n">_triple</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">_pair</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">UpsamplingTrilinear3d</span><span class="p">(</span><span class="n">_triple</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">scale_factor</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 4D and 5D input Tensors supported&quot;</span>
                                  <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">D) for the modes: nearest | bilinear | trilinear&quot;</span>
                                  <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">mode</span><span class="p">))</span></div>


<div class="viewcode-block" id="upsample_nearest"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample_nearest">[docs]</a><span class="k">def</span> <span class="nf">upsample_nearest</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Upsamples the input, using nearest neighbours&#39; pixel values.</span>

<span class="sd">    **Note:: This function is deprecated. Use nn.functional.upsample instead**</span>

<span class="sd">    Currently spatial and volumetric upsampling are supported (i.e. expected</span>
<span class="sd">    inputs are 4 or 5 dimensional).</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia</span>
<span class="sd">            size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_nearest is deprecated. Use nn.functional.upsample instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="upsample_bilinear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample_bilinear">[docs]</a><span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Upscales the input, using bilinear upsampling.</span>

<span class="sd">    **Note:: This function is deprecated. Use nn.functional.upsample instead**</span>

<span class="sd">    Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo</span>
<span class="sd">    volumetric (5 dimensional) inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input</span>
<span class="sd">        size (int or Tuple[int, int]): output spatial size.</span>
<span class="sd">        scale_factor (int or Tuple[int, int]): multiplier for spatial size</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_bilinear is deprecated. Use nn.functional.upsample instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="grid_sample"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.grid_sample">[docs]</a><span class="k">def</span> <span class="nf">grid_sample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Given an :attr:`input` and a flow-field :attr:`grid`, computes the</span>
<span class="sd">    `output` using input pixel locations from the grid.</span>

<span class="sd">    Uses bilinear interpolation to sample the input pixels.</span>
<span class="sd">    Currently, only spatial (4 dimensional) inputs are supported.</span>

<span class="sd">    For each output location, :attr:`grid` has `x` and `y`</span>
<span class="sd">    input pixel locations which are used to compute output.</span>

<span class="sd">    :attr:`grid` has values in the range of `[-1, 1]`. This is because the</span>
<span class="sd">    pixel locations are normalized by the input height and width.</span>

<span class="sd">    For example, values: x: -1, y: -1 is the left-top pixel of the input</span>
<span class="sd">                 values: x: 1, y: 1 is the right-bottom pixel of the input</span>

<span class="sd">    If :attr:`grid` has values outside the range of `[-1, 1]`, those locations</span>
<span class="sd">    are ignored (i.e. 0 is used as a contribution to the bilinear interpolation)</span>

<span class="sd">    .. Note:: This function is used in building Spatial Transformer Networks</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): input batch of images (N x C x IH x IW)</span>
<span class="sd">        grid (Variable): flow-field of size (N x OH x OW x 2)</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Variable): output Tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">GridSampler</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span></div>


<div class="viewcode-block" id="affine_grid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.affine_grid">[docs]</a><span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates a 2d flow field, given a batch of affine matrices :attr:`theta`</span>
<span class="sd">    Generally used in conjunction with :func:`grid_sample` to</span>
<span class="sd">    implement Spatial Transformer Networks.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta (Variable): input batch of affine matrices (N x 2 x 3)</span>
<span class="sd">        size (torch.Size): the target output image size (N x C x H x W)</span>
<span class="sd">                           Example: torch.Size(32, 3, 24, 24)</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Variable): output Tensor of size (N x H x W x 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">AffineGridGenerator</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pad">[docs]</a><span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pads tensor.</span>

<span class="sd">    Currently only 2D and 3D padding supported.</span>
<span class="sd">    In case of 4D input tensor pad should be in form</span>
<span class="sd">    (pad_l, pad_r, pad_t, pad_b ).</span>
<span class="sd">    In case of 5D pad should be (pleft, pright, ptop, pbottom, pfront, pback)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Variable): 4D or 5D tensor</span>
<span class="sd">        pad (tuple): 4-elem or 6-elem tuple</span>
<span class="sd">        mode: &#39;constant&#39;, &#39;reflect&#39; or &#39;replicate&#39;</span>
<span class="sd">        value: fill value for &#39;constant&#39; padding</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;4D tensors expect 4 values for padding&#39;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ConstantPad2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">pad</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">pad</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;5D tensors expect 6 values for padding&#39;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_functions</span><span class="o">.</span><span class="n">thnn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">pad</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only 4D and 5D padding is supported for now&quot;</span><span class="p">)</span></div>


<span class="c1"># distance</span>

<div class="viewcode-block" id="pairwise_distance"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pairwise_distance">[docs]</a><span class="k">def</span> <span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the batchwise pairwise distance between vectors v1,v2:</span>

<span class="sd">        .. math ::</span>
<span class="sd">            \Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}</span>

<span class="sd">        Args:</span>
<span class="sd">            x1: first input tensor</span>
<span class="sd">            x2: second input tensor</span>
<span class="sd">            p: the norm degree. Default: 2</span>

<span class="sd">        Shape:</span>
<span class="sd">            - Input: :math:`(N, D)` where `D = vector dimension`</span>
<span class="sd">            - Output: :math:`(N, 1)`</span>

<span class="sd">        &gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; output = F.pairwise_distance(input1, input2, p=2)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">x1</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">x2</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">x1</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Input must be a 2D matrix.&quot;</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">diff</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">p</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosine_similarity"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cosine_similarity">[docs]</a><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns cosine similarity between x1 and x2, computed along dim.</span>

<span class="sd">    .. math ::</span>
<span class="sd">        \text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Variable): First input.</span>
<span class="sd">        x2 (Variable): Second input (of size matching x1).</span>
<span class="sd">        dim (int, optional): Dimension of vectors. Default: 1</span>
<span class="sd">        eps (float, optional): Small value to avoid division by zero.</span>
<span class="sd">            Default: 1e-8</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(\ast_1, D, \ast_2)` where D is at position `dim`.</span>
<span class="sd">        - Output: :math:`(\ast_1, \ast_2)` where 1 is at position `dim`.</span>

<span class="sd">    &gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">    &gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">    &gt;&gt;&gt; output = F.cosine_similarity(input1, input2)</span>
<span class="sd">    &gt;&gt;&gt; print(output)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w12</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">w12</span> <span class="o">/</span> <span class="p">(</span><span class="n">w1</span> <span class="o">*</span> <span class="n">w2</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span></div>


<div class="viewcode-block" id="triplet_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.triplet_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors x1, x2, x3 and a margin with a value greater than 0.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n`: anchor, positive examples and negative</span>
<span class="sd">    example respectively. The shape of all input variables should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow</span>
<span class="sd">    convolutional feature descriptors with triplet losses`_ by</span>
<span class="sd">    V. Balntas, E. Riba et al.</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \frac{1}{N} \left( \sum_{i=1}^N \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\} \right)</span>

<span class="sd">    where :math:`d(x_i, y_i) = \| {\bf x}_i - {\bf y}_i \|_2^2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        anchor: anchor input tensor</span>
<span class="sd">        positive: positive input tensor</span>
<span class="sd">        negative: negative input tensor</span>
<span class="sd">        p: the norm degree. Default: 2</span>
<span class="sd">        eps: small epsilon value to avoid numerical issues</span>
<span class="sd">        swap: compute distance swap</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, D)` where `D = vector dimension`</span>
<span class="sd">        - Output: :math:`(N, 1)`</span>

<span class="sd">        &gt;&gt;&gt; input1 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input2 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; input3 = autograd.Variable(torch.randn(100, 128))</span>
<span class="sd">        &gt;&gt;&gt; output = F.triplet_margin_loss(input1, input2, input3, p=2)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">    .. _Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">positive</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between positive and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">negative</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between anchor and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">positive</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">negative</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="s2">&quot;Input sizes between positive and negative must be equal.&quot;</span>
    <span class="k">assert</span> <span class="n">anchor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Inputd must be a 2D matrix.&quot;</span>
    <span class="k">assert</span> <span class="n">margin</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;Margin should be positive value.&#39;</span>
    <span class="n">d_p</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">d_n</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">swap</span><span class="p">:</span>
        <span class="n">d_s</span> <span class="o">=</span> <span class="n">pairwise_distance</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">d_n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">d_n</span><span class="p">,</span> <span class="n">d_s</span><span class="p">)</span>

    <span class="n">dist_hinge</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">margin</span> <span class="o">+</span> <span class="n">d_p</span> <span class="o">-</span> <span class="n">d_n</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dist_hinge</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="normalize"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.normalize">[docs]</a><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Performs :math:`L_p` normalization of inputs over specified dimension.</span>

<span class="sd">    Does:</span>

<span class="sd">    .. math::</span>
<span class="sd">        v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}</span>

<span class="sd">    for each subtensor v over dimension dim of input. Each subtensor is</span>
<span class="sd">    flattened into a vector, i.e. :math:`\lVert v \rVert_p` is not a matrix</span>
<span class="sd">    norm.</span>

<span class="sd">    With default arguments normalizes over the second dimension with Euclidean</span>
<span class="sd">    norm.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of any shape</span>
<span class="sd">        p (float): the exponent value in the norm formulation</span>
<span class="sd">        dim (int): the dimension to reduce</span>
<span class="sd">        eps (float): small value to avoid division by zero</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>
</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>

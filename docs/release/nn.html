


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/nn.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.optim" href="optim.html" />
    <link rel="prev" title="torch.Storage" href="storage.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.0.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_deprecated.html">torch.distributed.deprecated</a></li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.nn</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/nn.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="module-torch.nn">
<span id="torch-nn"></span><h1>torch.nn<a class="headerlink" href="#module-torch.nn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="parameters">
<h2>Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch.nn.Parameter">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Parameter</code><a class="reference internal" href="_modules/torch/nn/parameter.html#Parameter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>A kind of Tensor that is to be considered a module parameter.</p>
<p>Parameters are <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> subclasses, that have a
very special property when used with <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> s - when they’re
assigned as Module attributes they are automatically added to the list of
its parameters, and will appear e.g. in <a class="reference internal" href="#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parameters()</span></code></a> iterator.
Assigning a Tensor doesn’t have such effect. This is because one might
want to cache some temporary state, like last hidden state of the RNN, in
the model. If there was no such class as <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>, these
temporaries would get registered too.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – parameter tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if the parameter requires gradient. See
<a class="reference internal" href="notes/autograd.html#excluding-subgraphs"><span class="std std-ref">Excluding subgraphs from backward</span></a> for more details. Default: <cite>True</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="containers">
<h2>Containers<a class="headerlink" href="#containers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module">
<h3><span class="hidden-section">Module</span><a class="headerlink" href="#module" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Module">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Module</code><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
       <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
       <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call <a class="reference internal" href="#torch.nn.Module.to" title="torch.nn.Module.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code></a>, etc.</p>
<dl class="method">
<dt id="torch.nn.Module.add_module">
<code class="descname">add_module</code><span class="sig-paren">(</span><em>name</em>, <em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.add_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</li>
<li><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – child module to be added to the module.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.apply">
<code class="descname">apply</code><span class="sig-paren">(</span><em>fn</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.apply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">torch-nn-init</span>).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>fn</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> -&gt; None) – function to be applied to each submodule</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="go">        print(m)</span>
<span class="go">        if type(m) == nn.Linear:</span>
<span class="go">            m.weight.data.fill_(1.0)</span>
<span class="go">            print(m.weight)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.buffers">
<code class="descname">buffers</code><span class="sig-paren">(</span><em>recurse=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.buffers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</td>
</tr>
<tr class="field-even field"><th class="field-name">Yields:</th><td class="field-body"><em>torch.Tensor</em> – module buffer</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.children">
<code class="descname">children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.children"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>Module</em> – a child module</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.cpu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.cuda"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.double"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="torch.nn.Module.dump_patches">
<code class="descname">dump_patches</code><em class="property"> = False</em><a class="headerlink" href="#torch.nn.Module.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.eval">
<code class="descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <a class="reference internal" href="#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.extra_repr">
<code class="descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.extra_repr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.float"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to float datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>*input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.half"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.load_state_dict">
<code class="descname">load_state_dict</code><span class="sig-paren">(</span><em>state_dict</em>, <em>strict=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – a dict containing parameters and
persistent buffers.</li>
<li><strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.modules">
<code class="descname">modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>Module</em> – a module in the network</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear (2 -&gt; 2)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_buffers">
<code class="descname">named_buffers</code><span class="sig-paren">(</span><em>prefix=''</em>, <em>recurse=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_buffers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – prefix to prepend to all buffer names.</li>
<li><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Yields:</th><td class="field-body"><p class="first last"><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_children">
<code class="descname">named_children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_children"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>(string, Module)</em> – Tuple containing a name and child module</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_modules">
<code class="descname">named_modules</code><span class="sig-paren">(</span><em>memo=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Yields:</th><td class="field-body"><em>(string, Module)</em> – Tuple of name and module</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear (2 -&gt; 2))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_parameters">
<code class="descname">named_parameters</code><span class="sig-paren">(</span><em>prefix=''</em>, <em>recurse=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.named_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – prefix to prepend to all parameter names.</li>
<li><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Yields:</th><td class="field-body"><p class="first last"><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><em>recurse=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</td>
</tr>
<tr class="field-even field"><th class="field-name">Yields:</th><td class="field-body"><em>Parameter</em> – module parameter</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_backward_hook">
<code class="descname">register_backward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_backward_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> may be tuples if the
module has multiple inputs or outputs. The hook should not modify its
arguments, but it can optionally return a new gradient with respect to
input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in subsequent
computations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">The current implementation will not have the presented behavior
for complex <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> that perform many operations.
In some failure cases, <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will only
contain the gradients for a subset of the inputs and outputs.
For such <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a>, you should use <a class="reference internal" href="autograd.html#torch.Tensor.register_hook" title="torch.Tensor.register_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.register_hook()</span></code></a>
directly on a specific input or output to get the required gradients.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_buffer">
<code class="descname">register_buffer</code><span class="sig-paren">(</span><em>name</em>, <em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_buffer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a persistent buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the persistent state.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</li>
<li><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – buffer to be registered.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_hook">
<code class="descname">register_forward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_forward_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input or output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_pre_hook">
<code class="descname">register_forward_pre_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_parameter">
<code class="descname">register_parameter</code><span class="sig-paren">(</span><em>name</em>, <em>param</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.register_parameter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</li>
<li><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>Parameter</em></a>) – parameter to be added to the module.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.state_dict">
<code class="descname">state_dict</code><span class="sig-paren">(</span><em>destination=None</em>, <em>prefix=''</em>, <em>keep_vars=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a dictionary containing a whole state of the module</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)">dict</a></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.to">
<code class="descname">to</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>device=None</em>, <em>dtype=None</em>, <em>non_blocking=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a></dt>
<dd></dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>dtype</em>, <em>non_blocking=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a></dt>
<dd></dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>tensor</em>, <em>non_blocking=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a></dt>
<dd></dd></dl>

<p>Its signature is similar to <a class="reference internal" href="tensors.html#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code></a>, but only accepts
floating point desired <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> s. In addition, this method will
only cast the floating point parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>
(if given). The integral parameters and buffers will be moved
<code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This method modifies the module in-place.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – the desired device of the parameters
and buffers in this module</li>
<li><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – the desired floating point type of
the floating point parameters and buffers in this module</li>
<li><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">self</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>mode=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <a class="reference internal" href="#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>dst_type</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.type"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dst_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a><em> or </em><em>string</em>) – the desired type</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">self</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.zero_grad">
<code class="descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/module.html#Module.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Module.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sequential">
<h3><span class="hidden-section">Sequential</span><a class="headerlink" href="#sequential" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Sequential">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sequential</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#Sequential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, here is a small example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example of using Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

<span class="c1"># Example of using Sequential with OrderedDict</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
          <span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
          <span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="p">]))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="modulelist">
<h3><span class="hidden-section">ModuleList</span><a class="headerlink" href="#modulelist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ModuleList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ModuleList</code><span class="sig-paren">(</span><em>modules=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a list.</p>
<p>ModuleList can be indexed like a regular Python list, but modules it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>iterable</em><em>, </em><em>optional</em>) – an iterable of modules to add</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ModuleList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ModuleList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given module to the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>modules</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList.extend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends modules from a Python iterable to the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>iterable</em>) – iterable of modules to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleList.insert">
<code class="descname">insert</code><span class="sig-paren">(</span><em>index</em>, <em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleList.insert"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleList.insert" title="Permalink to this definition">¶</a></dt>
<dd><p>Insert a given module before a given index in the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – index to insert.</li>
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module to insert</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="moduledict">
<h3><span class="hidden-section">ModuleDict</span><a class="headerlink" href="#moduledict" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ModuleDict">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ModuleDict</code><span class="sig-paren">(</span><em>modules=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a dictionary.</p>
<p>ModuleDict can be indexed like a regular Python dictionary, but modules it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>iterable</em><em>, </em><em>optional</em>) – a mapping (dictionary) of (string: module)
or an iterable of key/value pairs of type (string, module)</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">choices</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span>
                <span class="s1">&#39;conv&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                <span class="s1">&#39;pool&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">([</span>
                <span class="p">[</span><span class="s1">&#39;lrelu&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">()],</span>
                <span class="p">[</span><span class="s1">&#39;prelu&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()]</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">choice</span><span class="p">,</span> <span class="n">act</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="n">choice</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activations</span><span class="p">[</span><span class="n">act</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ModuleDict.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.clear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all items from the ModuleDict.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.items">
<code class="descname">items</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.items"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.items" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict key/value pairs.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.keys">
<code class="descname">keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.keys"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict keys.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.pop">
<code class="descname">pop</code><span class="sig-paren">(</span><em>key</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.pop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove key from the ModuleDict and return its module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>key</strong> (<em>string</em>) – key to pop from the ModuleDict</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>modules</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the ModuleDict with the key/value pairs from a mapping or
an iterable, overwriting existing keys.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<em>iterable</em>) – a mapping (dictionary) of (string: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module`</span></code>) or
an iterable of key/value pairs of type (string, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module`</span></code>)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.values">
<code class="descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ModuleDict.values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ModuleDict.values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict values.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="parameterlist">
<h3><span class="hidden-section">ParameterList</span><a class="headerlink" href="#parameterlist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ParameterList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ParameterList</code><span class="sig-paren">(</span><em>parameters=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds parameters in a list.</p>
<p>ParameterList can be indexed like a regular Python list, but parameters it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>iterable</em><em>, </em><em>optional</em>) – an iterable of <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter`</span></code> to add</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ParameterList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ParameterList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>parameter</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given parameter at the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameter</strong> (<a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><em>nn.Parameter</em></a>) – parameter to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterList.extend"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends parameters from a Python iterable to the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>iterable</em>) – iterable of parameters to append</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="parameterdict">
<h3><span class="hidden-section">ParameterDict</span><a class="headerlink" href="#parameterdict" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ParameterDict">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ParameterDict</code><span class="sig-paren">(</span><em>parameters=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds parameters in a dictionary.</p>
<p>ParameterDict can be indexed like a regular Python dictionary, but parameters it
contains are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>iterable</em><em>, </em><em>optional</em>) – a mapping (dictionary) of
(string : <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>) or an iterable of key,value pairs
of type (string, <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>)</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">({</span>
                <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
                <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        <span class="p">})</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">choice</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ParameterDict.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.clear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all items from the ParameterDict.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.items">
<code class="descname">items</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.items"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.items" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict key/value pairs.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.keys">
<code class="descname">keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.keys"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict keys.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.pop">
<code class="descname">pop</code><span class="sig-paren">(</span><em>key</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.pop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove key from the ParameterDict and return its parameter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>key</strong> (<em>string</em>) – key to pop from the ParameterDict</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the ParameterDict with the key/value pairs from a mapping or
an iterable, overwriting existing keys.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>iterable</em>) – a mapping (dictionary) of
(string : <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>) or an iterable of
key/value pairs of type (string, <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.values">
<code class="descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/container.html#ParameterDict.values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ParameterDict.values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict values.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="convolution-layers">
<h2>Convolution layers<a class="headerlink" href="#convolution-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="conv1d">
<h3><span class="hidden-section">Conv1d</span><a class="headerlink" href="#conv1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math">\((N, C_{\text{in}}, L)\)</span> and output <span class="math">\((N, C_{\text{out}}, L_{\text{out}})\)</span> can be
precisely described as:</p>
<div class="math">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
\star \text{input}(N_i, k)

\]</div>
<p>where <span class="math">\(\star\)</span> is the valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math">\(N\)</span> is a batch size, <span class="math">\(C\)</span> denotes a number of channels,
<span class="math">\(L\)</span> is a length of signal sequence.</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a one-element tuple.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters,
of size
<span class="math">\(\left\lfloor\frac{C_\text{out}}{C_\text{in}}\right\rfloor\)</span></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid
<a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also termed in
literature as depthwise convolution.</p>
<p class="last">In other words, for an input of size <span class="math">\((N, C_{in}, L_{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite>, can be constructed by arguments
<span class="math">\((C_\text{in}=C_{in}, C_\text{out}=C_{in} \times K, ..., \text{groups}=C_{in})\)</span>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C_{in}, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C_{out}, L_{out})\)</span> where</p>
<div class="math">
\[L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
          \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(out_channels, in_channels, kernel_size). The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \text{kernel\_size}}\)</span></li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape
(out_channels). If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \text{kernel\_size}}\)</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv2d">
<h3><span class="hidden-section">Conv2d</span><a class="headerlink" href="#conv2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math">\((N, C_{\text{in}}, H, W)\)</span> and output <span class="math">\((N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)</span>
can be precisely described as:</p>
<div class="math">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)

\]</div>
<p>where <span class="math">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math">\(N\)</span> is a batch size, <span class="math">\(C\)</span> denotes a number of channels,
<span class="math">\(H\)</span> is a height of input planes in pixels, and <span class="math">\(W\)</span> is
width in pixels.</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters, of size:
<span class="math">\(\left\lfloor\frac{C_\text{out}}{C_\text{in}}\right\rfloor\)</span>.</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also termed in
literature as depthwise convolution.</p>
<p class="last">In other words, for an input of size <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite>, can be constructed by arguments
<span class="math">\((in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})\)</span>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to both sides of the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
          \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
          \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(out_channels, in_channels, kernel_size[0], kernel_size[1]).
The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}\)</span></li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels). If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}\)</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv3d">
<h3><span class="hidden-section">Conv3d</span><a class="headerlink" href="#conv3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Conv3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#Conv3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C_{in}, D, H, W)\)</span>
and output <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[out(N_i, C_{out_j}) = bias(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)

\]</div>
<p>where <span class="math">\(\star\)</span> is the valid 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters, of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>.</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also termed in
literature as depthwise convolution.</p>
<blockquote class="last">
<div>In other words, for an input of size <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite>, can be constructed by arguments
<span class="math">\((in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})\)</span>.</div></blockquote>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to all three sides of the input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
      \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
      \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
      \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])
The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels). If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose1d">
<h3><span class="hidden-section">ConvTranspose1d</span><a class="headerlink" href="#convtranspose1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv1d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of the output shape. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C_{in}, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C_{out}, L_{out})\)</span> where</p>
<div class="math">
\[L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding}
      + \text{kernel\_size} + \text{output\_padding}

\]</div>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1]). The values
of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \text{kernel\_size}}\)</span></li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels).
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \text{kernel\_size}}\)</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="convtranspose2d">
<h3><span class="hidden-section">ConvTranspose2d</span><a class="headerlink" href="#convtranspose2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimensions</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of each dimension in the output shape. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> where</li>
</ul>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0]
      + \text{kernel\_size}[0] + \text{output\_padding}[0]

\]</div>
<div class="last math">
\[W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1]
      + \text{kernel\_size}[1] + \text{output\_padding}[1]

\]</div>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1])
The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}\)</span></li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}\)</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># exact output size can be also specified as an argument</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 6, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose3d">
<h3><span class="hidden-section">ConvTranspose3d</span><a class="headerlink" href="#convtranspose3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConvTranspose3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/conv.html#ConvTranspose3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image composed of several input
planes.
The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.</p>
<p>This module can be seen as the gradient of Conv3d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li>At groups=1, all inputs are convolved to all outputs.</li>
<li>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</li>
<li>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>).</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimensions</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of each dimension in the output shape. Default: 0</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where</li>
</ul>
<div class="math">
\[D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0]
      + \text{kernel\_size}[0] + \text{output\_padding}[0]

\]</div>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1]
      + \text{kernel\_size}[1] + \text{output\_padding}[1]

\]</div>
<div class="last math">
\[W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2]
      + \text{kernel\_size}[2] + \text{output\_padding}[2]

\]</div>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])
The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></li>
<li><strong>bias</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="unfold">
<h3><span class="hidden-section">Unfold</span><a class="headerlink" href="#unfold" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Unfold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Unfold</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/fold.html#Unfold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts sliding local blocks from a batched input tensor.</p>
<p>Consider an batched <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor of shape <span class="math">\((N, C, *)\)</span>,
where <span class="math">\(N\)</span> is the batch dimension, <span class="math">\(C\)</span> is the channel dimension,
and <span class="math">\(*\)</span> represent arbitrary spatial dimensions. This operation flattens
each sliding <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>-sized block within the spatial dimensions
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into a column (i.e., last dimension) of a 3-D <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code>
tensor of shape <span class="math">\((N, C \times \prod(\text{kernel\_size}), L)\)</span>, where
<span class="math">\(C \times \prod(\text{kernel\_size})\)</span> is the total number of values
with in each block (a block has <span class="math">\(\prod(\text{kernel\_size})\)</span> spatial
locations each containing a <span class="math">\(C\)</span>-channeled vector), and <span class="math">\(L\)</span> is
the total number of such blocks:</p>
<div class="math">
\[L = \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times \text{padding}[d] %
    - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,

\]</div>
<p>where <span class="math">\(\text{spatial\_size}\)</span> is formed by the spatial dimensions
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (<span class="math">\(*\)</span> above), and <span class="math">\(d\)</span> is over all spatial
dimensions.</p>
<p>Therefore, indexing <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code> at the last dimension (column dimension)
gives all values within a certain block.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> arguments specify
how the sliding blocks are retrieved.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the sliding blocks.</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension before
reshaping.</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the sliding blocks</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the stride of the sliding blocks in the input
spatial dimensions. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – implicit zero padding to be added on
both sides of input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a parameter that controls the
stride of elements within the
neighborhood. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> or
<code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> is an int or a tuple of length 1, their values will be
replicated across all spatial dimensions.</li>
<li>For the case of two input spatial dimensions this operation is sometimes
called <code class="docutils literal notranslate"><span class="pre">im2col</span></code>.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Fold</span></code></a> calculates each combined value in the resulting
large tensor by summing all values from all containing blocks.
<a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a> extracts the values in the local blocks by
copying from the large tensor. So, if the blocks overlap, they are not
inverses of each other.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Currently, only 4-D input tensors (batched image-like tensors) are
supported.</p>
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, *)\)</span></li>
<li>Output: <span class="math">\((N, C \times \prod(\text{kernel\_size}), L)\)</span> as described above</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unfold</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Unfold</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">unfold</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each patch contains 30 values (2x3=6 vectors, each of 5 channels)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 4 blocks (2x3 kernels) in total in the 3x4 input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 30, 4])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp_unf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_unf</span> <span class="o">=</span> <span class="n">inp_unf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">())</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">fold</span><span class="p">(</span><span class="n">out_unf</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or equivalently (and avoiding a copy),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># out = out_unf.view(1, 2, 7, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="go">tensor(1.9073e-06)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fold">
<h3><span class="hidden-section">Fold</span><a class="headerlink" href="#fold" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Fold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Fold</code><span class="sig-paren">(</span><em>output_size</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/fold.html#Fold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines an array of sliding local blocks into a large containing
tensor.</p>
<p>Consider a batched <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor containing sliding local blocks,
e.g., patches of images, of shape <span class="math">\((N, C \times  \prod(\text{kernel\_size}), L)\)</span>,
where <span class="math">\(N\)</span> is batch dimension, <span class="math">\(C \times \prod(\text{kernel\_size})\)</span>
is the number of values with in a block (a block has <span class="math">\(\prod(\text{kernel\_size})\)</span>
spatial locations each containing a <span class="math">\(C\)</span>-channeled vector), and
<span class="math">\(L\)</span> is the total number of blocks. (This is exacly the
same specification as the output shape of <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a>.) This
operation combines these local blocks into the large <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code> tensor
of shape <span class="math">\((N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)\)</span>
by summing the overlapping values. Similar to <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a>, the
arguments must satisfy</p>
<div class="math">
\[L = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] %
    - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,

\]</div>
<p>where <span class="math">\(d\)</span> is over all spatial dimensions.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> describes the spatial shape of the large containing
tensor of the sliding local blocks. It is useful to resolve the ambiguity
when multiple input shapes map to same number of sliding blocks, e.g.,
with <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>.</li>
</ul>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> arguments specify
how the sliding blocks are retrieved.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the sliding blocks.</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension before
reshaping.</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the shape of the spatial dimensions of the
output (i.e., <code class="docutils literal notranslate"><span class="pre">input.sizes()[2:]</span></code>)</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the sliding blocks</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the stride of the sliding blocks in the input
spatial dimensions. Default: 1</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – implicit zero padding to be added on
both sides of input. Default: 0</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a parameter that controls the
stride of elements within the
neighborhood. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<ul class="simple">
<li>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code>,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> or <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> is an int or a tuple of length 1 then
their values will be replicated across all spatial dimensions.</li>
<li>For the case of two output spatial dimensions this operation is sometimes
called <code class="docutils literal notranslate"><span class="pre">col2im</span></code>.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Fold</span></code></a> calculates each combined value in the resulting
large tensor by summing all values from all containing blocks.
<a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a> extracts the values in the local blocks by
copying from the large tensor. So, if the blocks overlap, they are not
inverses of each other.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Currently, only 4-D output tensors (batched image-like tensors) are
supported.</p>
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C \times \prod(\text{kernel\_size}), L)\)</span></li>
<li>Output: <span class="math">\((N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)\)</span> as described above</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fold</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Fold</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">fold</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-layers">
<h2>Pooling layers<a class="headerlink" href="#pooling-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="maxpool1d">
<h3><span class="hidden-section">MaxPool1d</span><a class="headerlink" href="#maxpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, L)\)</span>
and output <span class="math">\((N, C, L_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[out(N_i, C_j, k) = \max_{m=0, \ldots, \text{kernel\_size} - 1}
        input(N_i, C_j, stride \times k + m)

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window to take a max over</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> – a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool1d</span></code></a> later</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, L_{out})\)</span>, where</p>
<div class="math">
\[L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation}
      \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool2d">
<h3><span class="hidden-section">MaxPool2d</span><a class="headerlink" href="#maxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, H, W)\)</span>,
output <span class="math">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{aligned}
    out(N_i, C_j, h, w) ={} & \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                            & \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
                                           \text{stride[1]} \times w + n)
\end{aligned}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window to take a max over</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> – a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool2d</span></code></a> later</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}
      \times (\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}
      \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool3d">
<h3><span class="hidden-section">MaxPool3d</span><a class="headerlink" href="#maxpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes. This is not a test</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, D, H, W)\)</span>,
output <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{aligned}
    \text{out}(N_i, C_j, d, h, w) ={} & \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                      & \text{input}(N_i, C_j, \text{stride[0]} \times k + d,
                                                     \text{stride[1]} \times h + m, \text{stride[2]} \times w + n)
\end{aligned}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window to take a max over</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on all three sides</li>
<li><strong>dilation</strong> – a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool3d</span></code></a> later</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times
  (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times
  (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times
  (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool1d">
<h3><span class="hidden-section">MaxUnpool1d</span><a class="headerlink" href="#maxunpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool1d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the forward call.
See the Inputs and Example below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a></li>
<li><cite>output_size</cite> (optional): the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, H_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, H_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]

\]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example showcasing the use of output_size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool2d">
<h3><span class="hidden-section">MaxUnpool2d</span><a class="headerlink" href="#maxunpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool2d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the forward call.
See the Inputs and Example below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a></li>
<li><cite>output_size</cite> (optional): the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}

\]</div>
<div class="math">
\[W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}

\]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
<span class="go">                            [ 5,  6,  7,  8],</span>
<span class="go">                            [ 9, 10, 11, 12],</span>
<span class="go">                            [13, 14, 15, 16]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[[  0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,   6.,   0.,   8.],</span>
<span class="go">          [  0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,  14.,   0.,  16.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># specify a different output size than input size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="go">tensor([[[[  0.,   0.,   0.,   0.,   0.],</span>
<span class="go">          [  6.,   0.,   8.,   0.,   0.],</span>
<span class="go">          [  0.,   0.,   0.,  14.,   0.],</span>
<span class="go">          [ 16.,   0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,   0.,   0.,   0.,   0.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool3d">
<h3><span class="hidden-section">MaxUnpool3d</span><a class="headerlink" href="#maxunpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MaxUnpool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#MaxUnpool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MaxUnpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> is not fully invertible, since the non-maximal values are lost.
<a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool3d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the forward call.
See the Inputs section below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a></li>
<li><cite>output_size</cite> (optional): the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[D_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}

\]</div>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}

\]</div>
<div class="math">
\[W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}

\]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([20, 16, 51, 33, 15])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool1d">
<h3><span class="hidden-section">AvgPool1d</span><a class="headerlink" href="#avgpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, L)\)</span>,
output <span class="math">\((N, C, L_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\(k\)</span>
can be precisely described as:</p>
<div class="math">
\[\text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k}
                       \text{input}(N_i, C_j, \text{stride} \times l + m)\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> can each be
an <code class="docutils literal notranslate"><span class="pre">int</span></code> or a one-element tuple.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, L_{out})\)</span>, where</p>
<div class="math">
\[L_{out} = \left\lfloor \frac{L_{in} +
2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool with window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]]))</span>
<span class="go">tensor([[[ 2.,  4.,  6.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool2d">
<h3><span class="hidden-section">AvgPool2d</span><a class="headerlink" href="#avgpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, H, W)\)</span>,
output <span class="math">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] -
  \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] -
  \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool3d">
<h3><span class="hidden-section">AvgPool3d</span><a class="headerlink" href="#avgpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AvgPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, D, H, W)\)</span>,
output <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{aligned}
    \text{out}(N_i, C_j, d, h, w) ={} & \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\
                                      & \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k,
                                              \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)}
                                             {kD \times kH \times kW}
\end{aligned}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on all three sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero padding to be added on all three sides</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] -
      \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] -
      \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] -
      \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fractionalmaxpool2d">
<h3><span class="hidden-section">FractionalMaxPool2d</span><a class="headerlink" href="#fractionalmaxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.FractionalMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">FractionalMaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>output_size=None</em>, <em>output_ratio=None</em>, <em>return_indices=False</em>, <em>_random_samples=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#FractionalMaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.FractionalMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D fractional max pooling over an input signal composed of several input planes.</p>
<p>Fractional MaxPooling is described in detail in the paper <a class="reference external" href="http://arxiv.org/abs/1412.6071">Fractional MaxPooling</a> by Ben Graham</p>
<p>The max-pooling operation is applied in <span class="math">\(kHxkW\)</span> regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window to take a max over.
Can be a single number k (for a square kernel of k x k) or a tuple <cite>(kh x kw)</cite></li>
<li><strong>output_size</strong> – the target output size of the image of the form <cite>oH x oW</cite>.
Can be a tuple <cite>(oH, oW)</cite> or a single number oH for a square image <cite>oH x oH</cite></li>
<li><strong>output_ratio</strong> – If one wants to have an output size as a ratio of the input size, this option can be given.
This has to be a number or tuple in the range (0, 1)</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to <code class="xref py py-meth docutils literal notranslate"><span class="pre">nn.MaxUnpool2d()</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, and target output size 13x12</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window and target output size being half of input image size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lppool1d">
<h3><span class="hidden-section">LPPool1d</span><a class="headerlink" href="#lppool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LPPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LPPool1d</code><span class="sig-paren">(</span><em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#LPPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LPPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D power-average pooling over an input signal composed of several input
planes.</p>
<p>On each window, the function computed is:</p>
<div class="math">
\[f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}

\]</div>
<ul class="simple">
<li>At p = infinity, one gets Max Pooling</li>
<li>At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the sum to the power of <cite>p</cite> is zero, the gradient of this function is
not defined. This implementation will set the gradient to zero in this case.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – a single int, the size of the window</li>
<li><strong>stride</strong> – a single int, the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, L_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, L_{out})\)</span>, where</p>
<div class="math">
\[L_{out} = \left\lfloor\frac{L_{in} +
2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
<dt>Examples::</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># power-2 pool of window of length 3, with stride 2.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="lppool2d">
<h3><span class="hidden-section">LPPool2d</span><a class="headerlink" href="#lppool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LPPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LPPool2d</code><span class="sig-paren">(</span><em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#LPPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LPPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of several input
planes.</p>
<p>On each window, the function computed is:</p>
<div class="math">
\[f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}

\]</div>
<ul class="simple">
<li>At p = <span class="math">\(\infty\)</span>, one gets Max Pooling</li>
<li>At p = 1, one gets Sum Pooling (which is proportional to average pooling)</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the sum to the power of <cite>p</cite> is zero, the gradient of this function is
not defined. This implementation will set the gradient to zero in this case.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> – the size of the window</li>
<li><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0] \times
      (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1] \times
      (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># power-2 pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window of power 1.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool1d">
<h3><span class="hidden-section">AdaptiveMaxPool1d</span><a class="headerlink" href="#adaptivemaxpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool1d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size H</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool1d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool2d">
<h3><span class="hidden-section">AdaptiveMaxPool2d</span><a class="headerlink" href="#adaptivemaxpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool2d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H.
H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool2d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 10x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool3d">
<h3><span class="hidden-section">AdaptiveMaxPool3d</span><a class="headerlink" href="#adaptivemaxpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool3d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveMaxPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveMaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size of the image of the form D x H x W.
Can be a tuple (D, H, W) or a single D for a cube D x D x D.
D, H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</li>
<li><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool3d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7x9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7x7 (cube)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x9x8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool1d">
<h3><span class="hidden-section">AdaptiveAvgPool1d</span><a class="headerlink" href="#adaptiveavgpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool1d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size H</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool2d">
<h3><span class="hidden-section">AdaptiveAvgPool2d</span><a class="headerlink" href="#adaptiveavgpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool2d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H
H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 10x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool3d">
<h3><span class="hidden-section">AdaptiveAvgPool3d</span><a class="headerlink" href="#adaptiveavgpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool3d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pooling.html#AdaptiveAvgPool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveAvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size of the form D x H x W.
Can be a tuple (D, H, W) or a single number D for a cube D x D x D
D, H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7x9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7x7 (cube)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x9x8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="padding-layers">
<h2>Padding layers<a class="headerlink" href="#padding-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="reflectionpad1d">
<h3><span class="hidden-section">ReflectionPad1d</span><a class="headerlink" href="#reflectionpad1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReflectionPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReflectionPad1d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReflectionPad1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReflectionPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using the reflection of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 2-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, W_{out})\)</span> where
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[0., 1., 2., 3.],</span>
<span class="go">         [4., 5., 6., 7.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],</span>
<span class="go">         [6., 5., 4., 5., 6., 7., 6., 5.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],</span>
<span class="go">         [6., 5., 4., 5., 6., 7., 6., 5.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],</span>
<span class="go">         [7., 6., 5., 4., 5., 6., 7., 6.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="reflectionpad2d">
<h3><span class="hidden-section">ReflectionPad2d</span><a class="headerlink" href="#reflectionpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReflectionPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReflectionPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReflectionPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReflectionPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using the reflection of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math">\(\text{padding\_left}\)</span>,
<span class="math">\(\text{padding\_right}\)</span>, <span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last">
<li><p class="first">Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p>
</li>
<li><p class="first">Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</p>
<p><span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span>
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[0., 1., 2.],</span>
<span class="go">          [3., 4., 5.],</span>
<span class="go">          [6., 7., 8.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[8., 7., 6., 7., 8., 7., 6.],</span>
<span class="go">          [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="go">          [2., 1., 0., 1., 2., 1., 0.],</span>
<span class="go">          [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="go">          [8., 7., 6., 7., 8., 7., 6.],</span>
<span class="go">          [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="go">          [2., 1., 0., 1., 2., 1., 0.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[7., 6., 7., 8., 7.],</span>
<span class="go">          [4., 3., 4., 5., 4.],</span>
<span class="go">          [1., 0., 1., 2., 1.],</span>
<span class="go">          [4., 3., 4., 5., 4.],</span>
<span class="go">          [7., 6., 7., 8., 7.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad1d">
<h3><span class="hidden-section">ReplicationPad1d</span><a class="headerlink" href="#replicationpad1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReplicationPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad1d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReplicationPad1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReplicationPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 2-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, W_{out})\)</span> where
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[0., 1., 2., 3.],</span>
<span class="go">         [4., 5., 6., 7.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],</span>
<span class="go">         [4., 4., 4., 5., 6., 7., 7., 7.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],</span>
<span class="go">         [4., 4., 4., 4., 5., 6., 7., 7.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad2d">
<h3><span class="hidden-section">ReplicationPad2d</span><a class="headerlink" href="#replicationpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReplicationPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReplicationPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReplicationPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math">\(\text{padding\_left}\)</span>,
<span class="math">\(\text{padding\_right}\)</span>, <span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span>
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[0., 1., 2.],</span>
<span class="go">          [3., 4., 5.],</span>
<span class="go">          [6., 7., 8.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[0., 0., 0., 1., 2., 2., 2.],</span>
<span class="go">          [0., 0., 0., 1., 2., 2., 2.],</span>
<span class="go">          [0., 0., 0., 1., 2., 2., 2.],</span>
<span class="go">          [3., 3., 3., 4., 5., 5., 5.],</span>
<span class="go">          [6., 6., 6., 7., 8., 8., 8.],</span>
<span class="go">          [6., 6., 6., 7., 8., 8., 8.],</span>
<span class="go">          [6., 6., 6., 7., 8., 8., 8.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[0., 0., 1., 2., 2.],</span>
<span class="go">          [0., 0., 1., 2., 2.],</span>
<span class="go">          [0., 0., 1., 2., 2.],</span>
<span class="go">          [3., 3., 4., 5., 5.],</span>
<span class="go">          [6., 6., 7., 8., 8.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad3d">
<h3><span class="hidden-section">ReplicationPad3d</span><a class="headerlink" href="#replicationpad3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReplicationPad3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad3d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ReplicationPad3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReplicationPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 6-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>,
<span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>,
<span class="math">\(\text{padding\_front}\)</span>, <span class="math">\(\text{padding\_back}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}\)</span>
<span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span>
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">480</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="zeropad2d">
<h3><span class="hidden-section">ZeroPad2d</span><a class="headerlink" href="#zeropad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ZeroPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ZeroPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ZeroPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ZeroPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with zero.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math">\(\text{padding\_left}\)</span>,
<span class="math">\(\text{padding\_right}\)</span>, <span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span>
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[-0.1678, -0.4418,  1.9466],</span>
<span class="go">          [ 0.9604, -0.4219, -0.5241],</span>
<span class="go">          [-0.9162, -0.5436, -0.6446]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],</span>
<span class="go">          [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],</span>
<span class="go">          [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad1d">
<h3><span class="hidden-section">ConstantPad1d</span><a class="headerlink" href="#constantpad1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConstantPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad1d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ConstantPad1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConstantPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in both boundaries. If a 2-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, W_{out})\)</span> where
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],</span>
<span class="go">         [-1.3287,  1.8966,  0.1466, -0.2771]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,</span>
<span class="go">           3.5000],</span>
<span class="go">         [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,</span>
<span class="go">           3.5000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[ 1.6616,  1.4523, -1.1255],</span>
<span class="go">         [-3.6372,  0.1182, -1.8652]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad2d">
<h3><span class="hidden-section">ConstantPad2d</span><a class="headerlink" href="#constantpad2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConstantPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad2d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ConstantPad2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConstantPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math">\(\text{padding\_left}\)</span>,
<span class="math">\(\text{padding\_right}\)</span>, <span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span>
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[ 1.6585,  0.4320],</span>
<span class="go">         [-0.8701, -0.4649]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad3d">
<h3><span class="hidden-section">ConstantPad3d</span><a class="headerlink" href="#constantpad3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ConstantPad3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad3d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/padding.html#ConstantPad3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ConstantPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 6-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>,
<span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>,
<span class="math">\(\text{padding\_front}\)</span>, <span class="math">\(\text{padding\_back}\)</span>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}\)</span>
<span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span>
<span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activations-weighted-sum-nonlinearity">
<h2>Non-linear activations (weighted sum, nonlinearity)<a class="headerlink" href="#non-linear-activations-weighted-sum-nonlinearity" title="Permalink to this headline">¶</a></h2>
<div class="section" id="elu">
<h3><span class="hidden-section">ELU</span><a class="headerlink" href="#elu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ELU</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>alpha</strong> – the <span class="math">\(\alpha\)</span> value for the ELU formulation. Default: 1.0</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/ELU.png" src="_images/ELU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hardshrink">
<h3><span class="hidden-section">Hardshrink</span><a class="headerlink" href="#hardshrink" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Hardshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Hardshrink</code><span class="sig-paren">(</span><em>lambd=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Hardshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hard shrinkage function element-wise:</p>
<div class="math">
\[\text{HardShrink}(x) =
\begin{cases}
x, & \text{ if } x > \lambda \\
x, & \text{ if } x < -\lambda \\
0, & \text{ otherwise }
\end{cases}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>lambd</strong> – the <span class="math">\(\lambda\)</span> value for the Hardshrink formulation. Default: 0.5</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Hardshrink.png" src="_images/Hardshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hardtanh">
<h3><span class="hidden-section">Hardtanh</span><a class="headerlink" href="#hardtanh" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Hardtanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Hardtanh</code><span class="sig-paren">(</span><em>min_val=-1.0</em>, <em>max_val=1.0</em>, <em>inplace=False</em>, <em>min_value=None</em>, <em>max_value=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Hardtanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise</p>
<p>HardTanh is defined as:</p>
<div class="math">
\[\text{HardTanh}(x) = \begin{cases}
    1 & \text{ if } x > 1 \\
    -1 & \text{ if } x < -1 \\
    x & \text{ otherwise } \\
\end{cases}

\]</div>
<p>The range of the linear region <span class="math">\([-1, 1]\)</span> can be adjusted using
<code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code>.</p>
<img alt="_images/Hardtanh.png" src="_images/Hardtanh.png" />
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>min_val</strong> – minimum value of the linear region range. Default: -1</li>
<li><strong>max_val</strong> – maximum value of the linear region range. Default: 1</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Keyword arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_value</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_value</span></code>
have been deprecated in favor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code>.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="leakyrelu">
<h3><span class="hidden-section">LeakyReLU</span><a class="headerlink" href="#leakyrelu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LeakyReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LeakyReLU</code><span class="sig-paren">(</span><em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LeakyReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)

\]</div>
<p>or</p>
<div class="math">
\[\text{LeakyRELU}(x) =
\begin{cases}
x, & \text{ if } x \geq 0 \\
\text{negative\_slope} \times x, & \text{ otherwise }
\end{cases}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>negative_slope</strong> – Controls the angle of the negative slope. Default: 1e-2</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/LeakyReLU.png" src="_images/LeakyReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsigmoid">
<h3><span class="hidden-section">LogSigmoid</span><a class="headerlink" href="#logsigmoid" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LogSigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSigmoid</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LogSigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/LogSigmoid.png" src="_images/LogSigmoid.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="prelu">
<h3><span class="hidden-section">PReLU</span><a class="headerlink" href="#prelu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PReLU</code><span class="sig-paren">(</span><em>num_parameters=1</em>, <em>init=0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#PReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{PReLU}(x) = \max(0,x) + a * \min(0,x)

\]</div>
<p>or</p>
<div class="math">
\[\text{PReLU}(x) =
\begin{cases}
x, & \text{ if } x \geq 0 \\
ax, & \text{ otherwise }
\end{cases}

\]</div>
<p>Here <span class="math">\(a\)</span> is a learnable parameter. When called without arguments, <cite>nn.PReLU()</cite> uses a single
parameter <span class="math">\(a\)</span> across all input channels. If called with <cite>nn.PReLU(nChannels)</cite>,
a separate <span class="math">\(a\)</span> is used for each input channel.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">weight decay should not be used when learning <span class="math">\(a\)</span> for good performance.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is
no channel dim and the number of channels = 1.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_parameters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of <span class="math">\(a\)</span> to learn.
Although it takes an int as input, there is only two values are legitimate:
1, or the number of channels at input. Default: 1</li>
<li><strong>init</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the initial value of <span class="math">\(a\)</span>. Default: 0.25</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of shape (attr:<cite>num_parameters</cite>).
The attr:<cite>dtype</cite> is default to</td>
</tr>
</tbody>
</table>
<img alt="_images/PReLU.png" src="_images/PReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="relu">
<h3><span class="hidden-section">ReLU</span><a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise
<span class="math">\(\text{ReLU}(x)= \max(0, x)\)</span></p>
<img alt="_images/ReLU.png" src="_images/ReLU.png" />
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="relu6">
<h3><span class="hidden-section">ReLU6</span><a class="headerlink" href="#relu6" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.ReLU6">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU6</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#ReLU6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{ReLU6}(x) = \min(\max(0,x), 6)

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/ReLU6.png" src="_images/ReLU6.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="rrelu">
<h3><span class="hidden-section">RReLU</span><a class="headerlink" href="#rrelu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.RReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RReLU</code><span class="sig-paren">(</span><em>lower=0.125</em>, <em>upper=0.3333333333333333</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#RReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.RReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the randomized leaky rectified liner unit function, element-wise,
as described in the paper:</p>
<p><a class="reference external" href="https://arxiv.org/abs/1505.00853">Empirical Evaluation of Rectified Activations in Convolutional Network</a>.</p>
<p>The function is defined as:</p>
<div class="math">
\[\text{RReLU}(x) =
\begin{cases}
    x & \text{if } x \geq 0 \\
    ax & \text{ otherwise }
\end{cases}

\]</div>
<p>where <span class="math">\(a\)</span> is randomly sampled from uniform distribution
<span class="math">\(\mathcal{U}(\text{lower}, \text{upper})\)</span>.</p>
<blockquote>
<div>See: <a class="reference external" href="https://arxiv.org/pdf/1505.00853.pdf">https://arxiv.org/pdf/1505.00853.pdf</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>lower</strong> – lower bound of the uniform distribution. Default: <span class="math">\(\frac{1}{8}\)</span></li>
<li><strong>upper</strong> – upper bound of the uniform distribution. Default: <span class="math">\(\frac{1}{3}\)</span></li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="selu">
<h3><span class="hidden-section">SELU</span><a class="headerlink" href="#selu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SELU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#SELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applied element-wise, as:</p>
<div class="math">
\[\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))

\]</div>
<p>with <span class="math">\(\alpha = 1.6732632423543772848170429916717\)</span> and
<span class="math">\(\text{scale} = 1.0507009873554804934193349852946\)</span>.</p>
<img alt="_images/SELU.png" src="_images/SELU.png" />
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="celu">
<h3><span class="hidden-section">CELU</span><a class="headerlink" href="#celu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CELU</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#CELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))

\]</div>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>alpha</strong> – the <span class="math">\(\alpha\)</span> value for the CELU formulation. Default: 1.0</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/CELU.png" src="_images/CELU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="sigmoid">
<h3><span class="hidden-section">Sigmoid</span><a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Sigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sigmoid</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}

\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Sigmoid.png" src="_images/Sigmoid.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softplus">
<h3><span class="hidden-section">Softplus</span><a class="headerlink" href="#softplus" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softplus">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softplus</code><span class="sig-paren">(</span><em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softplus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))

\]</div>
<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.</p>
<p>For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> – the <span class="math">\(\beta\)</span> value for the Softplus formulation. Default: 1</li>
<li><strong>threshold</strong> – values above this revert to a linear function. Default: 20</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Softplus.png" src="_images/Softplus.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softshrink">
<h3><span class="hidden-section">Softshrink</span><a class="headerlink" href="#softshrink" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softshrink</code><span class="sig-paren">(</span><em>lambd=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise:</p>
<div class="math">
\[\text{SoftShrinkage}(x) =
\begin{cases}
x - \lambda, & \text{ if } x > \lambda \\
x + \lambda, & \text{ if } x < -\lambda \\
0, & \text{ otherwise }
\end{cases}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>lambd</strong> – the <span class="math">\(\lambda\)</span> value for the Softshrink formulation. Default: 0.5</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Softshrink.png" src="_images/Softshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softsign">
<h3><span class="hidden-section">Softsign</span><a class="headerlink" href="#softsign" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softsign">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softsign</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{SoftSign}(x) = \frac{x}{ 1 + |x|}

\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Softsign.png" src="_images/Softsign.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanh">
<h3><span class="hidden-section">Tanh</span><a class="headerlink" href="#tanh" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Tanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanh</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{Tanh}(x) = \tanh(x) = \frac{e^x - e^{-x}} {e^x + e^{-x}}

\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Tanh.png" src="_images/Tanh.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanhshrink">
<h3><span class="hidden-section">Tanhshrink</span><a class="headerlink" href="#tanhshrink" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Tanhshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanhshrink</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Tanhshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{Tanhshrink}(x) = x - \text{Tanh}(x)

\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<img alt="_images/Tanhshrink.png" src="_images/Tanhshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="threshold">
<h3><span class="hidden-section">Threshold</span><a class="headerlink" href="#threshold" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Threshold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Threshold</code><span class="sig-paren">(</span><em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor</p>
<p>Threshold is defined as:</p>
<div class="math">
\[y =
\begin{cases}
x, &\text{ if } x > \text{threshold} \\
\text{value}, &\text{ otherwise }
\end{cases}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>threshold</strong> – The value to threshold at</li>
<li><strong>value</strong> – The value to replace with</li>
<li><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activations-other">
<h2>Non-linear activations (other)<a class="headerlink" href="#non-linear-activations-other" title="Permalink to this headline">¶</a></h2>
<div class="section" id="softmin">
<h3><span class="hidden-section">Softmin</span><a class="headerlink" href="#softmin" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmin">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmin</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmin function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range <cite>(0, 1)</cite> and sum to 1</p>
<div class="math">
\[\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}

\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which Softmin will be computed (so every slice
along dim will sum to 1).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input, with
values in the range [0, 1]</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax">
<h3><span class="hidden-section">Softmax</span><a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmax function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1</p>
<p>Softmax is defined as:</p>
<div class="math">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}

\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</td>
</tr>
<tr class="field-even field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This module doesn’t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use <cite>LogSoftmax</cite> instead (it’s faster and has better numerical properties).</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax2d">
<h3><span class="hidden-section">Softmax2d</span><a class="headerlink" href="#softmax2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Softmax2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax2d</code><a class="reference internal" href="_modules/torch/nn/modules/activation.html#Softmax2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Softmax2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies SoftMax over features to each spatial location.</p>
<p>When given an image of <code class="docutils literal notranslate"><span class="pre">Channels</span> <span class="pre">x</span> <span class="pre">Height</span> <span class="pre">x</span> <span class="pre">Width</span></code>, it will
apply <cite>Softmax</cite> to each location <span class="math">\((Channels, h_i, w_j)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax2d</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># you softmax over the 2nd dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsoftmax">
<h3><span class="hidden-section">LogSoftmax</span><a class="headerlink" href="#logsoftmax" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LogSoftmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSoftmax</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/activation.html#LogSoftmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LogSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <span class="math">\(\log(\text{Softmax}(x))\)</span> function to an n-dimensional
input Tensor. The LogSoftmax formulation can be simplified as:</p>
<div class="math">
\[\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)

\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: any shape</li>
<li>Output: same as input</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [-inf, 0)</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivelogsoftmaxwithloss">
<h3><span class="hidden-section">AdaptiveLogSoftmaxWithLoss</span><a class="headerlink" href="#adaptivelogsoftmaxwithloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveLogSoftmaxWithLoss</code><span class="sig-paren">(</span><em>in_features</em>, <em>n_classes</em>, <em>cutoffs</em>, <em>div_value=4.0</em>, <em>head_bias=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Efficient softmax approximation as described in
<a class="reference external" href="https://arxiv.org/abs/1609.04309">Efficient softmax approximation for GPUs</a> by Edouard Grave, Armand Joulin,
Moustapha Cissé, David Grangier, and Hervé Jégou.</p>
<p>Adaptive softmax is an approximate strategy for training models with large
output spaces. It is most effective when the label distribution is highly
imbalanced, for example in natural language modelling, where the word
frequency distribution approximately follows the <a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipf’s law</a>.</p>
<p>Adaptive softmax partitions the labels into several clusters, according to
their frequency. These clusters may contain different number of targets
each.
Additionally, clusters containing less frequent labels assign lower
dimensional embeddings to those labels, which speeds up the computation.
For each minibatch, only clusters for which at least one target is
present are evaluated.</p>
<p>The idea is that the clusters which are accessed frequently
(like the first one, containing most frequent labels), should also be cheap
to compute – that is, contain a small number of assigned labels.</p>
<p>We highly recommend taking a look at the original paper for more details.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">cutoffs</span></code> should be an ordered Sequence of integers sorted
in the increasing order.
It controls number of clusters and the partitioning of targets into
clusters. For example setting <code class="docutils literal notranslate"><span class="pre">cutoffs</span> <span class="pre">=</span> <span class="pre">[10,</span> <span class="pre">100,</span> <span class="pre">1000]</span></code>
means that first <cite>10</cite> targets will be assigned
to the ‘head’ of the adaptive softmax, targets <cite>11, 12, …, 100</cite> will be
assigned to the first cluster, and targets <cite>101, 102, …, 1000</cite> will be
assigned to the second cluster, while targets
<cite>1001, 1002, …, n_classes - 1</cite> will be assigned
to the last, third cluster</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">div_value</span></code> is used to compute the size of each additional cluster,
which is given as
<span class="math">\(\left\lfloor\frac{in\_features}{div\_value^{idx}}\right\rfloor\)</span>,
where <span class="math">\(idx\)</span> is the cluster index (with clusters
for less frequent words having larger indices,
and indices starting from <span class="math">\(1\)</span>).</li>
<li><code class="xref py py-attr docutils literal notranslate"><span class="pre">head_bias</span></code> if set to True, adds a bias term to the ‘head’ of the
adaptive softmax. See paper for details. Set to False in the official
implementation.</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Labels passed as inputs to this module should be sorted accoridng to
their frequency. This means that the most frequent label should be
represented by the index <cite>0</cite>, and the least frequent
label should be represented by the index <cite>n_classes - 1</cite>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This module returns a <code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">output</span></code>
and <code class="docutils literal notranslate"><span class="pre">loss</span></code> fields. See further documentation for details.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To compute log-probabilities for all classes, the <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>
method can be used.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of features in the input tensor</li>
<li><strong>n_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of classes in the dataset.</li>
<li><strong>cutoffs</strong> (<em>Sequence</em>) – Cutoffs used to assign targets to their buckets.</li>
<li><strong>div_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – value used as an exponent to compute sizes
of the clusters. Default: 4.0</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><ul class="simple">
<li><strong>output</strong> is a Tensor of size <code class="docutils literal notranslate"><span class="pre">N</span></code> containing computed target
log probabilities for each example</li>
<li><strong>loss</strong> is a Scalar representing the computed negative
log likelihood loss</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">output</span></code> and <code class="docutils literal notranslate"><span class="pre">loss</span></code> fields</p>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>input: <span class="math">\((N, in\_features)\)</span></li>
<li>target: <span class="math">\((N)\)</span> where each value satisfies <span class="math">\(0 &lt;= target[i] &lt;= n\_classes\)</span></li>
<li>output: <span class="math">\((N)\)</span></li>
<li>loss: <code class="docutils literal notranslate"><span class="pre">Scalar</span></code></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.log_prob"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes log probabilities for all <span class="math">\(n\_classes\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a minibatch of examples</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">log-probabilities of for each class <span class="math">\(c\)</span>
in range <span class="math">\(0 &lt;= c &lt;= n\_classes\)</span>, where <span class="math">\(n\_classes\)</span> is a
parameter passed to <code class="docutils literal notranslate"><span class="pre">AdaptiveLogSoftmaxWithLoss</span></code> constructor.</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, in\_features)\)</span></li>
<li>Output: <span class="math">\((N, n\_classes)\)</span></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/adaptive.html#AdaptiveLogSoftmaxWithLoss.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>This is equivalent to <cite>self.log_pob(input).argmax(dim=1)</cite>,
but is more efficient in some cases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a minibatch of examples</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a class with the highest probability for each example</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">output (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, in\_features)\)</span></li>
<li>Output: <span class="math">\((N)\)</span></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="normalization-layers">
<h2>Normalization layers<a class="headerlink" href="#normalization-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="batchnorm1d">
<h3><span class="hidden-section">BatchNorm1d</span><a class="headerlink" href="#batchnorm1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math">\(\gamma\)</span> are sampled
from <span class="math">\(\mathcal{U}(0, 1)\)</span> and the elements of <span class="math">\(\beta\)</span> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, L)</cite> slices, it’s common terminology to call this Temporal Batch Normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, L)\)</span> or <span class="math">\(L\)</span> from input of size <span class="math">\((N, L)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C)\)</span> or <span class="math">\((N, C, L)\)</span></li>
<li>Output: <span class="math">\((N, C)\)</span> or <span class="math">\((N, C, L)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm2d">
<h3><span class="hidden-section">BatchNorm2d</span><a class="headerlink" href="#batchnorm2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math">\(\gamma\)</span> are sampled
from <span class="math">\(\mathcal{U}(0, 1)\)</span> and the elements of <span class="math">\(\beta\)</span> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, H, W)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm3d">
<h3><span class="hidden-section">BatchNorm3d</span><a class="headerlink" href="#batchnorm3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BatchNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/batchnorm.html#BatchNorm3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math">\(\gamma\)</span> are sampled
from <span class="math">\(\mathcal{U}(0, 1)\)</span> and the elements of <span class="math">\(\beta\)</span> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, D, H, W)</cite> slices, it’s common terminology to call this Volumetric Batch Normalization
or Spatio-temporal Batch Normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, D, H, W)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="groupnorm">
<h3><span class="hidden-section">GroupNorm</span><a class="headerlink" href="#groupnorm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.GroupNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GroupNorm</code><span class="sig-paren">(</span><em>num_groups</em>, <em>num_channels</em>, <em>eps=1e-05</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/normalization.html#GroupNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.GroupNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Group Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1803.08494">Group Normalization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

\]</div>
<p>The input channels are separated into <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_groups</span></code> groups, each containing
<code class="docutils literal notranslate"><span class="pre">num_channels</span> <span class="pre">/</span> <span class="pre">num_groups</span></code> channels. The mean and standard-deviation are calculated
separately over the each group. <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable
per-channel affine transform parameter vectorss of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_channels</span></code> if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of groups to separate the channels into</li>
<li><strong>num_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of channels expected in input</li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-channel affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, num\_channels, *)\)</span></li>
<li>Output: <span class="math">\((N, num\_channels, *)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 3 groups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 6 groups (equivalent with InstanceNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Put all 6 channels into a single group (equivalent with LayerNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activating the module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm1d">
<h3><span class="hidden-section">InstanceNorm1d</span><a class="headerlink" href="#instancenorm1d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> and <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> is applied
on each channel of channeled data like multidimensional time series, but
<a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionaly, <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, L)\)</span> or <span class="math">\(L\)</span> from input of size <span class="math">\((N, L)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, L)\)</span></li>
<li>Output: <span class="math">\((N, C, L)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm2d">
<h3><span class="hidden-section">InstanceNorm2d</span><a class="headerlink" href="#instancenorm2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> and <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> is applied
on each channel of channeled data like RGB images, but
<a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionaly, <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, H, W)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm3d">
<h3><span class="hidden-section">InstanceNorm3d</span><a class="headerlink" href="#instancenorm3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.InstanceNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/instancenorm.html#InstanceNorm3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.InstanceNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size C (where C is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> and <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> is applied
on each channel of channeled data like 3D models with RGB color, but
<a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionaly, <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, D, H, W)\)</span></li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="layernorm">
<h3><span class="hidden-section">LayerNorm</span><a class="headerlink" href="#layernorm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LayerNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LayerNorm</code><span class="sig-paren">(</span><em>normalized_shape</em>, <em>eps=1e-05</em>, <em>elementwise_affine=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/normalization.html#LayerNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

\]</div>
<p>The mean and standard-deviation are calculated separately over the last
certain number dimensions which have to be of the shape specified by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code>.
<span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable affine transform parameters of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Unlike Batch Normalization and Instance Normalization, which applies
scalar scale and bias for each entire channel/plane with the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> option, Layer Normalization applies per-element scale and
bias with <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code>.</p>
</div>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>normalized_shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em> or </em><em>torch.Size</em>) – <p>input shape from an expected input
of size</p>
<div class="math">
\[[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1]
    \times \ldots \times \text{normalized\_shape}[-1]]

\]</div>
<p>If a single integer is used, it is treated as a singleton list, and this module will
normalize over the last dimension which is expected to be of that specific size.</p>
</li>
<li><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>elementwise_affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-element affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span></li>
<li>Output: <span class="math">\((N, *)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Normalize over last two dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Normalize over last dimension of size 10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activating the module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="localresponsenorm">
<h3><span class="hidden-section">LocalResponseNorm</span><a class="headerlink" href="#localresponsenorm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LocalResponseNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LocalResponseNorm</code><span class="sig-paren">(</span><em>size</em>, <em>alpha=0.0001</em>, <em>beta=0.75</em>, <em>k=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/normalization.html#LocalResponseNorm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LocalResponseNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies local response normalization over an input signal composed
of several input planes, where channels occupy the second dimension.
Applies normalization across channels.</p>
<div class="math">
\[b_{c} = a_{c}\left(k + \frac{\alpha}{n}
\sum_{c'=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c'}^2\right)^{-\beta}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> – amount of neighbouring channels used for normalization</li>
<li><strong>alpha</strong> – multiplicative factor. Default: 0.0001</li>
<li><strong>beta</strong> – exponent. Default: 0.75</li>
<li><strong>k</strong> – additive factor. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, ...)\)</span></li>
<li>Output: <span class="math">\((N, C, ...)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lrn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LocalResponseNorm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">signal_2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">signal_4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_2d</span> <span class="o">=</span> <span class="n">lrn</span><span class="p">(</span><span class="n">signal_2d</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_4d</span> <span class="o">=</span> <span class="n">lrn</span><span class="p">(</span><span class="n">signal_4d</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="recurrent-layers">
<h2>Recurrent layers<a class="headerlink" href="#recurrent-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="rnn">
<h3><span class="hidden-section">RNN</span><a class="headerlink" href="#rnn" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.RNN">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNN</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#RNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.RNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer Elman RNN with <span class="math">\(tanh\)</span> or <span class="math">\(ReLU\)</span> non-linearity to an
input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[h_t = \text{tanh}(w_{ih} x_t + b_{ih} + w_{hh} h_{(t-1)} + b_{hh})

\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is
the input at time <cite>t</cite>, and <span class="math">\(h_{(t-1)}\)</span> is the hidden state of the
previous layer at time <cite>t-1</cite> or the initial hidden state at time <cite>0</cite>.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">nonlinearity</span></code> is <cite>‘relu’</cite>, then <cite>ReLU</cite> is used instead of <cite>tanh</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two RNNs together to form a <cite>stacked RNN</cite>,
with the second RNN taking in outputs of the first RNN and
computing the final results. Default: 1</li>
<li><strong>nonlinearity</strong> – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’</li>
<li><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as <cite>(batch, seq, feature)</cite>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
RNN layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</li>
<li><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional RNN. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, h_0</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
or <a class="reference internal" href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided. If the RNN is bidirectional,
num_directions should be 2, else it should be 1.</li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt>
<dd><ul class="first last">
<li><p class="first"><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features (<cite>h_k</cite>) from the last layer of the RNN,
for each <cite>k</cite>.  If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has
been given as the input, the output will also be a packed sequence.</p>
<p>For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.
Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p class="first"><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the hidden state for <cite>k = seq_len</cite>.</p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code>.</p>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> – the learnable input-hidden weights of the k-th layer,
of shape <cite>(hidden_size * input_size)</cite> for <cite>k = 0</cite>. Otherwise, the shape is
<cite>(hidden_size * hidden_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the k-th layer,
of shape <cite>(hidden_size * hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> – the learnable input-hidden bias of the k-th layer,
of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the k-th layer,
of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the following conditions are satisfied:
1) cudnn is enabled,
2) input data is on the GPU
3) input data has dtype <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>
4) V100 GPU is used,
5) input data is not in <code class="docutils literal notranslate"><span class="pre">PackedSequence</span></code> format
persistent algorithm can be selected to improve performance.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstm">
<h3><span class="hidden-section">LSTM</span><a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LSTM">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#LSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input
sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[\begin{array}{ll} \\
    i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
    f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
    g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\
    o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
    c_t = f_t c_{(t-1)} + i_t g_t \\
    h_t = o_t \tanh(c_t) \\
\end{array}

\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(c_t\)</span> is the cell
state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the input at time <cite>t</cite>, <span class="math">\(h_{(t-1)}\)</span>
is the hidden state of the layer at time <cite>t-1</cite> or the initial hidden
state at time <cite>0</cite>, and <span class="math">\(i_t\)</span>, <span class="math">\(f_t\)</span>, <span class="math">\(g_t\)</span>,
<span class="math">\(o_t\)</span> are the input, forget, cell, and output gates, respectively.
<span class="math">\(\sigma\)</span> is the sigmoid function.</p>
<p>In a multilayer LSTM, the input <span class="math">\(i^{(l)}_t\)</span> of the <span class="math">\(l\)</span> -th layer
(<span class="math">\(l &gt;= 2\)</span>) is the hidden state <span class="math">\(h^{(l-1)}_t\)</span> of the previous layer multiplied by
dropout <span class="math">\(\delta^{(l-1)}_t\)</span> where each <span class="math">\(\delta^{(l-1)_t}\)</span> is a Bernoulli random
variable which is <span class="math">\(0\)</span> with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two LSTMs together to form a <cite>stacked LSTM</cite>,
with the second LSTM taking in outputs of the first LSTM and
computing the final results. Default: 1</li>
<li><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
LSTM layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</li>
<li><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional LSTM. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, (h_0, c_0)</dt>
<dd><ul class="first last">
<li><p class="first"><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence.
The input can also be a packed variable length sequence.
See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a> or
<a class="reference internal" href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_sequence()</span></code></a> for details.</p>
</li>
<li><p class="first"><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.
If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p>
</li>
<li><p class="first"><strong>c_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial cell state for each element in the batch.</p>
<p>If <cite>(h_0, c_0)</cite> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
</dd>
<dt>Outputs: output, (h_n, c_n)</dt>
<dd><ul class="first last">
<li><p class="first"><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features <cite>(h_t)</cite> from the last layer of the LSTM,
for each t. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been
given as the input, the output will also be a packed sequence.</p>
<p>For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.
Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p class="first"><strong>h_n</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the hidden state for <cite>t = seq_len</cite>.</p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code> and similarly for <em>c_n</em>.</p>
</li>
<li><p class="first"><strong>c_n</strong> (num_layers * num_directions, batch, hidden_size): tensor
containing the cell state for <cite>t = seq_len</cite></p>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> – the learnable input-hidden weights of the <span class="math">\(\text{k}^{th}\)</span> layer
<cite>(W_ii|W_if|W_ig|W_io)</cite>, of shape <cite>(4*hidden_size x input_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the <span class="math">\(\text{k}^{th}\)</span> layer
<cite>(W_hi|W_hf|W_hg|W_ho)</cite>, of shape <cite>(4*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> – the learnable input-hidden bias of the <span class="math">\(\text{k}^{th}\)</span> layer
<cite>(b_ii|b_if|b_ig|b_io)</cite>, of shape <cite>(4*hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the <span class="math">\(\text{k}^{th}\)</span> layer
<cite>(b_hi|b_hf|b_hg|b_ho)</cite>, of shape <cite>(4*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the following conditions are satisfied:
1) cudnn is enabled,
2) input data is on the GPU
3) input data has dtype <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>
4) V100 GPU is used,
5) input data is not in <code class="docutils literal notranslate"><span class="pre">PackedSequence</span></code> format
persistent algorithm can be selected to improve performance.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">hn</span><span class="p">,</span> <span class="n">cn</span><span class="p">)</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="gru">
<h3><span class="hidden-section">GRU</span><a class="headerlink" href="#gru" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.GRU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#GRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.GRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[\begin{array}{ll}
    r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
    z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\
    n_t = \tanh(W_{in} x_t + b_{in} + r_t (W_{hn} h_{(t-1)}+ b_{hn})) \\
    h_t = (1 - z_t) n_t + z_t h_{(t-1)}
\end{array}

\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the input
at time <cite>t</cite>, <span class="math">\(h_{(t-1)}\)</span> is the hidden state of the layer
at time <cite>t-1</cite> or the initial hidden state at time <cite>0</cite>, and <span class="math">\(r_t\)</span>,
<span class="math">\(z_t\)</span>, <span class="math">\(n_t\)</span> are the reset, update, and new gates, respectively.
<span class="math">\(\sigma\)</span> is the sigmoid function.</p>
<p>In a multilayer GRU, the input <span class="math">\(i^{(l)}_t\)</span> of the <span class="math">\(l\)</span> -th layer
(<span class="math">\(l &gt;= 2\)</span>) is the hidden state <span class="math">\(h^{(l-1)}_t\)</span> of the previous layer multiplied by
dropout <span class="math">\(\delta^{(l-1)}_t\)</span> where each <span class="math">\(\delta^{(l-1)_t}\)</span> is a Bernoulli random
variable which is <span class="math">\(0\)</span> with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two GRUs together to form a <cite>stacked GRU</cite>,
with the second GRU taking in outputs of the first GRU and
computing the final results. Default: 1</li>
<li><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
GRU layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</li>
<li><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional GRU. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, h_0</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided. If the RNN is bidirectional,
num_directions should be 2, else it should be 1.</li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt>
<dd><ul class="first last">
<li><p class="first"><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features h_t from the last layer of the GRU,
for each t. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been
given as the input, the output will also be a packed sequence.
For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.</p>
<p>Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p class="first"><strong>h_n</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the hidden state for <cite>t = seq_len</cite></p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code>.</p>
</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> – the learnable input-hidden weights of the <span class="math">\(\text{k}^{th}\)</span> layer
(W_ir|W_iz|W_in), of shape <cite>(3*hidden_size x input_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the <span class="math">\(\text{k}^{th}\)</span> layer
(W_hr|W_hz|W_hn), of shape <cite>(3*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> – the learnable input-hidden bias of the <span class="math">\(\text{k}^{th}\)</span> layer
(b_ir|b_iz|b_in), of shape <cite>(3*hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the <span class="math">\(\text{k}^{th}\)</span> layer
(b_hr|b_hz|b_hn), of shape <cite>(3*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the following conditions are satisfied:
1) cudnn is enabled,
2) input data is on the GPU
3) input data has dtype <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>
4) V100 GPU is used,
5) input data is not in <code class="docutils literal notranslate"><span class="pre">PackedSequence</span></code> format
persistent algorithm can be selected to improve performance.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="rnncell">
<h3><span class="hidden-section">RNNCell</span><a class="headerlink" href="#rnncell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.RNNCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNNCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em>, <em>nonlinearity='tanh'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#RNNCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.RNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>An Elman RNN cell with tanh or ReLU non-linearity.</p>
<div class="math">
\[h' = \tanh(w_{ih} x + b_{ih}  +  w_{hh} h + b_{hh})\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">nonlinearity</span></code> is <cite>‘relu’</cite>, then ReLU is used in place of tanh.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>nonlinearity</strong> – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, hidden</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</li>
<li><strong>hidden</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.
Defaults to zero if not provided.</li>
</ul>
</dd>
<dt>Outputs: h’</dt>
<dd><ul class="first last simple">
<li><strong>h’</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(hidden_size x input_size)</cite></li>
<li><strong>weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx = rnn(input[i], hx)</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstmcell">
<h3><span class="hidden-section">LSTMCell</span><a class="headerlink" href="#lstmcell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.LSTMCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTMCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#LSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.LSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A long short-term memory (LSTM) cell.</p>
<div class="math">
\[\begin{array}{ll}
i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\
f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\
g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\
o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\
c' = f * c + i * g \\
h' = o \tanh(c') \\
\end{array}\]</div>
<p>where <span class="math">\(\sigma\)</span> is the sigmoid function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>bias</strong> – If <cite>False</cite>, then the layer does not use bias weights <cite>b_ih</cite> and
<cite>b_hh</cite>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, (h_0, c_0)</dt>
<dd><ul class="first last">
<li><p class="first"><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</p>
</li>
<li><p class="first"><strong>h_0</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.</p>
</li>
<li><p class="first"><strong>c_0</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial cell state
for each element in the batch.</p>
<p>If <cite>(h_0, c_0)</cite> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
</dd>
<dt>Outputs: h_1, c_1</dt>
<dd><ul class="first last simple">
<li><strong>h_1</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</li>
<li><strong>c_1</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next cell state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(4*hidden_size x input_size)</cite></li>
<li><strong>weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(4*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(4*hidden_size)</cite></li>
<li><strong>bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(4*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx, cx = rnn(input[i], (hx, cx))</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="grucell">
<h3><span class="hidden-section">GRUCell</span><a class="headerlink" href="#grucell" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.GRUCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRUCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/rnn.html#GRUCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.GRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A gated recurrent unit (GRU) cell</p>
<div class="math">
\[\begin{array}{ll}
r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\
z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\
n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\
h' = (1 - z) * n + z * h
\end{array}\]</div>
<p>where <span class="math">\(\sigma\)</span> is the sigmoid function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></li>
<li><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></li>
<li><strong>bias</strong> – If <cite>False</cite>, then the layer does not use bias weights <cite>b_ih</cite> and
<cite>b_hh</cite>. Default: <cite>True</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, hidden</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</li>
<li><strong>hidden</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.
Defaults to zero if not provided.</li>
</ul>
</dd>
<dt>Outputs: h’</dt>
<dd><ul class="first last simple">
<li><strong>h’</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(3*hidden_size x input_size)</cite></li>
<li><strong>weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(3*hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(3*hidden_size)</cite></li>
<li><strong>bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(3*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx = rnn(input[i], hx)</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="linear-layers">
<h2>Linear layers<a class="headerlink" href="#linear-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="linear">
<h3><span class="hidden-section">Linear</span><a class="headerlink" href="#linear" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Linear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Linear</code><span class="sig-paren">(</span><em>in_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/linear.html#Linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math">\(y = xA^T + b\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_features</strong> – size of each input sample</li>
<li><strong>out_features</strong> – size of each output sample</li>
<li><strong>bias</strong> – If set to False, the layer will not learn an additive bias.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *, \text{in\_features})\)</span> where <span class="math">\(*\)</span> means any number of
additional dimensions</li>
<li>Output: <span class="math">\((N, *, \text{out\_features})\)</span> where all but the last dimension
are the same shape as the input.</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> – the learnable weights of the module of shape
<span class="math">\((\text{out\_features}, \text{in\_features})\)</span>. The values are
initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where
<span class="math">\(k = \frac{1}{\text{in\_features}}\)</span></li>
<li><strong>bias</strong> – the learnable bias of the module of shape <span class="math">\((\text{out\_features})\)</span>.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the values are initialized from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{\text{in\_features}}\)</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([128, 30])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="bilinear">
<h3><span class="hidden-section">Bilinear</span><a class="headerlink" href="#bilinear" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Bilinear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Bilinear</code><span class="sig-paren">(</span><em>in1_features</em>, <em>in2_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/linear.html#Bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a bilinear transformation to the incoming data:
<span class="math">\(y = x_1 A x_2 + b\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in1_features</strong> – size of each first input sample</li>
<li><strong>in2_features</strong> – size of each second input sample</li>
<li><strong>out_features</strong> – size of each output sample</li>
<li><strong>bias</strong> – If set to False, the layer will not learn an additive bias.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *, \text{in1\_features})\)</span>, <span class="math">\((N, *, \text{in2\_features})\)</span>
where <span class="math">\(*\)</span> means any number of additional dimensions. All but the last
dimension of the inputs should be the same.</li>
<li>Output: <span class="math">\((N, *, \text{out\_features})\)</span> where all but the last dimension
are the same shape as the input.</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> – the learnable weights of the module of shape
<span class="math">\((\text{out\_features} x \text{in1\_features} x \text{in2\_features})\)</span>.
The values are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where
<span class="math">\(k = \frac{1}{\text{in1\_features}}\)</span></li>
<li><strong>bias</strong> – the learnable bias of the module of shape <span class="math">\((\text{out\_features})\)</span>
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the values are initialized from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where
<span class="math">\(k = \frac{1}{\text{in1\_features}}\)</span></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Bilinear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([128, 40])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dropout-layers">
<h2>Dropout layers<a class="headerlink" href="#dropout-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dropout">
<h3><span class="hidden-section">Dropout</span><a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli
distribution. Each channel will be zeroed out independently on every forward
call.</p>
<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature
detectors</a> .</p>
<p>Furthermore, the outputs are scaled by a factor of <span class="math">\(\frac{1}{1-p}\)</span> during
training. This means that during evaluation the module simply computes an
identity function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> – probability of an element to be zeroed. Default: 0.5</li>
<li><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <cite>Any</cite>. Input can be of any shape</li>
<li>Output: <cite>Same</cite>. Output is of the same shape as input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout2d">
<h3><span class="hidden-section">Dropout2d</span><a class="headerlink" href="#dropout2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout2d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 2D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 2D tensor <span class="math">\(\text{input}[i, j]\)</span>) of the input tensor).
Each channel will be zeroed out independently on every forward call.
with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>Usually the input comes from <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> modules.</p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.Dropout2d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – probability of an element to be zero-ed.</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout3d">
<h3><span class="hidden-section">Dropout3d</span><a class="headerlink" href="#dropout3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Dropout3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout3d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#Dropout3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 3D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 3D tensor <span class="math">\(\text{input}[i, j]\)</span>) of the input tensor).
Each channel will be zeroed out independently on every forward call.
with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>Usually the input comes from <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Conv3d</span></code> modules.</p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.Dropout3d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – probability of an element to be zeroed.</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout3d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="alphadropout">
<h3><span class="hidden-section">AlphaDropout</span><a class="headerlink" href="#alphadropout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.AlphaDropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AlphaDropout</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/dropout.html#AlphaDropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.AlphaDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Alpha Dropout over the input.</p>
<p>Alpha Dropout is a type of Dropout that maintains the self-normalizing
property.
For an input with zero mean and unit standard deviation, the output of
Alpha Dropout maintains the original mean and standard deviation of the
input.
Alpha Dropout goes hand-in-hand with SELU activation function, which ensures
that the outputs have zero mean and unit standard deviation.</p>
<p>During training, it randomly masks some of the elements of the input
tensor with probability <em>p</em> using samples from a bernoulli distribution.
The elements to masked are randomized on every forward call, and scaled
and shifted to maintain zero mean and unit standard deviation.</p>
<p>During evaluation the module simply computes an identity function.</p>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – probability of an element to be dropped. Default: 0.5</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <cite>Any</cite>. Input can be of any shape</li>
<li>Output: <cite>Same</cite>. Output is of the same shape as input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AlphaDropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="sparse-layers">
<h2>Sparse layers<a class="headerlink" href="#sparse-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="embedding">
<h3><span class="hidden-section">Embedding</span><a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Embedding">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Embedding</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em>, <em>_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/sparse.html#Embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of each embedding vector</li>
<li><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – If given, pads the output with the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code>
(initialized to zeros) whenever it encounters the index.</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> matrix will be a sparse tensor.
See Notes for more details regarding sparse gradients.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape (num_embeddings, embedding_dim)
initialized from <span class="math">\(\mathcal{N}(0, 1)\)</span></p>
</td>
</tr>
</tbody>
</table>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: LongTensor of arbitrary shape containing the indices to extract</li>
<li>Output: <cite>(*, embedding_dim)</cite>, where <cite>*</cite> is the input shape</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Keep in mind that only a limited number of optimizers support
sparse gradients: currently it’s <code class="xref py py-class docutils literal notranslate"><span class="pre">optim.SGD</span></code> (<cite>CUDA</cite> and <cite>CPU</cite>),
<code class="xref py py-class docutils literal notranslate"><span class="pre">optim.SparseAdam</span></code> (<cite>CUDA</cite> and <cite>CPU</cite>) and <code class="xref py py-class docutils literal notranslate"><span class="pre">optim.Adagrad</span></code> (<cite>CPU</cite>)</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">With <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> set, the embedding vector at
<code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> is initialized to all zeros. However, note that this
vector can be modified afterwards, e.g., using a customized
initialization method, and thus changing the vector used to pad the
output. The gradient for this vector from <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a>
is always zero.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[-0.0251, -1.6902,  0.7172],</span>
<span class="go">         [-0.6431,  0.0748,  0.6969],</span>
<span class="go">         [ 1.4970,  1.3448, -0.9685],</span>
<span class="go">         [-0.3677, -2.7265, -0.1685]],</span>

<span class="go">        [[ 1.4970,  1.3448, -0.9685],</span>
<span class="go">         [ 0.4362, -0.4004,  0.9400],</span>
<span class="go">         [-0.6431,  0.0748,  0.6969],</span>
<span class="go">         [ 0.9124, -2.3616,  1.1151]]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.1535, -2.0309,  0.9315],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [-0.1655,  0.9897,  0.0635]]])</span>
</pre></div>
</div>
<dl class="classmethod">
<dt id="torch.nn.Embedding.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>embeddings</em>, <em>freeze=True</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/sparse.html#Embedding.from_pretrained"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Embedding.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates Embedding instance from given 2-dimensional FloatTensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>embeddings</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – FloatTensor containing weights for the Embedding.
First dimension is being passed to Embedding as ‘num_embeddings’, second as ‘embedding_dim’.</li>
<li><strong>freeze</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the tensor does not get updated in the learning process.
Equivalent to <code class="docutils literal notranslate"><span class="pre">embedding.weight.requires_grad</span> <span class="pre">=</span> <span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. weight matrix will be a sparse tensor.
See Notes for more details regarding sparse gradients.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># FloatTensor containing pretrained weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">,</span> <span class="mf">6.3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Get embeddings for index 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[ 4.0000,  5.1000,  6.3000]])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="embeddingbag">
<h3><span class="hidden-section">EmbeddingBag</span><a class="headerlink" href="#embeddingbag" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.EmbeddingBag">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">EmbeddingBag</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/sparse.html#EmbeddingBag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.EmbeddingBag" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sums or means of ‘bags’ of embeddings, without instantiating the
intermediate embeddings.</p>
<p>For bags of constant length, this class</p>
<blockquote>
<div><ul class="simple">
<li>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;sum&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.sum(dim=1)</span></code>,</li>
<li>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;mean&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.mean(dim=1)</span></code>,</li>
<li>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.max(dim=1)</span></code>.</li>
</ul>
</div></blockquote>
<p>However, <a class="reference internal" href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingBag</span></code></a> is much more time and memory efficient than using a chain of these
operations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of each embedding vector</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</li>
<li><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>. Specifies the way to reduce the bag.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> matrix will be a sparse tensor. See
Notes for more details regarding sparse gradients. Note: this option is not
supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape <code class="docutils literal notranslate"><span class="pre">(num_embeddings</span> <span class="pre">x</span> <span class="pre">embedding_dim)</span></code>
initialized from <span class="math">\(\mathcal{N}(0, 1)\)</span>.</p>
</td>
</tr>
</tbody>
</table>
<p>Inputs: <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (LongTensor) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> (LongTensor, optional)</p>
<blockquote>
<div><ul>
<li><p class="first">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 2D of shape <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span></code>,</p>
<p>it will be treated as <code class="docutils literal notranslate"><span class="pre">B</span></code> bags (sequences) each of fixed length <code class="docutils literal notranslate"><span class="pre">N</span></code>, and
this will return <code class="docutils literal notranslate"><span class="pre">B</span></code> values aggregated in a way depending on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is ignored and required to be <code class="docutils literal notranslate"><span class="pre">None</span></code> in this case.</p>
</li>
<li><p class="first">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D of shape <code class="docutils literal notranslate"><span class="pre">N</span></code>,</p>
<p>it will be treated as a concatenation of multiple bags (sequences).
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is required to be a 1D tensor containing the
starting index positions of each bag in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. Therefore,
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> of shape <code class="docutils literal notranslate"><span class="pre">B</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be viewed as
having <code class="docutils literal notranslate"><span class="pre">B</span></code> bags. Empty bags (i.e., having 0-length) will have
returned vectors filled by zeros.</p>
</li>
</ul>
</div></blockquote>
<p>Output shape: <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">embedding_dim</span></code></p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="go">tensor([[-0.8861, -5.4350, -0.0523],</span>
<span class="go">        [ 1.1306, -2.5798, -1.0044]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="distance-functions">
<h2>Distance functions<a class="headerlink" href="#distance-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="cosinesimilarity">
<h3><span class="hidden-section">CosineSimilarity</span><a class="headerlink" href="#cosinesimilarity" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CosineSimilarity">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineSimilarity</code><span class="sig-paren">(</span><em>dim=1</em>, <em>eps=1e-08</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/distance.html#CosineSimilarity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CosineSimilarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between <span class="math">\(x_1\)</span> and <span class="math">\(x_2\)</span>, computed along dim.</p>
<div class="math">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension where cosine similarity is computed. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-8</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input1: <span class="math">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite></li>
<li>Input2: <span class="math">\((\ast_1, D, \ast_2)\)</span>, same shape as the Input1</li>
<li>Output: <span class="math">\((\ast_1, \ast_2)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CosineSimilarity</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pairwisedistance">
<h3><span class="hidden-section">PairwiseDistance</span><a class="headerlink" href="#pairwisedistance" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PairwiseDistance">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PairwiseDistance</code><span class="sig-paren">(</span><em>p=2.0</em>, <em>eps=1e-06</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/distance.html#PairwiseDistance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PairwiseDistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the batchwise pairwise distance between vectors <span class="math">\(v_1\)</span>, <span class="math">\(v_2\)</span> using the p-norm:</p>
<div class="math">
\[\Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<em>real</em>) – the norm degree. Default: 2</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-6</li>
<li><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Determines whether or not to keep the batch dimension.
Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input1: <span class="math">\((N, D)\)</span> where <cite>D = vector dimension</cite></li>
<li>Input2: <span class="math">\((N, D)\)</span>, same shape as the Input1</li>
<li>Output: <span class="math">\((N)\)</span>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then <span class="math">\((N, 1)\)</span>.</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pdist</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PairwiseDistance</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="l1loss">
<h3><span class="hidden-section">L1Loss</span><a class="headerlink" href="#l1loss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.L1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">L1Loss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#L1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean absolute error (MAE) between each element in
the input <cite>x</cite> and target <cite>y</cite>.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left| x_n - y_n \right|,

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then:</p>
<div class="math">
\[\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), & \text{if size\_average} = \text{True;}\\
    \operatorname{sum}(L),  & \text{if size\_average} = \text{False.}
\end{cases}

\]</div>
<p><cite>x</cite> and <cite>y</cite> are tensors of arbitrary shapes with a total of <cite>n</cite> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the constructor argument
<cite>size_average=False</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
<li>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then
<span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="mseloss">
<h3><span class="hidden-section">MSELoss</span><a class="headerlink" href="#mseloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MSELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MSELoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MSELoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean squared error (squared L2 norm) between
each element in the input <cite>x</cite> and target <cite>y</cite>.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left( x_n - y_n \right)^2,

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then:</p>
<div class="math">
\[\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), & \text{if}\; \text{size\_average} = \text{True},\\
    \operatorname{sum}(L),  & \text{if}\; \text{size\_average} = \text{False}.
\end{cases}

\]</div>
<p>The sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>To get a batch of losses, a loss per batch element, set <cite>reduce</cite> to
<code class="docutils literal notranslate"><span class="pre">False</span></code>. These losses are not averaged and are not affected by
<cite>size_average</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="crossentropyloss">
<h3><span class="hidden-section">CrossEntropyLoss</span><a class="headerlink" href="#crossentropyloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CrossEntropyLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CrossEntropyLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#CrossEntropyLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.LogSoftmax()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.NLLLoss()</span></code> in one single class.</p>
<p>It is useful when training a classification problem with <cite>C</cite> classes.
If provided, the optional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> should be a 1D <cite>Tensor</cite>
assigning weight to each of the classes.
This is particularly useful when you have an unbalanced training set.</p>
<p>The <cite>input</cite> is expected to contain scores for each class.</p>
<p><cite>input</cite> has to be a Tensor of size either <span class="math">\((minibatch, C)\)</span> or
<span class="math">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math">\(K \geq 2\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>This criterion expects a class index (0 to <cite>C-1</cite>) as the
<cite>target</cite> for each value of a 1D tensor of size <cite>minibatch</cite></p>
<p>The loss can be described as:</p>
<div class="math">
\[\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
               = -x[class] + \log\left(\sum_j \exp(x[j])\right)

\]</div>
<p>or in the case of the <cite>weight</cite> argument being specified:</p>
<div class="math">
\[\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)

\]</div>
<p>The losses are averaged across observations for each minibatch.</p>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 2\)</span>,
where <span class="math">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each class.
If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <cite>size_average</cite> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets.</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>Input: <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite>, or</dt>
<dd><span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 2\)</span>
in the case of <cite>K</cite>-dimensional loss.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Target: <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>, or</dt>
<dd><span class="math">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 2\)</span> in the case of
K-dimensional loss.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the same size</dt>
<dd>as the target: <span class="math">\((N)\)</span>, or
<span class="math">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 2\)</span> in the case
of K-dimensional loss.</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="ctcloss">
<h3><span class="hidden-section">CTCLoss</span><a class="headerlink" href="#ctcloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CTCLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CTCLoss</code><span class="sig-paren">(</span><em>blank=0</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#CTCLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CTCLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The Connectionist Temporal Classification loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>blank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – blank label. Default <span class="math">\(0\)</span>.</li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the output losses will be divided by the target lengths and
then the mean over the batch is taken. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><dl class="first last docutils">
<dt>log_probs: Tensor of size <span class="math">\((T, N, C)\)</span> where <cite>C = number of characters in alphabet including blank</cite>,</dt>
<dd><cite>T = input length</cite>, and <cite>N = batch size</cite>.
The logarithmized probabilities of the outputs
(e.g. obtained with <a class="reference internal" href="#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.log_softmax()</span></code></a>).</dd>
<dt>targets: Tensor of size <span class="math">\((N, S)\)</span> or <cite>(sum(target_lengths))</cite>.</dt>
<dd>Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.</dd>
<dt>input_lengths: Tuple or tensor of size <span class="math">\((N)\)</span>.</dt>
<dd>Lengths of the inputs (must each be <span class="math">\(\leq T\)</span>)</dd>
<dt>target_lengths: Tuple or tensor of size  <span class="math">\((N)\)</span>.</dt>
<dd>Lengths of the targets</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ctc_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">16</span><span class="p">,),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">,(</span><span class="mi">16</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="docutils">
<dt>Reference:</dt>
<dd>A. Graves et al.: Connectionist Temporal Classification:
Labelling Unsegmented Sequence Data with Recurrent Neural Networks:
<a class="reference external" href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></dd>
</dl>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>In order to use CuDNN, the following must be satisfied: <code class="xref py py-attr docutils literal notranslate"><span class="pre">targets</span></code> must be
in concatenated format, all <code class="xref py py-attr docutils literal notranslate"><span class="pre">input_lengths</span></code> must be <cite>T</cite>.  <span class="math">\(blank=0\)</span>,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">target_lengths</span></code> <span class="math">\(\leq 256\)</span>, the integer arguments must be of
dtype <code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.int32</span></code>.</p>
<p class="last">The regular implementation uses the (more common in PyTorch) <cite>torch.long</cite> dtype.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="nllloss">
<h3><span class="hidden-section">NLLLoss</span><a class="headerlink" href="#nllloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.NLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">NLLLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#NLLLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.NLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss. It is useful to train a classification
problem with <cite>C</cite> classes.</p>
<p>If provided, the optional argument <cite>weight</cite> should be a 1D Tensor assigning
weight to each of the classes. This is particularly useful when you have an
unbalanced training set.</p>
<p>The input given through a forward call is expected to contain
log-probabilities of each class. <cite>input</cite> has to be a Tensor of size either
<span class="math">\((minibatch, C)\)</span> or <span class="math">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math">\(K \geq 2\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>Obtaining log-probabilities in a neural network is easily achieved by
adding a  <cite>LogSoftmax</cite>  layer in the last layer of your network.
You may use <cite>CrossEntropyLoss</cite> instead, if you prefer not to add an extra
layer.</p>
<p>The target that this loss expects is a class index
<cite>(0 to C-1, where C = number of classes)</cite></p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_{y_n} x_{n,y_n}, \quad
w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> (default),
then</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, & \text{if}\;
    \text{size\_average} = \text{True},\\
    \sum_{n=1}^N l_n,  & \text{if}\;
    \text{size\_average} = \text{False}.
\end{cases}

\]</div>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 2\)</span>,
where <span class="math">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below). In the case of images, it computes NLL loss per-pixel.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over
non-ignored targets.</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>Input: <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite>, or</dt>
<dd><span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 2\)</span>
in the case of <cite>K</cite>-dimensional loss.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Target: <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>, or</dt>
<dd><span class="math">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 2\)</span> in the case of
K-dimensional loss.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the same size</dt>
<dd>as the target: <span class="math">\((N)\)</span>, or
<span class="math">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 2\)</span> in the case
of K-dimensional loss.</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2D loss example (used, for example, with image inputs)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C x height x width</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">conv</span><span class="p">(</span><span class="n">data</span><span class="p">)),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poissonnllloss">
<h3><span class="hidden-section">PoissonNLLLoss</span><a class="headerlink" href="#poissonnllloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PoissonNLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PoissonNLLLoss</code><span class="sig-paren">(</span><em>log_input=True</em>, <em>full=False</em>, <em>size_average=None</em>, <em>eps=1e-08</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#PoissonNLLLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PoissonNLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Negative log likelihood loss with Poisson distribution of target.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\text{target} \sim \mathrm{Poisson}(\text{input})

\text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})
                            + \log(\text{target!})\]</div>
<p>The last term can be omitted or approximated with Stirling formula. The
approximation is used for target values more than 1. For targets less or
equal to 1 zeros are added to the loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>log_input</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> the loss is computed as
<span class="math">\(\exp(\text{input}) - \text{target}*\text{input}\)</span>, if <code class="docutils literal notranslate"><span class="pre">False</span></code> the loss is
<span class="math">\(\text{input} - \text{target}*\log(\text{input}+\text{eps})\)</span>.</li>
<li><strong>full</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – <p>whether to compute full loss, i. e. to add the
Stirling approximation term</p>
<div class="math">
\[\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).

\]</div>
</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid evaluation of <span class="math">\(\log(0)\)</span> when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">log_input</span> <span class="pre">==</span> <span class="pre">False</span></code>. Default: 1e-8</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">log_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="kldivloss">
<h3><span class="hidden-section">KLDivLoss</span><a class="headerlink" href="#kldivloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.KLDivLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">KLDivLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#KLDivLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.KLDivLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss</p>
<p>KL divergence is a useful distance measure for continuous distributions
and is often useful when performing direct regression over the space of
(discretely sampled) continuous output distributions.</p>
<p>As with <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a>, the <cite>input</cite> given is expected to contain
<em>log-probabilities</em>. However, unlike <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a>, <cite>input</cite> is not
restricted to a 2D Tensor.
The targets are given as <em>probabilities</em> (i.e. without taking the logarithm).</p>
<p>This criterion expects a <cite>target</cite> <cite>Tensor</cite> of the same size as the
<cite>input</cite> <cite>Tensor</cite>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> set to <code class="docutils literal notranslate"><span class="pre">False</span></code>) loss can be described as:</p>
<div class="math">
\[l(x,y) = L := \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right)

\]</div>
<p>where the index <span class="math">\(N\)</span> spans all dimensions of <code class="docutils literal notranslate"><span class="pre">input</span></code> and <span class="math">\(L\)</span> has the same
shape as <code class="docutils literal notranslate"><span class="pre">input</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> (the default), then:</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), & \text{if}\; \text{size\_average} = \text{True},\\
    \operatorname{sum}(L),  & \text{if}\; \text{size\_average} = \text{False}.
\end{cases}

\]</div>
<p>In default reduction mode ‘mean’, the losses are averaged for each minibatch over observations
<strong>as well as</strong> over dimensions. ‘batchmean’ mode gives the correct KL divergence where losses
are averaged over batch dimension only. ‘mean’ mode’s behavior will be changed to the same as
‘batchmean’ in the next major release.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘batchmean’ | ‘sum’ | ‘mean’.
‘none’: no reduction will be applied.
‘batchmean’: the sum of the output will be divided by batchsize.
‘sum’: the output will be summed.
‘mean’: the output will be divided by the number of elements in the output.
Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>:param .. note:: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated,: and in the meantime, specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>.
:param .. note:: <cite>reduction=’mean’</cite> doesn’t return the true kl divergence value, please use: <cite>reduction=’batchmean’</cite> which aligns with KL math definition.</p>
<blockquote>
<div>In the next major release, ‘mean’ will be changed to be the same as ‘batchmean’.</div></blockquote>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
<li><dl class="first docutils">
<dt>output: scalar by default. If <cite>reduce</cite> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then <span class="math">\((N, *)\)</span>,</dt>
<dd>the same shape as the input</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="bceloss">
<h3><span class="hidden-section">BCELoss</span><a class="headerlink" href="#bceloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BCELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCELoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#BCELoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BCELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the Binary Cross Entropy
between the target and the output:</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), & \text{if}\; \text{size\_average} = \text{True},\\
    \operatorname{sum}(L),  & \text{if}\; \text{size\_average} = \text{False}.
\end{cases}

\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <cite>y</cite> should be numbers
between 0 and 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size
“nbatch”.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N, *)</cite>, same shape as
input.</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="bcewithlogitsloss">
<h3><span class="hidden-section">BCEWithLogitsLoss</span><a class="headerlink" href="#bcewithlogitsloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.BCEWithLogitsLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCEWithLogitsLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em>, <em>pos_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#BCEWithLogitsLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.BCEWithLogitsLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This loss combines a <cite>Sigmoid</cite> layer and the <cite>BCELoss</cite> in one single
class. This version is more numerically stable than using a plain <cite>Sigmoid</cite>
followed by a <cite>BCELoss</cite> as, by combining the operations into one layer,
we take advantage of the log-sum-exp trick for numerical stability.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
+ (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If reduce is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), & \text{if size\_average} = \text{True},\\
    \operatorname{sum}(L),  & \text{if size\_average} = \text{False}.
\end{cases}

\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <cite>t[i]</cite> should be numbers
between 0 and 1.</p>
<p>It’s possible to trade off recall and precision by adding weights to positive examples.
In this case the loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ p_n y_n \cdot \log \sigma(x_n)
+ (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],

\]</div>
<p>where <span class="math">\(p_n\)</span> is the positive weight of class <span class="math">\(n\)</span>.
<span class="math">\(p_n &gt; 1\)</span> increases the recall, <span class="math">\(p_n &lt; 1\)</span> increases the precision.</p>
<p>For example, if a dataset contains 100 positive and 300 negative examples of a single class,
then <cite>pos_weight</cite> for the class should be equal to <span class="math">\(\frac{300}{100}=3\)</span>.
The loss would act as if the dataset contains <span class="math">\(3\times 100=300\)</span> positive examples.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size
“nbatch”.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
<li><strong>pos_weight</strong> – a weight of positive examples.
Must be a vector with length equal to the number of classes.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="marginrankingloss">
<h3><span class="hidden-section">MarginRankingLoss</span><a class="headerlink" href="#marginrankingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MarginRankingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MarginRankingLoss</code><span class="sig-paren">(</span><em>margin=0.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MarginRankingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MarginRankingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given
inputs <cite>x1</cite>, <cite>x2</cite>, two 1D mini-batch <cite>Tensor`s,
and a label 1D mini-batch tensor `y</cite> with values (<cite>1</cite> or <cite>-1</cite>).</p>
<p>If <cite>y == 1</cite> then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for <cite>y == -1</cite>.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math">
\[\text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>0</cite>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, D)\)</span> where <cite>N</cite> is the batch size and <cite>D</cite> is the size of a sample.</li>
<li>Target: <span class="math">\((N)\)</span></li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N)</cite>.</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="hingeembeddingloss">
<h3><span class="hidden-section">HingeEmbeddingLoss</span><a class="headerlink" href="#hingeembeddingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.HingeEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">HingeEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#HingeEmbeddingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.HingeEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the loss given an input tensor <cite>x</cite> and a labels tensor <cite>y</cite>
containing values (<cite>1</cite> or <cite>-1</cite>).
This is usually used for measuring whether two inputs are similar or
dissimilar, e.g. using the L1 pairwise distance as <cite>x</cite>, and is typically
used for learning nonlinear embeddings or semi-supervised learning.</p>
<p>The loss function for <span class="math">\(n\)</span>-th sample in the mini-batch is</p>
<div class="math">
\[l_n = \begin{cases}
    x_n, & \text{if}\; y_n = 1,\\
    \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1,
\end{cases}

\]</div>
<p>and the total loss functions is</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), & \text{if size\_average} = \text{True},\\
    \operatorname{sum}(L),  & \text{if size\_average} = \text{False}.
\end{cases}

\]</div>
<p>where <span class="math">\(L = \{l_1,\dots,l_N\}^\top\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>1</cite>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: Tensor of arbitrary shape. The sum operation operates over all the elements.</li>
<li>Target: Same shape as input.</li>
<li>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then same shape as the input</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="multilabelmarginloss">
<h3><span class="hidden-section">MultiLabelMarginLoss</span><a class="headerlink" href="#multilabelmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiLabelMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelMarginLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiLabelMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiLabelMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class multi-classification
hinge loss (margin-based loss) between input <cite>x</cite>  (a 2D mini-batch <cite>Tensor</cite>)
and output <cite>y</cite> (which is a 2D <cite>Tensor</cite> of target class indices).
For each sample in the mini-batch:</p>
<div class="math">
\[\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}

\]</div>
<p>where <span class="math">\(i == 0\)</span> to <span class="math">\(x.size(0)\)</span>, <span class="math">\(j == 0\)</span> to <span class="math">\(y.size(0)\)</span>, <span class="math">\(y[j] \geq 0\)</span>, and <span class="math">\(i \neq y[j]\)</span> for all <span class="math">\(i\)</span> and <span class="math">\(j\)</span>.</p>
<p><cite>y</cite> and <cite>x</cite> must have the same size.</p>
<p>The criterion only considers a contiguous block of non-negative targets that
starts at the front.</p>
<p>This allows for different samples to have variable amounts of target classes</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((C)\)</span> or <span class="math">\((N, C)\)</span> where <cite>N</cite> is the batch size and <cite>C</cite>
is the number of classes.</li>
<li>Target: <span class="math">\((C)\)</span> or <span class="math">\((N, C)\)</span>, same shape as the input.</li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N)</cite>.</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="smoothl1loss">
<h3><span class="hidden-section">SmoothL1Loss</span><a class="headerlink" href="#smoothl1loss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SmoothL1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SmoothL1Loss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#SmoothL1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.
It is less sensitive to outliers than the <cite>MSELoss</cite> and in some cases
prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick).
Also known as the Huber loss:</p>
<div class="math">
\[\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}

\]</div>
<p>where <span class="math">\(z_{i}\)</span> is given by:</p>
<div class="math">
\[z_{i} =
\begin{cases}
0.5 (x_i - y_i)^2, & \text{if } |x_i - y_i| < 1 \\
|x_i - y_i| - 0.5, & \text{otherwise }
\end{cases}

\]</div>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each
the sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</li>
<li>Target: <span class="math">\((N, *)\)</span>, same shape as the input</li>
<li>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then
<span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="softmarginloss">
<h3><span class="hidden-section">SoftMarginLoss</span><a class="headerlink" href="#softmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.SoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SoftMarginLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#SoftMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.SoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a two-class classification
logistic loss between input tensor <cite>x</cite> and target tensor <cite>y</cite> (containing 1 or
-1).</p>
<div class="math">
\[\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: Tensor of arbitrary shape.</li>
<li>Target: Same shape as input.</li>
<li>Output: scalar. If reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then same shape as the input</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="multilabelsoftmarginloss">
<h3><span class="hidden-section">MultiLabelSoftMarginLoss</span><a class="headerlink" href="#multilabelsoftmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiLabelSoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelSoftMarginLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiLabelSoftMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiLabelSoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-label one-versus-all
loss based on max-entropy, between input <cite>x</cite> and target <cite>y</cite> of size <cite>(N, C)</cite>.
For each sample in the minibatch:</p>
<div class="math">
\[loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})
                 + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)

\]</div>
<p>where <cite>i == 0</cite> to <cite>x.nElement()-1</cite>, <cite>y[i]  in {0,1}</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C)\)</span> where <cite>N</cite> is the batch size and <cite>C</cite> is the number of classes.</li>
<li>Target: <span class="math">\((N, C)\)</span>, same shape as the input.</li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N)</cite>.</li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="cosineembeddingloss">
<h3><span class="hidden-section">CosineEmbeddingLoss</span><a class="headerlink" href="#cosineembeddingloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.CosineEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=0.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#CosineEmbeddingLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.CosineEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given input tensors
<span class="math">\(x_1\)</span>, <span class="math">\(x_2\)</span> and a <cite>Tensor</cite> label <cite>y</cite> with values 1 or -1.
This is used for measuring whether two inputs are similar or dissimilar,
using the cosine distance, and is typically used for learning nonlinear
embeddings or semi-supervised learning.</p>
<p>The loss function for each sample is:</p>
<div class="math">
\[\text{loss}(x, y) =
\begin{cases}
1 - \cos(x_1, x_2), & \text{if } y == 1 \\
\max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y == -1
\end{cases}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Should be a number from <cite>-1</cite> to <cite>1</cite>, <cite>0</cite> to <cite>0.5</cite>
is suggested. If <cite>margin</cite> is missing, the default value is <cite>0</cite>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="multimarginloss">
<h3><span class="hidden-section">MultiMarginLoss</span><a class="headerlink" href="#multimarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.MultiMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiMarginLoss</code><span class="sig-paren">(</span><em>p=1</em>, <em>margin=1.0</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#MultiMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.MultiMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class classification hinge
loss (margin-based loss) between input <cite>x</cite> (a 2D mini-batch <cite>Tensor</cite>) and
output <cite>y</cite> (which is a 1D tensor of target class indices,
<span class="math">\(0 \leq y \leq \text{x.size}(1)\)</span>):</p>
<p>For each mini-batch sample, the loss in terms of the 1D input <cite>x</cite> and scalar
output <cite>y</cite> is:</p>
<div class="math">
\[\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}

\]</div>
<p>where <cite>i == 0</cite> to <cite>x.size(0)</cite> and <span class="math">\(i \neq y\)</span>.</p>
<p>Optionally, you can give non-equal weighting on the classes by passing
a 1D <cite>weight</cite> tensor into the constructor.</p>
<p>The loss function then becomes:</p>
<div class="math">
\[\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>1</cite>. <cite>1</cite> and <cite>2</cite> are the only
supported values</li>
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>1</cite>.</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="tripletmarginloss">
<h3><span class="hidden-section">TripletMarginLoss</span><a class="headerlink" href="#tripletmarginloss" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.TripletMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">TripletMarginLoss</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>p=2.0</em>, <em>eps=1e-06</em>, <em>swap=False</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/loss.html#TripletMarginLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.TripletMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the triplet loss given an input
tensors x1, x2, x3 and a margin with a value greater than 0.
This is used for measuring a relative similarity between samples. A triplet
is composed by <cite>a</cite>, <cite>p</cite> and <cite>n</cite>: anchor, positive examples and negative
example respectively. The shapes of all input tensors should be
<span class="math">\((N, D)\)</span>.</p>
<p>The distance swap is described in detail in the paper <a class="reference external" href="http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf">Learning shallow
convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math">
\[L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}

\]</div>
<p>where</p>
<div class="math">
\[d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Default: <cite>1</cite>.</li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – The norm degree for pairwise distance. Default: <cite>2</cite>.</li>
<li><strong>swap</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The distance swap is described in detail in the paper
<cite>Learning shallow convolutional feature descriptors with triplet losses</cite> by
V. Balntas, E. Riba et al. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, D)\)</span> where <cite>D</cite> is the vector dimension.</li>
<li>Output: scalar. If <cite>reduce</cite> is False, then <cite>(N)</cite>.</li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">triplet_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">input3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="vision-layers">
<h2>Vision layers<a class="headerlink" href="#vision-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pixelshuffle">
<h3><span class="hidden-section">PixelShuffle</span><a class="headerlink" href="#pixelshuffle" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.PixelShuffle">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PixelShuffle</code><span class="sig-paren">(</span><em>upscale_factor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/pixelshuffle.html#PixelShuffle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.PixelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a tensor of shape <span class="math">\((*, C \times r^2, H, W)\)</span>
to a tensor of shape <span class="math">\((C, H \times r, W \times r)\)</span>.</p>
<p>This is useful for implementing efficient sub-pixel convolution
with a stride of <span class="math">\(1/r\)</span>.</p>
<p>Look at the paper:
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a>
by Shi et. al (2016) for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – factor to increase spatial resolution by</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C \times \text{upscale_factor}^2, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H \times \text{upscale_factor}, W \times \text{upscale_factor})\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pixel_shuffle</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsample">
<h3><span class="hidden-section">Upsample</span><a class="headerlink" href="#upsample" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Upsample">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Upsample</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#Upsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.Upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p>
<p>The input data is assumed to be of the form
<cite>minibatch x channels x [optional depth] x [optional height] x width</cite>.
Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</p>
<p>The algorithms available for upsampling are nearest neighbor and linear, bilinear and trilinear
for 3D, 4D and 5D input Tensor, respectively.</p>
<p>One can either give a <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code> or the target output <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> to
calculate the output size. (You cannot give both, as it is ambiguous)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a tuple of ints <cite>([optional D_out], [optional H_out], W_out)</cite> output sizes</li>
<li><strong>scale_factor</strong> (<em>int / tuple of python:ints</em><em>, </em><em>optional</em>) – the multiplier for the image height / width / depth</li>
<li><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – the upsampling algorithm: one of <cite>nearest</cite>, <cite>linear</cite>, <cite>bilinear</cite> and <cite>trilinear</cite>.
Default: <cite>nearest</cite></li>
<li><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if True, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <cite>linear</cite>,
<cite>bilinear</cite>, or <cite>trilinear</cite>. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, W_{in})\)</span>, <span class="math">\((N, C, H_{in}, W_{in})\)</span> or <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, W_{out})\)</span>, <span class="math">\((N, C, H_{out}, W_{out})\)</span>
or <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</li>
</ul>
</dd>
</dl>
<div class="math">
\[D_{out} = \left\lfloor D_{in} \times \text{scale\_factor} \right\rfloor \text{ or size}[-3]

\]</div>
<div class="math">
\[H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor \text{ or size}[-2]

\]</div>
<div class="math">
\[W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor \text{ or size}[-1]

\]</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See below for concrete examples on how this affects the outputs.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you want downsampling/general resizing, you should use <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span>  <span class="c1"># align_corners=False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],</span>
<span class="go">          [ 1.5000,  1.7500,  2.2500,  2.5000],</span>
<span class="go">          [ 2.5000,  2.7500,  3.2500,  3.5000],</span>
<span class="go">          [ 3.0000,  3.2500,  3.7500,  4.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],</span>
<span class="go">          [ 1.6667,  2.0000,  2.3333,  2.6667],</span>
<span class="go">          [ 2.3333,  2.6667,  3.0000,  3.3333],</span>
<span class="go">          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Try scaling the same data in a larger tensor</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span>
<span class="go">tensor([[[[ 1.,  2.,  0.],</span>
<span class="go">          [ 3.,  4.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span>  <span class="c1"># align_corners=False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Notice that values in top left corner are the same with the small input (except at boundary)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">input_3x3</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],</span>
<span class="go">          [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],</span>
<span class="go">          [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],</span>
<span class="go">          [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],</span>
<span class="go">          [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Notice that values in top left corner are now changed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">input_3x3</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],</span>
<span class="go">          [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],</span>
<span class="go">          [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],</span>
<span class="go">          [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],</span>
<span class="go">          [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingnearest2d">
<h3><span class="hidden-section">UpsamplingNearest2d</span><a class="headerlink" href="#upsamplingnearest2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.UpsamplingNearest2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingNearest2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#UpsamplingNearest2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.UpsamplingNearest2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <cite>size</cite> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a tuple of ints <cite>(H_out, W_out)</cite> output sizes</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the multiplier for the image height or width</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</li>
</ul>
</dd>
</dl>
<div class="math">
\[H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingbilinear2d">
<h3><span class="hidden-section">UpsamplingBilinear2d</span><a class="headerlink" href="#upsamplingbilinear2d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.UpsamplingBilinear2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingBilinear2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/modules/upsampling.html#UpsamplingBilinear2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.UpsamplingBilinear2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D bilinear upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <cite>size</cite> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a tuple of ints <cite>(H_out, W_out)</cite> output sizes</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the multiplier for the image height or width</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>. It is
equivalent to <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</li>
</ul>
</dd>
</dl>
<div class="math">
\[H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],</span>
<span class="go">          [ 1.6667,  2.0000,  2.3333,  2.6667],</span>
<span class="go">          [ 2.3333,  2.6667,  3.0000,  3.3333],</span>
<span class="go">          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dataparallel-layers-multi-gpu-distributed">
<h2>DataParallel layers (multi-GPU, distributed)<a class="headerlink" href="#dataparallel-layers-multi-gpu-distributed" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dataparallel">
<h3><span class="hidden-section">DataParallel</span><a class="headerlink" href="#dataparallel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.DataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">DataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/data_parallel.html#DataParallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.DataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements data parallelism at the module level.</p>
<p>This container parallelizes the application of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> by
splitting the input across the specified devices by chunking in the batch
dimension (other objects will be copied once per device). In the forward
pass, the module is replicated on each device, and each replica handles a
portion of the input. During the backwards pass, gradients from each replica
are summed into the original module.</p>
<p>The batch size should be larger than the number of GPUs used.</p>
<p>See also: <a class="reference internal" href="notes/cuda.html#cuda-nn-dataparallel-instead"><span class="std std-ref">Use nn.DataParallel instead of multiprocessing</span></a></p>
<p>Arbitrary positional and keyword inputs are allowed to be passed into
DataParallel EXCEPT Tensors. All tensors will be scattered on dim
specified (default 0). Primitive types will be broadcasted, but all
other types will be a shallow copy and can be corrupted if written to in
the model’s forward pass.</p>
<p>The parallelized <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> must have its parameters and buffers on
<code class="docutils literal notranslate"><span class="pre">device_ids[0]</span></code> before running this <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a>
module.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">In each forward, <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> is <strong>replicated</strong> on each device, so any
updates to the runing module in <code class="docutils literal notranslate"><span class="pre">forward</span></code> will be lost. For example,
if <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> has a counter attribute that is incremented in each
<code class="docutils literal notranslate"><span class="pre">forward</span></code>, it will always stay at the initial value becasue the update
is done on the replicas which are destroyed after <code class="docutils literal notranslate"><span class="pre">forward</span></code>. However,
<a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a> guarantees that the replica on
<code class="docutils literal notranslate"><span class="pre">device[0]</span></code> will have its parameters and buffers sharing storage with
the base parallelized <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code>. So <strong>in-place</strong> updates to the
parameters or buffers on <code class="docutils literal notranslate"><span class="pre">device[0]</span></code> will be recorded. E.g.,
<a class="reference internal" href="#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a> and <a class="reference internal" href="#torch.nn.utils.spectral_norm" title="torch.nn.utils.spectral_norm"><code class="xref py py-func docutils literal notranslate"><span class="pre">spectral_norm()</span></code></a>
rely on this behavior to update the buffers.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Forward and backward hooks defined on <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and its submodules
will be invoked <code class="docutils literal notranslate"><span class="pre">len(device_ids)</span></code> times, each with inputs located on
a particular device. Particularly, the hooks are only guaranteed to be
executed in correct order with respect to operations on corresponding
devices. For example, it is not guaranteed that hooks set via
<a class="reference internal" href="#torch.nn.Module.register_forward_pre_hook" title="torch.nn.Module.register_forward_pre_hook"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_forward_pre_hook()</span></code></a> be executed before
<cite>all</cite> <code class="docutils literal notranslate"><span class="pre">len(device_ids)</span></code> <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> calls, but
that each such hook be executed before the corresponding
<a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> call of that device.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">When <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> returns a scalar (i.e., 0-dimensional tensor) in
<code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code>, this wrapper will return a vector of length equal to
number of devices used in data parallelism, containing the result from
each device.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There is a subtlety in using the
<code class="docutils literal notranslate"><span class="pre">pack</span> <span class="pre">sequence</span> <span class="pre">-&gt;</span> <span class="pre">recurrent</span> <span class="pre">network</span> <span class="pre">-&gt;</span> <span class="pre">unpack</span> <span class="pre">sequence</span></code> pattern in a
<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> wrapped in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a>.
See <a class="reference internal" href="notes/faq.html#pack-rnn-unpack-with-data-parallelism"><span class="std std-ref">My recurrent network doesn’t work with data parallelism</span></a> section in FAQ for
details.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – module to be parallelized</li>
<li><strong>device_ids</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – device location of output (default: device_ids[0])</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to be parallelized</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="distributeddataparallel">
<h3><span class="hidden-section">DistributedDataParallel</span><a class="headerlink" href="#distributeddataparallel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.parallel.DistributedDataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.parallel.</code><code class="descname">DistributedDataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em>, <em>broadcast_buffers=True</em>, <em>process_group=None</em>, <em>bucket_cap_mb=25</em>, <em>check_reduction=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/distributed.html#DistributedDataParallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.parallel.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements distributed data parallelism that is based on
torch.distributed package at the module level.</p>
<p>This container parallelizes the application of the given module by
splitting the input across the specified devices by chunking in the batch
dimension. The module is replicated on each machine and each device, and
each such replica handles a portion of the input. During the backwards
pass, gradients from each node are averaged.</p>
<p>The batch size should be larger than the number of GPUs used locally. It
should also be an integer multiple of the number of GPUs so that each chunk
is the same size (so that each GPU processes the same number of samples).</p>
<p>See also: <a class="reference internal" href="distributed.html#distributed-basics"><span class="std std-ref">Basics</span></a> and <a class="reference internal" href="notes/cuda.html#cuda-nn-dataparallel-instead"><span class="std std-ref">Use nn.DataParallel instead of multiprocessing</span></a>.
The same constraints on input as in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code></a> apply.</p>
<p>Creation of this class requires that <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> to be already
initialized, by calling <a class="reference internal" href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.init_process_group()</span></code></a></p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> can be used in the following two ways:</p>
<ol class="arabic simple">
<li>Single-Process Multi-GPU</li>
</ol>
<p>In this case, a single process will be
spawned on each host/node and each process will operate on all the GPUs
of the node where it’s running. To use <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> in
this way, you can simply construct the model as the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># device_ids will include all GPU devices be default</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>Multi-Process Single-GPU</li>
</ol>
<p>This is the highly recommended way to use <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>, with
multiple processes, each of which operates on a single GPU. This is
currently the fastest approach to do data parallel training using PyTorch
and applies to both single-node(multi-GPU) and multi-node data
parallel training. It is proven to be significantly faster than
<a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code></a> for single-node multi-GPU data
parallel training.</p>
<p>Here is how to use it: on each host with N GPUs, you should spawn up N
processes, while ensuring that each process invidually works on a single GPU
from 0 to N-1. Therefore, it is your job to ensure that your training script
operates on a single given GPU by calling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
<p>where i is from 0 to N-1. In each process, you should refer the following
to construct this module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to spawn up multiple processes per node, you can use either
<code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.multiprocessing.spawn</span></code></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal notranslate"><span class="pre">nccl</span></code> backend is currently the fastest and
highly recommended backend to be used with Multi-Process Single-GPU
distributed training and this applies to both single-node and multi-node
distributed training</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module works only with the <code class="docutils literal notranslate"><span class="pre">gloo</span></code> and <code class="docutils literal notranslate"><span class="pre">nccl</span></code> backends.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Constructor, forward method, and differentiation of the output (or a
function of the output of this module) is a distributed synchronization
point. Take that into account in case different processes might be
executing different code.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all parameters are registered in the model by the
time it is created. No parameters should be added nor removed later.
Same applies to buffers.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all parameters are registered in the model of each
distributed processes are in the same order. The module itself will
conduct gradient all-reduction following the reverse order of the
registered parameters of the model. In other wise, it is users’
responsibility to ensure that each distributed process has the exact
same model and thus the exact parameter registeration order.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all buffers and gradients are dense.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module doesn’t work with <a class="reference internal" href="autograd.html#torch.autograd.grad" title="torch.autograd.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a> (i.e. it will
only work if gradients are to be accumulated in <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes of
parameters).</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If you plan on using this module with a <code class="docutils literal notranslate"><span class="pre">nccl</span></code> backend or a <code class="docutils literal notranslate"><span class="pre">gloo</span></code>
backend (that uses Infiniband), together with a DataLoader that uses
multiple workers, please change the multiprocessing start method to
<code class="docutils literal notranslate"><span class="pre">forkserver</span></code> (Python 3 only) or <code class="docutils literal notranslate"><span class="pre">spawn</span></code>. Unfortunately
Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will
likely experience deadlocks if you don’t change this setting.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Forward and backward hooks defined on <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and its submodules
won’t be invoked anymore, unless the hooks are initialized in the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">You should never try to change your model’s parameters after wrapping
up your model with DistributedDataParallel. In other words, when
wrapping up your model with DistributedDataParallel, the constructor of
DistributedDataParallel will register the additional gradient
reduction functions on all the parameters of the model itself at the
time of construction. If you change the model’s parameters after
the DistributedDataParallel construction, this is not supported and
unexpected behaviors can happen, since some parameters’ gradient
reduction functions might not get called.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Parameters are never broadcast between processes. The module performs
an all-reduce step on gradients and assumes that they will be modified
by the optimizer in all processes in the same way. Buffers
(e.g. BatchNorm stats) are broadcast from the module in process of rank
0, to all other replicas in the system in every iteration.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – module to be parallelized</li>
<li><strong>device_ids</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – device location of output (default: device_ids[0])</li>
<li><strong>broadcast_buffers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – flag that enables syncing (broadcasting) buffers of
the module at beginning of the forward function.
(default: True)</li>
<li><strong>process_group</strong> – the process group to be used for distributed data
all-reduction. If None, the default process group, which
is created by <code class="docutils literal notranslate"><span class="pre">`torch.distributed.init_process_group`</span></code>,
will be used. (default: None)</li>
<li><strong>bucket_cap_mb</strong> – DistributedDataParallel will bucket parameters into
multiple buckets so that gradient reduction of each
bucket can potentially overlap with backward computation.
bucket_cap_mb controls the bucket size in MegaBytes (MB)
(default: 25)</li>
<li><strong>check_reduction</strong> – when setting to True, it enables DistributedDataParallel
to automatically check if the previous iteration’s
backward reductions were successfully issued at the
beginning of every iteration’s forward function.
You normally don’t need this option enabled unless you
are observing weird behaviors such as different ranks
are getting different gradients, which should not
happen if DistributedDataParallel is corrected used.
(default: False)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to be parallelized</p>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Example::</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pg</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="distributeddataparallelcpu">
<h3><span class="hidden-section">DistributedDataParallelCPU</span><a class="headerlink" href="#distributeddataparallelcpu" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.parallel.DistributedDataParallelCPU">
<em class="property">class </em><code class="descclassname">torch.nn.parallel.</code><code class="descname">DistributedDataParallelCPU</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/distributed_cpu.html#DistributedDataParallelCPU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.parallel.DistributedDataParallelCPU" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements distributed data parallelism for CPU at the module level.</p>
<p>This module supports the <code class="docutils literal notranslate"><span class="pre">mpi</span></code> and <code class="docutils literal notranslate"><span class="pre">gloo</span></code> backends.</p>
<p>This container parallelizes the application of the given module by splitting
the input across the specified devices by chunking in the batch
dimension. The module is replicated on each machine, and each such replica
handles a portion of the input. During the backwards pass, gradients from
each node are averaged.</p>
<p>This module could be used in conjunction with the DistributedSampler,
(see :class <cite>torch.utils.data.distributed.DistributedSampler</cite>)
which will load a subset of the original datset for each node with the same
batch size. So strong scaling should be configured like this:</p>
<p>n = 1, batch size = 12</p>
<p>n = 2, batch size = 64</p>
<p>n = 4, batch size = 32</p>
<p>n = 8, batch size = 16</p>
<p>Creation of this class requires the distributed package to be already
initialized in the process group mode
(see <a class="reference internal" href="distributed.html#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.init_process_group()</span></code></a>).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Constructor, forward method, and differentiation of the output (or a
function of the output of this module) is a distributed synchronization
point. Take that into account in case different node might be
executing different code.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all parameters are registered in the model by the
time it is created. No parameters should be added nor removed later.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module assumes all gradients are dense.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This module doesn’t work with <a class="reference internal" href="autograd.html#torch.autograd.grad" title="torch.autograd.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a> (i.e. it will
only work if gradients are to be accumulated in <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes of
parameters).</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Forward and backward hooks defined on <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and its submodules
won’t be invoked anymore, unless the hooks are initialized in the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Parameters are broadcast between nodes in the __init__() function. The
module performs an all-reduce step on gradients and assumes that they
will be modified by the optimizer in all nodes in the same way.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>module</strong> – module to be parallelized</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DistributedDataParallelCPU</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="utilities">
<h2>Utilities<a class="headerlink" href="#utilities" title="Permalink to this headline">¶</a></h2>
<div class="section" id="clip-grad-norm">
<h3><span class="hidden-section">clip_grad_norm_</span><a class="headerlink" href="#clip-grad-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.clip_grad_norm_">
<code class="descclassname">torch.nn.utils.</code><code class="descname">clip_grad_norm_</code><span class="sig-paren">(</span><em>parameters</em>, <em>max_norm</em>, <em>norm_type=2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/clip_grad.html#clip_grad_norm_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.clip_grad_norm_" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient norm of an iterable of parameters.</p>
<p>The norm is computed over all gradients together, as if they were
concatenated into a single vector. Gradients are modified in-place.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – max norm of the gradients</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – type of the used p-norm. Can be <code class="docutils literal notranslate"><span class="pre">'inf'</span></code> for
infinity norm.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Total norm of the parameters (viewed as a single vector).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="clip-grad-value">
<h3><span class="hidden-section">clip_grad_value_</span><a class="headerlink" href="#clip-grad-value" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.clip_grad_value_">
<code class="descclassname">torch.nn.utils.</code><code class="descname">clip_grad_value_</code><span class="sig-paren">(</span><em>parameters</em>, <em>clip_value</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/clip_grad.html#clip_grad_value_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.clip_grad_value_" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient of an iterable of parameters at specified value.</p>
<p>Gradients are modified in-place.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or </em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</li>
<li><strong>clip_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – maximum allowed value of the gradients
The gradients are clipped in the range [-clip_value, clip_value]</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="parameters-to-vector">
<h3><span class="hidden-section">parameters_to_vector</span><a class="headerlink" href="#parameters-to-vector" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.parameters_to_vector">
<code class="descclassname">torch.nn.utils.</code><code class="descname">parameters_to_vector</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/convert_parameters.html#parameters_to_vector"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.parameters_to_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert parameters to one vector</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterator of Tensors that are the
parameters of a model.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">The parameters represented by a single vector</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="vector-to-parameters">
<h3><span class="hidden-section">vector_to_parameters</span><a class="headerlink" href="#vector-to-parameters" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.vector_to_parameters">
<code class="descclassname">torch.nn.utils.</code><code class="descname">vector_to_parameters</code><span class="sig-paren">(</span><em>vec</em>, <em>parameters</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/convert_parameters.html#vector_to_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.vector_to_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert one vector to the parameters</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vec</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a single vector represents the parameters of a model.</li>
<li><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterator of Tensors that are the
parameters of a model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="weight-norm">
<h3><span class="hidden-section">weight_norm</span><a class="headerlink" href="#weight-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.weight_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">weight_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/weight_norm.html#weight_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies weight normalization to a parameter in the given module.</p>
<div class="math">
\[\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}

\]</div>
<p>Weight normalization is a reparameterization that decouples the magnitude
of a weight tensor from its direction. This replaces the parameter specified
by <cite>name</cite> (e.g. “weight”) with two parameters: one specifying the magnitude
(e.g. “weight_g”) and one specifying the direction (e.g. “weight_v”).
Weight normalization is implemented via a hook that recomputes the weight
tensor from the magnitude and direction before every <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>
call.</p>
<p>By default, with <cite>dim=0</cite>, the norm is computed independently per output
channel/plane. To compute a norm over the entire weight tensor, use
<cite>dim=None</cite>.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – dimension over which to compute the norm</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The original module with the weight norm hook</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="go">Linear (20 -&gt; 40)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_g</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_v</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40, 20])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="remove-weight-norm">
<h3><span class="hidden-section">remove_weight_norm</span><a class="headerlink" href="#remove-weight-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.remove_weight_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">remove_weight_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/weight_norm.html#remove_weight_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.remove_weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the weight normalization reparameterization from a module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="spectral-norm">
<h3><span class="hidden-section">spectral_norm</span><a class="headerlink" href="#spectral-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.spectral_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">spectral_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em>, <em>n_power_iterations=1</em>, <em>eps=1e-12</em>, <em>dim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/spectral_norm.html#spectral_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.spectral_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies spectral normalization to a parameter in the given module.</p>
<div class="math">
\[\mathbf{W} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})} \\
\sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}

\]</div>
<p>Spectral normalization stabilizes the training of discriminators (critics)
in Generaive Adversarial Networks (GANs) by rescaling the weight tensor
with spectral norm <span class="math">\(\sigma\)</span> of the weight matrix calculated using
power iteration method. If the dimension of the weight tensor is greater
than 2, it is reshaped to 2D in power iteration method to get spectral
norm. This is implemented via a hook that calculates spectral norm and
rescales weight before every <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> call.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1802.05957">Spectral Normalization for Generative Adversarial Networks</a> .</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</li>
<li><strong>n_power_iterations</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – number of power iterations to
calculate spectal norm</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – epsilon for numerical stability in
calculating norms</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – dimension corresponding to number of outputs,
the default is 0, except for modules that are instances of
ConvTranspose1/2/3d, when it is 1</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The original module with the spectal norm hook</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="go">Linear (20 -&gt; 40)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_u</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([20])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="remove-spectral-norm">
<h3><span class="hidden-section">remove_spectral_norm</span><a class="headerlink" href="#remove-spectral-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.remove_spectral_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">remove_spectral_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/spectral_norm.html#remove_spectral_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.remove_spectral_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the spectral normalization reparameterization from a module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">remove_spectral_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="packedsequence">
<h3><span class="hidden-section">PackedSequence</span><a class="headerlink" href="#packedsequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.PackedSequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">PackedSequence</code><span class="sig-paren">(</span><em>data</em>, <em>batch_sizes=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#PackedSequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.PackedSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds the data and list of <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_sizes</span></code> of a packed sequence.</p>
<p>All RNN modules accept packed sequences as inputs.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Instances of this class should never be created manually. They are meant
to be instantiated by functions like <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
<p class="last">Batch sizes represent the number elements at each sequence step in
the batch, not the varying sequence lengths passed to
<a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.  For instance, given data  <code class="docutils literal notranslate"><span class="pre">abc</span></code> and <cite>x</cite>
the <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> would contain data <code class="docutils literal notranslate"><span class="pre">axbc</span></code> with
<code class="docutils literal notranslate"><span class="pre">batch_sizes=[2,1,1]</span></code>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor containing packed sequence</li>
<li><strong>batch_sizes</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor of integers holding
information about the batch size at each sequence step</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pack-padded-sequence">
<h3><span class="hidden-section">pack_padded_sequence</span><a class="headerlink" href="#pack-padded-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pack_padded_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pack_padded_sequence</code><span class="sig-paren">(</span><em>input</em>, <em>lengths</em>, <em>batch_first=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pack_padded_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pack_padded_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs a Tensor containing padded sequences of variable length.</p>
<p>Input can be of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> where <cite>T</cite> is the length of the longest sequence
(equal to <code class="docutils literal notranslate"><span class="pre">lengths[0]</span></code>), <cite>B</cite> is the batch size, and <cite>*</cite> is any number of
dimensions (including 0). If <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> is True <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> inputs are
expected.</p>
<p>The sequences should be sorted by length in a decreasing order, i.e.
<code class="docutils literal notranslate"><span class="pre">input[:,0]</span></code> should be the longest sequence, and <code class="docutils literal notranslate"><span class="pre">input[:,B-1]</span></code> the
shortest one.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function accepts any input that has at least two dimensions. You
can apply it to pack the labels, and use the output of the RNN with
them to compute the loss directly. A Tensor can be retrieved from
a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object by accessing its <code class="docutils literal notranslate"><span class="pre">.data</span></code> attribute.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – padded batch of variable length sequences.</li>
<li><strong>lengths</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – list of sequences lengths of each batch element.</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input is expected in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code>
format.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pad-packed-sequence">
<h3><span class="hidden-section">pad_packed_sequence</span><a class="headerlink" href="#pad-packed-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pad_packed_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pad_packed_sequence</code><span class="sig-paren">(</span><em>sequence</em>, <em>batch_first=False</em>, <em>padding_value=0.0</em>, <em>total_length=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pad_packed_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pad_packed_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads a packed batch of variable length sequences.</p>
<p>It is an inverse operation to <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
<p>The returned Tensor’s data will be of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code>, where <cite>T</cite> is the length
of the longest sequence and <cite>B</cite> is the batch size. If <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> is True,
the data will be transposed into <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> format.</p>
<p>Batch elements will be ordered decreasingly by their length.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code> is useful to implement the
<code class="docutils literal notranslate"><span class="pre">pack</span> <span class="pre">sequence</span> <span class="pre">-&gt;</span> <span class="pre">recurrent</span> <span class="pre">network</span> <span class="pre">-&gt;</span> <span class="pre">unpack</span> <span class="pre">sequence</span></code> pattern in a
<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> wrapped in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a>.
See <a class="reference internal" href="notes/faq.html#pack-rnn-unpack-with-data-parallelism"><span class="std std-ref">this FAQ section</span></a> for
details.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>sequence</strong> (<em>PackedSequence</em>) – batch to pad</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output will be in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code>
format.</li>
<li><strong>padding_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – values for padded elements.</li>
<li><strong>total_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – if not <code class="docutils literal notranslate"><span class="pre">None</span></code>, the output will be padded to
have length <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code>. This method will throw <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a>
if <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code> is less than the max sequence length in
<code class="xref py py-attr docutils literal notranslate"><span class="pre">sequence</span></code>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple of Tensor containing the padded sequence, and a Tensor
containing the list of lengths of each sequence in the batch.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pad-sequence">
<h3><span class="hidden-section">pad_sequence</span><a class="headerlink" href="#pad-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pad_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pad_sequence</code><span class="sig-paren">(</span><em>sequences</em>, <em>batch_first=False</em>, <em>padding_value=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pad_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pad_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Pad a list of variable length Tensors with zero</p>
<p><code class="docutils literal notranslate"><span class="pre">pad_sequence</span></code> stacks a list of Tensors along a new dimension,
and pads them to equal length. For example, if the input is list of
sequences with size <code class="docutils literal notranslate"><span class="pre">L</span> <span class="pre">x</span> <span class="pre">*</span></code> and if batch_first is False, and <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code>
otherwise.</p>
<p><cite>B</cite> is batch size. It is equal to the number of elements in <code class="docutils literal notranslate"><span class="pre">sequences</span></code>.
<cite>T</cite> is length of the longest sequence.
<cite>L</cite> is length of the sequence.
<cite>*</cite> is any number of trailing dimensions, including none.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">pad_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([25, 3, 300])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function returns a Tensor of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> or <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code>
where <cite>T</cite> is the length of the longest sequence. This function assumes
trailing dimensions and type of all the Tensors in sequences are same.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>sequences</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – list of variable length sequences.</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – output will be in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> if True, or in
<code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> otherwise</li>
<li><strong>padding_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – value for padded elements. Default: 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tensor of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_first</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Tensor of size <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> otherwise</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pack-sequence">
<h3><span class="hidden-section">pack_sequence</span><a class="headerlink" href="#pack-sequence" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.utils.rnn.pack_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pack_sequence</code><span class="sig-paren">(</span><em>sequences</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/utils/rnn.html#pack_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.utils.rnn.pack_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs a list of variable length Tensors</p>
<p><code class="docutils literal notranslate"><span class="pre">sequences</span></code> should be a list of Tensors of size <code class="docutils literal notranslate"><span class="pre">L</span> <span class="pre">x</span> <span class="pre">*</span></code>, where <cite>L</cite> is
the length of a sequence and <cite>*</cite> is any number of trailing dimensions,
including zero. They should be sorted in the order of decreasing length.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">pack_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pack_sequence</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
<span class="go">PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sequences</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – A list of sequences of decreasing length.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-functional">
<h1>torch.nn.functional<a class="headerlink" href="#torch-nn-functional" title="Permalink to this headline">¶</a></h1>
<div class="section" id="convolution-functions">
<h2>Convolution functions<a class="headerlink" href="#convolution-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id20">
<h3><span class="hidden-section">conv1d</span><a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iW)\)</span></li>
<li><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} \times \frac{\text{in\_channels}}{\text{groups}} \times kW)\)</span></li>
<li><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or
a one-element tuple <cite>(sW,)</cite>. Default: 1</li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a one-element tuple <cite>(padW,)</cite>. Default: 0</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a one-element tuple <cite>(dW,)</cite>. Default: 1</li>
<li><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by
the number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id21">
<h3><span class="hidden-section">conv2d</span><a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iH \times iW)\)</span></li>
<li><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} \times \frac{\text{in\_channels}}{\text{groups}} \times kH \times kW)\)</span></li>
<li><strong>bias</strong> – optional bias tensor of shape <span class="math">\((\text{out\_channels})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: 1</li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padH, padW)</cite>. Default: 0</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dH, dW)</cite>. Default: 1</li>
<li><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id22">
<h3><span class="hidden-section">conv3d</span><a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iT \times iH \times iW)\)</span></li>
<li><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} \times \frac{\text{in\_channels}}{\text{groups}} \times kT \times kH \times kW)\)</span></li>
<li><strong>bias</strong> – optional bias tensor of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: 1</li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padT, padH, padW)</cite>. Default: 0</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</li>
<li><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by
the number of groups. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose1d">
<h3><span class="hidden-section">conv_transpose1d</span><a class="headerlink" href="#conv-transpose1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input signal
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iW)\)</span></li>
<li><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} \times \frac{\text{out\_channels}}{\text{groups}} \times kW)\)</span></li>
<li><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sW,)</span></code>. Default: 1</li>
<li><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padW,)</span></code>. Default: 0</li>
<li><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padW)</span></code>. Default: 0</li>
<li><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dW,)</span></code>. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose2d">
<h3><span class="hidden-section">conv_transpose2d</span><a class="headerlink" href="#conv-transpose2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iH \times iW)\)</span></li>
<li><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} \times \frac{\text{out\_channels}}{\text{groups}} \times kH \times kW)\)</span></li>
<li><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sH,</span> <span class="pre">sW)</span></code>. Default: 1</li>
<li><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padH,</span> <span class="pre">padW)</span></code>. Default: 0</li>
<li><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padH,</span> <span class="pre">out_padW)</span></code>.
Default: 0</li>
<li><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dH,</span> <span class="pre">dW)</span></code>. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose3d">
<h3><span class="hidden-section">conv_transpose3d</span><a class="headerlink" href="#conv-transpose3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iT \times iH \times iW)\)</span></li>
<li><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} \times \frac{\text{out\_channels}}{\text{groups}} \times kT \times kH \times kW)\)</span></li>
<li><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</li>
<li><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sT,</span> <span class="pre">sH,</span> <span class="pre">sW)</span></code>. Default: 1</li>
<li><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">kernel_size</span> <span class="pre">-</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padT,</span> <span class="pre">padH,</span> <span class="pre">padW)</span></code>. Default: 0</li>
<li><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(out_padT,</span> <span class="pre">out_padH,</span> <span class="pre">out_padW)</span></code>. Default: 0</li>
<li><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</li>
<li><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id23">
<h3><span class="hidden-section">unfold</span><a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.unfold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">unfold</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#unfold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts sliding local blocks from an batched input tensor.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Currently, only 4-D input tensors (batched image-like tensors) are
supported.</p>
</div>
<p>See <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Unfold</span></code></a> for details</p>
</dd></dl>

</div>
<div class="section" id="id24">
<h3><span class="hidden-section">fold</span><a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.fold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">fold</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#fold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines an array of sliding local blocks into a large containing
tensor.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Currently, only 4-D output tensors (batched image-like tensors) are
supported.</p>
</div>
<p>See <a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Fold</span></code></a> for details</p>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-functions">
<h2>Pooling functions<a class="headerlink" href="#pooling-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="avg-pool1d">
<h3><span class="hidden-section">avg_pool1d</span><a class="headerlink" href="#avg-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool1d" title="torch.nn.AvgPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iW)\)</span></li>
<li><strong>kernel_size</strong> – the size of the window. Can be a single number or a
tuple <span class="math">\((kW,)\)</span></li>
<li><strong>stride</strong> – the stride of the window. Can be a single number or a tuple
<cite>(sW,)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padW,)</cite>. Default: 0</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the
output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Examples::</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[ 2.,  4.,  6.]]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="avg-pool2d">
<h3><span class="hidden-section">avg_pool2d</span><a class="headerlink" href="#avg-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 2D average-pooling operation in <span class="math">\(kH \times kW\)</span> regions by step size
<span class="math">\(sH \times sW\)</span> steps. The number of output features is equal to the number of
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool2d" title="torch.nn.AvgPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iH \times iW)\)</span></li>
<li><strong>kernel_size</strong> – size of the pooling region. Can be a single number or a
tuple <span class="math">\((kH \times kW)\)</span></li>
<li><strong>stride</strong> – stride of the pooling operation. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padH, padW)</cite>. Default: 0</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="avg-pool3d">
<h3><span class="hidden-section">avg_pool3d</span><a class="headerlink" href="#avg-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 3D average-pooling operation in <span class="math">\(kT \times kH \times kW\)</span> regions by step
size <span class="math">\(sT \times sH \times sW\)</span> steps. The number of output features is equal to
<span class="math">\(\lfloor\frac{\text{input planes}}{sT}\rfloor\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor <span class="math">\((\text{minibatch} \times \text{in\_channels} \times iT \times iH \times iW)\)</span></li>
<li><strong>kernel_size</strong> – size of the pooling region. Can be a single number or a
tuple <span class="math">\((kT \times kH \times kW)\)</span></li>
<li><strong>stride</strong> – stride of the pooling operation. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padT, padH, padW)</cite>, Default: 0</li>
<li><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape</li>
<li><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="max-pool1d">
<h3><span class="hidden-section">max_pool1d</span><a class="headerlink" href="#max-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool1d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool2d">
<h3><span class="hidden-section">max_pool2d</span><a class="headerlink" href="#max-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool2d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool3d">
<h3><span class="hidden-section">max_pool3d</span><a class="headerlink" href="#max-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool3d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool1d">
<h3><span class="hidden-section">max_unpool1d</span><a class="headerlink" href="#max-unpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool1d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool2d">
<h3><span class="hidden-section">max_unpool2d</span><a class="headerlink" href="#max-unpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool2d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool3d">
<h3><span class="hidden-section">max_unpool3d</span><a class="headerlink" href="#max-unpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool3d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool1d">
<h3><span class="hidden-section">lp_pool1d</span><a class="headerlink" href="#lp-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.lp_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#lp_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.lp_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <cite>p</cite> is
zero, the gradient is set to zero as well.</p>
<p>See <a class="reference internal" href="#torch.nn.LPPool1d" title="torch.nn.LPPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">LPPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool2d">
<h3><span class="hidden-section">lp_pool2d</span><a class="headerlink" href="#lp-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.lp_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#lp_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.lp_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <cite>p</cite> is
zero, the gradient is set to zero as well.</p>
<p>See <a class="reference internal" href="#torch.nn.LPPool2d" title="torch.nn.LPPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">LPPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool1d">
<h3><span class="hidden-section">adaptive_max_pool1d</span><a class="headerlink" href="#adaptive-max-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool1d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool1d" title="torch.nn.AdaptiveMaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size (single integer)</li>
<li><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool2d">
<h3><span class="hidden-section">adaptive_max_pool2d</span><a class="headerlink" href="#adaptive-max-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool2d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size (single integer or
double-integer tuple)</li>
<li><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool3d">
<h3><span class="hidden-section">adaptive_max_pool3d</span><a class="headerlink" href="#adaptive-max-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool3d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool3d" title="torch.nn.AdaptiveMaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> – the target output size (single integer or
triple-integer tuple)</li>
<li><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool1d">
<h3><span class="hidden-section">adaptive_avg_pool1d</span><a class="headerlink" href="#adaptive-avg-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool1d" title="torch.nn.AdaptiveAvgPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size (single integer)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool2d">
<h3><span class="hidden-section">adaptive_avg_pool2d</span><a class="headerlink" href="#adaptive-avg-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size (single integer or
double-integer tuple)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool3d">
<h3><span class="hidden-section">adaptive_avg_pool3d</span><a class="headerlink" href="#adaptive-avg-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> – the target output size (single integer or
triple-integer tuple)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activation-functions">
<h2>Non-linear activation functions<a class="headerlink" href="#non-linear-activation-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id25">
<h3><span class="hidden-section">threshold</span><a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.threshold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor.</p>
<p>See <a class="reference internal" href="#torch.nn.Threshold" title="torch.nn.Threshold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Threshold</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.threshold_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold_</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.threshold_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.threshold" title="torch.nn.functional.threshold"><code class="xref py py-func docutils literal notranslate"><span class="pre">threshold()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id26">
<h3><span class="hidden-section">relu</span><a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise. See
<a class="reference internal" href="#torch.nn.ReLU" title="torch.nn.ReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.relu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu_</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.relu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.relu" title="torch.nn.functional.relu"><code class="xref py py-func docutils literal notranslate"><span class="pre">relu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id27">
<h3><span class="hidden-section">hardtanh</span><a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hardtanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hardtanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise. See <a class="reference internal" href="#torch.nn.Hardtanh" title="torch.nn.Hardtanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardtanh</span></code></a> for more
details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.hardtanh_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh_</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.hardtanh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardtanh()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id28">
<h3><span class="hidden-section">relu6</span><a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.relu6">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu6</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#relu6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.relu6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(\text{ReLU6}(x) = \min(\max(0,x), 6)\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.ReLU6" title="torch.nn.ReLU6"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU6</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id29">
<h3><span class="hidden-section">elu</span><a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.elu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#elu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.elu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ELU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.elu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu_</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.elu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.elu" title="torch.nn.functional.elu"><code class="xref py py-func docutils literal notranslate"><span class="pre">elu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id30">
<h3><span class="hidden-section">selu</span><a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.selu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">selu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#selu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\)</span>,
with <span class="math">\(\alpha=1.6732632423543772848170429916717\)</span> and
<span class="math">\(scale=1.0507009873554804934193349852946\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.SELU" title="torch.nn.SELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">SELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id31">
<h3><span class="hidden-section">celu</span><a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.celu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">celu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#celu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.celu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.CELU" title="torch.nn.CELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">CELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="leaky-relu">
<h3><span class="hidden-section">leaky_relu</span><a class="headerlink" href="#leaky-relu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.leaky_relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#leaky_relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.leaky_relu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu_</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.leaky_relu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.leaky_relu" title="torch.nn.functional.leaky_relu"><code class="xref py py-func docutils literal notranslate"><span class="pre">leaky_relu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id32">
<h3><span class="hidden-section">prelu</span><a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.prelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">prelu</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#prelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.prelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise the function
<span class="math">\(\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)\)</span> where weight is a
learnable parameter.</p>
<p>See <a class="reference internal" href="#torch.nn.PReLU" title="torch.nn.PReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">PReLU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id33">
<h3><span class="hidden-section">rrelu</span><a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.rrelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#rrelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.rrelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomized leaky ReLU.</p>
<p>See <a class="reference internal" href="#torch.nn.RReLU" title="torch.nn.RReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">RReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.rrelu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu_</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.rrelu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.rrelu" title="torch.nn.functional.rrelu"><code class="xref py py-func docutils literal notranslate"><span class="pre">rrelu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="glu">
<h3><span class="hidden-section">glu</span><a class="headerlink" href="#glu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.glu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">glu</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#glu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.glu" title="Permalink to this definition">¶</a></dt>
<dd><p>The gated linear unit. Computes:</p>
<div class="math">
\[H = A \times \sigma(B)\]</div>
<p>where <cite>input</cite> is split in half along <cite>dim</cite> to form <cite>A</cite> and <cite>B</cite>.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1612.08083">Language Modeling with Gated Convolutional Networks</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input tensor</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension on which to split the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id34">
<h3><span class="hidden-section">logsigmoid</span><a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.logsigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">logsigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.logsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math">\(\text{LogSigmoid}(x) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.LogSigmoid" title="torch.nn.LogSigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogSigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id35">
<h3><span class="hidden-section">hardshrink</span><a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hardshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hardshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hard shrinkage function element-wise</p>
<p>See <a class="reference internal" href="#torch.nn.Hardshrink" title="torch.nn.Hardshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id36">
<h3><span class="hidden-section">tanhshrink</span><a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.tanhshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanhshrink</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#tanhshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(\text{Tanhshrink}(x) = x - \text{Tanh}(x)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Tanhshrink" title="torch.nn.Tanhshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanhshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id37">
<h3><span class="hidden-section">softsign</span><a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softsign">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softsign</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, the function <span class="math">\(\text{SoftSign}(x) = \frac{x}{1 + |x|}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Softsign" title="torch.nn.Softsign"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softsign</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id38">
<h3><span class="hidden-section">softplus</span><a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softplus">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softplus</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softplus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id39">
<h3><span class="hidden-section">softmin</span><a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softmin">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmin</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#softmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmin function.</p>
<p>Note that <span class="math">\(\text{Softmin}(x) = \text{Softmax}(-x)\)</span>. See softmax definition for mathematical formula.</p>
<p>See <a class="reference internal" href="#torch.nn.Softmin" title="torch.nn.Softmin"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmin</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmin will be computed (so every slice
along dim will sum to 1).</li>
<li><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>:param If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation:
:param is performed. This is useful for preventing data type overflows. Default: None.</p>
</dd></dl>

</div>
<div class="section" id="id40">
<h3><span class="hidden-section">softmax</span><a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax function.</p>
<p>Softmax is defined as:</p>
<p><span class="math">\(\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}\)</span></p>
<p>It is applied to all slices along dim, and will re-scale them so that the elements
lie in the range <cite>(0, 1)</cite> and sum to 1.</p>
<p>See <a class="reference internal" href="#torch.nn.Softmax" title="torch.nn.Softmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmax</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmax will be computed.</li>
<li><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>:param If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation:
:param is performed. This is useful for preventing data type overflows. Default: None.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function doesn’t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use log_softmax instead (it’s faster and has better numerical properties).</p>
</div>
</dd></dl>

</div>
<div class="section" id="id41">
<h3><span class="hidden-section">softshrink</span><a class="headerlink" href="#id41" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise</p>
<p>See <a class="reference internal" href="#torch.nn.Softshrink" title="torch.nn.Softshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="gumbel-softmax">
<h3><span class="hidden-section">gumbel_softmax</span><a class="headerlink" href="#gumbel-softmax" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.gumbel_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">gumbel_softmax</code><span class="sig-paren">(</span><em>logits</em>, <em>tau=1.0</em>, <em>hard=False</em>, <em>eps=1e-10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#gumbel_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.gumbel_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Sample from the Gumbel-Softmax distribution and optionally discretize.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>logits</strong> – <cite>[batch_size, num_features]</cite> unnormalized log probabilities</li>
<li><strong>tau</strong> – non-negative scalar temperature</li>
<li><strong>hard</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the returned samples will be discretized as one-hot vectors,
but will be differentiated as if it is the soft sample in autograd</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Sampled tensor of shape <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">x</span> <span class="pre">num_features</span></code> from the Gumbel-Softmax distribution.
If <code class="docutils literal notranslate"><span class="pre">hard=True</span></code>, the returned samples will be one-hot, otherwise they will
be probability distributions that sum to 1 across features</p>
</td>
</tr>
</tbody>
</table>
<p>Constraints:</p>
<ul class="simple">
<li>Currently only work on 2D input <code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code> tensor of shape <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">x</span> <span class="pre">num_features</span></code></li>
</ul>
<p>Based on
<a class="reference external" href="https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb">https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb</a> ,
(MIT license)</p>
</dd></dl>

</div>
<div class="section" id="log-softmax">
<h3><span class="hidden-section">log_softmax</span><a class="headerlink" href="#log-softmax" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.log_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">log_softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax followed by a logarithm.</p>
<p>While mathematically equivalent to log(softmax(x)), doing these two
operations separately is slower, and numerically unstable. This function
uses an alternative formulation to compute the output and gradient correctly.</p>
<p>See <a class="reference internal" href="#torch.nn.LogSoftmax" title="torch.nn.LogSoftmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogSoftmax</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which log_softmax will be computed.</li>
<li><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>:param If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation:
:param is performed. This is useful for preventing data type overflows. Default: None.</p>
</dd></dl>

</div>
<div class="section" id="id42">
<h3><span class="hidden-section">tanh</span><a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.tanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Tanh" title="torch.nn.Tanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanh</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id43">
<h3><span class="hidden-section">sigmoid</span><a class="headerlink" href="#id43" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.sigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
</div>
<div class="section" id="normalization-functions">
<h2>Normalization functions<a class="headerlink" href="#normalization-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="batch-norm">
<h3><span class="hidden-section">batch_norm</span><a class="headerlink" href="#batch-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.batch_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">batch_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean</em>, <em>running_var</em>, <em>weight=None</em>, <em>bias=None</em>, <em>training=False</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#batch_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.batch_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization for each channel across a batch of data.</p>
<p>See <a class="reference internal" href="#torch.nn.BatchNorm1d" title="torch.nn.BatchNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm1d</span></code></a>, <a class="reference internal" href="#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a>,
<a class="reference internal" href="#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="instance-norm">
<h3><span class="hidden-section">instance_norm</span><a class="headerlink" href="#instance-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.instance_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">instance_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean=None</em>, <em>running_var=None</em>, <em>weight=None</em>, <em>bias=None</em>, <em>use_input_stats=True</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#instance_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.instance_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization for each channel in each data sample in a
batch.</p>
<p>See <a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a>, <a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a>,
<a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="layer-norm">
<h3><span class="hidden-section">layer_norm</span><a class="headerlink" href="#layer-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.layer_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">layer_norm</code><span class="sig-paren">(</span><em>input</em>, <em>normalized_shape</em>, <em>weight=None</em>, <em>bias=None</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#layer_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.layer_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization for last certain number of dimensions.</p>
<p>See <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="local-response-norm">
<h3><span class="hidden-section">local_response_norm</span><a class="headerlink" href="#local-response-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.local_response_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">local_response_norm</code><span class="sig-paren">(</span><em>input</em>, <em>size</em>, <em>alpha=0.0001</em>, <em>beta=0.75</em>, <em>k=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#local_response_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.local_response_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies local response normalization over an input signal composed of
several input planes, where channels occupy the second dimension.
Applies normalization across channels.</p>
<p>See <a class="reference internal" href="#torch.nn.LocalResponseNorm" title="torch.nn.LocalResponseNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocalResponseNorm</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="normalize">
<h3><span class="hidden-section">normalize</span><a class="headerlink" href="#normalize" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.normalize">
<code class="descclassname">torch.nn.functional.</code><code class="descname">normalize</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em>, <em>dim=1</em>, <em>eps=1e-12</em>, <em>out=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#normalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs <span class="math">\(L_p\)</span> normalization of inputs over specified dimension.</p>
<p>For a tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of sizes <span class="math">\((n_0, ..., n_{dim}, ..., n_k)\)</span>, each
<span class="math">\(n_{dim}\)</span> -element vector <span class="math">\(v\)</span> along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is transformed as</p>
<div class="math">
\[v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.

\]</div>
<p>With the default arguments it uses the Euclidean norm over vectors along dimension <span class="math">\(1\)</span> for normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of any shape</li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the exponent value in the norm formulation. Default: 2</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – small value to avoid division by zero. Default: 1e-12</li>
<li><strong>out</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is used, this
operation won’t be differentiable.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="linear-functions">
<h2>Linear functions<a class="headerlink" href="#linear-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id44">
<h3><span class="hidden-section">linear</span><a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.linear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">linear</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math">\(y = xA^T + b\)</span>.</p>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: <span class="math">\((N, *, in\_features)\)</span> where <cite>*</cite> means any number of
additional dimensions</li>
<li>Weight: <span class="math">\((out\_features, in\_features)\)</span></li>
<li>Bias: <span class="math">\((out\_features)\)</span></li>
<li>Output: <span class="math">\((N, *, out\_features)\)</span></li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="id45">
<h3><span class="hidden-section">bilinear</span><a class="headerlink" href="#id45" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">bilinear</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.bilinear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="dropout-functions">
<h2>Dropout functions<a class="headerlink" href="#dropout-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id46">
<h3><span class="hidden-section">dropout</span><a class="headerlink" href="#id46" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli
distribution.</p>
<p>See <a class="reference internal" href="#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> – probability of an element to be zeroed. Default: 0.5</li>
<li><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Defualt: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="alpha-dropout">
<h3><span class="hidden-section">alpha_dropout</span><a class="headerlink" href="#alpha-dropout" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.alpha_dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">alpha_dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#alpha_dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.alpha_dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies alpha dropout to the input.</p>
<p>See <a class="reference internal" href="#torch.nn.AlphaDropout" title="torch.nn.AlphaDropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlphaDropout</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="id47">
<h3><span class="hidden-section">dropout2d</span><a class="headerlink" href="#id47" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout2d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 2D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 2D tensor <span class="math">\(\text{input}[i, j]\)</span>) of the input tensor).
Each channel will be zeroed out independently on every forward call.
with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>See <a class="reference internal" href="#torch.nn.Dropout2d" title="torch.nn.Dropout2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout2d</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> – probability of a channel to be zeroed. Default: 0.5</li>
<li><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Defualt: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="id48">
<h3><span class="hidden-section">dropout3d</span><a class="headerlink" href="#id48" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout3d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 3D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 3D tensor <span class="math">\(\text{input}[i, j]\)</span>) of the input tensor).
Each channel will be zeroed out independently on every forward call.
with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>See <a class="reference internal" href="#torch.nn.Dropout3d" title="torch.nn.Dropout3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout3d</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> – probability of a channel to be zeroed. Default: 0.5</li>
<li><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Defualt: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="sparse-functions">
<h2>Sparse functions<a class="headerlink" href="#sparse-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id49">
<h3><span class="hidden-section">embedding</span><a class="headerlink" href="#id49" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.embedding">
<code class="descclassname">torch.nn.functional.</code><code class="descname">embedding</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that looks up embeddings in a fixed dictionary and size.</p>
<p>This module is often used to retrieve word embeddings using indices.
The input to the module is a list of indices, and the embedding matrix,
and the output is the corresponding word embeddings.</p>
<p>See <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<em>LongTensor</em>) – Tensor containing indices into the embedding matrix</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The embedding matrix with number of rows equal to the maximum possible index + 1,
and number of columns equal to the embedding size</li>
<li><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – If given, pads the output with the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code>
(initialized to zeros) whenever it encounters the index.</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.
Note: this will modify <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> in-place.</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> will be a sparse tensor. See Notes under
<a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details regarding sparse gradients.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: LongTensor of arbitrary shape containing the indices to extract</li>
<li><dl class="first docutils">
<dt>Weight: Embedding matrix of floating point type with shape <cite>(V, embedding_dim)</cite>,</dt>
<dd>where V = maximum index + 1 and embedding_dim = the embedding size</dd>
</dl>
</li>
<li>Output: <cite>(*, embedding_dim)</cite>, where <cite>*</cite> is the input shape</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># an embedding matrix containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">)</span>
<span class="go">tensor([[[ 0.8490,  0.9625,  0.6753],</span>
<span class="go">         [ 0.9666,  0.7761,  0.6108],</span>
<span class="go">         [ 0.6246,  0.9751,  0.3618],</span>
<span class="go">         [ 0.4161,  0.2419,  0.7383]],</span>

<span class="go">        [[ 0.6246,  0.9751,  0.3618],</span>
<span class="go">         [ 0.0237,  0.7794,  0.0528],</span>
<span class="go">         [ 0.9666,  0.7761,  0.6108],</span>
<span class="go">         [ 0.3385,  0.8612,  0.1867]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.5609,  0.5384,  0.8720],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.6262,  0.2438,  0.7471]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="embedding-bag">
<h3><span class="hidden-section">embedding_bag</span><a class="headerlink" href="#embedding-bag" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.embedding_bag">
<code class="descclassname">torch.nn.functional.</code><code class="descname">embedding_bag</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>offsets=None</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#embedding_bag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.embedding_bag" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sums, means or maxes of ‘bags’ of embeddings, without instantiating the
intermediate embeddings.</p>
<p>See <a class="reference internal" href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.EmbeddingBag</span></code></a> for more details.
.. include:: cuda_deterministic_backward.rst</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<em>LongTensor</em>) – Tensor containing bags of indices into the embedding matrix</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The embedding matrix with number of rows equal to the maximum possible index + 1,
and number of columns equal to the embedding size</li>
<li><strong>offsets</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) – Only used when <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D. <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> determines
the starting index position of each bag (sequence) in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.
Note: this will modify <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> in-place.</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The <code class="docutils literal notranslate"><span class="pre">p</span></code> in the <code class="docutils literal notranslate"><span class="pre">p</span></code>-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option.
Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</li>
<li><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>. Specifies the way to reduce the bag.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></li>
<li><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> will be a sparse tensor. See Notes under
<a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details regarding sparse gradients.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Shape:</p>
<blockquote>
<div><ul>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (LongTensor) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> (LongTensor, optional)</p>
<ul>
<li><p class="first">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 2D of shape <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">N</span></code>,</p>
<p>it will be treated as <code class="docutils literal notranslate"><span class="pre">B</span></code> bags (sequences) each of fixed length <code class="docutils literal notranslate"><span class="pre">N</span></code>, and
this will return <code class="docutils literal notranslate"><span class="pre">B</span></code> values aggregated in a way depending on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is ignored and required to be <code class="docutils literal notranslate"><span class="pre">None</span></code> in this case.</p>
</li>
<li><p class="first">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D of shape <code class="docutils literal notranslate"><span class="pre">N</span></code>,</p>
<p>it will be treated as a concatenation of multiple bags (sequences).
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is required to be a 1D tensor containing the
starting index positions of each bag in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. Therefore,
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> of shape <code class="docutils literal notranslate"><span class="pre">B</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be viewed as
having <code class="docutils literal notranslate"><span class="pre">B</span></code> bags. Empty bags (i.e., having 0-length) will have
returned vectors filled by zeros.</p>
</li>
</ul>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> (Tensor): the learnable weights of the module of
shape <code class="docutils literal notranslate"><span class="pre">(num_embeddings</span> <span class="pre">x</span> <span class="pre">embedding_dim)</span></code></p>
</li>
<li><p class="first"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code>: aggregated embedding values of shape <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">embedding_dim</span></code></p>
</li>
</ul>
</div></blockquote>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="go">tensor([[ 0.3397,  0.3552,  0.5545],</span>
<span class="go">        [ 0.5893,  0.4386,  0.5882]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="id50">
<h2>Distance functions<a class="headerlink" href="#id50" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pairwise-distance">
<h3><span class="hidden-section">pairwise_distance</span><a class="headerlink" href="#pairwise-distance" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pairwise_distance">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pairwise_distance</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>p=2.0</em>, <em>eps=1e-06</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pairwise_distance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pairwise_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.PairwiseDistance" title="torch.nn.PairwiseDistance"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.PairwiseDistance</span></code></a> for details</p>
</dd></dl>

</div>
<div class="section" id="cosine-similarity">
<h3><span class="hidden-section">cosine_similarity</span><a class="headerlink" href="#cosine-similarity" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cosine_similarity">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_similarity</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>dim=1</em>, <em>eps=1e-8</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between x1 and x2, computed along dim.</p>
<div class="math">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}

\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>x1</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – First input.</li>
<li><strong>x2</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Second input (of size matching x1).</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension of vectors. Default: 1</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-8</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite>.</li>
<li>Output: <span class="math">\((\ast_1, \ast_2)\)</span> where 1 is at position <cite>dim</cite>.</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pdist">
<h3><span class="hidden-section">pdist</span><a class="headerlink" href="#pdist" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pdist">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pdist</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.pdist" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the p-norm distance between every pair of row vectors in the input.
This is identical to the upper triangular portion, excluding the diagonal, of
<cite>torch.norm(input[:, None] - input, dim=2, p=p)</cite>. This function will be faster
if the rows are contiguous.</p>
<p>If input has shape <span class="math">\(N \times M\)</span> then the output will have shape
<span class="math">\(\frac{1}{2} N (N - 1)\)</span>.</p>
<p>This function is equivalent to <cite>scipy.spatial.distance.pdist(input,
‘minkowski’, p=p)</cite> if <span class="math">\(p \in (0, \infty)\)</span>. When <span class="math">\(p = 0\)</span> it is
equivalent to <cite>scipy.spatial.distance.pdist(input, ‘hamming’) * M</cite>.
When <span class="math">\(p = \infty\)</span>, the closest scipy function is
<cite>scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – input tensor of shape <span class="math">\(N \times M\)</span>.</li>
<li><strong>p</strong> – p value for the p-norm distance to calculate between each vector pair
<span class="math">\(\in [0, \infty]\)</span>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="id51">
<h2>Loss functions<a class="headerlink" href="#id51" title="Permalink to this headline">¶</a></h2>
<div class="section" id="binary-cross-entropy">
<h3><span class="hidden-section">binary_cross_entropy</span><a class="headerlink" href="#binary-cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#binary_cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures the Binary Cross Entropy
between the target and the output.</p>
<p>See <a class="reference internal" href="#torch.nn.BCELoss" title="torch.nn.BCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BCELoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – Tensor of arbitrary shape</li>
<li><strong>target</strong> – Tensor of the same shape as input</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight
if provided it’s repeated to match input tensor shape</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="binary-cross-entropy-with-logits">
<h3><span class="hidden-section">binary_cross_entropy_with_logits</span><a class="headerlink" href="#binary-cross-entropy-with-logits" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy_with_logits">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy_with_logits</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em>, <em>pos_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#binary_cross_entropy_with_logits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy_with_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures Binary Cross Entropy between target and output
logits.</p>
<p>See <a class="reference internal" href="#torch.nn.BCEWithLogitsLoss" title="torch.nn.BCEWithLogitsLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BCEWithLogitsLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – Tensor of arbitrary shape</li>
<li><strong>target</strong> – Tensor of the same shape as input</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight
if provided it’s repeated to match input tensor shape</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
<li><strong>pos_weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a weight of positive examples.
Must be a vector with length equal to the number of classes.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poisson-nll-loss">
<h3><span class="hidden-section">poisson_nll_loss</span><a class="headerlink" href="#poisson-nll-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.poisson_nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">poisson_nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>log_input=True</em>, <em>full=False</em>, <em>size_average=None</em>, <em>eps=1e-08</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#poisson_nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.poisson_nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Poisson negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.PoissonNLLLoss" title="torch.nn.PoissonNLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">PoissonNLLLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – expectation of underlying Poisson distribution.</li>
<li><strong>target</strong> – random sample <span class="math">\(target \sim \text{Poisson}(input)\)</span>.</li>
<li><strong>log_input</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code> the loss is computed as
<span class="math">\(\exp(\text{input}) - \text{target} * \text{input}\)</span>, if <code class="docutils literal notranslate"><span class="pre">False</span></code> then loss is
<span class="math">\(\text{input} - \text{target} * \log(\text{input}+\text{eps})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>full</strong> – whether to compute full loss, i. e. to add the Stirling
approximation term. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
<span class="math">\(\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})\)</span>.</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid evaluation of <span class="math">\(\log(0)\)</span> when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">log_input`=``False`</span></code>. Default: 1e-8</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="cosine-embedding-loss">
<h3><span class="hidden-section">cosine_embedding_loss</span><a class="headerlink" href="#cosine-embedding-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cosine_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_embedding_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#cosine_embedding_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cosine_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.CosineEmbeddingLoss" title="torch.nn.CosineEmbeddingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CosineEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="cross-entropy">
<h3><span class="hidden-section">cross_entropy</span><a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <cite>log_softmax</cite> and <cite>nll_loss</cite> in a single
function.</p>
<p>See <a class="reference internal" href="#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K &gt; 1\)</span>
in the case of K-dimensional loss.</li>
<li><strong>target</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>,
or <span class="math">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span> for
K-dimensional loss.</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="ctc-loss">
<h3><span class="hidden-section">ctc_loss</span><a class="headerlink" href="#ctc-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.ctc_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">ctc_loss</code><span class="sig-paren">(</span><em>log_probs</em>, <em>targets</em>, <em>input_lengths</em>, <em>target_lengths</em>, <em>blank=0</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#ctc_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.ctc_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The Connectionist Temporal Classification loss.</p>
<p>See <a class="reference internal" href="#torch.nn.CTCLoss" title="torch.nn.CTCLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTCLoss</span></code></a> for details.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>log_probs</strong> – <span class="math">\((T, N, C)\)</span> where <cite>C = number of characters in alphabet including blank</cite>,
<cite>T = input length</cite>, and <cite>N = batch size</cite>.
The logarithmized probabilities of the outputs
(e.g. obtained with <a class="reference internal" href="#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.log_softmax()</span></code></a>).</li>
<li><strong>targets</strong> – <span class="math">\((N, S)\)</span> or <cite>(sum(target_lengths))</cite>.
Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.</li>
<li><strong>input_lengths</strong> – <span class="math">\((N)\)</span>.
Lengths of the inputs (must each be <span class="math">\(\leq T\)</span>)</li>
<li><strong>target_lengths</strong> – <span class="math">\((N)\)</span>.
Lengths of the targets</li>
<li><strong>blank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Blank label. Default <span class="math">\(0\)</span>.</li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the output losses will be divided by the target lengths and
then the mean over the batch is taken. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">16</span><span class="p">,),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">,(</span><span class="mi">16</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hinge-embedding-loss">
<h3><span class="hidden-section">hinge_embedding_loss</span><a class="headerlink" href="#hinge-embedding-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hinge_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hinge_embedding_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>margin=1.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hinge_embedding_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hinge_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.HingeEmbeddingLoss" title="torch.nn.HingeEmbeddingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">HingeEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="kl-div">
<h3><span class="hidden-section">kl_div</span><a class="headerlink" href="#kl-div" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.kl_div">
<code class="descclassname">torch.nn.functional.</code><code class="descname">kl_div</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#kl_div"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.kl_div" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss.</p>
<p>See <a class="reference internal" href="#torch.nn.KLDivLoss" title="torch.nn.KLDivLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">KLDivLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – Tensor of arbitrary shape</li>
<li><strong>target</strong> – Tensor of the same shape as input</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘batchmean’ | ‘sum’ | ‘mean’.
‘none’: no reduction will be applied
‘batchmean’: the sum of the output will be divided by the batchsize
‘sum’: the output will be summed
‘mean’: the output will be divided by the number of elements in the output
Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>:param .. note:: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated,: and in the meantime, specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>.
:param .. note:: <cite>reduction=’mean’</cite> doesn’t return the true kl divergence value, please use: <cite>reduction=’batchmean’</cite> which aligns with KL math definition.</p>
<blockquote>
<div>In the next major release, ‘mean’ will be changed to be the same as ‘batchmean’.</div></blockquote>
</dd></dl>

</div>
<div class="section" id="l1-loss">
<h3><span class="hidden-section">l1_loss</span><a class="headerlink" href="#l1-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#l1_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that takes the mean element-wise absolute value difference.</p>
<p>See <a class="reference internal" href="#torch.nn.L1Loss" title="torch.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="mse-loss">
<h3><span class="hidden-section">mse_loss</span><a class="headerlink" href="#mse-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.mse_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">mse_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#mse_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.mse_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the element-wise mean squared error.</p>
<p>See <a class="reference internal" href="#torch.nn.MSELoss" title="torch.nn.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="margin-ranking-loss">
<h3><span class="hidden-section">margin_ranking_loss</span><a class="headerlink" href="#margin-ranking-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.margin_ranking_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">margin_ranking_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#margin_ranking_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.margin_ranking_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MarginRankingLoss" title="torch.nn.MarginRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarginRankingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-margin-loss">
<h3><span class="hidden-section">multilabel_margin_loss</span><a class="headerlink" href="#multilabel-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multilabel_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#multilabel_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multilabel_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiLabelMarginLoss" title="torch.nn.MultiLabelMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiLabelMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-soft-margin-loss">
<h3><span class="hidden-section">multilabel_soft_margin_loss</span><a class="headerlink" href="#multilabel-soft-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multilabel_soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#multilabel_soft_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multilabel_soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiLabelSoftMarginLoss" title="torch.nn.MultiLabelSoftMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiLabelSoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multi-margin-loss">
<h3><span class="hidden-section">multi_margin_loss</span><a class="headerlink" href="#multi-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multi_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multi_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>p=1</em>, <em>margin=1.0</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#multi_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multi_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,</dt>
<dd>reduce=None, reduction=’mean’) -&gt; Tensor</dd>
</dl>
<p>See <a class="reference internal" href="#torch.nn.MultiMarginLoss" title="torch.nn.MultiMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="nll-loss">
<h3><span class="hidden-section">nll_loss</span><a class="headerlink" href="#nll-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> – <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K &gt; 1\)</span>
in the case of K-dimensional loss.</li>
<li><strong>target</strong> – <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>,
or <span class="math">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span> for
K-dimensional loss.</li>
<li><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</li>
<li><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></li>
<li><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the sum of the output will be divided by the number of
elements in the output, ‘sum’: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: ‘mean’</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="smooth-l1-loss">
<h3><span class="hidden-section">smooth_l1_loss</span><a class="headerlink" href="#smooth-l1-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.smooth_l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">smooth_l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#smooth_l1_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.smooth_l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.</p>
<p>See <a class="reference internal" href="#torch.nn.SmoothL1Loss" title="torch.nn.SmoothL1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="soft-margin-loss">
<h3><span class="hidden-section">soft_margin_loss</span><a class="headerlink" href="#soft-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#soft_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.SoftMarginLoss" title="torch.nn.SoftMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="triplet-margin-loss">
<h3><span class="hidden-section">triplet_margin_loss</span><a class="headerlink" href="#triplet-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.triplet_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">triplet_margin_loss</code><span class="sig-paren">(</span><em>anchor</em>, <em>positive</em>, <em>negative</em>, <em>margin=1.0</em>, <em>p=2</em>, <em>eps=1e-06</em>, <em>swap=False</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#triplet_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.triplet_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.TripletMarginLoss" title="torch.nn.TripletMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">TripletMarginLoss</span></code></a> for details</p>
</dd></dl>

</div>
</div>
<div class="section" id="vision-functions">
<h2>Vision functions<a class="headerlink" href="#vision-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pixel-shuffle">
<h3><span class="hidden-section">pixel_shuffle</span><a class="headerlink" href="#pixel-shuffle" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pixel_shuffle">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pixel_shuffle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.pixel_shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a tensor of shape <span class="math">\((*, C \times r^2, H, W)\)</span> to a
tensor of shape <span class="math">\((C, H \times r, W \times r)\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code class="xref py py-class docutils literal notranslate"><span class="pre">PixelShuffle</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</li>
<li><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – factor to increase spatial resolution by</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pad">
<h3><span class="hidden-section">pad</span><a class="headerlink" href="#pad" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pad">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pad</code><span class="sig-paren">(</span><em>input</em>, <em>pad</em>, <em>mode='constant'</em>, <em>value=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads tensor.</p>
<dl class="docutils">
<dt>Pading size:</dt>
<dd>The number of dimensions to pad is <span class="math">\(\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor\)</span>
and the dimensions that get padded begins with the last dimension and moves forward.
For example, to pad the last dimension of the input tensor, then <cite>pad</cite> has form
<cite>(padLeft, padRight)</cite>; to pad the last 2 dimensions of the input tensor, then use
<cite>(padLeft, padRight, padTop, padBottom)</cite>; to pad the last 3 dimensions, use
<cite>(padLeft, padRight, padTop, padBottom, padFront, padBack)</cite>.</dd>
<dt>Padding mode:</dt>
<dd>See <a class="reference internal" href="#torch.nn.ConstantPad2d" title="torch.nn.ConstantPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConstantPad2d</span></code></a>, <a class="reference internal" href="#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReflectionPad2d</span></code></a>, and
<a class="reference internal" href="#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReplicationPad2d</span></code></a> for concrete examples on how each of the
padding modes works. Constant padding is implemented for arbitrary dimensions.
Replicate padding is implemented for padding the last 3 dimensions of 5D input
tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of
3D input tensor. Reflect padding is only implemented for padding the last 2
dimensions of 4D input tensor, or the last dimension of 3D input tensor.</dd>
</dl>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <cite>Nd</cite> tensor</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – m-elem tuple, where <span class="math">\(\frac{m}{2} \leq\)</span> input dimensions and <span class="math">\(m\)</span> is even.</li>
<li><strong>mode</strong> – ‘constant’, ‘reflect’ or ‘replicate’. Default: ‘constant’</li>
<li><strong>value</strong> – fill value for ‘constant’ padding. Default: 0</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p1d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># pad last dim by 1 on each side</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p1d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># effectively zero padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p2d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p2d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 8, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p3d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p3d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 9, 7, 3])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="interpolate">
<h3><span class="hidden-section">interpolate</span><a class="headerlink" href="#interpolate" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.interpolate">
<code class="descclassname">torch.nn.functional.</code><code class="descname">interpolate</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#interpolate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.interpolate" title="Permalink to this definition">¶</a></dt>
<dd><p>Down/up samples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p>
<p>The algorithm used for interpolation is determined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric sampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [optional depth] x [optional height] x width</cite>.</p>
<p>The modes available for resizing are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite> (4D-only), <cite>trilinear</cite> (5D-only), <cite>area</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</li>
<li><strong>mode</strong> (<em>string</em>) – algorithm used for upsampling:
‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’ | ‘area’. Default: ‘nearest’</li>
<li><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if True, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <cite>linear</cite>,
<cite>bilinear</cite>, or <cite>trilinear</cite>. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference internal" href="#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code></a> for concrete examples on how this
affects the outputs.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="id52">
<h3><span class="hidden-section">upsample</span><a class="headerlink" href="#id52" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...)</span></code>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<p>The algorithm used for upsampling is determined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric upsampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [optional depth] x [optional height] x width</cite>.</p>
<p>The modes available for upsampling are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite> (4D-only), <cite>trilinear</cite> (5D-only)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – multiplier for spatial size. Has to be an integer.</li>
<li><strong>mode</strong> (<em>string</em>) – algorithm used for upsampling:
‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’. Default: ‘nearest’</li>
<li><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if True, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <cite>linear</cite>,
<cite>bilinear</cite>, or <cite>trilinear</cite>. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference internal" href="#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code></a> for concrete examples on how this
affects the outputs.</p>
</div>
</dd></dl>

</div>
<div class="section" id="upsample-nearest">
<h3><span class="hidden-section">upsample_nearest</span><a class="headerlink" href="#upsample-nearest" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample_nearest">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_nearest</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample_nearest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample_nearest" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using nearest neighbours’ pixel values.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='nearest')</span></code>.</p>
</div>
<p>Currently spatial and volumetric upsampling are supported (i.e. expected
inputs are 4 or 5 dimensional).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatia
size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – multiplier for spatial size. Has to be an integer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="upsample-bilinear">
<h3><span class="hidden-section">upsample_bilinear</span><a class="headerlink" href="#upsample-bilinear" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample_bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_bilinear</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample_bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample_bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using bilinear upsampling.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with
<code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<p>Expected inputs are spatial (4 dimensional). Use <cite>upsample_trilinear</cite> fo
volumetric (5 dimensional) inputs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – multiplier for spatial size</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="grid-sample">
<h3><span class="hidden-section">grid_sample</span><a class="headerlink" href="#grid-sample" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.grid_sample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">grid_sample</code><span class="sig-paren">(</span><em>input</em>, <em>grid</em>, <em>mode='bilinear'</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#grid_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.grid_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and a flow-field <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code>, computes the
<code class="docutils literal notranslate"><span class="pre">output</span></code> using <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> values and pixel locations from <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code>.</p>
<p>Currently, only spatial (4-D) and volumetric (5-D) <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> are
supported.</p>
<p>In the spatial (4-D) case, for <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with shape
<span class="math">\((N, C, H_\text{in}, W_\text{in})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> with shape
<span class="math">\((N, H_\text{out}, W_\text{out}, 2)\)</span>, the output will have shape
<span class="math">\((N, C, H_\text{out}, W_\text{out})\)</span>.</p>
<p>For each output location <code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>, the size-2 vector
<code class="docutils literal notranslate"><span class="pre">grid[n,</span> <span class="pre">h,</span> <span class="pre">w]</span></code> specifies <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> pixel locations <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>,
which are used to interpolate the output value <code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>.
In the case of 5D inputs, <code class="docutils literal notranslate"><span class="pre">grid[n,</span> <span class="pre">d,</span> <span class="pre">h,</span> <span class="pre">w]</span></code> specifies the
<code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">z</span></code> pixel locations for interpolating
<code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">d,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> argument specifies <code class="docutils literal notranslate"><span class="pre">nearest</span></code> or
<code class="docutils literal notranslate"><span class="pre">bilinear</span></code> interpolation method to sample the input pixels.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> should have most values in the range of <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>. This is
because the pixel locations are normalized by the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> spatial
dimensions. For example, values <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">-1,</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">-1</span></code> is the left-top pixel of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, and values  <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">1</span></code> is the right-bottom pixel of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> has values outside the range of <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>, those locations
are handled as defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_mode</span></code>. Options are</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;zeros&quot;</span></code>: use <code class="docutils literal notranslate"><span class="pre">0</span></code> for out-of-bound values,</li>
<li><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;border&quot;</span></code>: use border values for out-of-bound values,</li>
<li><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;reflection&quot;</span></code>: use values at locations reflected by
the border for out-of-bound values. For location far away from the
border, it will keep being reflected until becoming in bound, e.g.,
(normalized) pixel location <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">-3.5</span></code> reflects by <code class="docutils literal notranslate"><span class="pre">-1</span></code> and
becomes <code class="docutils literal notranslate"><span class="pre">x'</span> <span class="pre">=</span> <span class="pre">2.5</span></code>, then reflects by border <code class="docutils literal notranslate"><span class="pre">1</span></code> and becomes
<code class="docutils literal notranslate"><span class="pre">x''</span> <span class="pre">=</span> <span class="pre">-0.5</span></code>.</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is often used in building Spatial Transformer Networks.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input of shape <span class="math">\((N, C, H_\text{in}, W_\text{in})\)</span> (4-D case)
or <span class="math">\((N, C, D_\text{in}, H_\text{in}, W_\text{in})\)</span> (5-D case)</li>
<li><strong>grid</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – flow-field of shape <span class="math">\((N, H_\text{out}, W_\text{out}, 2)\)</span> (4-D case)
or <span class="math">\((N, D_\text{out}, H_\text{out}, W_\text{out}, 3)\)</span> (5-D case)</li>
<li><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – interpolation mode to calculate output values
‘bilinear’ | ‘nearest’. Default: ‘bilinear’</li>
<li><strong>padding_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – padding mode for outside grid values
‘zeros’ | ‘border’ | ‘reflection’. Default: ‘zeros’</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">output Tensor</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="affine-grid">
<h3><span class="hidden-section">affine_grid</span><a class="headerlink" href="#affine-grid" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.affine_grid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">affine_grid</code><span class="sig-paren">(</span><em>theta</em>, <em>size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#affine_grid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.affine_grid" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a 2d flow field, given a batch of affine matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">theta</span></code>
Generally used in conjunction with <a class="reference internal" href="#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code class="xref py py-func docutils literal notranslate"><span class="pre">grid_sample()</span></code></a> to
implement Spatial Transformer Networks.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>theta</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input batch of affine matrices (<span class="math">\(N \times 2 \times 3\)</span>)</li>
<li><strong>size</strong> (<em>torch.Size</em>) – the target output image size (<span class="math">\(N \times C \times H \times W\)</span>)
Example: torch.Size((32, 3, 24, 24))</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">output Tensor of size (<span class="math">\(N \times H \times W \times 2\)</span>)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">output (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="dataparallel-functions-multi-gpu-distributed">
<h2>DataParallel functions (multi-GPU, distributed)<a class="headerlink" href="#dataparallel-functions-multi-gpu-distributed" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-parallel">
<h3><span class="hidden-section">data_parallel</span><a class="headerlink" href="#data-parallel" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.parallel.data_parallel">
<code class="descclassname">torch.nn.parallel.</code><code class="descname">data_parallel</code><span class="sig-paren">(</span><em>module</em>, <em>inputs</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em>, <em>module_kwargs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/data_parallel.html#data_parallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.parallel.data_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates module(input) in parallel across the GPUs given in device_ids.</p>
<p>This is the functional version of the DataParallel module.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to evaluate in parallel</li>
<li><strong>inputs</strong> (<em>tensor</em>) – inputs to the module</li>
<li><strong>device_ids</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – GPU ids on which to replicate module</li>
<li><strong>output_device</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – GPU location of the output  Use -1 to indicate the CPU.
(default: device_ids[0])</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a Tensor containing the result of module(input) located on
output_device</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-init">
<h1>torch.nn.init<a class="headerlink" href="#torch-nn-init" title="Permalink to this headline">¶</a></h1>
<dl class="function">
<dt id="torch.nn.init.calculate_gain">
<code class="descclassname">torch.nn.init.</code><code class="descname">calculate_gain</code><span class="sig-paren">(</span><em>nonlinearity</em>, <em>param=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#calculate_gain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.calculate_gain" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the recommended gain value for the given nonlinearity function.
The values are as follows:</p>
<table border="1" class="docutils">
<colgroup>
<col width="24%" />
<col width="76%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">nonlinearity</th>
<th class="head">gain</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Linear / Identity</td>
<td><span class="math">\(1\)</span></td>
</tr>
<tr class="row-odd"><td>Conv{1,2,3}D</td>
<td><span class="math">\(1\)</span></td>
</tr>
<tr class="row-even"><td>Sigmoid</td>
<td><span class="math">\(1\)</span></td>
</tr>
<tr class="row-odd"><td>Tanh</td>
<td><span class="math">\(\frac{5}{3}\)</span></td>
</tr>
<tr class="row-even"><td>ReLU</td>
<td><span class="math">\(\sqrt{2}\)</span></td>
</tr>
<tr class="row-odd"><td>Leaky Relu</td>
<td><span class="math">\(\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}\)</span></td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name)</li>
<li><strong>param</strong> – optional parameter for the non-linear function</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gain</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>b=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#uniform_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with values drawn from the uniform
distribution <span class="math">\(\mathcal{U}(a, b)\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>a</strong> – the lower bound of the uniform distribution</li>
<li><strong>b</strong> – the upper bound of the uniform distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>mean=0</em>, <em>std=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#normal_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with values drawn from the normal
distribution <span class="math">\(\mathcal{N}(\text{mean}, \text{std})\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>mean</strong> – the mean of the normal distribution</li>
<li><strong>std</strong> – the standard deviation of the normal distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.constant_">
<code class="descclassname">torch.nn.init.</code><code class="descname">constant_</code><span class="sig-paren">(</span><em>tensor</em>, <em>val</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#constant_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.constant_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with the value <span class="math">\(\text{val}\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>val</strong> – the value to fill the tensor with</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.eye_">
<code class="descclassname">torch.nn.init.</code><code class="descname">eye_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#eye_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.eye_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2-dimensional input <cite>Tensor</cite> with the identity
matrix. Preserves the identity of the inputs in <cite>Linear</cite> layers, where as
many inputs are preserved as possible.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tensor</strong> – a 2-dimensional <cite>torch.Tensor</cite></td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">eye_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.dirac_">
<code class="descclassname">torch.nn.init.</code><code class="descname">dirac_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#dirac_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.dirac_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the {3, 4, 5}-dimensional input <cite>Tensor</cite> with the Dirac
delta function. Preserves the identity of the inputs in <cite>Convolutional</cite>
layers, where as many input channels are preserved as possible.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tensor</strong> – a {3, 4, 5}-dimensional <cite>torch.Tensor</cite></td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">dirac_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#xavier_uniform_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.xavier_uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in “Understanding the difficulty of training deep feedforward
neural networks” - Glorot, X. &amp; Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled from
<span class="math">\(\mathcal{U}(-a, a)\)</span> where</p>
<div class="math">
\[a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}

\]</div>
<p>Also known as Glorot initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>gain</strong> – an optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#xavier_normal_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.xavier_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in “Understanding the difficulty of training deep feedforward
neural networks” - Glorot, X. &amp; Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled from
<span class="math">\(\mathcal{N}(0, \text{std})\)</span> where</p>
<div class="math">
\[\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}}

\]</div>
<p>Also known as Glorot initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>gain</strong> – an optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em>, <em>nonlinearity='leaky_relu'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#kaiming_uniform_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.kaiming_uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in “Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification” - He, K. et al. (2015), using a
uniform distribution. The resulting tensor will have values sampled from
<span class="math">\(\mathcal{U}(-\text{bound}, \text{bound})\)</span> where</p>
<div class="math">
\[\text{bound} = \sqrt{\frac{6}{(1 + a^2) \times \text{fan\_in}}}

\]</div>
<p>Also known as He initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>a</strong> – the negative slope of the rectifier used after this layer (0 for ReLU
by default)</li>
<li><strong>mode</strong> – either ‘fan_in’ (default) or ‘fan_out’. Choosing <cite>fan_in</cite>
preserves the magnitude of the variance of the weights in the
forward pass. Choosing <cite>fan_out</cite> preserves the magnitudes in the
backwards pass.</li>
<li><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name),
recommended to use only with ‘relu’ or ‘leaky_relu’ (default).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em>, <em>nonlinearity='leaky_relu'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#kaiming_normal_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.kaiming_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in “Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification” - He, K. et al. (2015), using a
normal distribution. The resulting tensor will have values sampled from
<span class="math">\(\mathcal{N}(0, \text{std})\)</span> where</p>
<div class="math">
\[\text{std} = \sqrt{\frac{2}{(1 + a^2) \times \text{fan\_in}}}

\]</div>
<p>Also known as He initialization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>a</strong> – the negative slope of the rectifier used after this layer (0 for ReLU
by default)</li>
<li><strong>mode</strong> – either ‘fan_in’ (default) or ‘fan_out’. Choosing <cite>fan_in</cite>
preserves the magnitude of the variance of the weights in the
forward pass. Choosing <cite>fan_out</cite> preserves the magnitudes in the
backwards pass.</li>
<li><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name),
recommended to use only with ‘relu’ or ‘leaky_relu’ (default).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.orthogonal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">orthogonal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#orthogonal_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.orthogonal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with a (semi) orthogonal matrix, as
described in “Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks” - Saxe, A. et al. (2013). The input tensor must have
at least 2 dimensions, and for tensors with more than 2 dimensions the
trailing dimensions are flattened.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite>, where <span class="math">\(n \geq 2\)</span></li>
<li><strong>gain</strong> – optional scaling factor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.sparse_">
<code class="descclassname">torch.nn.init.</code><code class="descname">sparse_</code><span class="sig-paren">(</span><em>tensor</em>, <em>sparsity</em>, <em>std=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/init.html#sparse_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.init.sparse_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2D input <cite>Tensor</cite> as a sparse matrix, where the
non-zero elements will be drawn from the normal distribution
<span class="math">\(\mathcal{N}(0, 0.01)\)</span>, as described in “Deep learning via
Hessian-free optimization” - Martens, J. (2010).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></li>
<li><strong>sparsity</strong> – The fraction of elements in each column to be set to zero</li>
<li><strong>std</strong> – the standard deviation of the normal distribution used to generate
the non-zero values</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">sparse_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">sparsity</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="optim.html" class="btn btn-neutral float-right" title="torch.optim" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="storage.html" class="btn btn-neutral" title="torch.Storage" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.nn</a><ul>
<li><a class="reference internal" href="#parameters">Parameters</a></li>
<li><a class="reference internal" href="#containers">Containers</a><ul>
<li><a class="reference internal" href="#module"><span class="hidden-section">Module</span></a></li>
<li><a class="reference internal" href="#sequential"><span class="hidden-section">Sequential</span></a></li>
<li><a class="reference internal" href="#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li><a class="reference internal" href="#moduledict"><span class="hidden-section">ModuleDict</span></a></li>
<li><a class="reference internal" href="#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
<li><a class="reference internal" href="#parameterdict"><span class="hidden-section">ParameterDict</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#convolution-layers">Convolution layers</a><ul>
<li><a class="reference internal" href="#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li><a class="reference internal" href="#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li><a class="reference internal" href="#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li><a class="reference internal" href="#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li><a class="reference internal" href="#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li><a class="reference internal" href="#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
<li><a class="reference internal" href="#unfold"><span class="hidden-section">Unfold</span></a></li>
<li><a class="reference internal" href="#fold"><span class="hidden-section">Fold</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pooling-layers">Pooling layers</a><ul>
<li><a class="reference internal" href="#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li><a class="reference internal" href="#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li><a class="reference internal" href="#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li><a class="reference internal" href="#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li><a class="reference internal" href="#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li><a class="reference internal" href="#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li><a class="reference internal" href="#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li><a class="reference internal" href="#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li><a class="reference internal" href="#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li><a class="reference internal" href="#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li><a class="reference internal" href="#lppool1d"><span class="hidden-section">LPPool1d</span></a></li>
<li><a class="reference internal" href="#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li><a class="reference internal" href="#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li><a class="reference internal" href="#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li><a class="reference internal" href="#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li><a class="reference internal" href="#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li><a class="reference internal" href="#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li><a class="reference internal" href="#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#padding-layers">Padding layers</a><ul>
<li><a class="reference internal" href="#reflectionpad1d"><span class="hidden-section">ReflectionPad1d</span></a></li>
<li><a class="reference internal" href="#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li><a class="reference internal" href="#replicationpad1d"><span class="hidden-section">ReplicationPad1d</span></a></li>
<li><a class="reference internal" href="#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li><a class="reference internal" href="#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li><a class="reference internal" href="#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li><a class="reference internal" href="#constantpad1d"><span class="hidden-section">ConstantPad1d</span></a></li>
<li><a class="reference internal" href="#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
<li><a class="reference internal" href="#constantpad3d"><span class="hidden-section">ConstantPad3d</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#non-linear-activations-weighted-sum-nonlinearity">Non-linear activations (weighted sum, nonlinearity)</a><ul>
<li><a class="reference internal" href="#elu"><span class="hidden-section">ELU</span></a></li>
<li><a class="reference internal" href="#hardshrink"><span class="hidden-section">Hardshrink</span></a></li>
<li><a class="reference internal" href="#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li><a class="reference internal" href="#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li><a class="reference internal" href="#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li><a class="reference internal" href="#prelu"><span class="hidden-section">PReLU</span></a></li>
<li><a class="reference internal" href="#relu"><span class="hidden-section">ReLU</span></a></li>
<li><a class="reference internal" href="#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li><a class="reference internal" href="#rrelu"><span class="hidden-section">RReLU</span></a></li>
<li><a class="reference internal" href="#selu"><span class="hidden-section">SELU</span></a></li>
<li><a class="reference internal" href="#celu"><span class="hidden-section">CELU</span></a></li>
<li><a class="reference internal" href="#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li><a class="reference internal" href="#softplus"><span class="hidden-section">Softplus</span></a></li>
<li><a class="reference internal" href="#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li><a class="reference internal" href="#softsign"><span class="hidden-section">Softsign</span></a></li>
<li><a class="reference internal" href="#tanh"><span class="hidden-section">Tanh</span></a></li>
<li><a class="reference internal" href="#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li><a class="reference internal" href="#threshold"><span class="hidden-section">Threshold</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#non-linear-activations-other">Non-linear activations (other)</a><ul>
<li><a class="reference internal" href="#softmin"><span class="hidden-section">Softmin</span></a></li>
<li><a class="reference internal" href="#softmax"><span class="hidden-section">Softmax</span></a></li>
<li><a class="reference internal" href="#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li><a class="reference internal" href="#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
<li><a class="reference internal" href="#adaptivelogsoftmaxwithloss"><span class="hidden-section">AdaptiveLogSoftmaxWithLoss</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#normalization-layers">Normalization layers</a><ul>
<li><a class="reference internal" href="#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li><a class="reference internal" href="#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li><a class="reference internal" href="#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li><a class="reference internal" href="#groupnorm"><span class="hidden-section">GroupNorm</span></a></li>
<li><a class="reference internal" href="#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li><a class="reference internal" href="#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li><a class="reference internal" href="#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
<li><a class="reference internal" href="#layernorm"><span class="hidden-section">LayerNorm</span></a></li>
<li><a class="reference internal" href="#localresponsenorm"><span class="hidden-section">LocalResponseNorm</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#recurrent-layers">Recurrent layers</a><ul>
<li><a class="reference internal" href="#rnn"><span class="hidden-section">RNN</span></a></li>
<li><a class="reference internal" href="#lstm"><span class="hidden-section">LSTM</span></a></li>
<li><a class="reference internal" href="#gru"><span class="hidden-section">GRU</span></a></li>
<li><a class="reference internal" href="#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li><a class="reference internal" href="#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li><a class="reference internal" href="#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#linear-layers">Linear layers</a><ul>
<li><a class="reference internal" href="#linear"><span class="hidden-section">Linear</span></a></li>
<li><a class="reference internal" href="#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#dropout-layers">Dropout layers</a><ul>
<li><a class="reference internal" href="#dropout"><span class="hidden-section">Dropout</span></a></li>
<li><a class="reference internal" href="#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li><a class="reference internal" href="#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li><a class="reference internal" href="#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-layers">Sparse layers</a><ul>
<li><a class="reference internal" href="#embedding"><span class="hidden-section">Embedding</span></a></li>
<li><a class="reference internal" href="#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#distance-functions">Distance functions</a><ul>
<li><a class="reference internal" href="#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li><a class="reference internal" href="#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#loss-functions">Loss functions</a><ul>
<li><a class="reference internal" href="#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li><a class="reference internal" href="#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li><a class="reference internal" href="#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li><a class="reference internal" href="#ctcloss"><span class="hidden-section">CTCLoss</span></a></li>
<li><a class="reference internal" href="#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li><a class="reference internal" href="#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li><a class="reference internal" href="#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li><a class="reference internal" href="#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li><a class="reference internal" href="#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li><a class="reference internal" href="#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li><a class="reference internal" href="#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li><a class="reference internal" href="#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li><a class="reference internal" href="#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li><a class="reference internal" href="#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li><a class="reference internal" href="#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li><a class="reference internal" href="#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li><a class="reference internal" href="#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li><a class="reference internal" href="#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#vision-layers">Vision layers</a><ul>
<li><a class="reference internal" href="#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li><a class="reference internal" href="#upsample"><span class="hidden-section">Upsample</span></a></li>
<li><a class="reference internal" href="#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li><a class="reference internal" href="#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#dataparallel-layers-multi-gpu-distributed">DataParallel layers (multi-GPU, distributed)</a><ul>
<li><a class="reference internal" href="#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li><a class="reference internal" href="#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
<li><a class="reference internal" href="#distributeddataparallelcpu"><span class="hidden-section">DistributedDataParallelCPU</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#utilities">Utilities</a><ul>
<li><a class="reference internal" href="#clip-grad-norm"><span class="hidden-section">clip_grad_norm_</span></a></li>
<li><a class="reference internal" href="#clip-grad-value"><span class="hidden-section">clip_grad_value_</span></a></li>
<li><a class="reference internal" href="#parameters-to-vector"><span class="hidden-section">parameters_to_vector</span></a></li>
<li><a class="reference internal" href="#vector-to-parameters"><span class="hidden-section">vector_to_parameters</span></a></li>
<li><a class="reference internal" href="#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li><a class="reference internal" href="#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li><a class="reference internal" href="#spectral-norm"><span class="hidden-section">spectral_norm</span></a></li>
<li><a class="reference internal" href="#remove-spectral-norm"><span class="hidden-section">remove_spectral_norm</span></a></li>
<li><a class="reference internal" href="#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li><a class="reference internal" href="#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li><a class="reference internal" href="#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
<li><a class="reference internal" href="#pad-sequence"><span class="hidden-section">pad_sequence</span></a></li>
<li><a class="reference internal" href="#pack-sequence"><span class="hidden-section">pack_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#torch-nn-functional">torch.nn.functional</a><ul>
<li><a class="reference internal" href="#convolution-functions">Convolution functions</a><ul>
<li><a class="reference internal" href="#id20"><span class="hidden-section">conv1d</span></a></li>
<li><a class="reference internal" href="#id21"><span class="hidden-section">conv2d</span></a></li>
<li><a class="reference internal" href="#id22"><span class="hidden-section">conv3d</span></a></li>
<li><a class="reference internal" href="#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li><a class="reference internal" href="#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li><a class="reference internal" href="#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
<li><a class="reference internal" href="#id23"><span class="hidden-section">unfold</span></a></li>
<li><a class="reference internal" href="#id24"><span class="hidden-section">fold</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pooling-functions">Pooling functions</a><ul>
<li><a class="reference internal" href="#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li><a class="reference internal" href="#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li><a class="reference internal" href="#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li><a class="reference internal" href="#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li><a class="reference internal" href="#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li><a class="reference internal" href="#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li><a class="reference internal" href="#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li><a class="reference internal" href="#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li><a class="reference internal" href="#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li><a class="reference internal" href="#lp-pool1d"><span class="hidden-section">lp_pool1d</span></a></li>
<li><a class="reference internal" href="#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li><a class="reference internal" href="#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li><a class="reference internal" href="#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li><a class="reference internal" href="#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li><a class="reference internal" href="#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li><a class="reference internal" href="#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li><a class="reference internal" href="#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li><a class="reference internal" href="#id25"><span class="hidden-section">threshold</span></a></li>
<li><a class="reference internal" href="#id26"><span class="hidden-section">relu</span></a></li>
<li><a class="reference internal" href="#id27"><span class="hidden-section">hardtanh</span></a></li>
<li><a class="reference internal" href="#id28"><span class="hidden-section">relu6</span></a></li>
<li><a class="reference internal" href="#id29"><span class="hidden-section">elu</span></a></li>
<li><a class="reference internal" href="#id30"><span class="hidden-section">selu</span></a></li>
<li><a class="reference internal" href="#id31"><span class="hidden-section">celu</span></a></li>
<li><a class="reference internal" href="#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li><a class="reference internal" href="#id32"><span class="hidden-section">prelu</span></a></li>
<li><a class="reference internal" href="#id33"><span class="hidden-section">rrelu</span></a></li>
<li><a class="reference internal" href="#glu"><span class="hidden-section">glu</span></a></li>
<li><a class="reference internal" href="#id34"><span class="hidden-section">logsigmoid</span></a></li>
<li><a class="reference internal" href="#id35"><span class="hidden-section">hardshrink</span></a></li>
<li><a class="reference internal" href="#id36"><span class="hidden-section">tanhshrink</span></a></li>
<li><a class="reference internal" href="#id37"><span class="hidden-section">softsign</span></a></li>
<li><a class="reference internal" href="#id38"><span class="hidden-section">softplus</span></a></li>
<li><a class="reference internal" href="#id39"><span class="hidden-section">softmin</span></a></li>
<li><a class="reference internal" href="#id40"><span class="hidden-section">softmax</span></a></li>
<li><a class="reference internal" href="#id41"><span class="hidden-section">softshrink</span></a></li>
<li><a class="reference internal" href="#gumbel-softmax"><span class="hidden-section">gumbel_softmax</span></a></li>
<li><a class="reference internal" href="#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li><a class="reference internal" href="#id42"><span class="hidden-section">tanh</span></a></li>
<li><a class="reference internal" href="#id43"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#normalization-functions">Normalization functions</a><ul>
<li><a class="reference internal" href="#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li><a class="reference internal" href="#instance-norm"><span class="hidden-section">instance_norm</span></a></li>
<li><a class="reference internal" href="#layer-norm"><span class="hidden-section">layer_norm</span></a></li>
<li><a class="reference internal" href="#local-response-norm"><span class="hidden-section">local_response_norm</span></a></li>
<li><a class="reference internal" href="#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#linear-functions">Linear functions</a><ul>
<li><a class="reference internal" href="#id44"><span class="hidden-section">linear</span></a></li>
<li><a class="reference internal" href="#id45"><span class="hidden-section">bilinear</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#dropout-functions">Dropout functions</a><ul>
<li><a class="reference internal" href="#id46"><span class="hidden-section">dropout</span></a></li>
<li><a class="reference internal" href="#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li><a class="reference internal" href="#id47"><span class="hidden-section">dropout2d</span></a></li>
<li><a class="reference internal" href="#id48"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-functions">Sparse functions</a><ul>
<li><a class="reference internal" href="#id49"><span class="hidden-section">embedding</span></a></li>
<li><a class="reference internal" href="#embedding-bag"><span class="hidden-section">embedding_bag</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#id50">Distance functions</a><ul>
<li><a class="reference internal" href="#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li><a class="reference internal" href="#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
<li><a class="reference internal" href="#pdist"><span class="hidden-section">pdist</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#id51">Loss functions</a><ul>
<li><a class="reference internal" href="#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li><a class="reference internal" href="#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li><a class="reference internal" href="#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li><a class="reference internal" href="#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li><a class="reference internal" href="#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li><a class="reference internal" href="#ctc-loss"><span class="hidden-section">ctc_loss</span></a></li>
<li><a class="reference internal" href="#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li><a class="reference internal" href="#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li><a class="reference internal" href="#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li><a class="reference internal" href="#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li><a class="reference internal" href="#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li><a class="reference internal" href="#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li><a class="reference internal" href="#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li><a class="reference internal" href="#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li><a class="reference internal" href="#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li><a class="reference internal" href="#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li><a class="reference internal" href="#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li><a class="reference internal" href="#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#vision-functions">Vision functions</a><ul>
<li><a class="reference internal" href="#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li><a class="reference internal" href="#pad"><span class="hidden-section">pad</span></a></li>
<li><a class="reference internal" href="#interpolate"><span class="hidden-section">interpolate</span></a></li>
<li><a class="reference internal" href="#id52"><span class="hidden-section">upsample</span></a></li>
<li><a class="reference internal" href="#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li><a class="reference internal" href="#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li><a class="reference internal" href="#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li><a class="reference internal" href="#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#dataparallel-functions-multi-gpu-distributed">DataParallel functions (multi-GPU, distributed)</a><ul>
<li><a class="reference internal" href="#data-parallel"><span class="hidden-section">data_parallel</span></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#torch-nn-init">torch.nn.init</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script type="text/javascript" src="_static/jquery.js"></script>
         <script type="text/javascript" src="_static/underscore.js"></script>
         <script type="text/javascript" src="_static/doctools.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>
<img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
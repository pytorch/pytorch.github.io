


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.utils.data.dataloader &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.0.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed_deprecated.html">torch.distributed.deprecated</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchvision/index.html">torchvision</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.utils.data.dataloader</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.utils.data.dataloader</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">multiprocessing</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="k">import</span> <span class="n">_set_worker_signal_handlers</span><span class="p">,</span> <span class="n">_update_worker_pids</span><span class="p">,</span> \
    <span class="n">_remove_worker_pids</span><span class="p">,</span> <span class="n">_error_if_any_worker_fails</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">SequentialSampler</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">BatchSampler</span>
<span class="kn">import</span> <span class="nn">signal</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">torch._six</span> <span class="k">import</span> <span class="n">container_abcs</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">atexit</span>
<span class="kn">from</span> <span class="nn">torch._six</span> <span class="k">import</span> <span class="n">string_classes</span><span class="p">,</span> <span class="n">int_classes</span><span class="p">,</span> <span class="ne">FileNotFoundError</span>

<span class="n">IS_WINDOWS</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s2">&quot;win32&quot;</span>
<span class="k">if</span> <span class="n">IS_WINDOWS</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">ctypes</span>
    <span class="kn">from</span> <span class="nn">ctypes.wintypes</span> <span class="k">import</span> <span class="n">DWORD</span><span class="p">,</span> <span class="n">BOOL</span><span class="p">,</span> <span class="n">HANDLE</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">Queue</span> <span class="k">as</span> <span class="nn">queue</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">queue</span>


<span class="c1"># NOTE [ Python Traceback Reference Cycle Problem ]</span>
<span class="c1">#</span>
<span class="c1"># When using sys.exc_info(), it is important to **not** store the exc_info[2],</span>
<span class="c1"># which is the traceback, because otherwise you will run into the traceback</span>
<span class="c1"># reference cycle problem, i.e., the traceback holding reference to the frame,</span>
<span class="c1"># and the frame (which holds reference to all the object in its temporary scope)</span>
<span class="c1"># holding reference the traceback.</span>


<span class="k">class</span> <span class="nc">ExceptionWrapper</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wraps an exception plus traceback to communicate across threads&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_info</span><span class="p">):</span>
        <span class="c1"># It is important that we don&#39;t store exc_info, see</span>
        <span class="c1"># NOTE [ Python Traceback Reference Cycle Problem ]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exc_type</span> <span class="o">=</span> <span class="n">exc_info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exc_msg</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">traceback</span><span class="o">.</span><span class="n">format_exception</span><span class="p">(</span><span class="o">*</span><span class="n">exc_info</span><span class="p">))</span>


<span class="n">_use_shared_memory</span> <span class="o">=</span> <span class="kc">False</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;Whether to use shared memory in default_collate&quot;&quot;&quot;</span>

<span class="n">MP_STATUS_CHECK_INTERVAL</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;Interval (in seconds) to check status of processes to avoid hanging in</span>
<span class="sd">    multiprocessing data loading. This is mainly used in getting data from</span>
<span class="sd">    another process, in which case we need to periodically check whether the</span>
<span class="sd">    sender is alive to prevent hanging.&quot;&quot;&quot;</span>

<span class="k">if</span> <span class="n">IS_WINDOWS</span><span class="p">:</span>
    <span class="c1"># On Windows, the parent ID of the worker process remains unchanged when the manager process</span>
    <span class="c1"># is gone, and the only way to check it through OS is to let the worker have a process handle</span>
    <span class="c1"># of the manager and ask if the process status has changed.</span>
    <span class="k">class</span> <span class="nc">ManagerWatchdog</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">manager_pid</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getppid</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">kernel32</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinDLL</span><span class="p">(</span><span class="s1">&#39;kernel32&#39;</span><span class="p">,</span> <span class="n">use_last_error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel32</span><span class="o">.</span><span class="n">OpenProcess</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">DWORD</span><span class="p">,</span> <span class="n">BOOL</span><span class="p">,</span> <span class="n">DWORD</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel32</span><span class="o">.</span><span class="n">OpenProcess</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">HANDLE</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel32</span><span class="o">.</span><span class="n">WaitForSingleObject</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">HANDLE</span><span class="p">,</span> <span class="n">DWORD</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel32</span><span class="o">.</span><span class="n">WaitForSingleObject</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">DWORD</span>

            <span class="c1"># Value obtained from https://msdn.microsoft.com/en-us/library/ms684880.aspx</span>
            <span class="n">SYNCHRONIZE</span> <span class="o">=</span> <span class="mh">0x00100000</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">manager_handle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel32</span><span class="o">.</span><span class="n">OpenProcess</span><span class="p">(</span><span class="n">SYNCHRONIZE</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">manager_pid</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">manager_handle</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinError</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">get_last_error</span><span class="p">())</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">manager_dead</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">def</span> <span class="nf">is_alive</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">manager_dead</span><span class="p">:</span>
                <span class="c1"># Value obtained from https://msdn.microsoft.com/en-us/library/windows/desktop/ms687032.aspx</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">manager_dead</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel32</span><span class="o">.</span><span class="n">WaitForSingleObject</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">manager_handle</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">manager_dead</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">ManagerWatchdog</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">manager_pid</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getppid</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">manager_dead</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">def</span> <span class="nf">is_alive</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">manager_dead</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">manager_dead</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getppid</span><span class="p">()</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">manager_pid</span>
            <span class="k">return</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">manager_dead</span>


<span class="k">def</span> <span class="nf">_worker_loop</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">index_queue</span><span class="p">,</span> <span class="n">data_queue</span><span class="p">,</span> <span class="n">done_event</span><span class="p">,</span> <span class="n">collate_fn</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">init_fn</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">):</span>
    <span class="c1"># See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on the</span>
    <span class="c1"># logic of this function.</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">global</span> <span class="n">_use_shared_memory</span>
        <span class="n">_use_shared_memory</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Intialize C side signal handlers for SIGBUS and SIGSEGV. Python signal</span>
        <span class="c1"># module&#39;s handlers are executed after Python returns from C low-level</span>
        <span class="c1"># handlers, likely when the same fatal signal happened again already.</span>
        <span class="c1"># https://docs.python.org/3/library/signal.html Sec. 18.8.1.1</span>
        <span class="n">_set_worker_signal_handlers</span><span class="p">()</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="n">data_queue</span><span class="o">.</span><span class="n">cancel_join_thread</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">init_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">init_fn</span><span class="p">(</span><span class="n">worker_id</span><span class="p">)</span>

        <span class="n">watchdog</span> <span class="o">=</span> <span class="n">ManagerWatchdog</span><span class="p">()</span>

        <span class="k">while</span> <span class="n">watchdog</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">index_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">MP_STATUS_CHECK_INTERVAL</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">r</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Received the final signal</span>
                <span class="k">assert</span> <span class="n">done_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">()</span>
                <span class="k">return</span>
            <span class="k">elif</span> <span class="n">done_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">():</span>
                <span class="c1"># Done event is set. But I haven&#39;t received the final signal</span>
                <span class="c1"># (None) yet. I will keep continuing until get it, and skip the</span>
                <span class="c1"># processing steps.</span>
                <span class="k">continue</span>
            <span class="n">idx</span><span class="p">,</span> <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">r</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">samples</span> <span class="o">=</span> <span class="n">collate_fn</span><span class="p">([</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">batch_indices</span><span class="p">])</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="c1"># It is important that we don&#39;t store exc_info in a variable,</span>
                <span class="c1"># see NOTE [ Python Traceback Reference Cycle Problem ]</span>
                <span class="n">data_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">ExceptionWrapper</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">samples</span><span class="p">))</span>
                <span class="k">del</span> <span class="n">samples</span>
    <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
        <span class="c1"># Main process will raise KeyboardInterrupt anyways.</span>
        <span class="k">pass</span>


<span class="k">def</span> <span class="nf">_pin_memory_loop</span><span class="p">(</span><span class="n">in_queue</span><span class="p">,</span> <span class="n">out_queue</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">done_event</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>

    <span class="c1"># See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on the</span>
    <span class="c1"># logic of this function.</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">in_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">MP_STATUS_CHECK_INTERVAL</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">done_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">():</span>
                <span class="c1"># Weird things can happen when shutting down, e.g., fd being</span>
                <span class="c1"># closed when tensors are shared via fds.</span>
                <span class="k">break</span>
            <span class="k">raise</span>
        <span class="k">if</span> <span class="n">r</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">done_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">()</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="n">done_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">():</span>
            <span class="c1"># Haven&#39;t seen the final signal yet. Keep getting until None.</span>
            <span class="k">continue</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ExceptionWrapper</span><span class="p">):</span>
            <span class="n">out_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">r</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">pin_memory_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="n">out_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">ExceptionWrapper</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">())))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">batch</span><span class="p">))</span>

<span class="n">numpy_type_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;float64&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">,</span>
    <span class="s1">&#39;float32&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span>
    <span class="s1">&#39;float16&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">HalfTensor</span><span class="p">,</span>
    <span class="s1">&#39;int64&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
    <span class="s1">&#39;int32&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">,</span>
    <span class="s1">&#39;int16&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">ShortTensor</span><span class="p">,</span>
    <span class="s1">&#39;int8&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">CharTensor</span><span class="p">,</span>
    <span class="s1">&#39;uint8&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span> <span class="nf">default_collate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Puts each data field into a tensor with outer dimension batch size&quot;&quot;&quot;</span>

    <span class="n">error_msg</span> <span class="o">=</span> <span class="s2">&quot;batch must contain tensors, numbers, dicts or lists; found </span><span class="si">{}</span><span class="s2">&quot;</span>
    <span class="n">elem_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">_use_shared_memory</span><span class="p">:</span>
            <span class="c1"># If we&#39;re in a background process, concatenate directly into a</span>
            <span class="c1"># shared memory tensor to avoid an extra copy</span>
            <span class="n">numel</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
            <span class="n">storage</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">_new_shared</span><span class="p">(</span><span class="n">numel</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">elem_type</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">==</span> <span class="s1">&#39;numpy&#39;</span> <span class="ow">and</span> <span class="n">elem_type</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">!=</span> <span class="s1">&#39;str_&#39;</span> \
            <span class="ow">and</span> <span class="n">elem_type</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">!=</span> <span class="s1">&#39;string_&#39;</span><span class="p">:</span>
        <span class="n">elem</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">elem_type</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;ndarray&#39;</span><span class="p">:</span>
            <span class="c1"># array of string classes and object</span>
            <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s1">&#39;[SaUO]&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">str</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">error_msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">elem</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">():</span>  <span class="c1"># scalars</span>
            <span class="n">py_type</span> <span class="o">=</span> <span class="nb">float</span> <span class="k">if</span> <span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;float&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="nb">int</span>
            <span class="k">return</span> <span class="n">numpy_type_map</span><span class="p">[</span><span class="n">elem</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">](</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">py_type</span><span class="p">,</span> <span class="n">batch</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">int_classes</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">string_classes</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">batch</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">container_abcs</span><span class="o">.</span><span class="n">Mapping</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">default_collate</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">container_abcs</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
        <span class="n">transposed</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">default_collate</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="k">for</span> <span class="n">samples</span> <span class="ow">in</span> <span class="n">transposed</span><span class="p">]</span>

    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">((</span><span class="n">error_msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]))))</span>


<span class="k">def</span> <span class="nf">pin_memory_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">batch</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">string_classes</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">batch</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">container_abcs</span><span class="o">.</span><span class="n">Mapping</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">pin_memory_batch</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">container_abcs</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">pin_memory_batch</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch</span>


<span class="n">_SIGCHLD_handler_set</span> <span class="o">=</span> <span class="kc">False</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;Whether SIGCHLD handler is set for DataLoader worker failures. Only one</span>
<span class="sd">handler needs to be set for all DataLoaders in a process.&quot;&quot;&quot;</span>


<span class="k">def</span> <span class="nf">_set_SIGCHLD_handler</span><span class="p">():</span>
    <span class="c1"># Windows doesn&#39;t support SIGCHLD handler</span>
    <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;win32&#39;</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="c1"># can&#39;t set signal in child threads</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">current_thread</span><span class="p">(),</span> <span class="n">threading</span><span class="o">.</span><span class="n">_MainThread</span><span class="p">):</span>
        <span class="k">return</span>
    <span class="k">global</span> <span class="n">_SIGCHLD_handler_set</span>
    <span class="k">if</span> <span class="n">_SIGCHLD_handler_set</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">previous_handler</span> <span class="o">=</span> <span class="n">signal</span><span class="o">.</span><span class="n">getsignal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGCHLD</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">previous_handler</span><span class="p">):</span>
        <span class="c1"># This doesn&#39;t catch default handler, but SIGCHLD default handler is a</span>
        <span class="c1"># no-op.</span>
        <span class="n">previous_handler</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">handler</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
        <span class="c1"># This following call uses `waitid` with WNOHANG from C side. Therefore,</span>
        <span class="c1"># Python can still get and update the process status successfully.</span>
        <span class="n">_error_if_any_worker_fails</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">previous_handler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">previous_handler</span><span class="p">(</span><span class="n">signum</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>

    <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGCHLD</span><span class="p">,</span> <span class="n">handler</span><span class="p">)</span>
    <span class="n">_SIGCHLD_handler_set</span> <span class="o">=</span> <span class="kc">True</span>


<span class="n">_python_exit_status</span> <span class="o">=</span> <span class="kc">False</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;Whether Python is shutting down. This flag is guaranteed to be set before</span>
<span class="sd">the Python core library resources are freed, but Python may already be exiting</span>
<span class="sd">for some time when this is set.</span>

<span class="sd">Hook to set this flag is `_set_python_exit_flag`, and is inspired by a similar</span>
<span class="sd">hook in Python 3.7 multiprocessing library:</span>
<span class="sd">https://github.com/python/cpython/blob/d4d60134b29290049e28df54f23493de4f1824b6/Lib/multiprocessing/util.py#L277-L327</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="k">def</span> <span class="nf">_set_python_exit_flag</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">_python_exit_status</span>
    <span class="n">_python_exit_status</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">atexit</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">_set_python_exit_flag</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_DataLoaderIter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Iterates once over the DataLoader&#39;s dataset, as specified by the sampler&quot;&quot;&quot;</span>

    <span class="c1"># NOTE [ Data Loader Multiprocessing Shutdown Logic ]</span>
    <span class="c1">#</span>
    <span class="c1"># Preliminary:</span>
    <span class="c1">#</span>
    <span class="c1"># Our data model looks like this (queues are indicated with curly brackets):</span>
    <span class="c1">#</span>
    <span class="c1">#                main process                              ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#               {index_queue}                              ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#              worker processes                            ||     DATA</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#            {worker_result_queue}                         ||     FLOW</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#      pin_memory_thread of main process                   ||   DIRECTION</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#               {data_queue}                               ||</span>
    <span class="c1">#                     |                                    ||</span>
    <span class="c1">#                data output                               \/</span>
    <span class="c1">#</span>
    <span class="c1"># P.S. `worker_result_queue` and `pin_memory_thread` part may be omitted if</span>
    <span class="c1">#      `pin_memory=False`.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1"># Terminating multiprocessing logic requires very careful design. In</span>
    <span class="c1"># particular, we need to make sure that</span>
    <span class="c1">#</span>
    <span class="c1">#   1. The iterator gracefully exits the workers when its last reference is</span>
    <span class="c1">#      gone or it is depleted.</span>
    <span class="c1">#</span>
    <span class="c1">#      In this case, the workers should be gracefully exited because the</span>
    <span class="c1">#      main process may still need to continue to run, and we want cleaning</span>
    <span class="c1">#      up code in the workers to be executed (e.g., releasing GPU memory).</span>
    <span class="c1">#      Naturally, we implement the shutdown logic in `__del__` of</span>
    <span class="c1">#      DataLoaderIterator.</span>
    <span class="c1">#</span>
    <span class="c1">#      We delay the discussion on the logic in this case until later.</span>
    <span class="c1">#</span>
    <span class="c1">#   2. The iterator exits the workers when the loader process and/or worker</span>
    <span class="c1">#      processes exits normally or with error.</span>
    <span class="c1">#</span>
    <span class="c1">#      We set all workers and `pin_memory_thread` to have `daemon=True`.</span>
    <span class="c1">#</span>
    <span class="c1">#      You may ask, why can&#39;t we make the workers non-daemonic, and</span>
    <span class="c1">#      gracefully exit using the same logic as we have in `__del__` when the</span>
    <span class="c1">#      iterator gets deleted (see 1 above)?</span>
    <span class="c1">#</span>
    <span class="c1">#      First of all, `__del__` is **not** guaranteed to be called when</span>
    <span class="c1">#      interpreter exits. Even if it is called, by the time it executes,</span>
    <span class="c1">#      many Python core library resources may alreay be freed, and even</span>
    <span class="c1">#      simple things like acquiring an internal lock of a queue may hang.</span>
    <span class="c1">#      Therefore, in this case, we actually need to prevent `__del__` from</span>
    <span class="c1">#      being executed, and rely on the automatic termination of daemonic</span>
    <span class="c1">#      children. Thus, we register an `atexit` hook that sets a global flag</span>
    <span class="c1">#      `_python_exit_status`. Since `atexit` hooks are executed in reverse</span>
    <span class="c1">#      order of registration, we are guaranteed that this flag is set before</span>
    <span class="c1">#      library resources we use are freed. (Hooks freeing those resources</span>
    <span class="c1">#      are registered at importing the Python core libraries at the top of</span>
    <span class="c1">#      this file.) So in `__del__`, we check if `_python_exit_status` is set</span>
    <span class="c1">#      or `None` (freed), and perform no-op if so.</span>
    <span class="c1">#</span>
    <span class="c1">#      Another problem with `__del__` is also related to the library cleanup</span>
    <span class="c1">#      calls. When a process ends, it shuts the all its daemonic children</span>
    <span class="c1">#      down with a SIGTERM (instead of joining them without a timeout).</span>
    <span class="c1">#      Simiarly for threads, but by a different mechanism. This fact,</span>
    <span class="c1">#      together with a few implementation details of multiprocessing, forces</span>
    <span class="c1">#      us to make workers daemonic. All of our problems arise when a</span>
    <span class="c1">#      DataLoader is used in a subprocess, and are caused by multiprocessing</span>
    <span class="c1">#      code which looks more or less like this:</span>
    <span class="c1">#</span>
    <span class="c1">#          try:</span>
    <span class="c1">#              your_function_using_a_dataloader()</span>
    <span class="c1">#          finally:</span>
    <span class="c1">#              multiprocessing.util._exit_function()</span>
    <span class="c1">#</span>
    <span class="c1">#      The joining/termination mentioned above happens inside</span>
    <span class="c1">#      `_exit_function()`. Now, if `your_function_using_a_dataloader()`</span>
    <span class="c1">#      throws, the stack trace stored in the exception will prevent the</span>
    <span class="c1">#      frame which uses `DataLoaderIter` to be freed. If the frame has any</span>
    <span class="c1">#      reference to the `DataLoaderIter` (e.g., in a method of the iter),</span>
    <span class="c1">#      its  `__del__`, which starts the shutdown procedure, will not be</span>
    <span class="c1">#      called. That, in turn, means that workers aren&#39;t notified. Attempting</span>
    <span class="c1">#      to join in `_exit_function` will then result in a hang.</span>
    <span class="c1">#</span>
    <span class="c1">#      For context, `_exit_function` is also registered as an `atexit` call.</span>
    <span class="c1">#      So it is unclear to me (@ssnl) why this is needed in a finally block.</span>
    <span class="c1">#      The code dates back to 2008 and there is no comment on the original</span>
    <span class="c1">#      PEP 371 or patch https://bugs.python.org/issue3050 (containing both</span>
    <span class="c1">#      the finally block and the `atexit` registration) that explains this.</span>
    <span class="c1">#</span>
    <span class="c1">#      Another choice is to just shutdown workers with logic in 1 above</span>
    <span class="c1">#      whenever we see an error in `next`. This isn&#39;t ideal because</span>
    <span class="c1">#        a. It prevents users from using try-catch to resume data loading.</span>
    <span class="c1">#        b. It doesn&#39;t prevent hanging if users have references to the</span>
    <span class="c1">#           iterator.</span>
    <span class="c1">#</span>
    <span class="c1">#   3. All processes exit if any of them die unexpectedly by fatal signals.</span>
    <span class="c1">#</span>
    <span class="c1">#      As shown above, the workers are set as daemonic children of the main</span>
    <span class="c1">#      process. However, automatic cleaning-up of such child processes only</span>
    <span class="c1">#      happens if the parent process exits gracefully (e.g., not via fatal</span>
    <span class="c1">#      signals like SIGKILL). So we must ensure that each process will exit</span>
    <span class="c1">#      even the process that should send/receive data to/from it were</span>
    <span class="c1">#      killed, i.e.,</span>
    <span class="c1">#</span>
    <span class="c1">#        a. A process won&#39;t hang when getting from a queue.</span>
    <span class="c1">#</span>
    <span class="c1">#           Even with carefully designed data dependencies (i.e., a `put()`</span>
    <span class="c1">#           always corresponding to a `get()`), hanging on `get()` can still</span>
    <span class="c1">#           happen when data in queue is corrupted (e.g., due to</span>
    <span class="c1">#           `cancel_join_thread` or unexpected exit).</span>
    <span class="c1">#</span>
    <span class="c1">#           For child exit, we register SIGCHLD handler on main process,</span>
    <span class="c1">#           which checks if any of the workers fail in the (Python) handler.</span>
    <span class="c1">#           See DataLoader.cpp.</span>
    <span class="c1">#</span>
    <span class="c1">#           For `.get()` calls where the sender(s) is not the workers, we</span>
    <span class="c1">#           guard them with timeouts, and check the status of the sender</span>
    <span class="c1">#           when timeout happens:</span>
    <span class="c1">#             + in the workers, the `ManagerWatchdog` class checks the main</span>
    <span class="c1">#               process status.</span>
    <span class="c1">#             + if `pin_memory=True`, when getting from `pin_memory_thread`,</span>
    <span class="c1">#               check `pin_memory_thread` status periodically until `.get()`</span>
    <span class="c1">#               returns or see that `pin_memory_thread` died.</span>
    <span class="c1">#</span>
    <span class="c1">#        b. A process won&#39;t hang when putting into a queue;</span>
    <span class="c1">#</span>
    <span class="c1">#           We use `mp.Queue` which has a separate background thread to put</span>
    <span class="c1">#           objects from an unbounded buffer array. The background thread is</span>
    <span class="c1">#           daemonic and usually automatically joined when the process</span>
    <span class="c1">#           exits.</span>
    <span class="c1">#</span>
    <span class="c1">#           However, in case that the receiver has ended abruptly while</span>
    <span class="c1">#           reading from the pipe, the join will hang forever. Therefore,</span>
    <span class="c1">#           for both `worker_result_queue` (worker -&gt; main process/pin_memory_thread)</span>
    <span class="c1">#           and each `index_queue` (main process -&gt; worker), we use</span>
    <span class="c1">#           `q.cancel_join_thread()` in sender process before any `q.put` to</span>
    <span class="c1">#           prevent this automatic join.</span>
    <span class="c1">#</span>
    <span class="c1">#           Moreover, having all queues called `cancel_join_thread` makes</span>
    <span class="c1">#           implementing graceful shutdown logic in `__del__` much easier.</span>
    <span class="c1">#           It won&#39;t need to get from any queue, which would also need to be</span>
    <span class="c1">#           guarded by periodic status checks.</span>
    <span class="c1">#</span>
    <span class="c1">#           Note that this may leave corrupted data in the queue, but we</span>
    <span class="c1">#           don&#39;t care about the data anyways once we are shutting down.</span>
    <span class="c1">#</span>
    <span class="c1">#</span>
    <span class="c1"># Now let&#39;s get back to 1:</span>
    <span class="c1">#   how we gracefully exit the workers when the last reference to the</span>
    <span class="c1">#   iteartor is gone.</span>
    <span class="c1">#</span>
    <span class="c1"># To achieve this, we implement the following logic along with the design</span>
    <span class="c1"># choices mentioned above:</span>
    <span class="c1">#</span>
    <span class="c1"># [worker processes]</span>
    <span class="c1">#   While loader process is alive:</span>
    <span class="c1">#     Get from index_queue.</span>
    <span class="c1">#       If got a `None`, exit.</span>
    <span class="c1">#       If get anything else,</span>
    <span class="c1">#          Check `done_event`.</span>
    <span class="c1">#            If set, continue to next iteration</span>
    <span class="c1">#                    i.e., keep getting until see the `None`, then exit.</span>
    <span class="c1">#            Otherwise, process data.</span>
    <span class="c1">#       If timed out,</span>
    <span class="c1">#          No matter `done_event` is set (still need to see `None`) or not,</span>
    <span class="c1">#          must continue to next iteration .</span>
    <span class="c1">#</span>
    <span class="c1"># [pin_memory_thread]</span>
    <span class="c1">#   # No need to check main thread. If this thread is alive, the main loader</span>
    <span class="c1">#   # thread must be alive, because this thread is set as daemonic.</span>
    <span class="c1">#   While True:</span>
    <span class="c1">#     Get from index_queue.</span>
    <span class="c1">#       If got a `None`, exit.</span>
    <span class="c1">#       If get anything else,</span>
    <span class="c1">#          Check `done_event`.</span>
    <span class="c1">#            If set, continue to next iteration</span>
    <span class="c1">#                    i.e., keep getting until see the `None`, then exit.</span>
    <span class="c1">#            Otherwise, process data.</span>
    <span class="c1">#</span>
    <span class="c1">#   NOTE: we don&#39;t check the status of the main thread because</span>
    <span class="c1">#           1. if the process is killed by fatal signal, `pin_memory_thread`</span>
    <span class="c1">#              ends.</span>
    <span class="c1">#           2. in other cases, either the cleaning-up in __del__ or the</span>
    <span class="c1">#              automatic exit of daemonic thread will take care of it.</span>
    <span class="c1">#              This won&#39;t busy-wait either because `.get(timeout)` does not</span>
    <span class="c1">#              busy-wait.</span>
    <span class="c1">#</span>
    <span class="c1"># [main process]</span>
    <span class="c1">#   In the DataLoader Iter&#39;s `__del__`</span>
    <span class="c1">#     a. Set `done_event` (shared with `pin_memory_thread` and workers).</span>
    <span class="c1">#</span>
    <span class="c1">#        Note: from here on, the workers &amp; `pin_memory_thread` may exit at</span>
    <span class="c1">#              any time after they receive `None`.</span>
    <span class="c1">#</span>
    <span class="c1">#     b. Exit `pin_memory_thread`</span>
    <span class="c1">#          i.   Put `None` in `worker_result_queue`.</span>
    <span class="c1">#          ii.  Join the `pin_memory_thread`.</span>
    <span class="c1">#</span>
    <span class="c1">#     c. Exit the workers.</span>
    <span class="c1">#          i.   Put `None` in each worker&#39;s `index_queue`.</span>
    <span class="c1">#          ii.  Join the workers.</span>
    <span class="c1">#</span>
    <span class="c1">#        NOTE: This has to be after (b) because it may leave corrupted data</span>
    <span class="c1">#              in `worker_result_queue`, which `pin_memory_thread` reads</span>
    <span class="c1">#              from.</span>
    <span class="c1">#</span>
    <span class="c1">#   NOTE: If `pin_memory=False`, there is no `pin_memory_thread` and (b)</span>
    <span class="c1">#         can be omitted</span>
    <span class="c1">#</span>
    <span class="c1"># NB: `done_event`s isn&#39;t strictly needed. E.g., we can just check for</span>
    <span class="c1">#     `None` from `index_queue`, but it allows us to skip wasting resources</span>
    <span class="c1">#     processing indices already in `index_queue` if we are already shutting</span>
    <span class="c1">#     down.</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">batch_sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">pin_memory</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">timeout</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sample_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span><span class="p">)</span>

        <span class="n">base_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_init_fn</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">worker_init_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_queue_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_pids_set</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">send_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reorder_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done_event</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">index_queues</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">):</span>
                <span class="n">index_queue</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
                <span class="n">index_queue</span><span class="o">.</span><span class="n">cancel_join_thread</span><span class="p">()</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
                    <span class="n">target</span><span class="o">=</span><span class="n">_worker_loop</span><span class="p">,</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">index_queue</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">done_event</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span> <span class="n">base_seed</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">worker_init_fn</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
                <span class="n">w</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># NB: Process.start() actually take some time as it needs to</span>
                <span class="c1">#     start a process and pass the arguments over via a pipe.</span>
                <span class="c1">#     Therefore, we only add a worker to self.workers list after</span>
                <span class="c1">#     it started, so that we do not call .join() if program dies</span>
                <span class="c1">#     before it starts, and __del__ tries to join but will get:</span>
                <span class="c1">#     AssertionError: can only join a started process.</span>
                <span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">index_queues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index_queue</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
                <span class="n">pin_memory_thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span>
                    <span class="n">target</span><span class="o">=</span><span class="n">_pin_memory_loop</span><span class="p">,</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span><span class="p">,</span>
                          <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">done_event</span><span class="p">))</span>
                <span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
                <span class="c1"># Similar to workers (see comment above), we only register</span>
                <span class="c1"># pin_memory_thread once it is started.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory_thread</span> <span class="o">=</span> <span class="n">pin_memory_thread</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span>

            <span class="n">_update_worker_pids</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">pid</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">))</span>
            <span class="n">_set_SIGCHLD_handler</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">worker_pids_set</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># prime the prefetch loop</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_put_indices</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># In the non-timeout case, worker exit is covered by SIGCHLD handler.</span>
        <span class="c1"># But if `pin_memory=True`, we still need account for the possibility</span>
        <span class="c1"># that `pin_memory_thread` dies.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">timeout</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;DataLoader timed out after </span><span class="si">{}</span><span class="s1"> seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">timeout</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">:</span>
            <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">MP_STATUS_CHECK_INTERVAL</span><span class="p">)</span>
                <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
                    <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># while condition is false, i.e., pin_memory_thread died.</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Pin memory thread exited unexpectedly&#39;</span><span class="p">)</span>
            <span class="c1"># In this case, `self.data_queue` is a `queue.Queue`,. But we don&#39;t</span>
            <span class="c1"># need to call `.task_done()` because we don&#39;t use `.join()`.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># same-process loading</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_iter</span><span class="p">)</span>  <span class="c1"># may raise StopIteration</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">pin_memory_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">batch</span>

        <span class="c1"># check if the next sample has already been generated</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reorder_dict</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reorder_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_next_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown_workers</span><span class="p">()</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span>

        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_batch</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span><span class="p">:</span>
                <span class="c1"># store out-of-order samples</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reorder_dict</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span>
                <span class="k">continue</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_next_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="nb">next</span> <span class="o">=</span> <span class="fm">__next__</span>  <span class="c1"># Python 2 compatibility</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_put_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_iter</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index_queues</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">worker_queue_idx</span><span class="p">]</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">send_idx</span><span class="p">,</span> <span class="n">indices</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker_queue_idx</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">worker_queue_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batches_outstanding</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">send_idx</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_process_next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rcvd_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_put_indices</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">ExceptionWrapper</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">batch</span><span class="o">.</span><span class="n">exc_type</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">exc_msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># TODO: add limited pickling support for sharing an iterator</span>
        <span class="c1"># across multiple threads for HOGWILD.</span>
        <span class="c1"># Probably the best way to do this is by moving the sample pushing</span>
        <span class="c1"># to a separate thread and then just sharing the data queue</span>
        <span class="c1"># but signalling the end is tricky without a non-blocking API</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;_DataLoaderIter cannot be pickled&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_shutdown_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on</span>
        <span class="c1"># the logic of this function.</span>
        <span class="k">if</span> <span class="n">_python_exit_status</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">or</span> <span class="n">_python_exit_status</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># See (2) of the note. If Python is shutting down, do no-op.</span>
            <span class="k">return</span>
        <span class="c1"># Normal exit when last reference is gone / iterator is depleted.</span>
        <span class="c1"># See (1) and the second half of the note.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># Removes pids from the C side data structure first so worker</span>
            <span class="c1"># termination afterwards won&#39;t trigger false positive error report.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">worker_pids_set</span><span class="p">:</span>
                <span class="n">_remove_worker_pids</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worker_pids_set</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">done_event</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>

            <span class="c1"># Exit `pin_memory_thread` first because exiting workers may leave</span>
            <span class="c1"># corrupted data in `worker_result_queue` which `pin_memory_thread`</span>
            <span class="c1"># reads from.</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;pin_memory_thread&#39;</span><span class="p">):</span>
                <span class="c1"># Use hasattr in case error happens before we set the attribute.</span>
                <span class="c1"># First time do `worker_result_queue.put` in this process.</span>

                <span class="c1"># `cancel_join_thread` in case that `pin_memory_thread` exited.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="o">.</span><span class="n">cancel_join_thread</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
                <span class="c1"># Indicate that no more data will be put on this queue by the</span>
                <span class="c1"># current process. This **must** be called after</span>
                <span class="c1"># `pin_memory_thread` is joined because that thread shares the</span>
                <span class="c1"># same pipe handles with this loader thread. If the handle is</span>
                <span class="c1"># closed, Py3 will error in this case, but Py2 will just time</span>
                <span class="c1"># out even if there is data in the queue.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">worker_result_queue</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

            <span class="c1"># Exit workers now.</span>
            <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">index_queues</span><span class="p">:</span>
                <span class="n">q</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                <span class="c1"># Indicate that no more data will be put on this queue by the</span>
                <span class="c1"># current process.</span>
                <span class="n">q</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">:</span>
                <span class="n">w</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_shutdown_workers</span><span class="p">()</span>


<div class="viewcode-block" id="DataLoader"><a class="viewcode-back" href="../../../../data.html#torch.utils.data.DataLoader">[docs]</a><span class="k">class</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Data loader. Combines a dataset and a sampler, and provides</span>
<span class="sd">    single- or multi-process iterators over the dataset.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        dataset (Dataset): dataset from which to load the data.</span>
<span class="sd">        batch_size (int, optional): how many samples per batch to load</span>
<span class="sd">            (default: ``1``).</span>
<span class="sd">        shuffle (bool, optional): set to ``True`` to have the data reshuffled</span>
<span class="sd">            at every epoch (default: ``False``).</span>
<span class="sd">        sampler (Sampler, optional): defines the strategy to draw samples from</span>
<span class="sd">            the dataset. If specified, ``shuffle`` must be False.</span>
<span class="sd">        batch_sampler (Sampler, optional): like sampler, but returns a batch of</span>
<span class="sd">            indices at a time. Mutually exclusive with :attr:`batch_size`,</span>
<span class="sd">            :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.</span>
<span class="sd">        num_workers (int, optional): how many subprocesses to use for data</span>
<span class="sd">            loading. 0 means that the data will be loaded in the main process.</span>
<span class="sd">            (default: ``0``)</span>
<span class="sd">        collate_fn (callable, optional): merges a list of samples to form a mini-batch.</span>
<span class="sd">        pin_memory (bool, optional): If ``True``, the data loader will copy tensors</span>
<span class="sd">            into CUDA pinned memory before returning them.</span>
<span class="sd">        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,</span>
<span class="sd">            if the dataset size is not divisible by the batch size. If ``False`` and</span>
<span class="sd">            the size of dataset is not divisible by the batch size, then the last batch</span>
<span class="sd">            will be smaller. (default: ``False``)</span>
<span class="sd">        timeout (numeric, optional): if positive, the timeout value for collecting a batch</span>
<span class="sd">            from workers. Should always be non-negative. (default: ``0``)</span>
<span class="sd">        worker_init_fn (callable, optional): If not ``None``, this will be called on each</span>
<span class="sd">            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as</span>
<span class="sd">            input, after seeding and before data loading. (default: ``None``)</span>

<span class="sd">    .. note:: By default, each worker will have its PyTorch seed set to</span>
<span class="sd">              ``base_seed + worker_id``, where ``base_seed`` is a long generated</span>
<span class="sd">              by main process using its RNG. However, seeds for other libraies</span>
<span class="sd">              may be duplicated upon initializing workers (w.g., NumPy), causing</span>
<span class="sd">              each worker to return identical random numbers. (See</span>
<span class="sd">              :ref:`dataloader-workers-random-seed` section in FAQ.) You may</span>
<span class="sd">              use :func:`torch.initial_seed()` to access the PyTorch seed for</span>
<span class="sd">              each worker in :attr:`worker_init_fn`, and use it to set other</span>
<span class="sd">              seeds before data loading.</span>

<span class="sd">    .. warning:: If ``spawn`` start method is used, :attr:`worker_init_fn` cannot be an</span>
<span class="sd">                 unpicklable object, e.g., a lambda function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__initialized</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">default_collate</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">timeout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">worker_init_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span> <span class="o">=</span> <span class="n">pin_memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">drop_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">=</span> <span class="n">timeout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">worker_init_fn</span> <span class="o">=</span> <span class="n">worker_init_fn</span>

        <span class="k">if</span> <span class="n">timeout</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;timeout option should be non-negative&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">batch_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">shuffle</span> <span class="ow">or</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">drop_last</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;batch_sampler option is mutually exclusive &#39;</span>
                                 <span class="s1">&#39;with batch_size, shuffle, sampler, and &#39;</span>
                                 <span class="s1">&#39;drop_last&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">shuffle</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;sampler option is mutually exclusive with &#39;</span>
                             <span class="s1">&#39;shuffle&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;num_workers option cannot be negative; &#39;</span>
                             <span class="s1">&#39;use num_workers=0 to disable multiprocessing.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">batch_sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
                    <span class="n">sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
            <span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">BatchSampler</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span> <span class="o">=</span> <span class="n">batch_sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__initialized</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__initialized</span> <span class="ow">and</span> <span class="n">attr</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="s1">&#39;sampler&#39;</span><span class="p">,</span> <span class="s1">&#39;drop_last&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> attribute should not be set after </span><span class="si">{}</span><span class="s1"> is &#39;</span>
                             <span class="s1">&#39;initialized&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">DataLoader</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">attr</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_DataLoaderIter</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../../../../_static/jquery.js"></script>
         <script type="text/javascript" src="../../../../_static/underscore.js"></script>
         <script type="text/javascript" src="../../../../_static/doctools.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="../../../../_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>
<img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.functional &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/functional.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.0.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed_deprecated.html">torch.distributed.deprecated</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torchvision/index.html">torchvision</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
          <li><a href="../torch.html">torch</a> &gt;</li>
        
      <li>torch.functional</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.functional</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch._six</span> <span class="k">import</span> <span class="n">inf</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="k">import</span> <span class="n">mul</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="k">import</span> <span class="n">reduce</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;argmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argmin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argsort&#39;</span><span class="p">,</span>
    <span class="s1">&#39;btrifact&#39;</span><span class="p">,</span>
    <span class="s1">&#39;btriunpack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;chain_matmul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;einsum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;broadcast_tensors&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isfinite&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isinf&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isnan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;norm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;meshgrid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;potrf&#39;</span><span class="p">,</span>
    <span class="s1">&#39;split&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stft&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensordot&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unique&#39;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="broadcast_tensors"><a class="viewcode-back" href="../../torch.html#torch.broadcast_tensors">[docs]</a><span class="k">def</span> <span class="nf">broadcast_tensors</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;broadcast_tensors(*tensors) -&gt; List of Tensors</span>

<span class="sd">    Broadcasts the given tensors according to :ref:`_broadcasting-semantics`.</span>

<span class="sd">    Args:</span>
<span class="sd">        *tensors: any number of tensors of the same type</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; x = torch.arange(3).view(1, 3)</span>
<span class="sd">        &gt;&gt;&gt; y = torch.arange(2).view(2, 1)</span>
<span class="sd">        &gt;&gt;&gt; a, b = torch.broadcast_tensors(x, y)</span>
<span class="sd">        &gt;&gt;&gt; a.size()</span>
<span class="sd">        torch.Size([2, 3])</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        tensor([[0, 1, 2],</span>
<span class="sd">                [0, 1, 2]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></div>


<div class="viewcode-block" id="split"><a class="viewcode-back" href="../../torch.html#torch.split">[docs]</a><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Splits the tensor into chunks.</span>

<span class="sd">    If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will</span>
<span class="sd">    be split into equally sized chunks (if possible). Last chunk will be smaller if</span>
<span class="sd">    the tensor size along the given dimension :attr:`dim` is not divisible by</span>
<span class="sd">    :attr:`split_size`.</span>

<span class="sd">    If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split</span>
<span class="sd">    into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according</span>
<span class="sd">    to :attr:`split_size_or_sections`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): tensor to split.</span>
<span class="sd">        split_size_or_sections (int) or (list(int)): size of a single chunk or</span>
<span class="sd">            list of sizes for each chunk</span>
<span class="sd">        dim (int): dimension along which to split the tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Overwriting reason:</span>
    <span class="c1"># This dispatches to two ATen functions depending on the type of</span>
    <span class="c1"># split_size_or_sections. The branching code is in tensor.py, which we</span>
    <span class="c1"># call here.</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="btrifact"><a class="viewcode-back" href="../../torch.html#torch.btrifact">[docs]</a><span class="k">def</span> <span class="nf">btrifact</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Batch LU factorization.</span>

<span class="sd">    Returns a tuple containing the LU factorization and pivots. Pivoting is done if</span>
<span class="sd">    :attr:`pivot` is set.</span>

<span class="sd">    The optional argument :attr:`info` stores information if the factorization</span>
<span class="sd">    succeeded for each minibatch example. The :attr:`info` is provided as an</span>
<span class="sd">    `IntTensor`, its values will be filled from dgetrf and a non-zero value</span>
<span class="sd">    indicates an error occurred. Specifically, the values are from cublas if cuda is</span>
<span class="sd">    being used, otherwise LAPACK.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The :attr:`info` argument is deprecated in favor of :meth:`torch.btrifact_with_info`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        A (Tensor): the tensor to factor</span>
<span class="sd">        info (IntTensor, optional): (deprecated) an `IntTensor` to store values</span>
<span class="sd">            indicating whether factorization succeeds</span>
<span class="sd">        pivot (bool, optional): controls whether pivoting is done</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple containing factorization and pivots.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; A = torch.randn(2, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; A_LU, pivots = torch.btrifact(A)</span>
<span class="sd">        &gt;&gt;&gt; A_LU</span>
<span class="sd">        tensor([[[ 1.3506,  2.5558, -0.0816],</span>
<span class="sd">                 [ 0.1684,  1.1551,  0.1940],</span>
<span class="sd">                 [ 0.1193,  0.6189, -0.5497]],</span>

<span class="sd">                [[ 0.4526,  1.2526, -0.3285],</span>
<span class="sd">                 [-0.7988,  0.7175, -0.9701],</span>
<span class="sd">                 [ 0.2634, -0.9255, -0.3459]]])</span>

<span class="sd">        &gt;&gt;&gt; pivots</span>
<span class="sd">        tensor([[ 3,  3,  3],</span>
<span class="sd">                [ 3,  3,  3]], dtype=torch.int32)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Overwriting reason:</span>
    <span class="c1"># `info` is being deprecated in favor of `btrifact_with_info`. This warning</span>
    <span class="c1"># is in tensor.py, which we call here.</span>
    <span class="k">return</span> <span class="n">A</span><span class="o">.</span><span class="n">btrifact</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">pivot</span><span class="p">)</span></div>


<div class="viewcode-block" id="btriunpack"><a class="viewcode-back" href="../../torch.html#torch.btriunpack">[docs]</a><span class="k">def</span> <span class="nf">btriunpack</span><span class="p">(</span><span class="n">LU_data</span><span class="p">,</span> <span class="n">LU_pivots</span><span class="p">,</span> <span class="n">unpack_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unpack_pivots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Unpacks the data and pivots from a batched LU factorization (btrifact) of a tensor.</span>

<span class="sd">    Returns a tuple of tensors as ``(the pivots, the L tensor, the U tensor)``.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        LU_data (Tensor): the packed LU factorization data</span>
<span class="sd">        LU_pivots (Tensor): the packed LU factorization pivots</span>
<span class="sd">        unpack_data (bool): flag indicating if the data should be unpacked</span>
<span class="sd">        unpack_pivots (bool): flag indicating if the pivots should be unpacked</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; A = torch.randn(2, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; A_LU, pivots = A.btrifact()</span>
<span class="sd">        &gt;&gt;&gt; P, A_L, A_U = torch.btriunpack(A_LU, pivots)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # can recover A from factorization</span>
<span class="sd">        &gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">nBatch</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">LU_data</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">unpack_data</span><span class="p">:</span>
        <span class="n">I_U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">LU_data</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">nBatch</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">)</span>
        <span class="n">I_L</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">I_U</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">LU_data</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">LU_data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">LU_data</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">LU_data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="n">I_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">sz</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">LU_data</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">nBatch</span><span class="p">,</span> <span class="n">sz</span><span class="p">,</span> <span class="n">sz</span><span class="p">)</span>
        <span class="n">L</span><span class="p">[</span><span class="n">I_diag</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">L</span><span class="p">[</span><span class="n">I_L</span><span class="p">]</span> <span class="o">=</span> <span class="n">LU_data</span><span class="p">[</span><span class="n">I_L</span><span class="p">]</span>
        <span class="n">U</span><span class="p">[</span><span class="n">I_U</span><span class="p">]</span> <span class="o">=</span> <span class="n">LU_data</span><span class="p">[</span><span class="n">I_U</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">U</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">unpack_pivots</span><span class="p">:</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">sz</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">LU_data</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">nBatch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nBatch</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sz</span><span class="p">):</span>
                <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">LU_pivots</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">k</span><span class="p">]</span>
                <span class="n">P</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">t</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">P</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">P</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">U</span></div>


<div class="viewcode-block" id="einsum"><a class="viewcode-back" href="../../torch.html#torch.einsum">[docs]</a><span class="k">def</span> <span class="nf">einsum</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="o">*</span><span class="n">operands</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;einsum(equation, *operands) -&gt; Tensor</span>

<span class="sd">This function provides a way of computing multilinear expressions (i.e. sums of products) using the</span>
<span class="sd">Einstein summation convention.</span>

<span class="sd">Args:</span>
<span class="sd">    equation (string): The equation is given in terms of lower case letters (indices) to be associated</span>
<span class="sd">           with each dimension of the operands and result. The left hand side lists the operands</span>
<span class="sd">           dimensions, separated by commas. There should be one index letter per tensor dimension.</span>
<span class="sd">           The right hand side follows after `-&gt;` and gives the indices for the output.</span>
<span class="sd">           If the `-&gt;` and right hand side are omitted, it implicitly defined as the alphabetically</span>
<span class="sd">           sorted list of all indices appearing exactly once in the left hand side.</span>
<span class="sd">           The indices not apprearing in the output are summed over after multiplying the operands</span>
<span class="sd">           entries.</span>
<span class="sd">           If an index appears several times for the same operand, a diagonal is taken.</span>
<span class="sd">           Ellipses `...` represent a fixed number of dimensions. If the right hand side is inferred,</span>
<span class="sd">           the ellipsis dimensions are at the beginning of the output.</span>
<span class="sd">    operands (list of Tensors): The operands to compute the Einstein sum of.</span>
<span class="sd">           Note that the operands are passed as a list, not as individual arguments.</span>

<span class="sd">Examples::</span>

<span class="sd">    &gt;&gt;&gt; x = torch.randn(5)</span>
<span class="sd">    &gt;&gt;&gt; y = torch.randn(4)</span>
<span class="sd">    &gt;&gt;&gt; torch.einsum(&#39;i,j-&gt;ij&#39;, x, y)  # outer product</span>
<span class="sd">    tensor([[-0.0570, -0.0286, -0.0231,  0.0197],</span>
<span class="sd">            [ 1.2616,  0.6335,  0.5113, -0.4351],</span>
<span class="sd">            [ 1.4452,  0.7257,  0.5857, -0.4984],</span>
<span class="sd">            [-0.4647, -0.2333, -0.1883,  0.1603],</span>
<span class="sd">            [-1.1130, -0.5588, -0.4510,  0.3838]])</span>


<span class="sd">    &gt;&gt;&gt; A = torch.randn(3,5,4)</span>
<span class="sd">    &gt;&gt;&gt; l = torch.randn(2,5)</span>
<span class="sd">    &gt;&gt;&gt; r = torch.randn(2,4)</span>
<span class="sd">    &gt;&gt;&gt; torch.einsum(&#39;bn,anm,bm-&gt;ba&#39;, l, A, r) # compare torch.nn.functional.bilinear</span>
<span class="sd">    tensor([[-0.3430, -5.2405,  0.4494],</span>
<span class="sd">            [ 0.3311,  5.5201, -3.0356]])</span>


<span class="sd">    &gt;&gt;&gt; As = torch.randn(3,2,5)</span>
<span class="sd">    &gt;&gt;&gt; Bs = torch.randn(3,5,4)</span>
<span class="sd">    &gt;&gt;&gt; torch.einsum(&#39;bij,bjk-&gt;bik&#39;, As, Bs) # batch matrix multiplication</span>
<span class="sd">    tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],</span>
<span class="sd">             [-1.6706, -0.8097, -0.8025, -2.1183]],</span>

<span class="sd">            [[ 4.2239,  0.3107, -0.5756, -0.2354],</span>
<span class="sd">             [-1.4558, -0.3460,  1.5087, -0.8530]],</span>

<span class="sd">            [[ 2.8153,  1.8787, -4.3839, -1.2112],</span>
<span class="sd">             [ 0.3728, -2.1131,  0.0921,  0.8305]]])</span>

<span class="sd">    &gt;&gt;&gt; A = torch.randn(3, 3)</span>
<span class="sd">    &gt;&gt;&gt; torch.einsum(&#39;ii-&gt;i&#39;, A) # diagonal</span>
<span class="sd">    tensor([-0.7825,  0.8291, -0.1936])</span>

<span class="sd">    &gt;&gt;&gt; A = torch.randn(4, 3, 3)</span>
<span class="sd">    &gt;&gt;&gt; torch.einsum(&#39;...ii-&gt;...i&#39;, A) # batch diagonal</span>
<span class="sd">    tensor([[-1.0864,  0.7292,  0.0569],</span>
<span class="sd">            [-0.9725, -1.0270,  0.6493],</span>
<span class="sd">            [ 0.5832, -1.1716, -1.5084],</span>
<span class="sd">            [ 0.4041, -1.1690,  0.8570]])</span>

<span class="sd">    &gt;&gt;&gt; A = torch.randn(2, 3, 4, 5)</span>
<span class="sd">    &gt;&gt;&gt; torch.einsum(&#39;...ij-&gt;...ji&#39;, A).shape # batch permute</span>
<span class="sd">    torch.Size([2, 3, 5, 4])</span>
<span class="sd">&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="c1"># the old interface of passing the operands as one list argument</span>
        <span class="n">operands</span> <span class="o">=</span> <span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="n">operands</span><span class="p">)</span></div>


<div class="viewcode-block" id="isfinite"><a class="viewcode-back" href="../../torch.html#torch.isfinite">[docs]</a><span class="k">def</span> <span class="nf">isfinite</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a new tensor with boolean elements representing if each element is `Finite` or not.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): A tensor to check</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A ``torch.ByteTensor`` containing a 1 at each location of finite elements and 0 otherwise</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; torch.isfinite(torch.tensor([1, float(&#39;inf&#39;), 2, float(&#39;-inf&#39;), float(&#39;nan&#39;)]))</span>
<span class="sd">        tensor([ 1,  0,  1,  0,  0], dtype=torch.uint8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The argument is not a tensor&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="p">))</span>

    <span class="c1"># Support int input, nan and inf are concepts in floating point numbers.</span>
    <span class="c1"># Numpy uses type &#39;Object&#39; when the int overflows long, but we don&#39;t</span>
    <span class="c1"># have a similar concept. It&#39;s safe to assume any created LongTensor doesn&#39;t</span>
    <span class="c1"># overflow and it&#39;s finite.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">tensor</span> <span class="o">==</span> <span class="n">tensor</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">!=</span> <span class="n">inf</span><span class="p">)</span></div>


<div class="viewcode-block" id="isinf"><a class="viewcode-back" href="../../torch.html#torch.isinf">[docs]</a><span class="k">def</span> <span class="nf">isinf</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a new tensor with boolean elements representing if each element is `+/-INF` or not.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): A tensor to check</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A ``torch.ByteTensor`` containing a 1 at each location of `+/-INF` elements and 0 otherwise</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; torch.isinf(torch.tensor([1, float(&#39;inf&#39;), 2, float(&#39;-inf&#39;), float(&#39;nan&#39;)]))</span>
<span class="sd">        tensor([ 0,  1,  0,  1,  0], dtype=torch.uint8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The argument is not a tensor&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">==</span> <span class="n">inf</span></div>


<div class="viewcode-block" id="meshgrid"><a class="viewcode-back" href="../../torch.html#torch.meshgrid">[docs]</a><span class="k">def</span> <span class="nf">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Take :math:`N` tensors, each of which can be either scalar or 1-dimensional</span>
<span class="sd">vector, and create :math:`N` N-dimensional grids, where the :math:`i`th grid is defined by</span>
<span class="sd">expanding the :math:`i`th input over dimensions defined by other inputs.</span>


<span class="sd">    Args:</span>
<span class="sd">        tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be</span>
<span class="sd">        treated as tensors of size :math:`(1,)` automatically</span>

<span class="sd">    Returns:</span>
<span class="sd">        seq (sequence of Tensors): If the input has :math:`k` tensors of size</span>
<span class="sd">        :math:`(N_1,), (N_2,), \ldots , (N_k,)`, then the output would also has :math:`k` tensors,</span>
<span class="sd">        where all tensors are of size :math:`(N_1, N_2, \ldots , N_k)`.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; x = torch.tensor([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; y = torch.tensor([4, 5, 6])</span>
<span class="sd">        &gt;&gt;&gt; grid_x, grid_y = torch.meshgrid(x, y)</span>
<span class="sd">        &gt;&gt;&gt; grid_x</span>
<span class="sd">        tensor([[1, 1, 1],</span>
<span class="sd">                [2, 2, 2],</span>
<span class="sd">                [3, 3, 3]])</span>
<span class="sd">        &gt;&gt;&gt; grid_y</span>
<span class="sd">        tensor([[4, 5, 6],</span>
<span class="sd">                [4, 5, 6],</span>
<span class="sd">                [4, 5, 6]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;meshgrid() got an unexpected keyword argument &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="c1"># the old interface of passing the operands as one list argument</span>
        <span class="n">tensors</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></div>


<div class="viewcode-block" id="stft"><a class="viewcode-back" href="../../torch.html#torch.stft">[docs]</a><span class="k">def</span> <span class="nf">stft</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">win_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
         <span class="n">center</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;reflect&#39;</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Short-time Fourier transform (STFT).</span>

<span class="sd">    Ignoring the optional batch dimension, this method computes the following</span>
<span class="sd">    expression:</span>

<span class="sd">    .. math::</span>
<span class="sd">        X[m, \omega] = \sum_{k = 0}^{\text{win\_length}}%</span>
<span class="sd">                            \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ %</span>
<span class="sd">                            \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),</span>

<span class="sd">    where :math:`m` is the index of the sliding window, and :math:`\omega` is</span>
<span class="sd">    the frequency that :math:`0 \leq \omega &lt; \text{n\_fft}`. When</span>
<span class="sd">    :attr:`onesided` is the default value ``True``,</span>

<span class="sd">    * :attr:`input` must be either a 1-D time sequence or a 2-D batch of time</span>
<span class="sd">      sequences.</span>

<span class="sd">    * If :attr:`hop_length` is ``None`` (default), it is treated as equal to</span>
<span class="sd">      ``floor(n_fft / 4)``.</span>

<span class="sd">    * If :attr:`win_length` is ``None`` (default), it is treated as equal to</span>
<span class="sd">      :attr:`n_fft`.</span>

<span class="sd">    * :attr:`window` can be a 1-D tensor of size :attr:`win_length`, e.g., from</span>
<span class="sd">      :meth:`torch.hann_window`. If :attr:`window` is ``None`` (default), it is</span>
<span class="sd">      treated as if having :math:`1` everywhere in the window. If</span>
<span class="sd">      :math:`\text{win\_length} &lt; \text{n\_fft}`, :attr:`window` will be padded on</span>
<span class="sd">      both sides to length :attr:`n_fft` before being applied.</span>

<span class="sd">    * If :attr:`center` is ``True`` (default), :attr:`input` will be padded on</span>
<span class="sd">      both sides so that the :math:`t`-th frame is centered at time</span>
<span class="sd">      :math:`t \times \text{hop\_length}`. Otherwise, the :math:`t`-th frame</span>
<span class="sd">      begins at time  :math:`t \times \text{hop\_length}`.</span>

<span class="sd">    * :attr:`pad_mode` determines the padding method used on :attr:`input` when</span>
<span class="sd">      :attr:`center` is ``True``. See :meth:`torch.nn.functional.pad` for</span>
<span class="sd">      all available options. Default is ``&quot;reflect&quot;``.</span>

<span class="sd">    * If :attr:`onesided` is ``True`` (default), only values for :math:`\omega`</span>
<span class="sd">      in :math:`\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]`</span>
<span class="sd">      are returned because the real-to-complex Fourier transform satisfies the</span>
<span class="sd">      conjugate symmetry, i.e., :math:`X[m, \omega] = X[m, \text{n\_fft} - \omega]^*`.</span>

<span class="sd">    * If :attr:`normalized` is ``True`` (default is ``False``), the function</span>
<span class="sd">      returns the normalized STFT results, i.e., multiplied by :math:`(\text{frame\_length})^{-0.5}`.</span>

<span class="sd">    Returns the real and the imaginary parts together as one tensor of size</span>
<span class="sd">    :math:`(* \times N \times T \times 2)`, where :math:`*` is the optional</span>
<span class="sd">    batch size of :attr:`input`, :math:`N` is the number of frequencies where</span>
<span class="sd">    STFT is applied, :math:`T` is the total number of frames used, and each pair</span>
<span class="sd">    in the last dimension represents a complex number as the real part and the</span>
<span class="sd">    imaginary part.</span>

<span class="sd">    .. warning::</span>
<span class="sd">      This function changed signature at version 0.4.1. Calling with the</span>
<span class="sd">      previous signature may cause error or return incorrect result.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        n_fft (int): size of Fourier transform</span>
<span class="sd">        hop_length (int, optional): the distance between neighboring sliding window</span>
<span class="sd">            frames. Default: ``None`` (treated as equal to ``floor(n_fft / 4)``)</span>
<span class="sd">        win_length (int, optional): the size of window frame and STFT filter.</span>
<span class="sd">            Default: ``None``  (treated as equal to :attr:`n_fft`)</span>
<span class="sd">        window (Tensor, optional): the optional window function.</span>
<span class="sd">            Default: ``None`` (treated as window of all :math:`1` s)</span>
<span class="sd">        center (bool, optional): whether to pad :attr:`input` on both sides so</span>
<span class="sd">            that the :math:`t`-th frame is centered at time :math:`t \times \text{hop\_length}`.</span>
<span class="sd">            Default: ``True``</span>
<span class="sd">        pad_mode (string, optional): controls the padding method used when</span>
<span class="sd">            :attr:`center` is ``True``. Default: ``&quot;reflect&quot;``</span>
<span class="sd">        normalized (bool, optional): controls whether to return the normalized STFT results</span>
<span class="sd">             Default: ``False``</span>
<span class="sd">        onesided (bool, optional): controls whether to return half of results to</span>
<span class="sd">            avoid redundancy Default: ``True``</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A tensor containing the STFT result with shape described above</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: after having proper ways to map Python strings to ATen Enum, move</span>
    <span class="c1">#       this and F.pad to ATen.</span>
    <span class="k">if</span> <span class="n">center</span><span class="p">:</span>
        <span class="n">signal_dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
        <span class="n">extended_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">3</span> <span class="o">-</span> <span class="n">signal_dim</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_fft</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">extended_shape</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">),</span> <span class="n">pad_mode</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="n">signal_dim</span><span class="p">:])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">stft</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">,</span> <span class="n">win_length</span><span class="p">,</span> <span class="n">window</span><span class="p">,</span> <span class="n">normalized</span><span class="p">,</span> <span class="n">onesided</span><span class="p">)</span></div>


<div class="viewcode-block" id="isnan"><a class="viewcode-back" href="../../torch.html#torch.isnan">[docs]</a><span class="k">def</span> <span class="nf">isnan</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a new tensor with boolean elements representing if each element is `NaN` or not.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): A tensor to check</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A ``torch.ByteTensor`` containing a 1 at each location of `NaN` elements.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; torch.isnan(torch.tensor([1, float(&#39;nan&#39;), 2]))</span>
<span class="sd">        tensor([ 0,  1,  0], dtype=torch.uint8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The argument is not a tensor&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tensor</span> <span class="o">!=</span> <span class="n">tensor</span></div>


<div class="viewcode-block" id="unique"><a class="viewcode-back" href="../../torch.html#torch.unique">[docs]</a><span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the unique scalar elements of the input tensor as a 1-D tensor.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        sorted (bool): Whether to sort the unique elements in ascending order</span>
<span class="sd">            before returning as output.</span>
<span class="sd">        return_inverse (bool): Whether to also return the indices for where</span>
<span class="sd">            elements in the original input ended up in the returned unique list.</span>
<span class="sd">        dim (int): the dimension to apply unique. If ``None``, the unique of the</span>
<span class="sd">            flattened input is returned. default: ``None``</span>

<span class="sd">    Returns:</span>
<span class="sd">        (Tensor, Tensor (optional)): A tensor or a tuple of tensors containing</span>

<span class="sd">            - **output** (*Tensor*): the output list of unique scalar elements.</span>
<span class="sd">            - **inverse_indices** (*Tensor*): (optional) if</span>
<span class="sd">              :attr:`return_inverse` is True, there will be a</span>
<span class="sd">              2nd returned tensor (same shape as input) representing the indices</span>
<span class="sd">              for where elements in the original input map to in the output;</span>
<span class="sd">              otherwise, this function will only return a single tensor.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([ 2,  3,  1])</span>

<span class="sd">        &gt;&gt;&gt; output, inverse_indices = torch.unique(</span>
<span class="sd">                torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([ 1,  2,  3])</span>
<span class="sd">        &gt;&gt;&gt; inverse_indices</span>
<span class="sd">        tensor([ 0,  2,  1,  2])</span>

<span class="sd">        &gt;&gt;&gt; output, inverse_indices = torch.unique(</span>
<span class="sd">                torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        tensor([ 1,  2,  3])</span>
<span class="sd">        &gt;&gt;&gt; inverse_indices</span>
<span class="sd">        tensor([[ 0,  2],</span>
<span class="sd">                [ 1,  2]])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">inverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_unique_dim</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="n">dim</span><span class="p">,</span>
            <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
            <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">inverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_unique</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span>
            <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">return_inverse</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">inverse_indices</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="argmax"><a class="viewcode-back" href="../../torch.html#torch.argmax">[docs]</a><span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the indices of the maximum values of a tensor across a dimension.</span>

<span class="sd">    This is the second value returned by :meth:`torch.max`. See its</span>
<span class="sd">    documentation for the exact semantics of this method.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        dim (int): the dimension to reduce. If ``None``, the argmax of the</span>
<span class="sd">            flattened input is returned.</span>
<span class="sd">        keepdim (bool): whether the output tensors have :attr:`dim`</span>
<span class="sd">            retained or not. Ignored if ``dim=None``.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; a = torch.randn(4, 4)</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],</span>
<span class="sd">                [-0.7401, -0.8805, -0.3402, -1.1936],</span>
<span class="sd">                [ 0.4907, -1.3948, -1.0691, -0.3132],</span>
<span class="sd">                [-1.6092,  0.5419, -0.2993,  0.3195]])</span>


<span class="sd">        &gt;&gt;&gt; torch.argmax(a, dim=1)</span>
<span class="sd">        tensor([ 0,  2,  0,  1])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_argmax</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span></div>


<div class="viewcode-block" id="argmin"><a class="viewcode-back" href="../../torch.html#torch.argmin">[docs]</a><span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the indices of the minimum values of a tensor across a dimension.</span>

<span class="sd">    This is the second value returned by :meth:`torch.min`. See its</span>
<span class="sd">    documentation for the exact semantics of this method.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        dim (int): the dimension to reduce. If ``None``, the argmin of the</span>
<span class="sd">            flattened input is returned.</span>
<span class="sd">        keepdim (bool): whether the output tensors have :attr:`dim`</span>
<span class="sd">            retained or not. Ignored if ``dim=None``.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; a = torch.randn(4, 4)</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],</span>
<span class="sd">                [ 1.0100, -1.1975, -0.0102, -0.4732],</span>
<span class="sd">                [-0.9240,  0.1207, -0.7506, -1.0213],</span>
<span class="sd">                [ 1.7809, -1.2960,  0.9384,  0.1438]])</span>


<span class="sd">        &gt;&gt;&gt; torch.argmin(a, dim=1)</span>
<span class="sd">        tensor([ 2,  1,  3,  1])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_argmin</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_argmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensordot"><a class="viewcode-back" href="../../torch.html#torch.tensordot">[docs]</a><span class="k">def</span> <span class="nf">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a contraction of a and b over multiple dimensions.</span>

<span class="sd">    :attr:`tensordot` implements a generalizes the matrix product.</span>

<span class="sd">    Args:</span>
<span class="sd">      a (Tensor): Left tensor to contract</span>
<span class="sd">      b (Tensor): Right tensor to contract</span>
<span class="sd">      dims (int or tuple of two lists of integers): number of dimensions to</span>
<span class="sd">         contract or explicit lists of dimensions for :attr:`a` and</span>
<span class="sd">         :attr:`b` respectively</span>

<span class="sd">    When called with an integer argument :attr:`dims` = :math:`d`, and the number of</span>
<span class="sd">    dimensions of :attr:`a` and :attr:`b` is :math:`m` and :math:`n`, respectively,</span>
<span class="sd">    it computes</span>

<span class="sd">    .. math::</span>
<span class="sd">        r_{i_0,...,i_{m-d}, i_d,...,i_n}</span>
<span class="sd">          = \sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \times b_{k_0,...,k_{d-1}, i_d,...,i_n}.</span>

<span class="sd">    When called with :attr:`dims` of the list form, the given dimensions will be contracted</span>
<span class="sd">    in place of the last :math:`d` of :attr:`a` and the first :math:`d` of :math:`b`. The sizes</span>
<span class="sd">    in these dimensions must match, but :attr:`tensordot` will deal with broadcasted</span>
<span class="sd">    dimensions.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; a = torch.arange(60.).reshape(3, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; b = torch.arange(24.).reshape(4, 3, 2)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensordot(a, b, dims=([1, 0], [0, 1]))</span>
<span class="sd">        tensor([[4400., 4730.],</span>
<span class="sd">                [4532., 4874.],</span>
<span class="sd">                [4664., 5018.],</span>
<span class="sd">                [4796., 5162.],</span>
<span class="sd">                [4928., 5306.]])</span>

<span class="sd">        &gt;&gt;&gt; a = torch.randn(3, 4, 5, device=&#39;cuda&#39;)</span>
<span class="sd">        &gt;&gt;&gt; b = torch.randn(4, 5, 6, device=&#39;cuda&#39;)</span>
<span class="sd">        &gt;&gt;&gt; c = torch.tensordot(a, b, dims=2).cpu()</span>
<span class="sd">        tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],</span>
<span class="sd">                [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],</span>
<span class="sd">                [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">or</span> \
       <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">dims</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">dims_a</span><span class="p">,</span> <span class="n">dims_b</span> <span class="o">=</span> <span class="n">dims</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">dims</span> <span class="o">=</span> <span class="n">dims</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">dims_a</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">dims</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">dims_b</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">dims</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dims_a</span><span class="p">,</span> <span class="n">dims_b</span><span class="p">)</span></div>


<div class="viewcode-block" id="argsort"><a class="viewcode-back" href="../../torch.html#torch.argsort">[docs]</a><span class="k">def</span> <span class="nf">argsort</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the indices that sort a tensor along a given dimension in ascending</span>
<span class="sd">    order by value.</span>

<span class="sd">    This is the second value returned by :meth:`torch.sort`.  See its documentation</span>
<span class="sd">    for the exact semantics of this method.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        dim (int, optional): the dimension to sort along</span>
<span class="sd">        descending (bool, optional): controls the sorting order (ascending or descending)</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; a = torch.randn(4, 4)</span>
<span class="sd">        &gt;&gt;&gt; a</span>
<span class="sd">        tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],</span>
<span class="sd">                [ 0.1598,  0.0788, -0.0745, -1.2700],</span>
<span class="sd">                [ 1.2208,  1.0722, -0.7064,  1.2564],</span>
<span class="sd">                [ 0.0669, -0.2318, -0.8229, -0.9280]])</span>


<span class="sd">        &gt;&gt;&gt; torch.argsort(a, dim=1)</span>
<span class="sd">        tensor([[2, 0, 3, 1],</span>
<span class="sd">                [3, 2, 1, 0],</span>
<span class="sd">                [2, 1, 0, 3],</span>
<span class="sd">                [3, 2, 1, 0]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">descending</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span></div>


<div class="viewcode-block" id="norm"><a class="viewcode-back" href="../../torch.html#torch.norm">[docs]</a><span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s2">&quot;fro&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the matrix norm or vector norm of a given tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        p (int, float, inf, -inf, &#39;fro&#39;, &#39;nuc&#39;, optional): the order of norm. Default: ``&#39;fro&#39;``</span>
<span class="sd">            The following norms can be calculated:</span>

<span class="sd">            =====  ============================  ==========================</span>
<span class="sd">            ord    matrix norm                   vector norm</span>
<span class="sd">            =====  ============================  ==========================</span>
<span class="sd">            None   Frobenius norm                2-norm</span>
<span class="sd">            &#39;fro&#39;  Frobenius norm                --</span>
<span class="sd">            &#39;nuc&#39;  nuclear norm                  --</span>
<span class="sd">            Other  as vec norm when dim is None  sum(abs(x)**ord)**(1./ord)</span>
<span class="sd">            =====  ============================  ==========================</span>

<span class="sd">        dim (int, 2-tuple of ints, 2-list of ints, optional): If it is an int,</span>
<span class="sd">            vector norm will be calculated, if it is 2-tuple of ints, matrix norm</span>
<span class="sd">            will be calculated. If the value is None, matrix norm will be calculated</span>
<span class="sd">            when the input tensor only has two dimensions, vector norm will be</span>
<span class="sd">            calculated when the input tensor only has one dimension. If the input</span>
<span class="sd">            tensor has more than two dimensions, the vector norm will be applied to</span>
<span class="sd">            last dimension.</span>
<span class="sd">        keepdim (bool, optional): whether the output tensors have :attr:`dim`</span>
<span class="sd">            retained or not. Ignored if :attr:`dim` = ``None`` and</span>
<span class="sd">            :attr:`out` = ``None``. Default: ``False``</span>
<span class="sd">        out (Tensor, optional): the output tensor. Ignored if</span>
<span class="sd">            :attr:`dim` = ``None`` and :attr:`out` = ``None``.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; a = torch.arange(9, dtype= torch.float) - 4</span>
<span class="sd">        &gt;&gt;&gt; b = a.reshape((3, 3))</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(a)</span>
<span class="sd">        tensor(7.7460)</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(b)</span>
<span class="sd">        tensor(7.7460)</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(a, float(&#39;inf&#39;))</span>
<span class="sd">        tensor(4.)</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(b, float(&#39;inf&#39;))</span>
<span class="sd">        tensor([4., 3., 4.])</span>
<span class="sd">        &gt;&gt;&gt; c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(c, dim=0)</span>
<span class="sd">        tensor([1.4142, 2.2361, 5.0000])</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(c, dim=1)</span>
<span class="sd">        tensor([3.7417, 4.2426])</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(c, p=1, dim=1)</span>
<span class="sd">        tensor([6., 6.])</span>
<span class="sd">        &gt;&gt;&gt; d = torch.arange(8, dtype= torch.float).reshape(2,2,2)</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(d, dim=(1,2))</span>
<span class="sd">        tensor([ 3.7417, 11.2250])</span>
<span class="sd">        &gt;&gt;&gt; torch.norm(d[0, :, :]), torch.norm(d[1, :, :])</span>
<span class="sd">        (tensor(3.7417), tensor(11.2250))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>

    <span class="c1"># catch default case</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="s2">&quot;fro&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">frobenius_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">p</span> <span class="o">!=</span> <span class="s2">&quot;nuc&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="s2">&quot;fro&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">frobenius_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">frobenius_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">p</span> <span class="o">==</span> <span class="s2">&quot;nuc&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">nuclear_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">nuclear_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span></div>


<div class="viewcode-block" id="chain_matmul"><a class="viewcode-back" href="../../torch.html#torch.chain_matmul">[docs]</a><span class="k">def</span> <span class="nf">chain_matmul</span><span class="p">(</span><span class="o">*</span><span class="n">matrices</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the matrix product of the :math:`N` 2-D tensors. This product is efficiently computed</span>
<span class="sd">    using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms</span>
<span class="sd">    of arithmetic operations (`[CLRS]`_). Note that since this is a function to compute the product, :math:`N`</span>
<span class="sd">    needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.</span>
<span class="sd">    If :math:`N` is 1, then this is a no-op - the original matrix is returned as is.</span>


<span class="sd">    Args:</span>
<span class="sd">        matrices (Tensors...): a sequence of 2 or more 2-D tensors whose product is to be determined.</span>


<span class="sd">    Returns:</span>
<span class="sd">        Tensor: if the :math:`i^{th}` tensor was of dimensions :math:`p_{i} \times p_{i + 1}`, then the product</span>
<span class="sd">        would be of dimensions :math:`p_{1} \times p_{N + 1}`.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; a = torch.randn(3, 4)</span>
<span class="sd">        &gt;&gt;&gt; b = torch.randn(4, 5)</span>
<span class="sd">        &gt;&gt;&gt; c = torch.randn(5, 6)</span>
<span class="sd">        &gt;&gt;&gt; d = torch.randn(6, 7)</span>
<span class="sd">        &gt;&gt;&gt; torch.chain_matmul(a, b, c, d)</span>
<span class="sd">        tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],</span>
<span class="sd">                [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],</span>
<span class="sd">                [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])</span>

<span class="sd">    .. _`[CLRS]`: https://mitpress.mit.edu/books/introduction-algorithms-third-edition</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">chain_matmul</span><span class="p">(</span><span class="n">matrices</span><span class="p">)</span></div>


<div class="viewcode-block" id="potrf"><a class="viewcode-back" href="../../torch.html#torch.potrf">[docs]</a><span class="k">def</span> <span class="nf">potrf</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the Cholesky decomposition of a symmetric positive-definite</span>
<span class="sd">    matrix :math:`A`.</span>

<span class="sd">    For more information, regarding :func:`torch.potrf`, please check :func:`torch.cholesky`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next</span>
<span class="sd">        release. Please use torch.cholesky instead and note that the :attr:`upper` argument in</span>
<span class="sd">        torch.cholesky defaults to ``False``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next &quot;</span>
                  <span class="s2">&quot;release. Please use torch.cholesky instead and note that the :attr:`upper` argument in&quot;</span>
                  <span class="s2">&quot; torch.cholesky defaults to ``False``.&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../../_static/jquery.js"></script>
         <script type="text/javascript" src="../../_static/underscore.js"></script>
         <script type="text/javascript" src="../../_static/doctools.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="../../_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>
<img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.distributed.deprecated &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/distributed/deprecated.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.0.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed_deprecated.html">torch.distributed.deprecated</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/index.html">torchvision</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../distributed.html">torch.distributed</a> &gt;</li>
        
      <li>torch.distributed.deprecated</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.distributed.deprecated</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">torch.distributed.deprecated provides an MPI-like interface for exchanging tensor</span>
<span class="sd">data across multi-machine networks. It supports a few different backends</span>
<span class="sd">and initialization methods.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">atexit</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">torch._utils</span> <span class="k">import</span> <span class="n">_flatten_dense_tensors</span><span class="p">,</span> <span class="n">_unflatten_dense_tensors</span>


<span class="k">class</span> <span class="nc">dist_backend</span><span class="p">:</span>
    <span class="n">UNDEFINED</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">TCP</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">MPI</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">GLOO</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">NCCL</span> <span class="o">=</span> <span class="mi">3</span>


<span class="n">_INITIALIZED_PG</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">_INITIALIZED_MW</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">_initialized</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">UNDEFINED</span>
<span class="n">_scope</span> <span class="o">=</span> <span class="nb">locals</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_extend_scope</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="n">_scope</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)})</span>


<span class="k">def</span> <span class="nf">is_available</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_distributed</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">destroy_process_group</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Destroy the initialized distributed package</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_backend</span>
    <span class="k">global</span> <span class="n">_initialized</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_destroy_process_group</span><span class="p">()</span>
    <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">UNDEFINED</span>
    <span class="n">_initialized</span> <span class="o">=</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">is_initialized</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Checking if the process group has been initialized</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span>


<div class="viewcode-block" id="init_process_group"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.init_process_group">[docs]</a><span class="k">def</span> <span class="nf">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initializes the distributed package.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        backend (str): Name of the backend to use. Depending on build-time configuration</span>
<span class="sd">            valid values include: ``tcp``, ``mpi``, ``gloo`` and ``nccl``.</span>
<span class="sd">        init_method (str, optional): URL specifying how to initialize the package.</span>
<span class="sd">        world_size (int, optional): Number of processes participating in the job.</span>
<span class="sd">        rank (int, optional): Rank of the current process.</span>
<span class="sd">        group_name (str, optional): Group name. See description of init methods.</span>

<span class="sd">    To enable ``backend == mpi``, PyTorch needs to built from source on a system that</span>
<span class="sd">    supports MPI. If you want to use Open MPI with CUDA-aware support, please use</span>
<span class="sd">    Open MPI major version 2 and above.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This method initializes CUDA context. Therefore, if multiple processes</span>
<span class="sd">        run on a single machine but use different GPUs, make sure to use</span>
<span class="sd">        :func:`torch.cuda.set_device` before this method to avoid unnecessarily</span>
<span class="sd">        creating context on the first visible device.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;world_size&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">group_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;group_name&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;rank&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;got unexpected keyword arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;PyTorch built without distributed support&quot;</span><span class="p">)</span>

    <span class="k">global</span> <span class="n">_initialized</span>
    <span class="k">if</span> <span class="n">_initialized</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trying to initialize torch.distributed.deprecated twice!&quot;</span><span class="p">)</span>

    <span class="c1"># Checking and assigning the distributed backend</span>
    <span class="k">global</span> <span class="n">_backend</span>

    <span class="n">backend</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;tcp&quot;</span><span class="p">:</span>
        <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">TCP</span>
    <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;mpi&quot;</span><span class="p">:</span>
        <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">MPI</span>
    <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;gloo&quot;</span><span class="p">:</span>
        <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">GLOO</span>
    <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;nccl&quot;</span><span class="p">:</span>
        <span class="n">_backend</span> <span class="o">=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">NCCL</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Invalid distributed backend name: &quot;</span> <span class="o">+</span> <span class="n">backend</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span>
                                      <span class="n">group_name</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    <span class="n">_initialized</span> <span class="o">=</span> <span class="n">_INITIALIZED_PG</span>

    <span class="k">if</span> <span class="n">_backend</span> <span class="o">==</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
        <span class="n">atexit</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">destroy_process_group</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_init_extension</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduce_op</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;distributed module initialization failed&quot;</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">init_master_worker</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ================================================================================</span>
<span class="s2">                                        WARNING</span>
<span class="s2">    ================================================================================</span>
<span class="s2">    Master-worker mode is still experimental. The API will change without</span>
<span class="s2">    notice and we do not guarantee full correctness and expected performance yet.</span>
<span class="s2">    We&#39;ll announce it once it&#39;s ready.</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;world_size&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">group_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;group_name&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;rank&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;got unexpected keyword arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_available</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;PyTorch built without distributed support&quot;</span><span class="p">)</span>

    <span class="k">global</span> <span class="n">_initialized</span>
    <span class="k">if</span> <span class="n">_initialized</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trying to initialize torch.distributed.deprecated twice!&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_init_master_worker</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span>
                                      <span class="n">group_name</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    <span class="n">_initialized</span> <span class="o">=</span> <span class="n">_INITIALIZED_MW</span>
    <span class="kn">import</span> <span class="nn">torch.distributed.deprecated.collectives</span> <span class="k">as</span> <span class="nn">collectives</span>
    <span class="kn">import</span> <span class="nn">torch.distributed.deprecated.remote_types</span> <span class="k">as</span> <span class="nn">remote_types</span>
    <span class="n">_extend_scope</span><span class="p">(</span><span class="n">collectives</span><span class="p">)</span>
    <span class="n">_extend_scope</span><span class="p">(</span><span class="n">remote_types</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_init_extension</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduce_op</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;distributed module initialization failed&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">reduce_op</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">SUM</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>
    <span class="n">PRODUCT</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>
    <span class="n">MAX</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>
    <span class="n">MIN</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">group</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="n">WORLD</span> <span class="o">=</span> <span class="nb">object</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_DistributedRequest</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">request</span> <span class="o">=</span> <span class="n">request</span>

    <span class="k">def</span> <span class="nf">is_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_request_is_completed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">request</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wait</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_request_wait</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">request</span><span class="p">)</span>


<div class="viewcode-block" id="get_rank"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.get_rank">[docs]</a><span class="k">def</span> <span class="nf">get_rank</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the rank of current process.</span>

<span class="sd">    Rank is a unique identifier assigned to each process within a distributed</span>
<span class="sd">    group. They are always consecutive integers ranging from ``0`` to</span>
<span class="sd">    ``world_size - 1`` (inclusive).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_get_rank</span><span class="p">()</span></div>


<div class="viewcode-block" id="get_world_size"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.get_world_size">[docs]</a><span class="k">def</span> <span class="nf">get_world_size</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the number of processes in the distributed group.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_get_num_processes</span><span class="p">()</span></div>


<div class="viewcode-block" id="isend"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.isend">[docs]</a><span class="k">def</span> <span class="nf">isend</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sends a tensor asynchronously.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Tensor to send.</span>
<span class="sd">        dst (int): Destination rank.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A distributed request object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">_DistributedRequest</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_isend</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">))</span></div>


<div class="viewcode-block" id="irecv"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.irecv">[docs]</a><span class="k">def</span> <span class="nf">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Receives a tensor asynchronously.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Tensor to fill with received data.</span>
<span class="sd">        src (int): Source rank.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A distributed request object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">_DistributedRequest</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_irecv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">))</span></div>


<div class="viewcode-block" id="send"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.send">[docs]</a><span class="k">def</span> <span class="nf">send</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sends a tensor synchronously.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Tensor to send.</span>
<span class="sd">        dst (int): Destination rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_send</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">)</span></div>


<div class="viewcode-block" id="recv"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.recv">[docs]</a><span class="k">def</span> <span class="nf">recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Receives a tensor synchronously.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Tensor to fill with received data.</span>
<span class="sd">        src (int, optional): Source rank. Will receive from any</span>
<span class="sd">            process if unspecified.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sender rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_recv_any_source</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_multigpu"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.broadcast_multigpu">[docs]</a><span class="k">def</span> <span class="nf">broadcast_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Broadcasts the tensor to the whole group with multiple GPU tensors</span>
<span class="sd">    per node.</span>

<span class="sd">    :attr:`tensor` must have the same number of elements in all the GPUs from</span>
<span class="sd">    all processes participating in the collective. each tensor in the list must</span>
<span class="sd">    be on a different GPU.</span>

<span class="sd">    .. note::</span>
<span class="sd">      Only NCCL backend is currently supported. :attr:`tensor_list` should only</span>
<span class="sd">      contain GPU tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor_list (List[Tensor]): Tensors that participate in the collective</span>
<span class="sd">            operation. if ``src`` is the rank, then the first element of</span>
<span class="sd">            ``tensor_list`` (``tensor_list[0]``) will be broadcasted to all</span>
<span class="sd">            other tensors (on different GPUs) in the src process and all tensors</span>
<span class="sd">            in ``tensor_list`` of other non-src processes. You also need to make</span>
<span class="sd">            sure that ``len(tensor_list)`` is the same for all the distributed</span>
<span class="sd">            processes calling this function.</span>

<span class="sd">        src (int): Source rank.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_broadcast_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.broadcast">[docs]</a><span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Broadcasts the tensor to the whole group.</span>

<span class="sd">    :attr:`tensor` must have the same number of elements in all processes</span>
<span class="sd">    participating in the collective.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Data to be sent if :attr:`src` is the rank of</span>
<span class="sd">            current process, and tensor to be used to save received data</span>
<span class="sd">            otherwise.</span>
<span class="sd">        src (int): Source rank.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="all_reduce_multigpu"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.all_reduce_multigpu">[docs]</a><span class="k">def</span> <span class="nf">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reduces the tensor data across all machines in such a way that all get</span>
<span class="sd">    the final result. This function reduces a number of tensors on every node,</span>
<span class="sd">    while each tensor resides on a different GPU.</span>
<span class="sd">    Therefore, the input tensor in the tensor list needs to be GPU tensors.</span>
<span class="sd">    Also, each tensor in the tensor list needs to reside on a different GPU.</span>

<span class="sd">    After the call, all tensors in :attr:`tensor_list` will be bitwise identical</span>
<span class="sd">    in all processes.</span>

<span class="sd">    .. note::</span>
<span class="sd">      Only NCCL backend is currently supported. :attr:`tensor_list` should only</span>
<span class="sd">      contain GPU tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor_list (List[Tensor]): List of input and output tensors of</span>
<span class="sd">            the collective. The function operates in-place and requires that</span>
<span class="sd">            each tensor to be a GPU tensor on different GPUs.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>

<span class="sd">        op (optional): One of the values from ``torch.distributed.deprecated.reduce_op``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="all_reduce"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.all_reduce">[docs]</a><span class="k">def</span> <span class="nf">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reduces the tensor data across all machines in such a way that all get</span>
<span class="sd">    the final result.</span>

<span class="sd">    After the call :attr:`tensor` will be bitwise identical in all processes.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        op (optional): One of the values from ``torch.distributed.deprecated.reduce_op``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="reduce_multigpu"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.reduce_multigpu">[docs]</a><span class="k">def</span> <span class="nf">reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reduces the tensor data on multiple GPUs across all machines. Each tensor</span>
<span class="sd">    in :attr`tensor_list` should reside on a separate GPU.</span>

<span class="sd">    Only the GPU of ``tensor_list[0]`` on the process with rank :attr:`dst` is</span>
<span class="sd">    going to receive the final result.</span>

<span class="sd">    .. note::</span>
<span class="sd">      Only NCCL backend is currently supported. :attr:`tensor_list` should only</span>
<span class="sd">      contain GPU tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor_list (List[Tensor]): Input and output GPU tensors of the</span>
<span class="sd">            collective. The function operates in-place.</span>
<span class="sd">            You also need to make sure that ``len(tensor_list)`` is the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>

<span class="sd">        dst (int): Destination rank</span>
<span class="sd">        op (optional): One of the values from ``torch.distributed.deprecated.reduce_op``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="reduce"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.reduce">[docs]</a><span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">reduce_op</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reduces the tensor data across all machines.</span>

<span class="sd">    Only the process with rank :attr:`dst` is going to receive the final result.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Input and output of the collective. The function</span>
<span class="sd">            operates in-place.</span>
<span class="sd">        dst (int): Destination rank</span>
<span class="sd">        op (optional): One of the values from ``torch.distributed.deprecated.reduce_op``</span>
<span class="sd">            enum.  Specifies an operation used for element-wise reductions.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="all_gather_multigpu"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.all_gather_multigpu">[docs]</a><span class="k">def</span> <span class="nf">all_gather_multigpu</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span>
                        <span class="n">input_tensor_list</span><span class="p">,</span>
                        <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gathers tensors from the whole group in a list.</span>
<span class="sd">    Each tensor in :attr:`input_tensor_list` should reside on a separate GPU.</span>

<span class="sd">    .. note::</span>
<span class="sd">      Only NCCL backend is currently supported. :attr:`output_tensor_lists` and</span>
<span class="sd">      :attr:`input_tensor_list` should only contain GPU tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        output_tensor_lists (List[List[Tensor]]): Output lists. It should</span>
<span class="sd">            contain correctly-sized tensors on each GPU to be used for output of</span>
<span class="sd">            the collective.</span>
<span class="sd">            e.g. ``output_tensor_lists[i]`` contains the all_gather</span>
<span class="sd">            result that resides on the GPU of ``input_tensor_list[i]``.</span>
<span class="sd">            Note that each element of ``output_tensor_lists[i]`` has the size of</span>
<span class="sd">            ``world_size * len(input_tensor_list)``, since the function all</span>
<span class="sd">            gathers the result from every single GPU in the group. To interpret</span>
<span class="sd">            each element of ``output_tensor_list[i]``, note that</span>
<span class="sd">            ``input_tensor_list[j]`` of rank k will be appear in</span>
<span class="sd">            ``output_tensor_list[i][rank * world_size + j]``</span>
<span class="sd">            Also note that ``len(output_tensor_lists)``, and the size of each</span>
<span class="sd">            element in ``output_tensor_lists`` (each element is a list,</span>
<span class="sd">            therefore ``len(output_tensor_lists[i])``) need to be the same</span>
<span class="sd">            for all the distributed processes calling this function.</span>

<span class="sd">        input_tensor_list (List[Tensor]): List of tensors (on different GPUs) to</span>
<span class="sd">            be broadcast from current process.</span>
<span class="sd">            Note that ``len(input_tensor_list)`` needs to be the same for</span>
<span class="sd">            all the distributed processes calling this function.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>

    <span class="n">flatten_tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">output_tensor_list</span> <span class="ow">in</span> <span class="n">output_tensor_lists</span><span class="p">:</span>
        <span class="n">flatten_tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_flatten_dense_tensors</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">))</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_all_gather_multigpu</span><span class="p">(</span><span class="n">flatten_tensor_list</span><span class="p">,</span>
                                             <span class="n">input_tensor_list</span><span class="p">,</span>
                                             <span class="n">group</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">output_tensor_list</span><span class="p">,</span> <span class="n">flatten_tensor</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span>
                                                  <span class="n">flatten_tensor_list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">output_tensor_list</span><span class="p">,</span>
                                 <span class="n">_unflatten_dense_tensors</span><span class="p">(</span><span class="n">flatten_tensor</span><span class="p">,</span>
                                                          <span class="n">output_tensor_list</span><span class="p">)):</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="all_gather"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.all_gather">[docs]</a><span class="k">def</span> <span class="nf">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gathers tensors from the whole group in a list.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor_list (list[Tensor]): Output list. It should contain</span>
<span class="sd">            correctly-sized tensors to be used for output of the collective.</span>
<span class="sd">        tensor (Tensor): Tensor to be broadcast from current process.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">if</span> <span class="n">_backend</span> <span class="o">!=</span> <span class="n">dist_backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">all_gather_multigpu</span><span class="p">([</span><span class="n">tensor_list</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">],</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.gather">[docs]</a><span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gathers a list of tensors in a single process.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Input tensor.</span>
<span class="sd">        dst (int): Destination rank. Required in all processes except the one that</span>
<span class="sd">            is receiveing the data.</span>
<span class="sd">        gather_list (list[Tensor]): List of appropriately-sized tensors to</span>
<span class="sd">            use for received data. Required only in the receiving process.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">dst</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;dst&#39;</span><span class="p">,</span> <span class="n">my_rank</span><span class="p">)</span>
    <span class="n">gather_list</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;gather_list&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">_group</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;got unexpected kwargs&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dst</span> <span class="o">==</span> <span class="n">my_rank</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gather_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;gather_list is a required argument in gather destination&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_gather_recv</span><span class="p">(</span><span class="n">gather_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">_group</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gather_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;non-empty gather_list can be given only to gather destination&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_gather_send</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">_group</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.scatter">[docs]</a><span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Scatters a list of tensors to all processes in a group.</span>

<span class="sd">    Each process will receive exactly one tensor and store its data in the</span>
<span class="sd">    :attr:`tensor` argument.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor (Tensor): Output tensor.</span>
<span class="sd">        src (int): Source rank. Required in all processes except the one that</span>
<span class="sd">            is sending the data.</span>
<span class="sd">        scatter_list (list[Tensor]): List of tensors to scatter. Required only</span>
<span class="sd">            in the process that is sending the data.</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="n">my_rank</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;src&#39;</span><span class="p">,</span> <span class="n">my_rank</span><span class="p">)</span>
    <span class="n">scatter_list</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;scatter_list&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">_group</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;got unexpected kwargs: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())))</span>
    <span class="k">if</span> <span class="n">src</span> <span class="o">==</span> <span class="n">my_rank</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scatter_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;scatter_list is a required argument in scatter source&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_scatter_send</span><span class="p">(</span><span class="n">scatter_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">_group</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scatter_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;non-empty can be given only to scatter source&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_scatter_recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">_group</span><span class="p">)</span></div>


<div class="viewcode-block" id="barrier"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.barrier">[docs]</a><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Synchronizes all processes.</span>

<span class="sd">    This collective blocks processes until the whole group enters this function.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_barrier</span><span class="p">(</span><span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="new_group"><a class="viewcode-back" href="../../../distributed_deprecated.html#torch.distributed.deprecated.new_group">[docs]</a><span class="k">def</span> <span class="nf">new_group</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a new distributed group.</span>

<span class="sd">    This function requires that all processes in the main group (i.e., all</span>
<span class="sd">    processes that are part of the distributed job) enter this function, even</span>
<span class="sd">    if they are not going to be members of the group. Additionally, groups</span>
<span class="sd">    should be created in the same order in all processes.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        ranks (list[int]): List of ranks of group members.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A handle of distributed group that can be given to collective calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">deprecated</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">==</span> <span class="n">_INITIALIZED_PG</span><span class="p">,</span> \
        <span class="s2">&quot;collective only supported in process-group mode&quot;</span>
    <span class="k">if</span> <span class="n">ranks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ranks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">get_world_size</span><span class="p">()))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_new_group</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_clear_group_cache</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Clear the created distributed group&#39;s cached resource.</span>

<span class="sd">    Only NCCL backend is currently supported.</span>

<span class="sd">    Cached resource includes NCCL communicators and CUDA events.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        group (optional): Group of the collective.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_clear_group_cache</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_register_stream</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_initialized</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;torch.distributed.deprecated needs to be initialized first&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_dist_register_stream</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../../../_static/jquery.js"></script>
         <script type="text/javascript" src="../../../_static/underscore.js"></script>
         <script type="text/javascript" src="../../../_static/doctools.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="../../../_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>
<img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
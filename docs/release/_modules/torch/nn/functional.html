


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.functional &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/functional.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.0.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed_deprecated.html">torch.distributed.deprecated</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torchvision/index.html">torchvision</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.functional</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.functional</h1><div class="highlight"><pre>
<span></span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Functional interface&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">types</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="k">import</span> <span class="n">_infer_size</span><span class="p">,</span> <span class="n">_add_docstr</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_reduction</span> <span class="k">as</span> <span class="n">_Reduction</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_functions</span>
<span class="kn">from</span> <span class="nn">.modules</span> <span class="k">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">._functions</span> <span class="k">import</span> <span class="n">vision</span>
<span class="kn">from</span> <span class="nn">._functions.thnn.fold</span> <span class="k">import</span> <span class="n">Col2Im</span><span class="p">,</span> <span class="n">Im2Col</span>
<span class="kn">from</span> <span class="nn">.modules.utils</span> <span class="k">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span><span class="p">,</span> <span class="n">_list_with_default</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">grad</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_VF</span>
<span class="kn">from</span> <span class="nn">.._jit_internal</span> <span class="k">import</span> <span class="n">weak_script</span><span class="p">,</span> <span class="n">List</span>


<span class="n">conv1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 1D convolution over an input signal composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">See :class:`~torch.nn.Conv1d` for details and output shape.</span>

<span class="s2">.. include:: cudnn_deterministic.rst</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{out\_channels} \times \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} \times kW)`</span>
<span class="s2">    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: ``None``</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or</span>
<span class="s2">      a one-element tuple `(sW,)`. Default: 1</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a one-element tuple `(padW,)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a one-element tuple `(dW,)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by</span>
<span class="s2">      the number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; filters = torch.randn(33, 16, 3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50)</span>
<span class="s2">    &gt;&gt;&gt; F.conv1d(inputs, filters)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 2D convolution over an input image composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">See :class:`~torch.nn.Conv2d` for details and output shape.</span>

<span class="s2">.. include:: cudnn_deterministic.rst</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iH \times iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{out\_channels} \times \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} \times kH \times kW)`</span>
<span class="s2">    bias: optional bias tensor of shape :math:`(\text{out\_channels})`. Default: ``None``</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple `(sH, sW)`. Default: 1</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padH, padW)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dH, dW)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">    &gt;&gt;&gt; filters = torch.randn(8,4,3,3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(1,4,5,5)</span>
<span class="s2">    &gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E501</span>

<span class="n">conv3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 3D convolution over an input image composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">See :class:`~torch.nn.Conv3d` for details and output shape.</span>

<span class="s2">.. include:: cudnn_deterministic.rst</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iT \times iH \times iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{out\_channels} \times \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} \times kT \times kH \times kW)`</span>
<span class="s2">    bias: optional bias tensor of shape :math:`(\text{out\_channels})`. Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple `(sT, sH, sW)`. Default: 1</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padT, padH, padW)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dT, dH, dW)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by</span>
<span class="s2">      the number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; filters = torch.randn(33, 16, 3, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)</span>
<span class="s2">    &gt;&gt;&gt; F.conv3d(inputs, filters)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E501</span>

<span class="n">conv_transpose1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 1D transposed convolution operator over an input signal</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose1d` for details and output shape.</span>

<span class="s2">.. include:: cudnn_deterministic.rst</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{in\_channels} \times \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} \times kW)`</span>
<span class="s2">    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sW,)``. Default: 1</span>
<span class="s2">    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padW,)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple ``(dW,)``. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(16, 33, 5)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose1d(inputs, weights)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv_transpose2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 2D transposed convolution operator over an input image</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose2d` for details and output shape.</span>

<span class="s2">.. include:: cudnn_deterministic.rst</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iH \times iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{in\_channels} \times \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} \times kH \times kW)`</span>
<span class="s2">    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sH, sW)``. Default: 1</span>
<span class="s2">    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padH, padW)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.</span>
<span class="s2">      Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple ``(dH, dW)``. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(4, 8, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose2d(inputs, weights, padding=1)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E501</span>

<span class="n">conv_transpose3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 3D transposed convolution operator over an input image</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose3d` for details and output shape.</span>

<span class="s2">.. include:: cudnn_deterministic.rst</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iT \times iH \times iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{in\_channels} \times \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} \times kT \times kH \times kW)`</span>
<span class="s2">    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sT, sH, sW)``. Default: 1</span>
<span class="s2">    padding: ``kernel_size - 1 - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padT, padH, padW)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple</span>
<span class="s2">      ``(out_padT, out_padH, out_padW)``. Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dT, dH, dW)`. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(16, 33, 3, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose3d(inputs, weights)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E501</span>

<span class="n">conv_tbc</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_tbc</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Applies a 1-dimensional sequence convolution over an input sequence.</span>
<span class="s2">Input and output dimensions are (Time, Batch, Channels) - hence TBC.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text{sequence length} \times batch \times \text{in\_channels})`</span>
<span class="s2">    weight: filter of shape (:math:`\text{kernel width} \times \text{in\_channels} \times \text{out\_channels}`)</span>
<span class="s2">    bias: bias of shape (:math:`\text{out\_channels}`)</span>
<span class="s2">    pad: number of timesteps to pad. Default: 0</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="c1"># Pooling</span>
<span class="n">avg_pool1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Tensor</span>

<span class="s2">Applies a 1D average pooling over an input signal composed of several</span>
<span class="s2">input planes.</span>

<span class="s2">See :class:`~torch.nn.AvgPool1d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iW)`</span>
<span class="s2">    kernel_size: the size of the window. Can be a single number or a</span>
<span class="s2">      tuple :math:`(kW,)`</span>
<span class="s2">    stride: the stride of the window. Can be a single number or a tuple</span>
<span class="s2">      `(sW,)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padW,)`. Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` to compute the</span>
<span class="s2">        output shape. Default: ``False``</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation. Default: ``True``</span>

<span class="s2">Examples::</span>
<span class="s2">    &gt;&gt;&gt; # pool of square window of size=3, stride=2</span>
<span class="s2">    &gt;&gt;&gt; input = torch.tensor([[[1,2,3,4,5,6,7]]])</span>
<span class="s2">    &gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)</span>
<span class="s2">    tensor([[[ 2.,  4.,  6.]]])</span>

<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="n">avg_pool2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Tensor</span>

<span class="s2">Applies 2D average-pooling operation in :math:`kH \times kW` regions by step size</span>
<span class="s2">:math:`sH \times sW` steps. The number of output features is equal to the number of</span>
<span class="s2">input planes.</span>

<span class="s2">See :class:`~torch.nn.AvgPool2d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iH \times iW)`</span>
<span class="s2">    kernel_size: size of the pooling region. Can be a single number or a</span>
<span class="s2">      tuple :math:`(kH \times kW)`</span>
<span class="s2">    stride: stride of the pooling operation. Can be a single number or a</span>
<span class="s2">      tuple `(sH, sW)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padH, padW)`. Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="s2">        to compute the output shape. Default: ``False``</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation. Default: ``True``</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">avg_pool3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Tensor</span>

<span class="s2">Applies 3D average-pooling operation in :math:`kT \times kH \times kW` regions by step</span>
<span class="s2">size :math:`sT \times sH \times sW` steps. The number of output features is equal to</span>
<span class="s2">:math:`\lfloor\frac{\text{input planes}}</span><span class="si">{sT}</span><span class="s2">\rfloor`.</span>

<span class="s2">See :class:`~torch.nn.AvgPool3d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> \times \text{in\_channels} \times iT \times iH \times iW)`</span>
<span class="s2">    kernel_size: size of the pooling region. Can be a single number or a</span>
<span class="s2">      tuple :math:`(kT \times kH \times kW)`</span>
<span class="s2">    stride: stride of the pooling operation. Can be a single number or a</span>
<span class="s2">      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padT, padH, padW)`, Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="s2">        to compute the output shape</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">fractional_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                       <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                       <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], Optional[BroadcastingList2[float]], bool, Optional[Tensor]) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies 2D fractional max pooling over an input signal composed of several input planes.</span>

<span class="sd">    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham</span>

<span class="sd">    The max-pooling operation is applied in :math:`kH \times kW` regions by a stochastic</span>
<span class="sd">    step size determined by the target output size.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size: the size of the window to take a max over.</span>
<span class="sd">                     Can be a single number :math:`k` (for a square kernel of :math:`k \times k`)</span>
<span class="sd">                     or a tuple :math:`(kH \times kW)`</span>
<span class="sd">        output_size: the target output size of the image of the form :math:`oH \times oW`.</span>
<span class="sd">                     Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \times oH`</span>
<span class="sd">        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.</span>
<span class="sd">                      This has to be a number or tuple in the range (0, 1)</span>
<span class="sd">        return_indices: if ``True``, will return the indices along with the outputs.</span>
<span class="sd">                        Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.</span>

<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12</span>
<span class="sd">        &gt;&gt;&gt; F.fractional_max_pool2d(input, 3, output_size=(13, 12))</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window and target output size being half of input image size</span>
<span class="sd">        &gt;&gt;&gt; F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))</span>

<span class="sd">    .. _Fractional MaxPooling:</span>
<span class="sd">        http://arxiv.org/abs/1412.6071</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;fractional_max_pool2d requires specifying either &quot;</span>
                         <span class="s2">&quot;an output_size or an output_ratio&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_output_ratio</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">))</span>
        <span class="n">_output_size</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">_output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                        <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">_output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_output_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_random_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">fractional_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">,</span> <span class="n">_random_samples</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_fractional_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], Optional[BroadcastingList2[float]], bool, Optional[Tensor]) -&gt; Tensor  # noqa</span>
    <span class="k">return</span> <span class="n">fractional_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                                              <span class="n">output_ratio</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">,</span>
                                              <span class="n">_random_samples</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">fractional_max_pool2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">fractional_max_pool2d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_fractional_max_pool2d</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], Optional[BroadcastingList1[int]], BroadcastingList1[int], BroadcastingList1[int], bool, bool) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">max_pool1d_with_indices</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], Optional[BroadcastingList1[int]], BroadcastingList1[int], BroadcastingList1[int], bool, bool) -&gt; Tensor  # noqa</span>
    <span class="k">return</span> <span class="n">max_pool1d_with_indices</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">max_pool1d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">max_pool1d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_max_pool1d</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], BroadcastingList2[int], BroadcastingList2[int], bool, bool) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], BroadcastingList2[int], BroadcastingList2[int], bool, bool) -&gt; Tensor  # noqa</span>
    <span class="k">return</span> <span class="n">max_pool2d_with_indices</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">max_pool2d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_max_pool2d</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int], Optional[BroadcastingList3[int]], BroadcastingList3[int], BroadcastingList3[int], bool, bool) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool3d_with_indices</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int], Optional[BroadcastingList3[int]], BroadcastingList3[int], BroadcastingList3[int], bool, bool) -&gt; Tensor  # noqa</span>
    <span class="k">return</span> <span class="n">max_pool3d_with_indices</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">max_pool3d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">max_pool3d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_max_pool3d</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int], List[int], List[int], Optional[List[int]]) -&gt; List[int]</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">default_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">default_size</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_size</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span>
                            <span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">default_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;output_size should be a sequence containing &quot;</span>
                             <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements, but it has a length of &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                                     <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
            <span class="n">min_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
            <span class="n">max_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">min_size</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_size</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;invalid output_size &quot;</span><span class="si">{}</span><span class="s1">&quot; (dim </span><span class="si">{}</span><span class="s1"> must be between </span><span class="si">{}</span><span class="s1"> and </span><span class="si">{}</span><span class="s1">)&#39;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">min_size</span><span class="p">,</span> <span class="n">max_size</span><span class="p">))</span>

        <span class="n">ret</span> <span class="o">=</span> <span class="n">output_size</span>
    <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="max_unpool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool1d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">max_unpool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, BroadcastingList1[int], Optional[BroadcastingList1[int]], BroadcastingList1[int], Optional[BroadcastingList1[int]]) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool1d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">output_size</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool2d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], BroadcastingList2[int], Optional[BroadcastingList2[int]]) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool2d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.max_unpool3d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">max_unpool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, BroadcastingList3[int], Optional[BroadcastingList3[int]], BroadcastingList3[int], Optional[BroadcastingList3[int]]) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool3d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool3d</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.lp_pool2d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">lp_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, int, Optional[BroadcastingList2[int]], bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D power-average pooling over an input signal composed of</span>
<span class="sd">    several input planes. If the sum of all inputs to the power of `p` is</span>
<span class="sd">    zero, the gradient is set to zero as well.</span>

<span class="sd">    See :class:`~torch.nn.LPPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kw</span><span class="p">,</span> <span class="n">kh</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kw</span> <span class="o">*</span> <span class="n">kh</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool1d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.lp_pool1d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">lp_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, int, Optional[BroadcastingList1[int]], bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D power-average pooling over an input signal composed of</span>
<span class="sd">    several input planes. If the sum of all inputs to the power of `p` is</span>
<span class="sd">    zero, the gradient is set to zero as well.</span>

<span class="sd">    See :class:`~torch.nn.LPPool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">adaptive_max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], bool) -&gt; Tuple[Tensor, Tensor]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], bool) -&gt; Tensor</span>
    <span class="k">return</span> <span class="n">adaptive_max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">adaptive_max_pool1d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">adaptive_max_pool1d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_adaptive_max_pool1d</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">adaptive_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], bool) -&gt; Tuple[Tensor, Tensor]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], bool) -&gt; Tensor</span>
    <span class="k">return</span> <span class="n">adaptive_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">adaptive_max_pool2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">adaptive_max_pool2d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_adaptive_max_pool2d</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">adaptive_max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], bool) -&gt; Tuple[Tensor, Tensor]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            triple-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_adaptive_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], bool) -&gt; Tensor</span>
    <span class="k">return</span> <span class="n">adaptive_max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">adaptive_max_pool3d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">adaptive_max_pool3d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_adaptive_max_pool3d</span><span class="p">)</span>


<span class="n">adaptive_avg_pool1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">adaptive_avg_pool1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">adaptive_avg_pool1d(input, output_size) -&gt; Tensor</span>

<span class="s2">Applies a 1D adaptive average pooling over an input signal composed of</span>
<span class="s2">several input planes.</span>

<span class="s2">See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    output_size: the target output size (single integer)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool2d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.adaptive_avg_pool3d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            triple-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">)</span></div>


<span class="c1"># Activation functions</span>
<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input</span>
<span class="sd">    tensor with probability :attr:`p` using samples from a Bernoulli</span>
<span class="sd">    distribution.</span>

<span class="sd">    See :class:`~torch.nn.Dropout` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p: probability of an element to be zeroed. Default: 0.5</span>
<span class="sd">        training: apply dropout if is ``True``. Defualt: ``True``</span>
<span class="sd">        inplace: If set to ``True``, will do this operation in-place. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<div class="viewcode-block" id="alpha_dropout"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.alpha_dropout">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies alpha dropout to the input.</span>

<span class="sd">    See :class:`~torch.nn.AlphaDropout` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">alpha_dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<div class="viewcode-block" id="dropout2d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout2d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">dropout2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly zero out entire channels (a channel is a 2D feature map,</span>
<span class="sd">    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the</span>
<span class="sd">    batched input is a 2D tensor :math:`\text{input}[i, j]`) of the input tensor).</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call.</span>
<span class="sd">    with probability :attr:`p` using samples from a Bernoulli distribution.</span>

<span class="sd">    See :class:`~torch.nn.Dropout2d` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p: probability of a channel to be zeroed. Default: 0.5</span>
<span class="sd">        training: apply dropout if is ``True``. Defualt: ``True``</span>
<span class="sd">        inplace: If set to ``True``, will do this operation in-place. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">feature_dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">feature_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<div class="viewcode-block" id="dropout3d"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.dropout3d">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">dropout3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly zero out entire channels (a channel is a 3D feature map,</span>
<span class="sd">    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the</span>
<span class="sd">    batched input is a 3D tensor :math:`\text{input}[i, j]`) of the input tensor).</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call.</span>
<span class="sd">    with probability :attr:`p` using samples from a Bernoulli distribution.</span>

<span class="sd">    See :class:`~torch.nn.Dropout3d` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p: probability of a channel to be zeroed. Default: 0.5</span>
<span class="sd">        training: apply dropout if is ``True``. Defualt: ``True``</span>
<span class="sd">        inplace: If set to ``True``, will do this operation in-place. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This is 100% the same code as dropout2d. We duplicate this code so that</span>
    <span class="c1"># stack traces are not confusing.</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">feature_dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">feature_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">feature_alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">feature_alpha_dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">feature_alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span>


<div class="viewcode-block" id="threshold"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.threshold">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Thresholds each element of the input Tensor.</span>

<span class="sd">    See :class:`~torch.nn.Threshold` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_VF</span><span class="o">.</span><span class="n">threshold_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_VF</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">threshold_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">threshold_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">threshold_(input, threshold, value) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~threshold`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;relu(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the rectified linear unit function element-wise. See</span>
<span class="sd">    :class:`~torch.nn.ReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">relu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">relu_(input) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~relu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="glu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.glu">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">glu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, int) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    glu(input, dim=-1) -&gt; Tensor</span>

<span class="sd">    The gated linear unit. Computes:</span>

<span class="sd">    .. math ::</span>

<span class="sd">        H = A \times \sigma(B)</span>

<span class="sd">    where `input` is split in half along `dim` to form `A` and `B`.</span>

<span class="sd">    See `Language Modeling with Gated Convolutional Networks &lt;https://arxiv.org/abs/1612.08083&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input tensor</span>
<span class="sd">        dim (int): dimension on which to split the input</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;glu does not suppport scalars because halving size must be even&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardtanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hardtanh">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    hardtanh(input, min_val=-1., max_val=1., inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more</span>
<span class="sd">    details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">hardtanh_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">hardtanh_(input, min_val=-1., max_val=1.) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~hardtanh`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.relu6">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;relu6(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the element-wise function :math:`\text{ReLU6}(x) = \min(\max(0,x), 6)`.</span>

<span class="sd">    See :class:`~torch.nn.ReLU6` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.elu">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies element-wise,</span>
<span class="sd">    :math:`\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`.</span>

<span class="sd">    See :class:`~torch.nn.ELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">elu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">elu_(input, alpha=1.) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~elu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.selu">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;selu(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))`,</span>
<span class="sd">    with :math:`\alpha=1.6732632423543772848170429916717` and</span>
<span class="sd">    :math:`scale=1.0507009873554804934193349852946`.</span>

<span class="sd">    See :class:`~torch.nn.SELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">selu_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">selu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">selu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">selu_(input) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~selu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="celu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.celu">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;celu(input, alpha=1., inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))`.</span>

<span class="sd">    See :class:`~torch.nn.CELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">celu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">celu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>

<span class="n">celu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">celu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">celu_(input, alpha=1.) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~celu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.leaky_relu">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    leaky_relu(input, negative_slope=0.01, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)`</span>

<span class="sd">    See :class:`~torch.nn.LeakyReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">leaky_relu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">leaky_relu_(input, negative_slope=0.01) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~leaky_relu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.prelu">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;prelu(input, weight) -&gt; Tensor</span>

<span class="sd">    Applies element-wise the function</span>
<span class="sd">    :math:`\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)` where weight is a</span>
<span class="sd">    learnable parameter.</span>

<span class="sd">    See :class:`~torch.nn.PReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="rrelu"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.rrelu">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">8</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -&gt; Tensor</span>

<span class="sd">    Randomized leaky ReLU.</span>

<span class="sd">    See :class:`~torch.nn.RReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rrelu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">rrelu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rrelu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">rrelu_(input, lower=1./8, upper=1./3, training=False) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~rrelu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">logsigmoid</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">log_sigmoid</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">logsigmoid(input) -&gt; Tensor</span>

<span class="s2">Applies element-wise :math:`\text</span><span class="si">{LogSigmoid}</span><span class="s2">(x) = \log \left(\frac</span><span class="si">{1}</span><span class="s2">{1 + \exp(-x_i)}\right)`</span>

<span class="s2">See :class:`~torch.nn.LogSigmoid` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="hardshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hardshrink">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    hardshrink(input, lambd=0.5) -&gt; Tensor</span>

<span class="sd">    Applies the hard shrinkage function element-wise</span>

<span class="sd">    See :class:`~torch.nn.Hardshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanhshrink"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanhshrink">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">tanhshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;tanhshrink(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise, :math:`\text{Tanhshrink}(x) = x - \text{Tanh}(x)`</span>

<span class="sd">    See :class:`~torch.nn.Tanhshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softsign">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;softsign(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise, the function :math:`\text{SoftSign}(x) = \frac{x}{1 + |x|}`</span>

<span class="sd">    See :class:`~torch.nn.Softsign` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span></div>


<span class="n">softplus</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">softplus(input, beta=1, threshold=20) -&gt; Tensor</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_get_softmax_dim</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">stacklevel</span><span class="p">):</span>
    <span class="c1"># type: (str, int, int) -&gt; int</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Implicit dimension choice for </span><span class="si">{}</span><span class="s2"> has been deprecated. &quot;</span>
                  <span class="s2">&quot;Change the call to include dim=X as an argument.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">stacklevel</span><span class="o">=</span><span class="n">stacklevel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="softmin"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmin">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">softmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[int], int, Optional[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmin function.</span>

<span class="sd">    Note that :math:`\text{Softmin}(x) = \text{Softmax}(-x)`. See softmax definition for mathematical formula.</span>

<span class="sd">    See :class:`~torch.nn.Softmin` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which softmin will be computed (so every slice</span>
<span class="sd">            along dim will sum to 1).</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>
<span class="sd">        If specified, the input tensor is casted to :attr:`dtype` before the operation</span>
<span class="sd">        is performed. This is useful for preventing data type overflows. Default: None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;softmin&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.softmax">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[int], int, Optional[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmax function.</span>

<span class="sd">    Softmax is defined as:</span>

<span class="sd">    :math:`\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}`</span>

<span class="sd">    It is applied to all slices along dim, and will re-scale them so that the elements</span>
<span class="sd">    lie in the range `(0, 1)` and sum to 1.</span>

<span class="sd">    See :class:`~torch.nn.Softmax` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which softmax will be computed.</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>
<span class="sd">        If specified, the input tensor is casted to :attr:`dtype` before the operation</span>
<span class="sd">        is performed. This is useful for preventing data type overflows. Default: None.</span>


<span class="sd">    .. note::</span>
<span class="sd">        This function doesn&#39;t work directly with NLLLoss,</span>
<span class="sd">        which expects the Log to be computed between the Softmax and itself.</span>
<span class="sd">        Use log_softmax instead (it&#39;s faster and has better numerical properties).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_sample_gumbel</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (List[int], float, Optional[Tensor]) -&gt; Tensor</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from Gumbel(0, 1)</span>

<span class="sd">    based on</span>
<span class="sd">    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,</span>
<span class="sd">    (MIT license)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">()</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">eps</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">U</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_gumbel_softmax_sample</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, float) -&gt; Tensor</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Draw a sample from the Gumbel-Softmax distribution</span>

<span class="sd">    based on</span>
<span class="sd">    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb</span>
<span class="sd">    (MIT license)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="n">gumbel_noise</span> <span class="o">=</span> <span class="n">_sample_gumbel</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">gumbel_noise</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">y</span> <span class="o">/</span> <span class="n">tau</span><span class="p">,</span> <span class="n">dims</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>


<div class="viewcode-block" id="gumbel_softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.gumbel_softmax">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sample from the Gumbel-Softmax distribution and optionally discretize.</span>

<span class="sd">    Args:</span>
<span class="sd">      logits: `[batch_size, num_features]` unnormalized log probabilities</span>
<span class="sd">      tau: non-negative scalar temperature</span>
<span class="sd">      hard: if ``True``, the returned samples will be discretized as one-hot vectors,</span>
<span class="sd">            but will be differentiated as if it is the soft sample in autograd</span>

<span class="sd">    Returns:</span>
<span class="sd">      Sampled tensor of shape ``batch_size x num_features`` from the Gumbel-Softmax distribution.</span>
<span class="sd">      If ``hard=True``, the returned samples will be one-hot, otherwise they will</span>
<span class="sd">      be probability distributions that sum to 1 across features</span>

<span class="sd">    Constraints:</span>

<span class="sd">    - Currently only work on 2D input :attr:`logits` tensor of shape ``batch_size x num_features``</span>

<span class="sd">    Based on</span>
<span class="sd">    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,</span>
<span class="sd">    (MIT license)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="n">y_soft</span> <span class="o">=</span> <span class="n">_gumbel_softmax_sample</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hard</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">y_soft</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># this bit is based on</span>
        <span class="c1"># https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5</span>
        <span class="n">y_hard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="c1"># this cool bit of code achieves two things:</span>
        <span class="c1"># - makes the output value exactly one-hot (since we add then</span>
        <span class="c1">#   subtract y_soft value)</span>
        <span class="c1"># - makes the gradient equal to y_soft gradient (since we strip</span>
        <span class="c1">#   all other gradients)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y_hard</span> <span class="o">-</span> <span class="n">y_soft</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">y_soft</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y_soft</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.log_softmax">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[int], int, Optional[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmax followed by a logarithm.</span>

<span class="sd">    While mathematically equivalent to log(softmax(x)), doing these two</span>
<span class="sd">    operations separately is slower, and numerically unstable. This function</span>
<span class="sd">    uses an alternative formulation to compute the output and gradient correctly.</span>

<span class="sd">    See :class:`~torch.nn.LogSoftmax` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which log_softmax will be computed.</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>
<span class="sd">        If specified, the input tensor is casted to :attr:`dtype` before the operation</span>
<span class="sd">        is performed. This is useful for preventing data type overflows. Default: None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;log_softmax&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="n">softshrink</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softshrink</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">softshrink(input, lambd=0.5) -&gt; Tensor</span>

<span class="s2">Applies the soft shrinkage function elementwise</span>

<span class="s2">See :class:`~torch.nn.Softshrink` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.tanh">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;tanh(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}`</span>

<span class="sd">    See :class:`~torch.nn.Tanh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.tanh is deprecated. Use torch.tanh instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span></div>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.sigmoid">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;sigmoid(input) -&gt; Tensor</span>

<span class="sd">    Applies the element-wise function :math:`\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}`</span>

<span class="sd">    See :class:`~torch.nn.Sigmoid` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span></div>


<div class="viewcode-block" id="linear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.linear">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.</span>

<span class="sd">    Shape:</span>

<span class="sd">        - Input: :math:`(N, *, in\_features)` where `*` means any number of</span>
<span class="sd">          additional dimensions</span>
<span class="sd">        - Weight: :math:`(out\_features, in\_features)`</span>
<span class="sd">        - Bias: :math:`(out\_features)`</span>
<span class="sd">        - Output: :math:`(N, *, out\_features)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># fused op is marginally faster</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">bias</span><span class="p">),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">output</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="bilinear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.bilinear">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, Optional[Tensor]) -&gt; Tensor</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_no_grad_embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, float, float) -&gt; Tensor</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>


<div class="viewcode-block" id="embedding"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.embedding">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span>
              <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[int], Optional[float], float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A simple lookup table that looks up embeddings in a fixed dictionary and size.</span>

<span class="sd">    This module is often used to retrieve word embeddings using indices.</span>
<span class="sd">    The input to the module is a list of indices, and the embedding matrix,</span>
<span class="sd">    and the output is the corresponding word embeddings.</span>

<span class="sd">    See :class:`torch.nn.Embedding` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (LongTensor): Tensor containing indices into the embedding matrix</span>
<span class="sd">        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,</span>
<span class="sd">            and number of columns equal to the embedding size</span>
<span class="sd">        padding_idx (int, optional): If given, pads the output with the embedding vector at :attr:`padding_idx`</span>
<span class="sd">                                         (initialized to zeros) whenever it encounters the index.</span>
<span class="sd">        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`</span>
<span class="sd">                                    is renormalized to have norm :attr:`max_norm`.</span>
<span class="sd">                                    Note: this will modify :attr:`weight` in-place.</span>
<span class="sd">        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.</span>
<span class="sd">        scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of</span>
<span class="sd">                                                the words in the mini-batch. Default ``False``.</span>
<span class="sd">        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under</span>
<span class="sd">                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: LongTensor of arbitrary shape containing the indices to extract</span>
<span class="sd">        - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,</span>
<span class="sd">                            where V = maximum index + 1 and embedding_dim = the embedding size</span>
<span class="sd">        - Output: `(*, embedding_dim)`, where `*` is the input shape</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([[1,2,4,5],[4,3,2,9]])</span>
<span class="sd">        &gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; F.embedding(input, embedding_matrix)</span>
<span class="sd">        tensor([[[ 0.8490,  0.9625,  0.6753],</span>
<span class="sd">                 [ 0.9666,  0.7761,  0.6108],</span>
<span class="sd">                 [ 0.6246,  0.9751,  0.3618],</span>
<span class="sd">                 [ 0.4161,  0.2419,  0.7383]],</span>

<span class="sd">                [[ 0.6246,  0.9751,  0.3618],</span>
<span class="sd">                 [ 0.0237,  0.7794,  0.0528],</span>
<span class="sd">                 [ 0.9666,  0.7761,  0.6108],</span>
<span class="sd">                 [ 0.3385,  0.8612,  0.1867]]])</span>

<span class="sd">        &gt;&gt;&gt; # example with padding_idx</span>
<span class="sd">        &gt;&gt;&gt; weights = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; weights[0, :].zero_()</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = weights</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([[0,2,0,5]])</span>
<span class="sd">        &gt;&gt;&gt; F.embedding(input, embedding_matrix, padding_idx=0)</span>
<span class="sd">        tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="sd">                 [ 0.5609,  0.5384,  0.8720],</span>
<span class="sd">                 [ 0.0000,  0.0000,  0.0000],</span>
<span class="sd">                 [ 0.6262,  0.2438,  0.7471]]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">padding_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">padding_idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">padding_idx</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">padding_idx</span> <span class="o">&lt;</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;Padding_idx must be within num_embeddings&#39;</span>
        <span class="k">elif</span> <span class="n">padding_idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">padding_idx</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;Padding_idx must be within num_embeddings&#39;</span>
            <span class="n">padding_idx</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">padding_idx</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">padding_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">if</span> <span class="n">max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">max_norm</span><span class="p">)</span>
        <span class="c1"># `embedding_renorm_` will call .contiguous() on input anyways, so we</span>
        <span class="c1"># call it here and take advantage of the improved locality in the</span>
        <span class="c1"># `embedding` call below too.</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># XXX: equivalent to</span>
        <span class="c1"># with torch.no_grad():</span>
        <span class="c1">#   torch.nembedding_renorm_</span>
        <span class="c1"># remove once script supports set_grad_enabled</span>
        <span class="n">_no_grad_embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="n">sparse</span><span class="p">)</span></div>


<div class="viewcode-block" id="embedding_bag"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.embedding_bag">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">embedding_bag</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offsets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                  <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[float], float, bool, str, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes sums, means or maxes of &#39;bags&#39; of embeddings, without instantiating the</span>
<span class="sd">    intermediate embeddings.</span>

<span class="sd">    See :class:`torch.nn.EmbeddingBag` for more details.</span>
<span class="sd">    .. include:: cuda_deterministic_backward.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        input (LongTensor): Tensor containing bags of indices into the embedding matrix</span>
<span class="sd">        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,</span>
<span class="sd">            and number of columns equal to the embedding size</span>
<span class="sd">        offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines</span>
<span class="sd">                             the starting index position of each bag (sequence) in :attr:`input`.</span>
<span class="sd">        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`</span>
<span class="sd">                                    is renormalized to have norm :attr:`max_norm`.</span>
<span class="sd">                                    Note: this will modify :attr:`weight` in-place.</span>
<span class="sd">        norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.</span>
<span class="sd">                                     Default ``2``.</span>
<span class="sd">        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the inverse of frequency of</span>
<span class="sd">                                                the words in the mini-batch. Default ``False``.</span>
<span class="sd">                                                Note: this option is not supported when ``mode=&quot;max&quot;``.</span>
<span class="sd">        mode (string, optional): ``&quot;sum&quot;``, ``&quot;mean&quot;`` or ``&quot;max&quot;``. Specifies the way to reduce the bag.</span>
<span class="sd">                                 Default: ``&quot;mean&quot;``</span>
<span class="sd">        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under</span>
<span class="sd">                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.</span>
<span class="sd">                                 Note: this option is not supported when ``mode=&quot;max&quot;``.</span>

<span class="sd">    Shape:</span>

<span class="sd">        - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)</span>

<span class="sd">          - If :attr:`input` is 2D of shape ``B x N``,</span>

<span class="sd">            it will be treated as ``B`` bags (sequences) each of fixed length ``N``, and</span>
<span class="sd">            this will return ``B`` values aggregated in a way depending on the :attr:`mode`.</span>
<span class="sd">            :attr:`offsets` is ignored and required to be ``None`` in this case.</span>

<span class="sd">          - If :attr:`input` is 1D of shape ``N``,</span>

<span class="sd">            it will be treated as a concatenation of multiple bags (sequences).</span>
<span class="sd">            :attr:`offsets` is required to be a 1D tensor containing the</span>
<span class="sd">            starting index positions of each bag in :attr:`input`. Therefore,</span>
<span class="sd">            for :attr:`offsets` of shape ``B``, :attr:`input` will be viewed as</span>
<span class="sd">            having ``B`` bags. Empty bags (i.e., having 0-length) will have</span>
<span class="sd">            returned vectors filled by zeros.</span>

<span class="sd">        - :attr:`weight` (Tensor): the learnable weights of the module of</span>
<span class="sd">          shape ``(num_embeddings x embedding_dim)``</span>

<span class="sd">        - :attr:`output`: aggregated embedding values of shape ``B x embedding_dim``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([1,2,4,5,4,3,2,9])</span>
<span class="sd">        &gt;&gt;&gt; offsets = torch.tensor([0,4])</span>
<span class="sd">        &gt;&gt;&gt; F.embedding_bag(embedding_matrix, input, offsets)</span>
<span class="sd">        tensor([[ 0.3397,  0.3552,  0.5545],</span>
<span class="sd">                [ 0.5893,  0.4386,  0.5882]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check for backward compatibility.</span>
    <span class="c1"># Used to be embedding_bag(weight, input, ...)</span>
    <span class="c1"># Now is     embedding_bag(input, weight, ...)</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Argument order of nn.functional.embedding_bag was changed. &quot;</span>
                      <span class="s2">&quot;Usage `embedding_bag(weight, input, ...)` is deprecated, &quot;</span>
                      <span class="s2">&quot;and should now be `embedding_bag(input, weight, ...)`.&quot;</span><span class="p">)</span>
        <span class="n">weight</span><span class="p">,</span> <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">offsets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;if input is 2D, then offsets has to be None&quot;</span>
                             <span class="s2">&quot;, as input is treated is a mini-batch of&quot;</span>
                             <span class="s2">&quot; fixed length sequences. However, found &quot;</span>
                             <span class="s2">&quot;offsets of type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">offsets</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                                   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">offsets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets has to be a 1D Tensor but got None&quot;</span><span class="p">)</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">offsets</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">offsets</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets has to be a 1D Tensor&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">offsets</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets[0] has to be 0, i.e., the first sequence &quot;</span>
                             <span class="s2">&quot;in the mini-batch has to start from position 0. &quot;</span>
                             <span class="s2">&quot;However, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">offsets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
        <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">offsets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets[-1] can not be greater than input&#39;s length&quot;</span>
                             <span class="s2">&quot; (</span><span class="si">{}</span><span class="s2">), but got offsets[-1] of </span><span class="si">{}</span><span class="s2">&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">offsets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input has to be 1D or 2D Tensor,&quot;</span>
                         <span class="s2">&quot; but got Tensor of dimension </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">offsets</span><span class="p">)</span>  <span class="c1"># TODO remove when exception control flow logic</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="n">scale_grad_by_freq</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max mode does not support scaling the gradient by the frequency&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max mode does not support sparse weights&quot;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># TODO when exception control flow logic</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode has to be one of sum, mean or max&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">max_norm</span><span class="p">)</span>
        <span class="c1"># XXX: equivalent to</span>
        <span class="c1"># with torch.no_grad():</span>
        <span class="c1">#   torch.nembedding_renorm_</span>
        <span class="c1"># remove once script supports set_grad_enabled</span>
        <span class="n">_no_grad_embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>

    <span class="n">ret</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="p">,</span>
        <span class="n">mode_enum</span><span class="p">,</span>
        <span class="n">sparse</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="batch_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.batch_norm">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, Optional[Tensor], Optional[Tensor], bool, float, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Batch Normalization for each channel across a batch of data.</span>

<span class="sd">    See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,</span>
<span class="sd">    :class:`~torch.nn.BatchNorm3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="c1"># XXX: JIT script does not support the reduce from functools, and mul op is a</span>
        <span class="c1"># builtin, which cannot be used as a value to a func yet, so rewrite this size</span>
        <span class="c1"># check to a simple equivalent for loop</span>
        <span class="c1">#</span>
        <span class="c1"># TODO: make use of reduce like below when JIT is ready with the missing features:</span>
        <span class="c1"># from operator import mul</span>
        <span class="c1"># from functools import reduce</span>
        <span class="c1">#</span>
        <span class="c1">#   if reduce(mul, size[2:], size[0]) == 1</span>
        <span class="n">size_prods</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">size_prods</span> <span class="o">*=</span> <span class="n">size</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">size_prods</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected more than 1 value per channel when training, got input size </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span>
        <span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="instance_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.instance_norm">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">instance_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">running_var</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_input_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor], bool, float, float) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Instance Normalization for each channel in each data sample in a</span>
<span class="sd">    batch.</span>

<span class="sd">    See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,</span>
<span class="sd">    :class:`~torch.nn.InstanceNorm3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">instance_norm</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span>
        <span class="n">use_input_stats</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="layer_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.layer_norm">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int], Optional[Tensor], Optional[Tensor], float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Layer Normalization for last certain number of dimensions.</span>

<span class="sd">    See :class:`~torch.nn.LayerNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span></div>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">group_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, int, Optional[Tensor], Optional[Tensor], float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Group Normalization for last certain number of dimensions.</span>

<span class="sd">    See :class:`~torch.nn.GroupNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>


<div class="viewcode-block" id="local_response_norm"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.local_response_norm">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">local_response_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, int, float, float, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies local response normalization over an input signal composed of</span>
<span class="sd">    several input planes, where channels occupy the second dimension.</span>
<span class="sd">    Applies normalization across channels.</span>

<span class="sd">    See :class:`~torch.nn.LocalResponseNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 3D or higher dimensionality </span><span class="se">\</span>
<span class="s1">                         input (got </span><span class="si">{}</span><span class="s1"> dimensions)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
    <span class="n">div</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sizes</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">avg_pool3d</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
    <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="n">div</span></div>


<span class="c1"># loss</span>

<div class="viewcode-block" id="ctc_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.ctc_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, Tensor, int, str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Connectionist Temporal Classification loss.</span>

<span class="sd">    See :class:`~torch.nn.CTCLoss` for details.</span>

<span class="sd">    .. include:: cudnn_deterministic.rst</span>
<span class="sd">    .. include:: cuda_deterministic_backward.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        log_probs: :math:`(T, N, C)` where `C = number of characters in alphabet including blank`,</span>
<span class="sd">            `T = input length`, and `N = batch size`.</span>
<span class="sd">            The logarithmized probabilities of the outputs</span>
<span class="sd">            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).</span>
<span class="sd">        targets: :math:`(N, S)` or `(sum(target_lengths))`.</span>
<span class="sd">            Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.</span>
<span class="sd">        input_lengths: :math:`(N)`.</span>
<span class="sd">            Lengths of the inputs (must each be :math:`\leq T`)</span>
<span class="sd">        target_lengths: :math:`(N)`.</span>
<span class="sd">            Lengths of the targets</span>
<span class="sd">        blank (int, optional):</span>
<span class="sd">            Blank label. Default :math:`0`.</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;mean&#39;: the output losses will be divided by the target lengths and</span>
<span class="sd">            then the mean over the batch is taken. Default: &#39;mean&#39;</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt; targets = torch.randint(1, 20, (16, 30), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.full((16,), 50, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(10,30,(16,), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">))</span></div>


<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.nll_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
             <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], int, Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.NLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K &gt; 1`</span>
<span class="sd">            in the case of K-dimensional loss.</span>
<span class="sd">        target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`,</span>
<span class="sd">            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for</span>
<span class="sd">            K-dimensional loss.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets. Default: -100</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;mean&#39;</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # input is of size N x C = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.tensor([1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 2 or more dimensions (got </span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected input batch_size (</span><span class="si">{}</span><span class="s1">) to match target batch_size (</span><span class="si">{}</span><span class="s1">).&#39;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># dim == 3 or dim &gt; 4</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected target size </span><span class="si">{}</span><span class="s1">, got </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">out_size</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="ow">not</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span>
                <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span>
                <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="poisson_nll_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.poisson_nll_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">poisson_nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                     <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, bool, bool, Optional[bool], float, Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Poisson negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.PoissonNLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: expectation of underlying Poisson distribution.</span>
<span class="sd">        target: random sample :math:`target \sim \text{Poisson}(input)`.</span>
<span class="sd">        log_input: if ``True`` the loss is computed as</span>
<span class="sd">            :math:`\exp(\text{input}) - \text{target} * \text{input}`, if ``False`` then loss is</span>
<span class="sd">            :math:`\text{input} - \text{target} * \log(\text{input}+\text{eps})`. Default: ``True``</span>
<span class="sd">        full: whether to compute full loss, i. e. to add the Stirling</span>
<span class="sd">            approximation term. Default: ``False``</span>
<span class="sd">            :math:`\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when</span>
<span class="sd">            :attr:`log_input`=``False``. Default: 1e-8</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;mean&#39;</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">log_input</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="nb">input</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">input</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">full</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="n">loss</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">target</span><span class="p">))[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="kl_div"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.kl_div">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The `Kullback-Leibler divergence`_ Loss.</span>

<span class="sd">    See :class:`~torch.nn.KLDivLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;batchmean&#39; | &#39;sum&#39; | &#39;mean&#39;.</span>
<span class="sd">            &#39;none&#39;: no reduction will be applied</span>
<span class="sd">            &#39;batchmean&#39;: the sum of the output will be divided by the batchsize</span>
<span class="sd">            &#39;sum&#39;: the output will be summed</span>
<span class="sd">            &#39;mean&#39;: the output will be divided by the number of elements in the output</span>
<span class="sd">            Default: &#39;mean&#39;</span>

<span class="sd">        .. note:: :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,</span>
<span class="sd">            and in the meantime, specifying either of those two args will override :attr:`reduction`.</span>

<span class="sd">        .. note:: `reduction=&#39;mean&#39;` doesn&#39;t return the true kl divergence value, please use</span>
<span class="sd">            `reduction=&#39;batchmean&#39;` which aligns with KL math definition.</span>
<span class="sd">            In the next major release, &#39;mean&#39; will be changed to be the same as &#39;batchmean&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;reduction: &#39;mean&#39; divides the total loss by both the batch size and the support size.&quot;</span>
                          <span class="s2">&quot;&#39;batchmean&#39; divides only by the batch size, and aligns with the KL div math definition.&quot;</span>
                          <span class="s2">&quot;&#39;mean&#39; will be changed to behave the same as &#39;batchmean&#39; in the next major release.&quot;</span><span class="p">)</span>

        <span class="c1"># special case for batchmean</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">:</span>
            <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>

    <span class="n">reduced</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;batchmean&#39;</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">reduced</span> <span class="o">=</span> <span class="n">reduced</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">reduced</span></div>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cross_entropy">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                  <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], int, Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines `log_softmax` and `nll_loss` in a single</span>
<span class="sd">    function.</span>

<span class="sd">    See :class:`~torch.nn.CrossEntropyLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor) : :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K &gt; 1`</span>
<span class="sd">            in the case of K-dimensional loss.</span>
<span class="sd">        target (Tensor) : :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`,</span>
<span class="sd">            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for</span>
<span class="sd">            K-dimensional loss.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets. Default: -100</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;mean&#39;</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(5, (3,), dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt; loss = F.cross_entropy(input, target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures the Binary Cross Entropy</span>
<span class="sd">    between the target and the output.</span>

<span class="sd">    See :class:`~torch.nn.BCELoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight</span>
<span class="sd">                if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;mean&#39;</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = torch.randn((3, 2), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.rand((3, 2), requires_grad=False)</span>
<span class="sd">        &gt;&gt;&gt; loss = F.binary_cross_entropy(F.sigmoid(input), target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Using a target size (</span><span class="si">{}</span><span class="s2">) that is different to the input size (</span><span class="si">{}</span><span class="s2">) is deprecated. &quot;</span>
                      <span class="s2">&quot;Please ensure they have the same size.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="n">target</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target and input must have the same number of elements. target nelement (</span><span class="si">{}</span><span class="s2">) &quot;</span>
                         <span class="s2">&quot;!= input nelement (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">()))</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">new_size</span> <span class="o">=</span> <span class="n">_infer_size</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">new_size</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy_with_logits"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.binary_cross_entropy_with_logits">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], Optional[bool], str, Optional[Tensor]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures Binary Cross Entropy between target and output</span>
<span class="sd">    logits.</span>

<span class="sd">    See :class:`~torch.nn.BCEWithLogitsLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight</span>
<span class="sd">            if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            &#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied,</span>
<span class="sd">            &#39;mean&#39;: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, &#39;sum&#39;: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: &#39;mean&#39;</span>
<span class="sd">        pos_weight (Tensor, optional): a weight of positive examples.</span>
<span class="sd">                Must be a vector with length equal to the number of classes.</span>

<span class="sd">    Examples::</span>

<span class="sd">         &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)</span>
<span class="sd">         &gt;&gt;&gt; target = torch.empty(3).random_(2)</span>
<span class="sd">         &gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)</span>
<span class="sd">         &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target size (</span><span class="si">{}</span><span class="s2">) must be the same as input size (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_pointwise_loss</span><span class="p">(</span><span class="n">lambd</span><span class="p">,</span> <span class="n">lambd_optimized</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">lambd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">d</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">lambd_optimized</span><span class="p">(</span><span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">))</span>


<span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">_smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor) -&gt; Tensor</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">t</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">t</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>


<div class="viewcode-block" id="smooth_l1_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.smooth_l1_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below 1 and an L1 term otherwise.</span>

<span class="sd">    See :class:`~torch.nn.SmoothL1Loss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="l1_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.l1_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;l1_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    Function that takes the mean element-wise absolute value difference.</span>

<span class="sd">    See :class:`~torch.nn.L1Loss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="mse_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.mse_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;mse_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    Measures the element-wise mean squared error.</span>

<span class="sd">    See :class:`~torch.nn.MSELoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="margin_ranking_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.margin_ranking_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, float, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MarginRankingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input1</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">input2</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">target</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">((</span><span class="s2">&quot;margin_ranking_loss does not support scalars, got sizes: &quot;</span>
                            <span class="s2">&quot;input1: </span><span class="si">{}</span><span class="s2">, input2: </span><span class="si">{}</span><span class="s2">, target: </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input1</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">input2</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">())))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="hinge_embedding_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.hinge_embedding_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, float, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.HingeEmbeddingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="multilabel_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multilabel_margin_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiLabelMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="soft_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.soft_margin_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;soft_margin_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.SoftMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="multilabel_soft_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multilabel_soft_margin_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multilabel_soft_margin_loss(input, target, weight=None, size_average=None) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">logsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">logsigmoid</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># only return N loss values</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">reduction</span> <span class="o">+</span> <span class="s2">&quot; is not valid&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="cosine_embedding_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.cosine_embedding_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, float, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.CosineEmbeddingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="multi_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.multi_margin_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, int, float, Optional[Tensor], Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,</span>
<span class="sd">                          reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;only p == 1 and p == 2 supported&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;weight must be one-dimensional&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<span class="n">pixel_shuffle</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pixel_shuffle</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Rearranges elements in a tensor of shape :math:`(*, C \times r^2, H, W)` to a</span>
<span class="s2">tensor of shape :math:`(C, H \times r, W \times r)`.</span>

<span class="s2">See :class:`~torch.nn.PixelShuffle` for details.</span>

<span class="s2">Args:</span>
<span class="s2">    input (Tensor): the input tensor</span>
<span class="s2">    upscale_factor (int): factor to increase spatial resolution by</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; input = torch.randn(1, 9, 4, 4)</span>
<span class="s2">    &gt;&gt;&gt; output = torch.nn.functional.pixel_shuffle(input, 3)</span>
<span class="s2">    &gt;&gt;&gt; print(output.size())</span>
<span class="s2">    torch.Size([1, 1, 12, 12])</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="upsample"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample">[docs]</a><span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input to either the given :attr:`size` or the given</span>
<span class="sd">    :attr:`scale_factor`</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with ``nn.functional.interpolate(...)``.</span>

<span class="sd">    .. include:: cuda_deterministic_backward.rst</span>

<span class="sd">    The algorithm used for upsampling is determined by :attr:`mode`.</span>

<span class="sd">    Currently temporal, spatial and volumetric upsampling are supported, i.e.</span>
<span class="sd">    expected inputs are 3-D, 4-D or 5-D in shape.</span>

<span class="sd">    The input dimensions are interpreted in the form:</span>
<span class="sd">    `mini-batch x channels x [optional depth] x [optional height] x width`.</span>

<span class="sd">    The modes available for upsampling are: `nearest`, `linear` (3D-only),</span>
<span class="sd">    `bilinear` (4D-only), `trilinear` (5D-only)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</span>
<span class="sd">            output spatial size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>
<span class="sd">        mode (string): algorithm used for upsampling:</span>
<span class="sd">            &#39;nearest&#39; | &#39;linear&#39; | &#39;bilinear&#39; | &#39;trilinear&#39;. Default: &#39;nearest&#39;</span>
<span class="sd">        align_corners (bool, optional): if True, the corner pixels of the input</span>
<span class="sd">            and output tensors are aligned, and thus preserving the values at</span>
<span class="sd">            those pixels. This only has effect when :attr:`mode` is `linear`,</span>
<span class="sd">            `bilinear`, or `trilinear`. Default: False</span>

<span class="sd">    .. warning::</span>
<span class="sd">        With ``align_corners = True``, the linearly interpolating modes</span>
<span class="sd">        (`linear`, `bilinear`, and `trilinear`) don&#39;t proportionally align the</span>
<span class="sd">        output and input pixels, and thus the output values can depend on the</span>
<span class="sd">        input size. This was the default behavior for these modes up to version</span>
<span class="sd">        0.3.1. Since then, the default behavior is ``align_corners = False``.</span>
<span class="sd">        See :class:`~torch.nn.Upsample` for concrete examples on how this</span>
<span class="sd">        affects the outputs.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span></div>


<div class="viewcode-block" id="interpolate"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.interpolate">[docs]</a><span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Down/up samples the input to either the given :attr:`size` or the given</span>
<span class="sd">    :attr:`scale_factor`</span>

<span class="sd">    The algorithm used for interpolation is determined by :attr:`mode`.</span>

<span class="sd">    Currently temporal, spatial and volumetric sampling are supported, i.e.</span>
<span class="sd">    expected inputs are 3-D, 4-D or 5-D in shape.</span>

<span class="sd">    The input dimensions are interpreted in the form:</span>
<span class="sd">    `mini-batch x channels x [optional depth] x [optional height] x width`.</span>

<span class="sd">    The modes available for resizing are: `nearest`, `linear` (3D-only),</span>
<span class="sd">    `bilinear` (4D-only), `trilinear` (5D-only), `area`</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</span>
<span class="sd">            output spatial size.</span>
<span class="sd">        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.</span>
<span class="sd">        mode (string): algorithm used for upsampling:</span>
<span class="sd">            &#39;nearest&#39; | &#39;linear&#39; | &#39;bilinear&#39; | &#39;trilinear&#39; | &#39;area&#39;. Default: &#39;nearest&#39;</span>
<span class="sd">        align_corners (bool, optional): if True, the corner pixels of the input</span>
<span class="sd">            and output tensors are aligned, and thus preserving the values at</span>
<span class="sd">            those pixels. This only has effect when :attr:`mode` is `linear`,</span>
<span class="sd">            `bilinear`, or `trilinear`. Default: False</span>

<span class="sd">    .. warning::</span>
<span class="sd">        With ``align_corners = True``, the linearly interpolating modes</span>
<span class="sd">        (`linear`, `bilinear`, and `trilinear`) don&#39;t proportionally align the</span>
<span class="sd">        output and input pixels, and thus the output values can depend on the</span>
<span class="sd">        input size. This was the default behavior for these modes up to version</span>
<span class="sd">        0.3.1. Since then, the default behavior is ``align_corners = False``.</span>
<span class="sd">        See :class:`~torch.nn.Upsample` for concrete examples on how this</span>
<span class="sd">        affects the outputs.</span>

<span class="sd">    .. include:: cuda_deterministic_backward.rst</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">numbers</span> <span class="k">import</span> <span class="n">Integral</span>
    <span class="kn">from</span> <span class="nn">.modules.utils</span> <span class="k">import</span> <span class="n">_ntuple</span>

    <span class="k">def</span> <span class="nf">_check_size_scale_factor</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;either size or scale_factor should be defined&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;only one of size or scale_factor should be defined&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>\
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;scale_factor shape must match input shape. &#39;</span>
                             <span class="s1">&#39;Input is </span><span class="si">{}</span><span class="s1">D, scale_factor size is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_output_size</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="n">_check_size_scale_factor</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">size</span>
        <span class="n">scale_factors</span> <span class="o">=</span> <span class="n">_ntuple</span><span class="p">(</span><span class="n">dim</span><span class="p">)(</span><span class="n">scale_factor</span><span class="p">)</span>
        <span class="c1"># math.floor might return float in py2.7</span>
        <span class="k">return</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="s1">&#39;area&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;align_corners option can only be set with the &quot;</span>
                             <span class="s2">&quot;interpolating modes: linear | bilinear | trilinear&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Default upsampling behavior when mode=</span><span class="si">{}</span><span class="s2"> is changed &quot;</span>
                          <span class="s2">&quot;to align_corners=False since 0.4.0. Please specify &quot;</span>
                          <span class="s2">&quot;align_corners=True if the old behavior is desired. &quot;</span>
                          <span class="s2">&quot;See the documentation of nn.Upsample for details.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
            <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_linear1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 3D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 3D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but linear mode needs 3D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_bilinear2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but linear mode needs 3D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_trilinear3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 3D, 4D and 5D input Tensors supported&quot;</span>
                                  <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">D) for the modes: nearest | linear | bilinear | trilinear&quot;</span>
                                  <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">mode</span><span class="p">))</span></div>


<div class="viewcode-block" id="upsample_nearest"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample_nearest">[docs]</a><span class="k">def</span> <span class="nf">upsample_nearest</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input, using nearest neighbours&#39; pixel values.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with ``nn.functional.interpolate(..., mode=&#39;nearest&#39;)``.</span>

<span class="sd">    Currently spatial and volumetric upsampling are supported (i.e. expected</span>
<span class="sd">    inputs are 4 or 5 dimensional).</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia</span>
<span class="sd">            size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>

<span class="sd">    .. include:: cuda_deterministic_backward.rst</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="upsample_bilinear"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.upsample_bilinear">[docs]</a><span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input, using bilinear upsampling.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with</span>
<span class="sd">        ``nn.functional.interpolate(..., mode=&#39;bilinear&#39;, align_corners=True)``.</span>

<span class="sd">    Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo</span>
<span class="sd">    volumetric (5 dimensional) inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        size (int or Tuple[int, int]): output spatial size.</span>
<span class="sd">        scale_factor (int or Tuple[int, int]): multiplier for spatial size</span>

<span class="sd">    .. include:: cuda_deterministic_backward.rst</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<span class="n">GRID_SAMPLE_INTERPOLATION_MODES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;nearest&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">GRID_SAMPLE_PADDING_MODES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;zeros&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;border&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;reflection&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
<span class="p">}</span>


<div class="viewcode-block" id="grid_sample"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.grid_sample">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">grid_sample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, str, str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given an :attr:`input` and a flow-field :attr:`grid`, computes the</span>
<span class="sd">    ``output`` using :attr:`input` values and pixel locations from :attr:`grid`.</span>

<span class="sd">    Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are</span>
<span class="sd">    supported.</span>

<span class="sd">    In the spatial (4-D) case, for :attr:`input` with shape</span>
<span class="sd">    :math:`(N, C, H_\text{in}, W_\text{in})` and :attr:`grid` with shape</span>
<span class="sd">    :math:`(N, H_\text{out}, W_\text{out}, 2)`, the output will have shape</span>
<span class="sd">    :math:`(N, C, H_\text{out}, W_\text{out})`.</span>

<span class="sd">    For each output location ``output[n, :, h, w]``, the size-2 vector</span>
<span class="sd">    ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``,</span>
<span class="sd">    which are used to interpolate the output value ``output[n, :, h, w]``.</span>
<span class="sd">    In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the</span>
<span class="sd">    ``x``, ``y``, ``z`` pixel locations for interpolating</span>
<span class="sd">    ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or</span>
<span class="sd">    ``bilinear`` interpolation method to sample the input pixels.</span>

<span class="sd">    :attr:`grid` should have most values in the range of ``[-1, 1]``. This is</span>
<span class="sd">    because the pixel locations are normalized by the :attr:`input` spatial</span>
<span class="sd">    dimensions. For example, values ``x = -1, y = -1`` is the left-top pixel of</span>
<span class="sd">    :attr:`input`, and values  ``x = 1, y = 1`` is the right-bottom pixel of</span>
<span class="sd">    :attr:`input`.</span>

<span class="sd">    If :attr:`grid` has values outside the range of ``[-1, 1]``, those locations</span>
<span class="sd">    are handled as defined by :attr:`padding_mode`. Options are</span>

<span class="sd">        * ``padding_mode=&quot;zeros&quot;``: use ``0`` for out-of-bound values,</span>
<span class="sd">        * ``padding_mode=&quot;border&quot;``: use border values for out-of-bound values,</span>
<span class="sd">        * ``padding_mode=&quot;reflection&quot;``: use values at locations reflected by</span>
<span class="sd">          the border for out-of-bound values. For location far away from the</span>
<span class="sd">          border, it will keep being reflected until becoming in bound, e.g.,</span>
<span class="sd">          (normalized) pixel location ``x = -3.5`` reflects by ``-1`` and</span>
<span class="sd">          becomes ``x&#39; = 2.5``, then reflects by border ``1`` and becomes</span>
<span class="sd">          ``x&#39;&#39; = -0.5``.</span>

<span class="sd">    .. Note:: This function is often used in building Spatial Transformer Networks.</span>
<span class="sd">    .. include:: cuda_deterministic_backward.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input of shape :math:`(N, C, H_\text{in}, W_\text{in})` (4-D case)</span>
<span class="sd">                        or :math:`(N, C, D_\text{in}, H_\text{in}, W_\text{in})` (5-D case)</span>
<span class="sd">        grid (Tensor): flow-field of shape :math:`(N, H_\text{out}, W_\text{out}, 2)` (4-D case)</span>
<span class="sd">                       or :math:`(N, D_\text{out}, H_\text{out}, W_\text{out}, 3)` (5-D case)</span>
<span class="sd">        mode (str): interpolation mode to calculate output values</span>
<span class="sd">            &#39;bilinear&#39; | &#39;nearest&#39;. Default: &#39;bilinear&#39;</span>
<span class="sd">        padding_mode (str): padding mode for outside grid values</span>
<span class="sd">            &#39;zeros&#39; | &#39;border&#39; | &#39;reflection&#39;. Default: &#39;zeros&#39;</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor): output Tensor</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;bilinear&#39;</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;nn.functional.grid_sample(): expected mode to be &quot;</span>
                         <span class="s2">&quot;&#39;bilinear&#39; or &#39;nearest&#39;, but got: &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span> <span class="ow">and</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;border&#39;</span> <span class="ow">and</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;reflection&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;nn.functional.grid_sample(): expected padding_mode &quot;</span>
                         <span class="s2">&quot;to be &#39;zeros&#39;, &#39;border&#39;, or &#39;reflection&#39;, &quot;</span>
                         <span class="s2">&quot;but got: &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">padding_mode</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
        <span class="n">padding_mode_enum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s1">&#39;border&#39;</span><span class="p">:</span>
        <span class="n">padding_mode_enum</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">padding_mode_enum</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">grid_sampler</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode_enum</span><span class="p">,</span> <span class="n">padding_mode_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="affine_grid"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.affine_grid">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Generates a 2d flow field, given a batch of affine matrices :attr:`theta`</span>
<span class="sd">    Generally used in conjunction with :func:`grid_sample` to</span>
<span class="sd">    implement Spatial Transformer Networks.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta (Tensor): input batch of affine matrices (:math:`N \times 2 \times 3`)</span>
<span class="sd">        size (torch.Size): the target output image size (:math:`N \times C \times H \times W`)</span>
<span class="sd">                           Example: torch.Size((32, 3, 24, 24))</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor): output Tensor of size (:math:`N \times H \times W \times 2`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">vision</span><span class="o">.</span><span class="n">affine_grid_generator</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pad">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int], str, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads tensor.</span>

<span class="sd">    Pading size:</span>
<span class="sd">        The number of dimensions to pad is :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor`</span>
<span class="sd">        and the dimensions that get padded begins with the last dimension and moves forward.</span>
<span class="sd">        For example, to pad the last dimension of the input tensor, then `pad` has form</span>
<span class="sd">        `(padLeft, padRight)`; to pad the last 2 dimensions of the input tensor, then use</span>
<span class="sd">        `(padLeft, padRight, padTop, padBottom)`; to pad the last 3 dimensions, use</span>
<span class="sd">        `(padLeft, padRight, padTop, padBottom, padFront, padBack)`.</span>

<span class="sd">    Padding mode:</span>
<span class="sd">        See :class:`torch.nn.ConstantPad2d`, :class:`torch.nn.ReflectionPad2d`, and</span>
<span class="sd">        :class:`torch.nn.ReplicationPad2d` for concrete examples on how each of the</span>
<span class="sd">        padding modes works. Constant padding is implemented for arbitrary dimensions.</span>
<span class="sd">        Replicate padding is implemented for padding the last 3 dimensions of 5D input</span>
<span class="sd">        tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of</span>
<span class="sd">        3D input tensor. Reflect padding is only implemented for padding the last 2</span>
<span class="sd">        dimensions of 4D input tensor, or the last dimension of 3D input tensor.</span>

<span class="sd">    .. include:: cuda_deterministic_backward.rst</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): `Nd` tensor</span>
<span class="sd">        pad (tuple): m-elem tuple, where :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.</span>
<span class="sd">        mode: &#39;constant&#39;, &#39;reflect&#39; or &#39;replicate&#39;. Default: &#39;constant&#39;</span>
<span class="sd">        value: fill value for &#39;constant&#39; padding. Default: 0</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)</span>
<span class="sd">        &gt;&gt;&gt; p1d = (1, 1) # pad last dim by 1 on each side</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p1d, &quot;constant&quot;, 0)  # effectively zero padding</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 3, 4, 4])</span>
<span class="sd">        &gt;&gt;&gt; p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p2d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 3, 8, 4])</span>
<span class="sd">        &gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)</span>
<span class="sd">        &gt;&gt;&gt; p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p3d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.data.size())</span>
<span class="sd">        torch.Size([3, 9, 7, 3])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Padding length must be divisible by 2&#39;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&lt;=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="s1">&#39;Padding length too large&#39;</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">_VF</span><span class="o">.</span><span class="n">constant_pad_nd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Padding mode &quot;</span><span class="si">{}</span><span class="s1">&quot;&quot; doesn</span><span class="se">\&#39;</span><span class="s1">t take in value argument&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;3D tensors expect 2 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span>  <span class="c1"># TODO: remove this when jit raise supports control flow</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;4D tensors expect 4 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span>  <span class="c1"># TODO: remove this when jit raise supports control flow</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;5D tensors expect 6 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span>  <span class="c1"># TODO: remove this when jit raise supports control flow</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span>  <span class="c1"># TODO: remove this when jit raise supports control flow</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span>  <span class="c1"># TODO: remove this when jit raise supports control flow</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only 3D, 4D, 5D padding with non-constant padding are supported for now&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>

<span class="c1"># distance</span>


<div class="viewcode-block" id="pairwise_distance"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.pairwise_distance">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, float, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See :class:`torch.nn.PairwiseDistance` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span></div>


<span class="n">pdist</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pdist</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">pdist(input, p=2) -&gt; Tensor</span>

<span class="s2">Computes the p-norm distance between every pair of row vectors in the input.</span>
<span class="s2">This is identical to the upper triangular portion, excluding the diagonal, of</span>
<span class="s2">`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster</span>
<span class="s2">if the rows are contiguous.</span>

<span class="s2">If input has shape :math:`N \times M` then the output will have shape</span>
<span class="s2">:math:`\frac</span><span class="si">{1}{2}</span><span class="s2"> N (N - 1)`.</span>

<span class="s2">This function is equivalent to `scipy.spatial.distance.pdist(input,</span>
<span class="s2">&#39;minkowski&#39;, p=p)` if :math:`p \in (0, \infty)`. When :math:`p = 0` it is</span>
<span class="s2">equivalent to `scipy.spatial.distance.pdist(input, &#39;hamming&#39;) * M`.</span>
<span class="s2">When :math:`p = \infty`, the closest scipy function is</span>
<span class="s2">`scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())`.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`N \times M`.</span>
<span class="s2">    p: p value for the p-norm distance to calculate between each vector pair</span>
<span class="s2">        :math:`\in [0, \infty]`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="n">cosine_similarity</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">cosine_similarity(x1, x2, dim=1, eps=1e-8) -&gt; Tensor</span>

<span class="s2">Returns cosine similarity between x1 and x2, computed along dim.</span>

<span class="s2">.. math ::</span>
<span class="s2">    \text</span><span class="si">{similarity}</span><span class="s2"> = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}</span>

<span class="s2">Args:</span>
<span class="s2">    x1 (Tensor): First input.</span>
<span class="s2">    x2 (Tensor): Second input (of size matching x1).</span>
<span class="s2">    dim (int, optional): Dimension of vectors. Default: 1</span>
<span class="s2">    eps (float, optional): Small value to avoid division by zero.</span>
<span class="s2">        Default: 1e-8</span>

<span class="s2">Shape:</span>
<span class="s2">    - Input: :math:`(\ast_1, D, \ast_2)` where D is at position `dim`.</span>
<span class="s2">    - Output: :math:`(\ast_1, \ast_2)` where 1 is at position `dim`.</span>

<span class="s2">Example::</span>

<span class="s2">    &gt;&gt;&gt; input1 = torch.randn(100, 128)</span>
<span class="s2">    &gt;&gt;&gt; input2 = torch.randn(100, 128)</span>
<span class="s2">    &gt;&gt;&gt; output = F.cosine_similarity(input1, input2)</span>
<span class="s2">    &gt;&gt;&gt; print(output)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="triplet_margin_loss"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.triplet_margin_loss">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, float, float, float, bool, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See :class:`~torch.nn.TripletMarginLoss` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                                     <span class="n">swap</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="normalize"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.normalize">[docs]</a><span class="nd">@torch</span><span class="o">.</span><span class="n">_jit_internal</span><span class="o">.</span><span class="n">weak_script</span>
<span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, int, float, Optional[Tensor]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Performs :math:`L_p` normalization of inputs over specified dimension.</span>

<span class="sd">    For a tensor :attr:`input` of sizes :math:`(n_0, ..., n_{dim}, ..., n_k)`, each</span>
<span class="sd">    :math:`n_{dim}` -element vector :math:`v` along dimension :attr:`dim` is transformed as</span>

<span class="sd">    .. math::</span>
<span class="sd">        v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.</span>

<span class="sd">    With the default arguments it uses the Euclidean norm over vectors along dimension :math:`1` for normalization.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of any shape</span>
<span class="sd">        p (float): the exponent value in the norm formulation. Default: 2</span>
<span class="sd">        dim (int): the dimension to reduce. Default: 1</span>
<span class="sd">        eps (float): small value to avoid division by zero. Default: 1e-12</span>
<span class="sd">        out (Tensor, optional): the output tensor. If :attr:`out` is used, this</span>
<span class="sd">                                operation won&#39;t be differentiable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">/</span> <span class="n">denom</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="k">def</span> <span class="nf">assert_int_or_pair</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="n">message</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">arg_name</span><span class="p">)</span>


<div class="viewcode-block" id="unfold"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.unfold">[docs]</a><span class="k">def</span> <span class="nf">unfold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Extracts sliding local blocks from an batched input tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, only 4-D input tensors (batched image-like tensors) are</span>
<span class="sd">        supported.</span>

<span class="sd">    See :class:`torch.nn.Unfold` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> must be int or 2-tuple for 4D input&#39;</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Im2Col</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
                            <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 4D input Tensors are supported (got </span><span class="si">{}</span><span class="s2">D)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span></div>


<div class="viewcode-block" id="fold"><a class="viewcode-back" href="../../../nn.html#torch.nn.functional.fold">[docs]</a><span class="k">def</span> <span class="nf">fold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Combines an array of sliding local blocks into a large containing</span>
<span class="sd">    tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, only 4-D output tensors (batched image-like tensors) are</span>
<span class="sd">        supported.</span>

<span class="sd">    See :class:`torch.nn.Fold` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> must be int or 2-tuple for 3D input&#39;</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Col2Im</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_size</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
                            <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 3D input Tensors are supported (got </span><span class="si">{}</span><span class="s2">D)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../../../_static/jquery.js"></script>
         <script type="text/javascript" src="../../../_static/underscore.js"></script>
         <script type="text/javascript" src="../../../_static/doctools.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="../../../_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>
<img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
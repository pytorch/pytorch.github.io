


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.parallel.distributed &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.0.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed_deprecated.html">torch.distributed.deprecated</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torchvision/index.html">torchvision</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.parallel.distributed</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.parallel.distributed</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">torch.cuda.comm</span> <span class="k">import</span> <span class="n">broadcast_coalesced</span>
<span class="kn">from</span> <span class="nn">torch.cuda</span> <span class="k">import</span> <span class="n">nccl</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">torch.distributed.distributed_c10d</span> <span class="k">import</span> <span class="n">_get_default_group</span>

<span class="kn">from</span> <span class="nn">..modules</span> <span class="k">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">.replicate</span> <span class="k">import</span> <span class="n">replicate</span>
<span class="kn">from</span> <span class="nn">.scatter_gather</span> <span class="k">import</span> <span class="n">scatter_kwargs</span><span class="p">,</span> <span class="n">gather</span>
<span class="kn">from</span> <span class="nn">.parallel_apply</span> <span class="k">import</span> <span class="n">parallel_apply</span>
<span class="kn">from</span> <span class="nn">torch.cuda._utils</span> <span class="k">import</span> <span class="n">_get_device_index</span>


<div class="viewcode-block" id="DistributedDataParallel"><a class="viewcode-back" href="../../../../nn.html#torch.nn.parallel.DistributedDataParallel">[docs]</a><span class="k">class</span> <span class="nc">DistributedDataParallel</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements distributed data parallelism that is based on</span>
<span class="sd">    torch.distributed package at the module level.</span>

<span class="sd">    This container parallelizes the application of the given module by</span>
<span class="sd">    splitting the input across the specified devices by chunking in the batch</span>
<span class="sd">    dimension. The module is replicated on each machine and each device, and</span>
<span class="sd">    each such replica handles a portion of the input. During the backwards</span>
<span class="sd">    pass, gradients from each node are averaged.</span>

<span class="sd">    The batch size should be larger than the number of GPUs used locally. It</span>
<span class="sd">    should also be an integer multiple of the number of GPUs so that each chunk</span>
<span class="sd">    is the same size (so that each GPU processes the same number of samples).</span>

<span class="sd">    See also: :ref:`distributed-basics` and :ref:`cuda-nn-dataparallel-instead`.</span>
<span class="sd">    The same constraints on input as in :class:`torch.nn.DataParallel` apply.</span>

<span class="sd">    Creation of this class requires that ``torch.distributed`` to be already</span>
<span class="sd">    initialized, by calling :func:`torch.distributed.init_process_group`</span>

<span class="sd">    ``DistributedDataParallel`` can be used in the following two ways:</span>

<span class="sd">    (1) Single-Process Multi-GPU</span>

<span class="sd">    In this case, a single process will be</span>
<span class="sd">    spawned on each host/node and each process will operate on all the GPUs</span>
<span class="sd">    of the node where it&#39;s running. To use ``DistributedDataParallel`` in</span>
<span class="sd">    this way, you can simply construct the model as the following:</span>

<span class="sd">        &gt;&gt;&gt; torch.distributed.init_process_group(backend=&quot;nccl&quot;)</span>
<span class="sd">        &gt;&gt;&gt; model = DistributedDataParallel(model) # device_ids will include all GPU devices be default</span>

<span class="sd">    (2) Multi-Process Single-GPU</span>

<span class="sd">    This is the highly recommended way to use ``DistributedDataParallel``, with</span>
<span class="sd">    multiple processes, each of which operates on a single GPU. This is</span>
<span class="sd">    currently the fastest approach to do data parallel training using PyTorch</span>
<span class="sd">    and applies to both single-node(multi-GPU) and multi-node data</span>
<span class="sd">    parallel training. It is proven to be significantly faster than</span>
<span class="sd">    :class:`torch.nn.DataParallel` for single-node multi-GPU data</span>
<span class="sd">    parallel training.</span>

<span class="sd">    Here is how to use it: on each host with N GPUs, you should spawn up N</span>
<span class="sd">    processes, while ensuring that each process invidually works on a single GPU</span>
<span class="sd">    from 0 to N-1. Therefore, it is your job to ensure that your training script</span>
<span class="sd">    operates on a single given GPU by calling:</span>

<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(i)</span>

<span class="sd">    where i is from 0 to N-1. In each process, you should refer the following</span>
<span class="sd">    to construct this module:</span>

<span class="sd">        &gt;&gt;&gt; torch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=4, init_method=&#39;...&#39;)</span>
<span class="sd">        &gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)</span>

<span class="sd">    In order to spawn up multiple processes per node, you can use either</span>
<span class="sd">    ``torch.distributed.launch`` or ``torch.multiprocessing.spawn``</span>

<span class="sd">    .. note:: ``nccl`` backend is currently the fastest and</span>
<span class="sd">        highly recommended backend to be used with Multi-Process Single-GPU</span>
<span class="sd">        distributed training and this applies to both single-node and multi-node</span>
<span class="sd">        distributed training</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module works only with the ``gloo`` and ``nccl`` backends.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Constructor, forward method, and differentiation of the output (or a</span>
<span class="sd">        function of the output of this module) is a distributed synchronization</span>
<span class="sd">        point. Take that into account in case different processes might be</span>
<span class="sd">        executing different code.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module assumes all parameters are registered in the model by the</span>
<span class="sd">        time it is created. No parameters should be added nor removed later.</span>
<span class="sd">        Same applies to buffers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module assumes all parameters are registered in the model of each</span>
<span class="sd">        distributed processes are in the same order. The module itself will</span>
<span class="sd">        conduct gradient all-reduction following the reverse order of the</span>
<span class="sd">        registered parameters of the model. In other wise, it is users&#39;</span>
<span class="sd">        responsibility to ensure that each distributed process has the exact</span>
<span class="sd">        same model and thus the exact parameter registeration order.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module assumes all buffers and gradients are dense.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This module doesn&#39;t work with :func:`torch.autograd.grad` (i.e. it will</span>
<span class="sd">        only work if gradients are to be accumulated in ``.grad`` attributes of</span>
<span class="sd">        parameters).</span>

<span class="sd">    .. warning::</span>

<span class="sd">        If you plan on using this module with a ``nccl`` backend or a ``gloo``</span>
<span class="sd">        backend (that uses Infiniband), together with a DataLoader that uses</span>
<span class="sd">        multiple workers, please change the multiprocessing start method to</span>
<span class="sd">        ``forkserver`` (Python 3 only) or ``spawn``. Unfortunately</span>
<span class="sd">        Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will</span>
<span class="sd">        likely experience deadlocks if you don&#39;t change this setting.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Forward and backward hooks defined on :attr:`module` and its submodules</span>
<span class="sd">        won&#39;t be invoked anymore, unless the hooks are initialized in the</span>
<span class="sd">        :meth:`forward` method.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        You should never try to change your model&#39;s parameters after wrapping</span>
<span class="sd">        up your model with DistributedDataParallel. In other words, when</span>
<span class="sd">        wrapping up your model with DistributedDataParallel, the constructor of</span>
<span class="sd">        DistributedDataParallel will register the additional gradient</span>
<span class="sd">        reduction functions on all the parameters of the model itself at the</span>
<span class="sd">        time of construction. If you change the model&#39;s parameters after</span>
<span class="sd">        the DistributedDataParallel construction, this is not supported and</span>
<span class="sd">        unexpected behaviors can happen, since some parameters&#39; gradient</span>
<span class="sd">        reduction functions might not get called.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Parameters are never broadcast between processes. The module performs</span>
<span class="sd">        an all-reduce step on gradients and assumes that they will be modified</span>
<span class="sd">        by the optimizer in all processes in the same way. Buffers</span>
<span class="sd">        (e.g. BatchNorm stats) are broadcast from the module in process of rank</span>
<span class="sd">        0, to all other replicas in the system in every iteration.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (Module): module to be parallelized</span>
<span class="sd">        device_ids (list of int or torch.device): CUDA devices (default: all devices)</span>
<span class="sd">        output_device (int or torch.device): device location of output (default: device_ids[0])</span>
<span class="sd">        broadcast_buffers (bool): flag that enables syncing (broadcasting) buffers of</span>
<span class="sd">                           the module at beginning of the forward function.</span>
<span class="sd">                           (default: True)</span>
<span class="sd">        process_group: the process group to be used for distributed data</span>
<span class="sd">                       all-reduction. If None, the default process group, which</span>
<span class="sd">                       is created by ```torch.distributed.init_process_group```,</span>
<span class="sd">                       will be used. (default: None)</span>
<span class="sd">        bucket_cap_mb: DistributedDataParallel will bucket parameters into</span>
<span class="sd">                       multiple buckets so that gradient reduction of each</span>
<span class="sd">                       bucket can potentially overlap with backward computation.</span>
<span class="sd">                       bucket_cap_mb controls the bucket size in MegaBytes (MB)</span>
<span class="sd">                       (default: 25)</span>
<span class="sd">        check_reduction: when setting to True, it enables DistributedDataParallel</span>
<span class="sd">                         to automatically check if the previous iteration&#39;s</span>
<span class="sd">                         backward reductions were successfully issued at the</span>
<span class="sd">                         beginning of every iteration&#39;s forward function.</span>
<span class="sd">                         You normally don&#39;t need this option enabled unless you</span>
<span class="sd">                         are observing weird behaviors such as different ranks</span>
<span class="sd">                         are getting different gradients, which should not</span>
<span class="sd">                         happen if DistributedDataParallel is corrected used.</span>
<span class="sd">                         (default: False)</span>

<span class="sd">    Attributes:</span>
<span class="sd">        module (Module): the module to be parallelized</span>

<span class="sd">    Example::</span>
<span class="sd">        &gt;&gt;&gt; torch.distributed.init_process_group(backend=&#39;nccl&#39;, world_size=4, init_method=&#39;...&#39;)</span>
<span class="sd">        &gt;&gt;&gt; net = torch.nn.DistributedDataParallel(model, pg)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">output_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">broadcast_buffers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">process_group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bucket_cap_mb</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                 <span class="n">check_reduction</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedDataParallel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Use all devices by default</span>
        <span class="k">if</span> <span class="n">device_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>

        <span class="k">if</span> <span class="n">output_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_device</span> <span class="o">=</span> <span class="n">device_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">process_group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">process_group</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span> <span class="n">device_ids</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">output_device</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span> <span class="o">=</span> <span class="n">broadcast_buffers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_reduction</span> <span class="o">=</span> <span class="n">check_reduction</span>

        <span class="n">MB</span> <span class="o">=</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span>

        <span class="c1"># used for intra-node param sync and inter-node sync as well</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span> <span class="o">=</span> <span class="mi">250</span> <span class="o">*</span> <span class="n">MB</span>

        <span class="c1"># reduction bucket size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_bytes_cap</span> <span class="o">=</span> <span class="n">bucket_cap_mb</span> <span class="o">*</span> <span class="n">MB</span>

        <span class="c1"># Sync params and buffers</span>
        <span class="n">module_states</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">module_states</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dist_broadcast_coalesced</span><span class="p">(</span><span class="n">module_states</span><span class="p">,</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_ddp_init_helper</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_ddp_init_helper</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialization helper function that does the following:</span>

<span class="sd">        (1) replicating the module from device[0] to the other devices</span>
<span class="sd">        (2) bucketing the parameters for reductions</span>
<span class="sd">        (3) resetting the bucketing states</span>
<span class="sd">        (4) registering the grad hooks</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># TODO: we don&#39;t need to replicate params in here. they&#39;re always going to</span>
            <span class="c1"># be broadcasted using larger blocks in broadcast_coalesced, so it might be</span>
            <span class="c1"># better to not pollute the caches with these small blocks</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span> <span class="o">=</span> <span class="n">replicate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">,</span> <span class="n">detach</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>

            <span class="k">for</span> <span class="n">module_copy</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">copy_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">module_copy</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
                    <span class="n">copy_param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">modules_params_data</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">))]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers_data</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">))]</span>

        <span class="k">for</span> <span class="n">dev_idx</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">modules_params_data</span><span class="p">[</span><span class="n">dev_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers_data</span><span class="p">[</span><span class="n">dev_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">buffers</span><span class="p">()]</span>

        <span class="c1"># This is a triply-nested list where the &quot;dimensions&quot; are: devices, buckets, bucket_elems</span>
        <span class="n">param_buckets</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Split the parameters into buckets and by types as well</span>
        <span class="c1"># We only need to bucket and reduce parameters that require grad and</span>
        <span class="c1"># this is also true for backward since only the backward hooks for</span>
        <span class="c1"># parameters that require grad will be registered with gradient</span>
        <span class="c1"># reduction functions</span>
        <span class="n">params_to_bucket</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">dev_idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="n">params_to_bucket</span><span class="p">[</span><span class="n">dev_idx</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="n">param_buckets</span> <span class="o">=</span> <span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">_dist_bucket_tensors</span><span class="p">(</span><span class="n">dev_params_to_bucket</span><span class="p">,</span>
                                                   <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_bytes_cap</span><span class="p">),</span>
                                                   <span class="n">fine_grained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                         <span class="k">for</span> <span class="n">dev_params_to_bucket</span> <span class="ow">in</span> <span class="n">params_to_bucket</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_map</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># We transpose param_buckets, so the loop is over buckets.</span>
        <span class="c1"># param_buckets_tuple is a doubly-nested list with &quot;dims&quot;: devices, bucket_elems</span>
        <span class="k">for</span> <span class="n">bucket_idx</span><span class="p">,</span> <span class="n">param_buckets_tuple</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">param_buckets</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Now, we transpose again, so we iterate over bucket_elems, but getting tuples</span>
            <span class="c1"># of params from each device.</span>
            <span class="k">for</span> <span class="n">param_tuple</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">param_buckets_tuple</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_tuple</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_tuple</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bucket_map</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">bucket_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span> <span class="o">=</span> <span class="p">[[[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span>
                        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>
        <span class="c1"># The number of params ready in each bucket</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buckets_ready_size</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>

        <span class="c1"># coalesced bucket for only device 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buckets_coalesced</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>
        <span class="c1"># We will always reduce the bucket following the reverse order</span>
        <span class="c1"># that is, alway reduces following the order of: n - 1, n - 2, ..., 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="c1"># When all buckets are reduced, this will be set to True. This flag is</span>
        <span class="c1"># useful for sanity checks to ensure that each iteration&#39;s backward has</span>
        <span class="c1"># always reduced all buckets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_buckets_reduced</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_previous_reduction</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction_works</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">devs_ready</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_register_grad_hooks</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_default_group</span><span class="p">()</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">attrs</span><span class="p">[</span><span class="s1">&#39;process_group&#39;</span><span class="p">],</span> \
            <span class="n">attrs</span><span class="p">[</span><span class="s1">&#39;default_streams&#39;</span><span class="p">],</span> \
            <span class="n">attrs</span><span class="p">[</span><span class="s1">&#39;_grad_accs&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">attrs</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1"># If serializable, then the process group should be the default one</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">=</span> <span class="n">_get_default_group</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_previous_reduction</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedDataParallel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ddp_init_helper</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_check_default_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_group</span> <span class="o">!=</span> <span class="n">_get_default_group</span><span class="p">():</span>
                <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
            <span class="n">pickle_not_supported</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">pickle_not_supported</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;DDP Pickling/Unpickling are only supported &quot;</span>
                               <span class="s2">&quot;when using DDP with the default process &quot;</span>
                               <span class="s2">&quot;group. That is, when you have called &quot;</span>
                               <span class="s2">&quot;init_process_group and have not passed &quot;</span>
                               <span class="s2">&quot;process_group argument to DDP constructor&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_previous_reduction</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="c1"># self.check_previous_reduction will be False in the first iteration</span>
        <span class="c1"># and is then toggled to True for all future iterations.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_previous_reduction</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">check_previous_reduction</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_buckets_reduced</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Not all gradients have been reduced from &quot;</span>
                                   <span class="s2">&quot;the backward of the previous iteration. &quot;</span>
                                   <span class="s2">&quot;This is unexpected and fatal error. Please &quot;</span>
                                   <span class="s2">&quot;check and ensure that the model&#39;s &quot;</span>
                                   <span class="s2">&quot;parameters are not changed after you wrap &quot;</span>
                                   <span class="s2">&quot;up the model with DistributedDataParallel.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_buckets_reduced</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_reduction</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_previous_reduction</span><span class="p">()</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_params</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)],</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scatter_kwargs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parallel_apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replicas</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">parallel_apply</span><span class="p">(</span><span class="n">replicas</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">replicas</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">output_device</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">gather</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_device</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_previous_reduction</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedDataParallel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_dist_broadcast_coalesced</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">):</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">_dist_broadcast_coalesced</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sync_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># intra-node parameter sync</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">broadcast_coalesced</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules_params_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">module_params_data</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules_params_data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">param_data</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">module_params_data</span><span class="p">):</span>
                    <span class="n">param_data</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

        <span class="c1"># module buffer sync</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_buffers</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># cross-node buffer sync</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_dist_broadcast_coalesced</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                               <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># intra-node buffer sync</span>
                    <span class="n">result</span> <span class="o">=</span> <span class="n">broadcast_coalesced</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_bucket_size</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">module_buffers_data</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules_buffers_data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                        <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">buffer_data</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">module_buffers_data</span><span class="p">):</span>
                            <span class="n">buffer_data</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_register_grad_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad_accs</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># need to keep them in scope</span>

        <span class="c1"># default stream tracking to launch nccl reduce kernels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_streams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">dev_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">dev_id</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">default_streams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

        <span class="k">for</span> <span class="n">device_idx</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_module_copies</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="n">p_tmp</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
                    <span class="n">grad_acc</span> <span class="o">=</span> <span class="n">p_tmp</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">grad_acc</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_make_param_hook</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">device_idx</span><span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_grad_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_acc</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_param_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">device_idx</span><span class="p">):</span>
        <span class="n">bucket_idx</span><span class="p">,</span> <span class="n">bucket_offset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_map</span><span class="p">[</span><span class="n">param</span><span class="p">]</span>

        <span class="k">def</span> <span class="nf">distributed_data_parallel_hook</span><span class="p">(</span><span class="o">*</span><span class="n">unused</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;DistributedDataParallel only works &quot;</span>
                                   <span class="s2">&quot;with gradients that don&#39;t require grad&quot;</span><span class="p">)</span>
            <span class="n">bucket</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">][</span><span class="n">device_idx</span><span class="p">]</span>
            <span class="n">bucket</span><span class="p">[</span><span class="n">bucket_offset</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">buckets_ready_size</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">][</span><span class="n">device_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># We can flush these and save memory for replicas</span>
            <span class="k">if</span> <span class="n">device_idx</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">set_</span><span class="p">()</span>

            <span class="c1"># Current device&#39;s bucket is full</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buckets_ready_size</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">][</span><span class="n">device_idx</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">devs_ready</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">devs_ready</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">):</span>
                    <span class="k">return</span>

                <span class="c1"># Now all devices&#39;s buckets with index: bucket_idx are ready</span>
                <span class="k">if</span> <span class="n">bucket_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_queue_reduction</span><span class="p">(</span><span class="n">bucket_idx</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="c1"># Now reduce anything that is ready but not yet reduced</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">sorted_todo</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sorted_todo</span><span class="p">:</span>
                            <span class="c1"># Nothing can be reduced now</span>
                            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span><span class="p">:</span>
                                <span class="k">break</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_queue_reduction</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">bucket_idx</span><span class="p">)</span>

                <span class="c1"># When all devices&#39; buckets</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># A final sync for all the reduction works</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_sync_reduction_works</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">all_buckets_reduced</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">distributed_data_parallel_hook</span>

    <span class="k">def</span> <span class="nf">_queue_reduction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bucket_idx</span><span class="p">):</span>
        <span class="c1"># _queue_reduction will use a seperate CUDA stream to coalesce</span>
        <span class="c1"># the small tensors to achieve more parallelisms, before passing the</span>
        <span class="c1"># coalesced tensor into the c10d CUDA stream for reduction</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">_queue_reduction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
                                       <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">],</span>
                                       <span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction_works</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buckets_coalesced</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_sync_reduction_works</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Now only work on the first GPU of self.device_ids</span>
        <span class="c1"># _sync_reduction will use a seperate CUDA stream to uncoalesce</span>
        <span class="c1"># the coalesced tensors to achieve more parallelisms</span>
        <span class="k">for</span> <span class="n">bucket_idx</span><span class="p">,</span> <span class="n">grads_batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buckets</span><span class="p">):</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">_sync_reduction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction_works</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">],</span>
                                 <span class="n">grads_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">buckets_coalesced</span><span class="p">[</span><span class="n">bucket_idx</span><span class="p">])</span>

        <span class="c1"># Reset the module states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">next_bucket</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ready_buckets_not_reduced</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction_works</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">devs_ready</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">buckets</span> <span class="o">=</span> <span class="p">[[[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span>
                        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buckets_coalesced</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buckets_ready_size</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_ids</span><span class="p">))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket_sizes</span><span class="p">))]</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script type="text/javascript" src="../../../../_static/jquery.js"></script>
         <script type="text/javascript" src="../../../../_static/underscore.js"></script>
         <script type="text/javascript" src="../../../../_static/doctools.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="../../../../_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>
<img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
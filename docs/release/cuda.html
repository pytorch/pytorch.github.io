


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.cuda &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/cuda.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.Storage" href="storage.html" />
    <link rel="prev" title="torch.sparse" href="sparse.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.0.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_deprecated.html">torch.distributed.deprecated</a></li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/index.html">torchvision</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.cuda</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/cuda.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="module-torch.cuda">
<span id="torch-cuda"></span><h1>torch.cuda<a class="headerlink" href="#module-torch.cuda" title="Permalink to this headline">¶</a></h1>
<p>This package adds support for CUDA tensor types, that implement the same
function as CPU tensors, but they utilize GPUs for computation.</p>
<p>It is lazily initialized, so you can always import it, and use
<a class="reference internal" href="#torch.cuda.is_available" title="torch.cuda.is_available"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_available()</span></code></a> to determine if your system supports CUDA.</p>
<p><a class="reference internal" href="notes/cuda.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> has more details about working with CUDA.</p>
<dl class="function">
<dt id="torch.cuda.current_blas_handle">
<code class="descclassname">torch.cuda.</code><code class="descname">current_blas_handle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#current_blas_handle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.current_blas_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cublasHandle_t pointer to current cuBLAS handle</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.current_device">
<code class="descclassname">torch.cuda.</code><code class="descname">current_device</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#current_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.current_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the index of a currently selected device.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.current_stream">
<code class="descclassname">torch.cuda.</code><code class="descname">current_stream</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#current_stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.current_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a currently selected <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a>.</p>
</dd></dl>

<dl class="class">
<dt id="torch.cuda.device">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">device</code><span class="sig-paren">(</span><em>device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that changes the selected device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – device index to select. It’s a no-op if
this argument is a negative integer or <code class="docutils literal notranslate"><span class="pre">None</span></code>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.device_count">
<code class="descclassname">torch.cuda.</code><code class="descname">device_count</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#device_count"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of GPUs available.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.cuda.device_ctx_manager">
<code class="descclassname">torch.cuda.</code><code class="descname">device_ctx_manager</code><a class="headerlink" href="#torch.cuda.device_ctx_manager" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.device</span></code></a></p>
</dd></dl>

<dl class="class">
<dt id="torch.cuda.device_of">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">device_of</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#device_of"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.device_of" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that changes the current device to that of given object.</p>
<p>You can use both tensors and storages as arguments. If a given object is
not allocated on a GPU, this is a no-op.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>obj</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) – object allocated on the selected device.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.empty_cache">
<code class="descclassname">torch.cuda.</code><code class="descname">empty_cache</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#empty_cache"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.empty_cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Releases all unoccupied cached memory currently held by the caching
allocator so that those can be used in other GPU application and visible in
<cite>nvidia-smi</cite>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code class="xref py py-meth docutils literal notranslate"><span class="pre">empty_cache()</span></code></a> doesn’t increase the amount of GPU
memory available for PyTorch. See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for
more details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_device_capability">
<code class="descclassname">torch.cuda.</code><code class="descname">get_device_capability</code><span class="sig-paren">(</span><em>device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#get_device_capability"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_device_capability" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the cuda capability of a device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – device for which to return the
device capability. This function is a no-op if this argument is
a negative integer. Uses the current device, given by
<a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code>
(default).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">the major and minor cuda capability of the device</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)">tuple</a>(<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)">int</a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)">int</a>)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_device_name">
<code class="descclassname">torch.cuda.</code><code class="descname">get_device_name</code><span class="sig-paren">(</span><em>device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#get_device_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_device_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the name of a device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – device for which to return the
name. This function is a no-op if this argument is a negative
integer. Uses the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.init">
<code class="descclassname">torch.cuda.</code><code class="descname">init</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#init"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.init" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize PyTorch’s CUDA state.  You may need to call
this explicitly if you are interacting with PyTorch via
its C API, as Python bindings for CUDA functionality will not
be until this initialization takes place.  Ordinary users
should not need this, as all of PyTorch’s CUDA methods
automatically initialize CUDA state on-demand.</p>
<p>Does nothing if the CUDA state is already initialized.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.is_available">
<code class="descclassname">torch.cuda.</code><code class="descname">is_available</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#is_available"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.is_available" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a bool indicating if CUDA is currently available.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.max_memory_allocated">
<code class="descclassname">torch.cuda.</code><code class="descname">max_memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#max_memory_allocated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.max_memory_allocated" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the maximum GPU memory usage by tensors in bytes for a given
device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.max_memory_cached">
<code class="descclassname">torch.cuda.</code><code class="descname">max_memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#max_memory_cached"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.max_memory_cached" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the maximum GPU memory managed by the caching allocator in bytes
for a given device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_allocated">
<code class="descclassname">torch.cuda.</code><code class="descname">memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#memory_allocated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.memory_allocated" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current GPU memory usage by tensors in bytes for a given
device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is likely less than the amount shown in <cite>nvidia-smi</cite> since some
unused memory can be held by the caching allocator and some context
needs to be created on GPU. See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more
details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_cached">
<code class="descclassname">torch.cuda.</code><code class="descname">memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#memory_cached"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.memory_cached" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current GPU memory managed by the caching allocator in bytes
for a given device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_device">
<code class="descclassname">torch.cuda.</code><code class="descname">set_device</code><span class="sig-paren">(</span><em>device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#set_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.set_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the current device.</p>
<p>Usage of this function is discouraged in favor of <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref any py py-class docutils literal notranslate"><span class="pre">device</span></code></a>. In most
cases it’s better to use <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environmental variable.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – selected device. This function is a no-op
if this argument is negative.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.stream">
<code class="descclassname">torch.cuda.</code><code class="descname">stream</code><span class="sig-paren">(</span><em>stream</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that selects a given stream.</p>
<p>All CUDA kernels queued within its context will be enqueued on a selected
stream.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>stream</strong> (<a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a>) – selected stream. This manager is a no-op if it’s
<code class="docutils literal notranslate"><span class="pre">None</span></code>.</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Streams are per-device, and this function changes the “current
stream” only for the currently selected device.  It is illegal to select
a stream that belongs to a different device.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.synchronize">
<code class="descclassname">torch.cuda.</code><code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#synchronize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all kernels in all streams on current device to complete.</p>
</dd></dl>

<div class="section" id="random-number-generator">
<h2>Random Number Generator<a class="headerlink" href="#random-number-generator" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.cuda.get_rng_state">
<code class="descclassname">torch.cuda.</code><code class="descname">get_rng_state</code><span class="sig-paren">(</span><em>device=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/random.html#get_rng_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the random number generator state of the current
GPU as a ByteTensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – The device to return the RNG state of.
Default: -1 (i.e., use the current device).</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function eagerly initializes CUDA.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_rng_state">
<code class="descclassname">torch.cuda.</code><code class="descname">set_rng_state</code><span class="sig-paren">(</span><em>new_state</em>, <em>device=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/random.html#set_rng_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.set_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state of the current GPU.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>new_state</strong> (<a class="reference internal" href="tensors.html#torch.ByteTensor" title="torch.ByteTensor"><em>torch.ByteTensor</em></a>) – The desired state</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.manual_seed">
<code class="descclassname">torch.cuda.</code><code class="descname">manual_seed</code><span class="sig-paren">(</span><em>seed</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/random.html#manual_seed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.manual_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers for the current GPU.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The desired seed.</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If you are working with a multi-GPU model, this function is insufficient
to get determinism.  To seed all GPUs, use <a class="reference internal" href="#torch.cuda.manual_seed_all" title="torch.cuda.manual_seed_all"><code class="xref py py-func docutils literal notranslate"><span class="pre">manual_seed_all()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.manual_seed_all">
<code class="descclassname">torch.cuda.</code><code class="descname">manual_seed_all</code><span class="sig-paren">(</span><em>seed</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/random.html#manual_seed_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.manual_seed_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers on all GPUs.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The desired seed.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.seed">
<code class="descclassname">torch.cuda.</code><code class="descname">seed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/random.html#seed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers to a random number for the current GPU.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If you are working with a multi-GPU model, this function will only initialize
the seed on one GPU.  To initialize all GPUs, use <a class="reference internal" href="#torch.cuda.seed_all" title="torch.cuda.seed_all"><code class="xref py py-func docutils literal notranslate"><span class="pre">seed_all()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.seed_all">
<code class="descclassname">torch.cuda.</code><code class="descname">seed_all</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/random.html#seed_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.seed_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers to a random number on all GPUs.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.initial_seed">
<code class="descclassname">torch.cuda.</code><code class="descname">initial_seed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/random.html#initial_seed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.initial_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current random seed of the current GPU.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function eagerly initializes CUDA.</p>
</div>
</dd></dl>

</div>
<div class="section" id="communication-collectives">
<h2>Communication collectives<a class="headerlink" href="#communication-collectives" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.cuda.comm.broadcast">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">broadcast</code><span class="sig-paren">(</span><em>tensor</em>, <em>devices</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/comm.html#broadcast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts a tensor to a number of GPUs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor to broadcast.</li>
<li><strong>devices</strong> (<em>Iterable</em>) – an iterable of devices among which to broadcast.
Note that it should be like (src, dst1, dst2, …), the first element
of which is the source device to broadcast from.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tuple containing copies of the <code class="docutils literal notranslate"><span class="pre">tensor</span></code>, placed on devices
corresponding to indices from <code class="docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.broadcast_coalesced">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">broadcast_coalesced</code><span class="sig-paren">(</span><em>tensors</em>, <em>devices</em>, <em>buffer_size=10485760</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/comm.html#broadcast_coalesced"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.broadcast_coalesced" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts a sequence tensors to the specified GPUs.
Small tensors are first coalesced into a buffer to reduce the number
of synchronizations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensors</strong> (<em>sequence</em>) – tensors to broadcast.</li>
<li><strong>devices</strong> (<em>Iterable</em>) – an iterable of devices among which to broadcast.
Note that it should be like (src, dst1, dst2, …), the first element
of which is the source device to broadcast from.</li>
<li><strong>buffer_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – maximum size of the buffer used for coalescing</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tuple containing copies of the <code class="docutils literal notranslate"><span class="pre">tensor</span></code>, placed on devices
corresponding to indices from <code class="docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.reduce_add">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">reduce_add</code><span class="sig-paren">(</span><em>inputs</em>, <em>destination=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/comm.html#reduce_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.reduce_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Sums tensors from multiple GPUs.</p>
<p>All inputs should have matching shapes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>inputs</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterable of tensors to add.</li>
<li><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – a device on which the output will be
placed (default: current device).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tensor containing an elementwise sum of all inputs, placed on the
<code class="docutils literal notranslate"><span class="pre">destination</span></code> device.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.scatter">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">scatter</code><span class="sig-paren">(</span><em>tensor</em>, <em>devices</em>, <em>chunk_sizes=None</em>, <em>dim=0</em>, <em>streams=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/comm.html#scatter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatters tensor across multiple GPUs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor to scatter.</li>
<li><strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – iterable of ints, specifying among which
devices the tensor should be scattered.</li>
<li><strong>chunk_sizes</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – sizes of chunks to be placed on
each device. It should match <code class="docutils literal notranslate"><span class="pre">devices</span></code> in length and sum to
<code class="docutils literal notranslate"><span class="pre">tensor.size(dim)</span></code>. If not specified, the tensor will be divided
into equal chunks.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – A dimension along which to chunk the tensor.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tuple containing chunks of the <code class="docutils literal notranslate"><span class="pre">tensor</span></code>, spread across given
<code class="docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.gather">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">gather</code><span class="sig-paren">(</span><em>tensors</em>, <em>dim=0</em>, <em>destination=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/comm.html#gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers tensors from multiple GPUs.</p>
<p>Tensor sizes in all dimension different than <code class="docutils literal notranslate"><span class="pre">dim</span></code> have to match.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensors</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – iterable of tensors to gather.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – a dimension along which the tensors will be concatenated.</li>
<li><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – output device (-1 means CPU, default:
current device)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tensor located on <code class="docutils literal notranslate"><span class="pre">destination</span></code> device, that is a result of
concatenating <code class="docutils literal notranslate"><span class="pre">tensors</span></code> along <code class="docutils literal notranslate"><span class="pre">dim</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="streams-and-events">
<h2>Streams and events<a class="headerlink" href="#streams-and-events" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch.cuda.Stream">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">Stream</code><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around a CUDA stream.</p>
<p>A CUDA stream is a linear sequence of execution that belongs to a specific
device, independent from other streams.  See <a class="reference internal" href="notes/cuda.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> for
details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – a device on which to allocate
the stream. If <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default) or a negative
integer, this will use the current device.</li>
<li><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – priority of the stream. Lower numbers
represent higher priorities.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.cuda.Stream.query">
<code class="descname">query</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.query"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.query" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if all the work submitted has been completed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A boolean indicating if all kernels in this stream are completed.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.record_event">
<code class="descname">record_event</code><span class="sig-paren">(</span><em>event=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.record_event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.record_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Records an event.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>event</strong> (<a class="reference internal" href="#torch.cuda.Event" title="torch.cuda.Event"><em>Event</em></a><em>, </em><em>optional</em>) – event to record. If not given, a new one
will be allocated.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Recorded event.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.synchronize">
<code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.synchronize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Wait for all the kernels in this stream to complete.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is a wrapper around <code class="docutils literal notranslate"><span class="pre">cudaStreamSynchronize()</span></code>: see
<a class="reference external" href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html">CUDA documentation</a> for more info.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.wait_event">
<code class="descname">wait_event</code><span class="sig-paren">(</span><em>event</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.wait_event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.wait_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes all future work submitted to the stream wait for an event.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>event</strong> (<a class="reference internal" href="#torch.cuda.Event" title="torch.cuda.Event"><em>Event</em></a>) – an event to wait for.</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>This is a wrapper around <code class="docutils literal notranslate"><span class="pre">cudaStreamWaitEvent()</span></code>: see <a class="reference external" href="http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html">CUDA
documentation</a> for more info.</p>
<p class="last">This function returns without waiting for <code class="xref py py-attr docutils literal notranslate"><span class="pre">event</span></code>: only future
operations are affected.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.wait_stream">
<code class="descname">wait_stream</code><span class="sig-paren">(</span><em>stream</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.wait_stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.wait_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Synchronizes with another stream.</p>
<p>All future work submitted to this stream will wait until all kernels
submitted to a given stream at the time of call complete.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>stream</strong> (<a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a>) – a stream to synchronize.</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function returns without waiting for currently enqueued
kernels in <a class="reference internal" href="#torch.cuda.stream" title="torch.cuda.stream"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stream</span></code></a>: only future operations are affected.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.cuda.Event">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">Event</code><span class="sig-paren">(</span><em>enable_timing=False</em>, <em>blocking=False</em>, <em>interprocess=False</em>, <em>_handle=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around CUDA event.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>enable_timing</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – indicates if the event should measure time
(default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</li>
<li><strong>blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, <a class="reference internal" href="#torch.cuda.Event.wait" title="torch.cuda.Event.wait"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wait()</span></code></a> will be blocking (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</li>
<li><strong>interprocess</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the event can be shared between processes
(default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.cuda.Event.elapsed_time">
<code class="descname">elapsed_time</code><span class="sig-paren">(</span><em>end_event</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.elapsed_time"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.elapsed_time" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the time elapsed before the event was recorded.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.ipc_handle">
<code class="descname">ipc_handle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.ipc_handle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.ipc_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an IPC handle of this event.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.query">
<code class="descname">query</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.query"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.query" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if the event has been recorded.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A boolean indicating if the event has been recorded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.record">
<code class="descname">record</code><span class="sig-paren">(</span><em>stream=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.record"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.record" title="Permalink to this definition">¶</a></dt>
<dd><p>Records the event in a given stream.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.synchronize">
<code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.synchronize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Synchronizes with the event.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.wait">
<code class="descname">wait</code><span class="sig-paren">(</span><em>stream=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.wait"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.wait" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes a given stream wait for the event.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="memory-management">
<h2>Memory management<a class="headerlink" href="#memory-management" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">empty_cache</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#empty_cache"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Releases all unoccupied cached memory currently held by the caching
allocator so that those can be used in other GPU application and visible in
<cite>nvidia-smi</cite>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code class="xref py py-meth docutils literal notranslate"><span class="pre">empty_cache()</span></code></a> doesn’t increase the amount of GPU
memory available for PyTorch. See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for
more details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#memory_allocated"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Returns the current GPU memory usage by tensors in bytes for a given
device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is likely less than the amount shown in <cite>nvidia-smi</cite> since some
unused memory can be held by the caching allocator and some context
needs to be created on GPU. See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more
details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">max_memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#max_memory_allocated"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Returns the maximum GPU memory usage by tensors in bytes for a given
device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#memory_cached"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Returns the current GPU memory managed by the caching allocator in bytes
for a given device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">max_memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#max_memory_cached"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Returns the maximum GPU memory managed by the caching allocator in bytes
for a given device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

</div>
<div class="section" id="nvidia-tools-extension-nvtx">
<h2>NVIDIA Tools Extension (NVTX)<a class="headerlink" href="#nvidia-tools-extension-nvtx" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.cuda.nvtx.mark">
<code class="descclassname">torch.cuda.nvtx.</code><code class="descname">mark</code><span class="sig-paren">(</span><em>msg</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/nvtx.html#mark"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.nvtx.mark" title="Permalink to this definition">¶</a></dt>
<dd><p>Describe an instantaneous event that occurred at some point.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>msg</strong> (<em>string</em>) – ASCII message to associate with the event.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.nvtx.range_push">
<code class="descclassname">torch.cuda.nvtx.</code><code class="descname">range_push</code><span class="sig-paren">(</span><em>msg</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/nvtx.html#range_push"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.nvtx.range_push" title="Permalink to this definition">¶</a></dt>
<dd><p>Pushes a range onto a stack of nested range span.  Returns zero-based
depth of the range that is started.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>msg</strong> (<em>string</em>) – ASCII message to associate with range</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.nvtx.range_pop">
<code class="descclassname">torch.cuda.nvtx.</code><code class="descname">range_pop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/nvtx.html#range_pop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.nvtx.range_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Pops a range off of a stack of nested range spans.  Returns the
zero-based depth of the range that is ended.</p>
</dd></dl>

</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="storage.html" class="btn btn-neutral float-right" title="torch.Storage" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="sparse.html" class="btn btn-neutral" title="torch.sparse" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.cuda</a><ul>
<li><a class="reference internal" href="#random-number-generator">Random Number Generator</a></li>
<li><a class="reference internal" href="#communication-collectives">Communication collectives</a></li>
<li><a class="reference internal" href="#streams-and-events">Streams and events</a></li>
<li><a class="reference internal" href="#memory-management">Memory management</a></li>
<li><a class="reference internal" href="#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script type="text/javascript" src="_static/jquery.js"></script>
         <script type="text/javascript" src="_static/underscore.js"></script>
         <script type="text/javascript" src="_static/doctools.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.9.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>
<img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
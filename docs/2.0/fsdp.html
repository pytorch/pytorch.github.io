


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>FullyShardedDataParallel &mdash; PyTorch 2.0 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/fsdp.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Distributed Optimizers" href="distributed.optim.html" />
    <link rel="prev" title="TorchElastic Kubernetes" href="elastic/kubernetes.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>2.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dynamo/index.html">TorchDynamo Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamo/installation.html">Installing TorchDynamo</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamo/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamo/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamo/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamo/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamo/troubleshooting.html">TorchDynamo Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamo/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="ir.html">IRs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="_dynamo.html">torch._dynamo</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>FullyShardedDataParallel</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/fsdp.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="module-torch.distributed.fsdp">
<span id="fullyshardeddataparallel"></span><h1>FullyShardedDataParallel<a class="headerlink" href="#module-torch.distributed.fsdp" title="Permalink to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">FullyShardedDataParallel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharding_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cpu_offload</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_wrap_policy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_prefetch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">BackwardPrefetch.BACKWARD_PRE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixed_precision</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignored_modules</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_module_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_prefetch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">limit_all_gathers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_orig_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignored_parameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>A wrapper for sharding Module parameters across data parallel workers. This
is inspired by <a class="reference external" href="https://arxiv.org/abs/2004.13336">Xu et al.</a> as well as the ZeRO Stage 3 from <a class="reference external" href="https://www.deepspeed.ai/">DeepSpeed</a>.
FullyShardedDataParallel is commonly shortened to FSDP.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sharded_module</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">my_module</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">sharded_module</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">sharded_module</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The optimizer must be initialized <em>after</em> the module has been wrapped,
since FSDP will shard parameters in-place and this will break any
previously initialized optimizers.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the destination CUDA device has ID <code class="docutils literal notranslate"><span class="pre">dev_id</span></code>, either (1)
<code class="docutils literal notranslate"><span class="pre">module</span></code> should already be placed on that device, (2) the device
should be set using <code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device(dev_id)</span></code>, or (3)
<code class="docutils literal notranslate"><span class="pre">dev_id</span></code> should be passed into the <code class="docutils literal notranslate"><span class="pre">device_id</span></code> constructor
argument. This FSDP instance’s compute device will be that destination
device. For (1) and (3), the FSDP initialization always occurs on GPU.
For (2), the FSDP initialization happens on <code class="docutils literal notranslate"><span class="pre">module</span></code> ‘s current
device, which may be CPU.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>FSDP currently does not support gradient accumulation outside
<code class="docutils literal notranslate"><span class="pre">no_sync()</span></code> when using CPU offloading. Trying to do so yields
incorrect results since FSDP will use the newly-reduced gradient
instead of accumulating with any existing gradient.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Changing the original parameter variable names after construction will
lead to undefined behavior.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Passing in <cite>sync_module_states=True</cite> flag requires module to be put
on GPU, or to use <code class="docutils literal notranslate"><span class="pre">device_id</span></code> argument to specify a CUDA device that
FSDP will move module to. This is because <code class="docutils literal notranslate"><span class="pre">sync_module_states=True</span></code>
requires GPU communication.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As of PyTorch 1.12, FSDP only offers limited support for shared parameters
(for example, setting one <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer’s weight to another’s). In
particular, modules that share parameters must be wrapped as part of the
same FSDP unit. If enhanced shared parameter support is needed for your
use case, please ping <a class="reference external" href="https://github.com/pytorch/pytorch/issues/77724">https://github.com/pytorch/pytorch/issues/77724</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Inputs into FSDP <code class="docutils literal notranslate"><span class="pre">forward</span></code> function will be moved to compute device
(same device FSDP module is on) before running <code class="docutils literal notranslate"><span class="pre">forward</span></code>, so user does
not have to manually move inputs from CPU -&gt; GPU.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – This is the module to be wrapped with FSDP.</p></li>
<li><p><strong>process_group</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.11)"><em>Optional</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.11)"><em>Union</em></a><em>[</em><em>ProcessGroup</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.11)"><em>Tuple</em></a><em>[</em><em>ProcessGroup</em><em>, </em><em>ProcessGroup</em><em>]</em><em>]</em><em>]</em>) – Optional[Union[ProcessGroup, Tuple[ProcessGroup, ProcessGroup]]]
This is the process group used for collective communications and
the one over which the model is sharded. For hybrid sharding strategies such as
<code class="docutils literal notranslate"><span class="pre">ShardingStrategy.HYBRID_SHARD</span></code> users can
pass in a tuple of process groups representing the groups to shard and replicate across,
respectively.</p></li>
<li><p><strong>sharding_strategy</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.fsdp.ShardingStrategy" title="torch.distributed.fsdp.ShardingStrategy"><em>ShardingStrategy</em></a><em>]</em>) – This configures the sharding strategy used by FSDP, which may trade
off memory saving and communication overhead. See
<a class="reference internal" href="#torch.distributed.fsdp.ShardingStrategy" title="torch.distributed.fsdp.ShardingStrategy"><code class="xref py py-class docutils literal notranslate"><span class="pre">ShardingStrategy</span></code></a> for details. (Default: <code class="docutils literal notranslate"><span class="pre">FULL_SHARD</span></code>)</p></li>
<li><p><strong>cpu_offload</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.fsdp.CPUOffload" title="torch.distributed.fsdp.CPUOffload"><em>CPUOffload</em></a><em>]</em>) – This configures CPU offloading. If this is set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, then
no CPU offloading happens. See <a class="reference internal" href="#torch.distributed.fsdp.CPUOffload" title="torch.distributed.fsdp.CPUOffload"><code class="xref py py-class docutils literal notranslate"><span class="pre">CPUOffload</span></code></a> for details.
(Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>auto_wrap_policy</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>Callable</em><em>[</em><em>[</em><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>]</em><em>, </em><em>_FSDPPolicy</em><em>]</em><em>]</em>) – <p>This is either <code class="docutils literal notranslate"><span class="pre">None</span></code>, an <code class="docutils literal notranslate"><span class="pre">_FSDPPolicy</span></code>, or a callable of
a fixed signature. If it is <code class="docutils literal notranslate"><span class="pre">None</span></code>, then <code class="docutils literal notranslate"><span class="pre">module</span></code> is wrapped
with only a top-level FSDP instance without any nested wrapping. If
it is an <code class="docutils literal notranslate"><span class="pre">_FSDPPolicy</span></code>, then the wrapping follows the given
policy. <code class="docutils literal notranslate"><span class="pre">ModuleWrapPolicy</span></code> in <code class="docutils literal notranslate"><span class="pre">torch.distributed.fsdp.wrap.py</span></code>
is an example. If it is a callable, then it should take in three
arguments <code class="docutils literal notranslate"><span class="pre">module:</span> <span class="pre">nn.Module</span></code>, <code class="docutils literal notranslate"><span class="pre">recurse:</span> <span class="pre">bool</span></code>, and
<code class="docutils literal notranslate"><span class="pre">nonwrapped_numel:</span> <span class="pre">int</span></code> and should return a <code class="docutils literal notranslate"><span class="pre">bool</span></code> specifying
whether the passed-in <code class="docutils literal notranslate"><span class="pre">module</span></code> should be wrapped if
<code class="docutils literal notranslate"><span class="pre">recurse=False</span></code> or if the traversal should continue down the
subtree if <code class="docutils literal notranslate"><span class="pre">recurse=True</span></code>. Additional custom arguments may be
added to the callable. The <code class="docutils literal notranslate"><span class="pre">size_based_auto_wrap_policy</span></code> in
<code class="docutils literal notranslate"><span class="pre">torch.distributed.fsdp.wrap.py</span></code> gives an example callable that
wraps a module if the parameters in its subtree exceed 100M numel.
A good practice is to print the model after wrapping and adjust as
needed.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">custom_auto_wrap_policy</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">recurse</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">nonwrapped_numel</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Additional custom arguments</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">min_num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e8</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">nonwrapped_numel</span> <span class="o">&gt;=</span> <span class="n">min_num_params</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Configure a custom `min_num_params`</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">my_auto_wrap_policy</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">custom_auto_wrap_policy</span><span class="p">,</span> <span class="n">min_num_params</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">))</span>
</pre></div>
</div>
</p></li>
<li><p><strong>backward_prefetch</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.fsdp.BackwardPrefetch" title="torch.distributed.fsdp.BackwardPrefetch"><em>BackwardPrefetch</em></a><em>]</em>) – This configures explicit backward prefetching of all-gathers. See
<a class="reference internal" href="#torch.distributed.fsdp.BackwardPrefetch" title="torch.distributed.fsdp.BackwardPrefetch"><code class="xref py py-class docutils literal notranslate"><span class="pre">BackwardPrefetch</span></code></a> for details. (Default: <code class="docutils literal notranslate"><span class="pre">BACKWARD_PRE</span></code>)</p></li>
<li><p><strong>mixed_precision</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#torch.distributed.fsdp.MixedPrecision" title="torch.distributed.fsdp.MixedPrecision"><em>MixedPrecision</em></a><em>]</em>) – This configures native mixed precision for FSDP. If this is set to
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then no mixed precision is used. Otherwise, parameter,
buffer, and gradient reduction dtypes can be set. See
<a class="reference internal" href="#torch.distributed.fsdp.MixedPrecision" title="torch.distributed.fsdp.MixedPrecision"><code class="xref py py-class docutils literal notranslate"><span class="pre">MixedPrecision</span></code></a> for details. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>ignored_modules</strong> (<em>Optional</em><em>[</em><em>Iterable</em><em>[</em><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a><em>]</em><em>]</em>) – Modules whose
own parameters and child modules’ parameters and buffers are
ignored by this instance. None of the modules directly in
<code class="docutils literal notranslate"><span class="pre">ignored_modules</span></code> should be <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a>
instances, and any child modules that are already-constructed
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a> instances will not be ignored if
they are nested under this instance. This argument may be used to
avoid sharding specific parameters at module granularity when using an
<code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code> or if parameters’ sharding is not managed by
FSDP. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>param_init_fn</strong> (<em>Optional</em><em>[</em><em>Callable</em><em>[</em><em>[</em><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a><em>]</em><em>, </em><em>None</em><em>]</em><em>]</em>) – <p>A <code class="docutils literal notranslate"><span class="pre">Callable[torch.nn.Module]</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code> that
specifies how modules that are currently on the meta device should be initialized
onto an actual device. Note that as of v1.12, we detect modules on the meta
device via <code class="docutils literal notranslate"><span class="pre">is_meta</span></code> check and apply a default initialization that calls
<code class="docutils literal notranslate"><span class="pre">reset_parameters</span></code> method on the passed in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> if <code class="docutils literal notranslate"><span class="pre">param_init_fn</span></code>
is not specified, otherwise we run <code class="docutils literal notranslate"><span class="pre">param_init_fn</span></code> to initialize the passed
in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>. In particular, this means that if <code class="docutils literal notranslate"><span class="pre">is_meta=True</span></code> for any
module parameters for modules that will be wrapped with FSDP and <code class="docutils literal notranslate"><span class="pre">param_init_fn</span></code>
is not specified, we assume your module properly implements a <code class="docutils literal notranslate"><span class="pre">reset_parameters()</span></code>
and will throw errors if not. Note that additionally, we offer support for modules
initialized with torchdistX’s (<a class="reference external" href="https://github.com/pytorch/torchdistX">https://github.com/pytorch/torchdistX</a>)
<code class="docutils literal notranslate"><span class="pre">deferred_init</span></code> API. In this case, deferred modules would be initialized
by a default initialization function that calls torchdistX’s
<code class="docutils literal notranslate"><span class="pre">materialize_module</span></code>, or the passed in <code class="docutils literal notranslate"><span class="pre">param_init_fn</span></code>, if it is not
<code class="docutils literal notranslate"><span class="pre">None</span></code>. The same <code class="docutils literal notranslate"><span class="pre">Callable</span></code> is applied to initialize all meta modules.
Note that this initialization function is applied before doing any FSDP sharding
logic.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;meta&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_init_fn</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># responsible for initializing a module, such as with reset_parameters</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fsdp_model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">param_init_fn</span><span class="o">=</span><span class="n">my_init_fn</span><span class="p">,</span> <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">size_based_auto_wrap_policy</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">fsdp_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="c1"># current CUDA device</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With torchdistX</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">deferred_init</span><span class="o">.</span><span class="n">deferred_init</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Will initialize via deferred_init.materialize_module().</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fsdp_model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">size_based_auto_wrap_policy</span><span class="p">)</span>
</pre></div>
</div>
</p></li>
<li><p><strong>device_id</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a><em>, </em><a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a><em>]</em><em>]</em>) – An <code class="docutils literal notranslate"><span class="pre">int</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.device</span></code>
describing the CUDA device the FSDP module should be moved to determining where
initialization such as sharding takes place. If this argument is not specified
and <code class="docutils literal notranslate"><span class="pre">module</span></code> is on CPU, we issue a warning mentioning that this argument can
be specified for faster initialization. If specified, resulting FSDP instances
will reside on this device, including moving ignored modules’ parameters if
needed. Note that if <code class="docutils literal notranslate"><span class="pre">device_id</span></code> is specified but <code class="docutils literal notranslate"><span class="pre">module</span></code> is already on a
different CUDA device, an error will be thrown. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>sync_module_states</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, each individually wrapped FSDP unit will broadcast
module parameters from rank 0 to ensure they are the same across all ranks after
initialization. This helps ensure model parameters are the same across ranks
before starting training, but adds communication overhead to <code class="docutils literal notranslate"><span class="pre">__init__</span></code>, as at least
one broadcast is triggered per individually wrapped FSDP unit.
This can also help load checkpoints taken by <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> and to be loaded by
<code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> in a memory efficient way. See documentation for
<code class="xref py py-class docutils literal notranslate"><span class="pre">FullStateDictConfig</span></code> for an example of this. (Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>forward_prefetch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then FSDP <em>explicitly</em> prefetches
the next upcoming all-gather while executing in the forward pass.
This may improve communication and computation overlap for CPU
bound workloads. This should only be used for static graph models
since the forward order is fixed based on the first iteration’s
execution. (Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>limit_all_gathers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then FSDP allows the CPU
thread to schedule all-gathers without any extra synchronization.
If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then FSDP explicitly synchronizes the CPU thread to
prevent too many in-flight all-gathers. This <code class="docutils literal notranslate"><span class="pre">bool</span></code> only affects
the sharded strategies that schedule all-gathers. Enabling this can
help lower the number of CUDA malloc retries.</p></li>
<li><p><strong>ignored_parameters</strong> (<em>Optional</em><em>[</em><em>Iterable</em><em>[</em><em>torch.nn.Parameter</em><em>]</em><em>]</em>) – Ignored
parameters will not be managed by this FSDP instance,
that means these parameters will not be flattened and sharded by FSDP,
their gradients will not be synchronized as well. With this newly added
argument, <code class="docutils literal notranslate"><span class="pre">ignored_modules</span></code> could be deprecated soon. For backward compatibility,
both <code class="docutils literal notranslate"><span class="pre">ignored_parameters</span></code> and <code class="docutils literal notranslate"><span class="pre">ignored_modules</span></code> are kept for now,
but FSDP only allows one of them to be specified as not <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.apply"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <a class="reference internal" href="nn.init.html#nn-init-doc"><span class="std std-ref">torch.nn.init</span></a>).</p>
<p>Compared to <code class="docutils literal notranslate"><span class="pre">torch.nn.Module.apply</span></code>, this version additionally gathers
the full parameters before applying <code class="docutils literal notranslate"><span class="pre">fn</span></code>. It should not be called from
within another <code class="docutils literal notranslate"><span class="pre">summon_full_params</span></code> context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><p id="torch.nn.Module"/><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_">
<span class="sig-name descname"><span class="pre">clip_grad_norm_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_norm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.clip_grad_norm_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips the gradient norm of all parameters. The norm is computed over
all parameters’ gradients as viewed as a single vector, and the
gradients are modified in-place.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – max norm of the gradients</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.11)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.11)"><em>int</em></a>) – type of the used p-norm. Can be <code class="docutils literal notranslate"><span class="pre">'inf'</span></code>
for infinity norm.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Total norm of the parameters (viewed as a single vector).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If every FSDP instance uses <code class="docutils literal notranslate"><span class="pre">NO_SHARD</span></code>, meaning that no
gradients are sharded across ranks, then you may directly use
<a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_" title="torch.nn.utils.clip_grad_norm_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.clip_grad_norm_()</span></code></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If at least some FSDP instance uses a sharded strategy (i.e.
one other than <code class="docutils literal notranslate"><span class="pre">NO_SHARD</span></code>), then you should use this method
instead of <a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_" title="torch.nn.utils.clip_grad_norm_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.clip_grad_norm_()</span></code></a> since this method
handles the fact that gradients are sharded across ranks.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The total norm returned will have the “largest” dtype across
all parameters/gradients as defined by PyTorch’s type promotion
semantics. For example, if <em>all</em> parameters/gradients use a low
precision dtype, then the returned norm’s dtype will be that low
precision dtype, but if there exists at least one parameter/
gradient using FP32, then the returned norm’s dtype will be FP32.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This needs to be called on all ranks since it uses
collective communications.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">flatten_sharded_optim_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sharded_optim_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.flatten_sharded_optim_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>The API is similar to <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">shard_full_optim_state_dict()</span></code></a>. The only
difference is that the input <code class="docutils literal notranslate"><span class="pre">sharded_optim_state_dict</span></code> should be
returned from <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sharded_optim_state_dict()</span></code></a>. Therefore, there will
be all-gather calls on each rank to gather <code class="docutils literal notranslate"><span class="pre">ShardedTensor</span></code> s.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sharded_optim_state_dict</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em>, </em><em>Any</em><em>]</em>) – Optimizer state dict
corresponding to the unflattened parameters and holding the
sharded optimizer state.</p></li>
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Refer to :meth:<code class="docutils literal notranslate"><span class="pre">shard_full_optim_state_dict</span></code>.</p></li>
<li><p><strong>optim</strong> (<a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>torch.optim.Optimizer</em></a>) – Optimizer for <code class="docutils literal notranslate"><span class="pre">model</span></code> ‘s</p></li>
<li><p><strong>parameters.</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Refer to <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">shard_full_optim_state_dict()</span></code></a>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.11)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.11)"><em>Any</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs the forward pass for the wrapped module, inserting FSDP-specific
pre- and post-forward sharding logic.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.11)"><em>Any</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">fsdp_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.fsdp_modules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all nested FSDP instances, possibly including <code class="docutils literal notranslate"><span class="pre">module</span></code> itself
and only including FSDP root modules if <code class="docutils literal notranslate"><span class="pre">root_only=True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module, which may or may not be an
<code class="docutils literal notranslate"><span class="pre">FSDP</span></code> module.</p></li>
<li><p><strong>root_only</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – Whether to return only FSDP root modules.
(Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>FSDP modules that are nested in
the input <code class="docutils literal notranslate"><span class="pre">module</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel">FullyShardedDataParallel</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">full_optim_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank0_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.full_optim_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Consolidates the full optimizer state on rank 0 and returns it
as a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> following the convention of
<a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict" title="torch.optim.Optimizer.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.state_dict()</span></code></a>, i.e. with keys <code class="docutils literal notranslate"><span class="pre">&quot;state&quot;</span></code>
and <code class="docutils literal notranslate"><span class="pre">&quot;param_groups&quot;</span></code>. The flattened parameters in <code class="docutils literal notranslate"><span class="pre">FSDP</span></code> modules
contained in <code class="docutils literal notranslate"><span class="pre">model</span></code> are mapped back to their unflattened parameters.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This needs to be called on all ranks since it uses
collective communications. However, if <code class="docutils literal notranslate"><span class="pre">rank0_only=True</span></code>, then
the state dict is only populated on rank 0, and all other ranks
return an empty <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Unlike <code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer.state_dict()</span></code>, this method
uses full parameter names as keys instead of parameter IDs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Like in <a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict" title="torch.optim.Optimizer.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.state_dict()</span></code></a>, the tensors
contained in the optimizer state dict are not cloned, so there may
be aliasing surprises. For best practices, consider saving the
returned optimizer state dict immediately, e.g. using
<code class="docutils literal notranslate"><span class="pre">torch.save()</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module (which may or may not be a
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a> instance) whose parameters
were passed into the optimizer <code class="docutils literal notranslate"><span class="pre">optim</span></code>.</p></li>
<li><p><strong>optim</strong> (<a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>torch.optim.Optimizer</em></a>) – Optimizer for <code class="docutils literal notranslate"><span class="pre">model</span></code> ‘s
parameters.</p></li>
<li><p><strong>optim_input</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em>, </em><em>Any</em><em>]</em><em>]</em><em>, </em><em>Iterable</em><em>[</em><em>torch.nn.Parameter</em><em>]</em><em>]</em><em>]</em>) – Input passed into the optimizer <code class="docutils literal notranslate"><span class="pre">optim</span></code> representing either a
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a> of parameter groups or an iterable of parameters;
if <code class="docutils literal notranslate"><span class="pre">None</span></code>, then this method assumes the input was
<code class="docutils literal notranslate"><span class="pre">model.parameters()</span></code>. This argument is deprecated, and there
is no need to pass it in anymore. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>rank0_only</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, saves the populated <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>
only on rank 0; if <code class="docutils literal notranslate"><span class="pre">False</span></code>, saves it on all ranks. (Default:
<code class="docutils literal notranslate"><span class="pre">True</span></code>)</p></li>
<li><p><strong>group</strong> (<em>dist.ProcessGroup</em>) – Model’s process group or <code class="docutils literal notranslate"><span class="pre">None</span></code> if using
the default process group. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> containing the optimizer state for
<code class="docutils literal notranslate"><span class="pre">model</span></code> ‘s original unflattened parameters and including keys
“state” and “param_groups” following the convention of
<a class="reference internal" href="generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict" title="torch.optim.Optimizer.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.state_dict()</span></code></a>. If <code class="docutils literal notranslate"><span class="pre">rank0_only=True</span></code>,
then nonzero ranks return an empty <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.load_optim_state_dict_pre_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_optim_state_dict_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.load_optim_state_dict_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.load_optim_state_dict_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>This hook is intended be used by <code class="docutils literal notranslate"><span class="pre">torch.distributed.NamedOptimizer</span></code>.
The functionaility is identical to <code class="docutils literal notranslate"><span class="pre">:meth:optim_state_dict_to_load</span></code>
except for the different arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module (which may or may not be a
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a> instance) whose parameters
were passed into the optimizer <code class="docutils literal notranslate"><span class="pre">optim</span></code>.</p></li>
<li><p><strong>optim</strong> (<a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>torch.optim.Optimizer</em></a>) – Optimizer for <code class="docutils literal notranslate"><span class="pre">model</span></code> ‘s
parameters.</p></li>
<li><p><strong>optim_state_dict</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em>, </em><em>Any</em><em>]</em>) – The optimizer states to be loaded.</p></li>
<li><p><strong>group</strong> (<em>dist.ProcessGroup</em>) – Model’s process group across which parameters
are sharded or <code class="docutils literal notranslate"><span class="pre">None</span></code> if using the default process group. (
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.11)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.11)"><em>Any</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.module">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">module</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.modules.module.Module"><span class="pre">Module</span></a></em><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.module" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the wrapped module (like <code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.named_buffers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Overrides <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers" title="torch.distributed.fsdp.FullyShardedDataParallel.named_buffers"><code class="xref py py-meth docutils literal notranslate"><span class="pre">named_buffers()</span></code></a> to intercept buffer names and
remove all occurrences of the FSDP-specific flattened buffer prefix
when inside the <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params" title="torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params"><code class="xref py py-meth docutils literal notranslate"><span class="pre">summon_full_params()</span></code></a> context manager.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterator" title="(in Python v3.11)"><em>Iterator</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.11)"><em>Tuple</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.named_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Overrides <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters" title="torch.distributed.fsdp.FullyShardedDataParallel.named_parameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">named_parameters()</span></code></a> to intercept parameter names and
remove all occurrences of the FSDP-specific flattened parameter prefix
when inside the <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params" title="torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params"><code class="xref py py-meth docutils literal notranslate"><span class="pre">summon_full_params()</span></code></a> context manager.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterator" title="(in Python v3.11)"><em>Iterator</em></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.11)"><em>Tuple</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, <a class="reference internal" href="generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><em>Parameter</em></a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.no_sync">
<span class="sig-name descname"><span class="pre">no_sync</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.no_sync"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.no_sync" title="Permalink to this definition">¶</a></dt>
<dd><p>A context manager to disable gradient synchronizations across FSDP
instances. Within this context, gradients will be accumulated in module
variables, which will later be synchronized in the first
forward-backward pass after exiting the context. This should only be
used on the root FSDP instance and will recursively apply to all
children FSDP instances.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This likely results in higher memory usage because FSDP will
accumulate the full model gradients (instead of gradient shards)
until the eventual sync.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When used with CPU offloading, the gradients will not be
offloaded to CPU when inside the context manager. Instead, they
will only be offloaded right after the eventual sync.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Generator" title="(in Python v3.11)"><em>Generator</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">optim_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.optim_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state dict of <code class="docutils literal notranslate"><span class="pre">optim</span></code> for the <code class="docutils literal notranslate"><span class="pre">model</span></code> that is (partially)
sharded by FSDP. The state may be sharded, consolidated, or consolidated
on rank 0 only depending on the <code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code> set by
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type" title="torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_state_dict_type()</span></code></a> or <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type" title="torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict_type()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">StateDictType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullStateDictConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullOptimStateDictConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Save a checkpoint</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">FSDP</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">optim_state_dict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">save_a_checkpoint</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">optim_state_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Load a checkpoint</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span><span class="p">,</span> <span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">load_a_checkponit</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">FSDP</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">optim_state_dict_to_load</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optim_state_dict</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">optim_state_dict</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module (which may or may not be a
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a> instance) whose parameters
were passed into the optimizer <code class="docutils literal notranslate"><span class="pre">optim</span></code>.</p></li>
<li><p><strong>optim</strong> (<a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>torch.optim.Optimizer</em></a>) – Optimizer for <code class="docutils literal notranslate"><span class="pre">model</span></code> ‘s
parameters.</p></li>
<li><p><strong>group</strong> (<em>dist.ProcessGroup</em>) – Model’s process group across which parameters
are sharded or <code class="docutils literal notranslate"><span class="pre">None</span></code> if using the default process group. (
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> containing the optimizer state for
<code class="docutils literal notranslate"><span class="pre">model</span></code>. The sharding of the optimizer state is based on
<code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_post_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">optim_state_dict_post_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.optim_state_dict_post_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_post_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>This hook is intended be used by <code class="docutils literal notranslate"><span class="pre">torch.distributed.NamedOptimizer</span></code>.
The functionaility is identical to <code class="docutils literal notranslate"><span class="pre">:meth:optim_state_dict</span></code> except
for the different arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module (which may or may not be a
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a> instance) whose parameters
were passed into the optimizer <code class="docutils literal notranslate"><span class="pre">optim</span></code>.</p></li>
<li><p><strong>optim</strong> (<a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>torch.optim.Optimizer</em></a>) – Optimizer for <code class="docutils literal notranslate"><span class="pre">model</span></code> ‘s
parameters.</p></li>
<li><p><strong>(Dict[str</strong> (<em>optim</em>) – the optim_state_dict to be coverted. The value
is typically returned by <code class="docutils literal notranslate"><span class="pre">NamedOptimizer.state_dict()</span></code>.</p></li>
<li><p><strong>Any]</strong> – the optim_state_dict to be coverted. The value
is typically returned by <code class="docutils literal notranslate"><span class="pre">NamedOptimizer.state_dict()</span></code>.</p></li>
<li><p><strong>group</strong> (<em>dist.ProcessGroup</em>) – Model’s process group across which parameters
are sharded or <code class="docutils literal notranslate"><span class="pre">None</span></code> if using the default process group. (
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> containing the optimizer state for
<code class="docutils literal notranslate"><span class="pre">model</span></code>. The sharding of the optimizer state is based on
<code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">optim_state_dict_to_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optim_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_named_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.optim_state_dict_to_load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a saved <code class="docutils literal notranslate"><span class="pre">optim_state_dict</span></code>, converts it to the optimizer state_dict
that can be loaded to <code class="docutils literal notranslate"><span class="pre">optim</span></code> which is the optimizer for <code class="docutils literal notranslate"><span class="pre">model</span></code>.
<code class="docutils literal notranslate"><span class="pre">model</span></code> is (partially) sharded by FullyShardedDataParallel.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">StateDictType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullStateDictConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullOptimStateDictConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Save a checkpoint</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">FSDP</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">optim_state_dict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">save_a_checkpoint</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">optim_state_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Load a checkpoint</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span><span class="p">,</span> <span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">load_a_checkponit</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">FSDP</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">StateDictType</span><span class="o">.</span><span class="n">FULL_STATE_DICT</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">optim_state_dict_to_load</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optim_state_dict</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">optim_state_dict</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optim_state_dict</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em>, </em><em>Any</em><em>]</em>) – The optimizer states to be loaded.</p></li>
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module (which may or may not be a
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a> instance) whose parameters
were passed into the optimizer <code class="docutils literal notranslate"><span class="pre">optim</span></code>.</p></li>
<li><p><strong>optim</strong> (<a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>torch.optim.Optimizer</em></a>) – Optimizer for <code class="docutils literal notranslate"><span class="pre">model</span></code> ‘s
parameters.</p></li>
<li><p><strong>is_named_optimizer</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – Is this optimizer a NamedOptimizer or
KeyedOptimizer. Only set to True if <code class="docutils literal notranslate"><span class="pre">optim</span></code> is TorchRec’s
KeyedOptimizer or torch.distributed’s NamedOptimizer.</p></li>
<li><p><strong>group</strong> (<em>dist.ProcessGroup</em>) – Model’s process group across which parameters
are sharded or <code class="docutils literal notranslate"><span class="pre">None</span></code> if using the default process group. (
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.11)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.11)"><em>Any</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook">
<span class="sig-name descname"><span class="pre">register_comm_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.register_comm_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a communication hook which is an enhancement that provides a
flexible hook to users where they can specify how FSDP aggregates gradients
across multiple workers.
This hook can be used to implement several algorithms like
<a class="reference external" href="https://arxiv.org/abs/1803.05880">GossipGrad</a> and gradient compression
which involve different communication strategies for
parameter syncs while training with <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>FSDP communication hook should be registered before running an initial forward pass
and only once.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.11)"><em>object</em></a>) – <p>Passed to the hook to maintain any state information during the training process.
Examples include error feedback in gradient compression,
peers to communicate with next in <a class="reference external" href="https://arxiv.org/abs/1803.05880">GossipGrad</a>, etc.
It is locally stored by each worker
and shared by all the gradient tensors on the worker.</p>
</p></li>
<li><p><strong>hook</strong> (<em>Callable</em>) – Callable, which has one of the following signatures:
1) <code class="docutils literal notranslate"><span class="pre">hook:</span> <span class="pre">Callable[torch.Tensor]</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code>:
This function takes in a Python tensor, which represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units).
It then performs all necessary processing and returns <code class="docutils literal notranslate"><span class="pre">None</span></code>;
2) <code class="docutils literal notranslate"><span class="pre">hook:</span> <span class="pre">Callable[torch.Tensor,</span> <span class="pre">torch.Tensor]</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code>:
This function takes in two Python tensors, the first one represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units). The latter
represents a pre-sized tensor to store a chunk of a sharded gradient after
reduction.
In both cases, callable performs all necessary processing and returns <code class="docutils literal notranslate"><span class="pre">None</span></code>.
Callables with signature 1 are expected to handle gradient communication for a <cite>NO_SHARD</cite> case.
Callables with signature 2 are expected to handle gradient communication for sharded cases.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">rekey_optim_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optim_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_state_key_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.rekey_optim_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Re-keys the optimizer state dict <code class="docutils literal notranslate"><span class="pre">optim_state_dict</span></code> to use the key
type <code class="docutils literal notranslate"><span class="pre">optim_state_key_type</span></code>. This can be used to achieve
compatibility between optimizer state dicts from models with FSDP
instances and ones without.</p>
<p>To re-key an FSDP full optimizer state dict (i.e. from
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">full_optim_state_dict()</span></code></a>) to use parameter IDs and be loadable to
a non-wrapped model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">wrapped_model</span><span class="p">,</span> <span class="n">wrapped_optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">full_osd</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">full_optim_state_dict</span><span class="p">(</span><span class="n">wrapped_model</span><span class="p">,</span> <span class="n">wrapped_optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nonwrapped_model</span><span class="p">,</span> <span class="n">nonwrapped_optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rekeyed_osd</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">rekey_optim_state_dict</span><span class="p">(</span><span class="n">full_osd</span><span class="p">,</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_ID</span><span class="p">,</span> <span class="n">nonwrapped_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nonwrapped_optim</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">rekeyed_osd</span><span class="p">)</span>
</pre></div>
</div>
<p>To re-key a normal optimizer state dict from a non-wrapped model to be
loadable to a wrapped model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nonwrapped_model</span><span class="p">,</span> <span class="n">nonwrapped_optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">osd</span> <span class="o">=</span> <span class="n">nonwrapped_optim</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rekeyed_osd</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">rekey_optim_state_dict</span><span class="p">(</span><span class="n">osd</span><span class="p">,</span> <span class="n">OptimStateKeyType</span><span class="o">.</span><span class="n">PARAM_NAME</span><span class="p">,</span> <span class="n">nonwrapped_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wrapped_model</span><span class="p">,</span> <span class="n">wrapped_optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sharded_osd</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">shard_full_optim_state_dict</span><span class="p">(</span><span class="n">rekeyed_osd</span><span class="p">,</span> <span class="n">wrapped_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wrapped_optim</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">sharded_osd</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The optimizer state dict re-keyed using the
parameter keys specified by <code class="docutils literal notranslate"><span class="pre">optim_state_key_type</span></code>.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Dict[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">scatter_full_optim_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">full_optim_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.scatter_full_optim_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatters the full optimizer state dict from rank 0 to all other ranks,
returning the sharded optimizer state dict on each rank. The return
value is the same as <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">shard_full_optim_state_dict()</span></code></a>, and on rank
0, the first argument should be the return value of
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">full_optim_state_dict()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">full_osd</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">full_optim_state_dict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>  <span class="c1"># only non-empty on rank 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define new model with possibly different world size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_model</span><span class="p">,</span> <span class="n">new_optim</span><span class="p">,</span> <span class="n">new_group</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sharded_osd</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">scatter_full_optim_state_dict</span><span class="p">(</span><span class="n">full_osd</span><span class="p">,</span> <span class="n">new_model</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">new_group</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_optim</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">sharded_osd</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Both <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">shard_full_optim_state_dict()</span></code></a> and
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_full_optim_state_dict()</span></code></a> may be used to get the
sharded optimizer state dict to load. Assuming that the full
optimizer state dict resides in CPU memory, the former requires
each rank to have the full dict in CPU memory, where each rank
individually shards the dict without any communication, while the
latter requires only rank 0 to have the full dict in CPU memory,
where rank 0 moves each shard to GPU memory (for NCCL) and
communicates it to ranks appropriately. Hence, the former has
higher aggregate CPU memory cost, while the latter has higher
communication cost.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>full_optim_state_dict</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em>, </em><em>Any</em><em>]</em><em>]</em>) – Optimizer state
dict corresponding to the unflattened parameters and holding
the full non-sharded optimizer state if on rank 0; the argument
is ignored on nonzero ranks.</p></li>
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module (which may or may not be a
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a> instance) whose parameters
correspond to the optimizer state in <code class="docutils literal notranslate"><span class="pre">full_optim_state_dict</span></code>.</p></li>
<li><p><strong>optim_input</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em>, </em><em>Any</em><em>]</em><em>]</em><em>, </em><em>Iterable</em><em>[</em><em>torch.nn.Parameter</em><em>]</em><em>]</em><em>]</em>) – Input passed into the optimizer representing either a
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a> of parameter groups or an iterable of parameters;
if <code class="docutils literal notranslate"><span class="pre">None</span></code>, then this method assumes the input was
<code class="docutils literal notranslate"><span class="pre">model.parameters()</span></code>. This argument is deprecated, and there
is no need to pass it in anymore. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>optim</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>torch.optim.Optimizer</em></a><em>]</em>) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over <code class="docutils literal notranslate"><span class="pre">optim_input</span></code>. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>group</strong> (<em>dist.ProcessGroup</em>) – Model’s process group or <code class="docutils literal notranslate"><span class="pre">None</span></code> if
using the default process group. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">set_state_dict_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_dict_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_dict_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_state_dict_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_state_dict_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the <code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code> and the corresponding (optional)
configurations of all the descendant FSDP modules of the target module.
The target module does not have to be a FSDP module. If the target
module is a FSDP module, its <code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code> will also be changed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This API should be called for only the top-level (root)
module.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This API enables users to transparently use the conventional
<code class="docutils literal notranslate"><span class="pre">state_dict</span></code> API to take model checkpoints in cases where the
root FSDP module is wrapped by another <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>. For example,
the following will ensure <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> is called on all non-FSDP
instances, while dispatching into <cite>sharded_state_dict</cite> implementation
for FSDP:</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">FSDP</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">FSDP</span><span class="o">.</span><span class="n">set_state_dict_type</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">state_dict_config</span> <span class="o">=</span> <span class="n">ShardedStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optim_state_dict_config</span> <span class="o">=</span> <span class="n">OptimStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">param_state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim_state_dict</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">optim_state_dict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module.</p></li>
<li><p><strong>state_dict_type</strong> (<em>StateDictType</em>) – the desired <code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code> to set.</p></li>
<li><p><strong>state_dict_config</strong> (<em>Optional</em><em>[</em><em>StateDictConfig</em><em>]</em>) – the configuration for the
target <code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A StateDictSettings that include the previous state_dict type and
configuration for the module.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>StateDictSettings</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">shard_full_optim_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">full_optim_state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.shard_full_optim_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Shards the full optimizer state dict <code class="docutils literal notranslate"><span class="pre">full_optim_state_dict</span></code> by
remapping the state to flattened parameters instead of unflattened
parameters and restricting to only this rank’s part of the optimizer
state. The first argument should be the return value of
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">full_optim_state_dict()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">full_osd</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">full_optim_state_dict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">full_osd</span><span class="p">,</span> <span class="n">PATH</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define new model with possibly different world size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_model</span><span class="p">,</span> <span class="n">new_optim</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">full_osd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sharded_osd</span> <span class="o">=</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">shard_full_optim_state_dict</span><span class="p">(</span><span class="n">full_osd</span><span class="p">,</span> <span class="n">new_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_optim</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">sharded_osd</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Both <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">shard_full_optim_state_dict()</span></code></a> and
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_full_optim_state_dict()</span></code></a> may be used to get the
sharded optimizer state dict to load. Assuming that the full
optimizer state dict resides in CPU memory, the former requires
each rank to have the full dict in CPU memory, where each rank
individually shards the dict without any communication, while the
latter requires only rank 0 to have the full dict in CPU memory,
where rank 0 moves each shard to GPU memory (for NCCL) and
communicates it to ranks appropriately. Hence, the former has
higher aggregate CPU memory cost, while the latter has higher
communication cost.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>full_optim_state_dict</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em>, </em><em>Any</em><em>]</em>) – Optimizer state dict
corresponding to the unflattened parameters and holding the
full non-sharded optimizer state.</p></li>
<li><p><strong>model</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module (which may or may not be a
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a> instance) whose parameters
correspond to the optimizer state in <code class="docutils literal notranslate"><span class="pre">full_optim_state_dict</span></code>.</p></li>
<li><p><strong>optim_input</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)"><em>str</em></a><em>, </em><em>Any</em><em>]</em><em>]</em><em>, </em><em>Iterable</em><em>[</em><em>torch.nn.Parameter</em><em>]</em><em>]</em><em>]</em>) – Input passed into the optimizer representing either a
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.11)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a> of parameter groups or an iterable of parameters;
if <code class="docutils literal notranslate"><span class="pre">None</span></code>, then this method assumes the input was
<code class="docutils literal notranslate"><span class="pre">model.parameters()</span></code>. This argument is deprecated, and there
is no need to pass it in anymore. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>optim</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="optim.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>torch.optim.Optimizer</em></a><em>]</em>) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over <code class="docutils literal notranslate"><span class="pre">optim_input</span></code>. (Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">sharded_optim_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.sharded_optim_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>The API is similar to <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">full_optim_state_dict()</span></code></a> but this API chunks
all non-zero-dimension states to <code class="xref py py-class docutils literal notranslate"><span class="pre">ShardedTensor</span></code> to save memory.
This API should only be used when the model <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> is derived
with the context manager <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">state_dict_type(SHARDED_STATE_DICT):</span></code>.</p>
<p>For the detailed usage, refer to <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict" title="torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">full_optim_state_dict()</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The returned state dict contains <code class="docutils literal notranslate"><span class="pre">ShardedTensor</span></code> and
cannot be directly used by the regular <code class="docutils literal notranslate"><span class="pre">optim.load_state_dict</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.11)"><em>Dict</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.11)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.11)"><em>Any</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">state_dict_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_dict_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_dict_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_state_dict_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.state_dict_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type" title="Permalink to this definition">¶</a></dt>
<dd><p>A context manager to set the <code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code> of all the descendant
FSDP modules of the target module. This context manager has the same
functions as <a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type" title="torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_state_dict_type()</span></code></a>. Read the document of
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type" title="torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_state_dict_type()</span></code></a> for the detail.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">FSDP</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">FSDP</span><span class="o">.</span><span class="n">state_dict_type</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">StateDictType</span><span class="o">.</span><span class="n">SHARDED_STATE_DICT</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – Root module.</p></li>
<li><p><strong>state_dict_type</strong> (<em>StateDictType</em>) – the desired <code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code> to set.</p></li>
<li><p><strong>state_dict_config</strong> (<em>Optional</em><em>[</em><em>StateDictConfig</em><em>]</em>) – the configuration for the
target <code class="docutils literal notranslate"><span class="pre">state_dict_type</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Generator" title="(in Python v3.11)"><em>Generator</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">summon_full_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">writeback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank0_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offload_to_cpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/fully_sharded_data_parallel.html#FullyShardedDataParallel.summon_full_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params" title="Permalink to this definition">¶</a></dt>
<dd><p>A context manager to expose full params for FSDP instances.
Can be useful <em>after</em> forward/backward for a model to get
the params for additional processing or checking. It can take a non-FSDP
module and will summon full params for all contained FSDP modules as
well as their children, depending on the <code class="docutils literal notranslate"><span class="pre">recurse</span></code> argument.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This can be used on inner FSDPs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This can <em>not</em> be used within a forward or backward pass. Nor
can forward and backward be started from within this context.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parameters will revert to their local shards after the context
manager exits, storage behavior is the same as forward.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The full parameters can be modified, but only the portion
corresponding to the local param shard will persist after the
context manager exits (unless <code class="docutils literal notranslate"><span class="pre">writeback=False</span></code>, in which case
changes will be discarded). In the case where FSDP does not shard
the parameters, currently only when <code class="docutils literal notranslate"><span class="pre">world_size</span> <span class="pre">==</span> <span class="pre">1</span></code>, or <code class="docutils literal notranslate"><span class="pre">NO_SHARD</span></code>
config, the modification is persisted regardless of <code class="docutils literal notranslate"><span class="pre">writeback</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method works on modules which are not FSDP themselves but
may contain multiple independent FSDP units. In that case, the given
arguments will apply to all contained FSDP units.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that <code class="docutils literal notranslate"><span class="pre">rank0_only=True</span></code> in conjunction with
<code class="docutils literal notranslate"><span class="pre">writeback=True</span></code> is not currently supported and will raise an
error. This is because model parameter shapes would be different
across ranks within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that <code class="docutils literal notranslate"><span class="pre">offload_to_cpu</span></code> and <code class="docutils literal notranslate"><span class="pre">rank0_only=False</span></code> will
result in full parameters being redundantly copied to CPU memory for
GPUs that reside on the same machine, which may incur the risk of
CPU OOM. It is recommended to use <code class="docutils literal notranslate"><span class="pre">offload_to_cpu</span></code> with
<code class="docutils literal notranslate"><span class="pre">rank0_only=True</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>Optional</em>) – recursively summon all params for nested
FSDP instances (default: True).</p></li>
<li><p><strong>writeback</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>Optional</em>) – if <code class="docutils literal notranslate"><span class="pre">False</span></code>, modifications to params are
discarded after the context manager exits;
disabling this can be slightly more efficient (default: True)</p></li>
<li><p><strong>rank0_only</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>Optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, full parameters are
materialized on only global rank 0. This means that within the
context, only rank 0 will have full parameters and the other
ranks will have sharded parameters. Note that setting
<code class="docutils literal notranslate"><span class="pre">rank0_only=True</span></code> with <code class="docutils literal notranslate"><span class="pre">writeback=True</span></code> is not supported,
as model parameter shapes will be different across ranks
within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.</p></li>
<li><p><strong>offload_to_cpu</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>Optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, full parameters are
offloaded to CPU. Note that this offloading currently only
occurs if the parameter is sharded (which is only not the case
for world_size = 1 or <code class="docutils literal notranslate"><span class="pre">NO_SHARD</span></code> config). It is recommended
to use <code class="docutils literal notranslate"><span class="pre">offload_to_cpu</span></code> with <code class="docutils literal notranslate"><span class="pre">rank0_only=True</span></code> to avoid
redundant copies of model parameters being offloaded to the same CPU memory.</p></li>
<li><p><strong>with_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a><em>, </em><em>Optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradients are also
unsharded with the parameters. Currently, this is only
supported when passing <code class="docutils literal notranslate"><span class="pre">use_orig_params=True</span></code> to the FSDP
constructor and <code class="docutils literal notranslate"><span class="pre">offload_to_cpu=False</span></code> to this method.
(Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Generator" title="(in Python v3.11)"><em>Generator</em></a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.BackwardPrefetch">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">BackwardPrefetch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/api.html#BackwardPrefetch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.BackwardPrefetch" title="Permalink to this definition">¶</a></dt>
<dd><p>This configures explicit backward prefetching, which can improve throughput
but may slightly increase peak memory usage.</p>
<p>For NCCL backend, any collectives, even if issued in different streams,
contend for the same per-device NCCL stream, which is why the relative
order in which the collectives are issued matters for overlapping. The
different backward prefetching settings correspond to different orderings.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BACKWARD_PRE</span></code>: This prefetches the next set of parameters before the
current set of parameter’s gradient computation. This improves backward
pass throughput by overlapping communication (next all-gather) and
computation (current gradient computation).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BACKWARD_POST</span></code>: This prefetches the next set of parameters after the
current set of parameter’s gradient computation. This may improve
backward pass throughput by overlapping communication (current
reduce-scatter) and computation (next gradient computation).
Specifically, the next all-gather is reordered to be before the current
reduce-scatter.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the increase in peak memory usage from prefetching is an
issue, you may consider passing <code class="docutils literal notranslate"><span class="pre">limit_all_gathers=True</span></code> to the FSDP
constructor, which may help reduce peak memory usage in some cases.</p>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.ShardingStrategy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">ShardingStrategy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/api.html#ShardingStrategy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.ShardingStrategy" title="Permalink to this definition">¶</a></dt>
<dd><p>This specifies the sharding strategy to be used for distributed training by
<a class="reference internal" href="#torch.distributed.fsdp.FullyShardedDataParallel" title="torch.distributed.fsdp.FullyShardedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">FullyShardedDataParallel</span></code></a>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">FULL_SHARD</span></code>: Parameters, gradients, and optimizer states are sharded.
For the parameters, this strategy unshards (via all-gather) before the
forward, reshards after the forward, unshards before the backward
computation, and reshards after the backward computation. For gradients,
it synchronizes and shards them (via reduce-scatter) after the backward
computation. The sharded optimizer states are updated locally per rank.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SHARD_GRAD_OP</span></code>: Gradients and optimizer states are sharded during
computation, and additionally, parameters are sharded outside
computation. For the parameters, this strategy unshards before the
forward, does not reshard them after the forward, and only reshards them
after the backward computation. The sharded optimizer states are updated
locally per rank. Inside <code class="docutils literal notranslate"><span class="pre">no_sync()</span></code>, the parameters are not resharded
after the backward computation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NO_SHARD</span></code>: Parameters, gradients, and optimizer states are not sharded
but instead replicated across ranks similar to PyTorch’s
<code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> API. For gradients, this strategy
synchronizes them (via all-reduce) after the backward computation. The
unsharded optimizer states are updated locally per rank.</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">HYBRID_SHARD</span></code>: Apply <code class="docutils literal notranslate"><span class="pre">FULL_SHARD</span></code> within a node, and replicate parameters across</dt><dd><p>nodes. This results in reduced communication volume as expensive all-gathers and
reduce-scatters are only done within a node, which can be more performant for medium
-sized models.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">_HYBRID_SHARD_ZERO2</span></code>: Apply <code class="docutils literal notranslate"><span class="pre">SHARD_GRAD_OP</span></code> within a node, and replicate parameters across</dt><dd><p>nodes. This is like <code class="docutils literal notranslate"><span class="pre">HYBRID_SHARD</span></code>, except this may provide even higher throughput
since the unsharded parameters are not freed after the forward pass, saving the
all-gathers in the pre-backward.</p>
</dd>
</dl>
</li>
</ul>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.MixedPrecision">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">MixedPrecision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_low_precision_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cast_forward_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cast_root_forward_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/api.html#MixedPrecision"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.MixedPrecision" title="Permalink to this definition">¶</a></dt>
<dd><p>This configures FSDP-native mixed precision training.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><em>torch.dtype</em></a>) – This specifies the dtype for model
parameters, inputs (when <code class="docutils literal notranslate"><span class="pre">cast_forward_inputs</span></code> or
<code class="docutils literal notranslate"><span class="pre">cast_root_forward_inputs``is</span> <span class="pre">set</span> <span class="pre">to</span>
<span class="pre">``True</span></code>), and therefore the dtype for computation.
However, outside the forward and backward passes, parameters are in
full precision. Model checkpointing always happens in full
precision.</p></li>
<li><p><strong>reduce_dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><em>torch.dtype</em></a>) – This specifies the dtype for gradient
reduction, which is permitted to differ from <code class="docutils literal notranslate"><span class="pre">param_dtype</span></code>.</p></li>
<li><p><strong>buffer_dtype</strong> (<a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype"><em>torch.dtype</em></a>) – This specifies the dtype for buffers. FSDP
does not shard buffers, casts them to <code class="docutils literal notranslate"><span class="pre">buffer_dtype</span></code> in the first
forward pass, and keeps them in that dtype thereafter. Model
checkpointing always happens in full precision.</p></li>
<li><p><strong>keep_low_precision_grads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – This specifies whether to upcast
gradients back to the full parameter precision after the backward
pass. This may be set to <code class="docutils literal notranslate"><span class="pre">False</span></code> to save memory if using custom
optimizers that can perform the optimizer step in <code class="docutils literal notranslate"><span class="pre">reduce_dtype</span></code>.
(Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>cast_forward_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – Cast floating point tensors in the forward
arguments and keyword arguments to <code class="docutils literal notranslate"><span class="pre">param_dtype</span></code>.
(Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>cast_root_forward_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – Cast floating point tensors in the forward
arguments and keyword arguments to <code class="docutils literal notranslate"><span class="pre">param_dtype</span></code> for the root FSDP instance.
It takes precedence over <code class="docutils literal notranslate"><span class="pre">cast_forward_inputs</span></code> for the root FSDP instance.
(Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This API is experimental and subject to change.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only floating point tensors are cast to their specified dtypes.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <code class="docutils literal notranslate"><span class="pre">summon_full_params</span></code>, parameters are forced to full
precision, but buffers are not.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">state_dict</span></code> checkpoints parameters and buffers in full
precision. For buffers, this is only supported for
<code class="docutils literal notranslate"><span class="pre">StateDictType.FULL_STATE_DICT</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each low precision dtype must be specified explicitly. For
example, <code class="docutils literal notranslate"><span class="pre">MixedPrecision(reduce_dtype=torch.float16)</span></code> only specifies
the reduction dtype to be low precision, and FSDP will not cast
parameters or buffers.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a <code class="docutils literal notranslate"><span class="pre">reduce_dtype</span></code> is not specified, then gradient reduction
happens in <code class="docutils literal notranslate"><span class="pre">param_dtype</span></code> if specified or the original parameter dtype
otherwise.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the user passes a model with <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> modules and an
<code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code> to the FSDP constructor, then FSDP will disable
mixed precision for <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> modules by wrapping them separately
in their own FSDP instance with mixed precision disabled. This is due
to some missing low precision <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> kernels. If the user does
not use an <code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code>, then the user must take care to not
use mixed precision for FSDP instances containing <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code>
modules.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">MixedPrecision</span></code> has <code class="docutils literal notranslate"><span class="pre">cast_root_forward_inputs=True</span></code> and
<code class="docutils literal notranslate"><span class="pre">cast_forward_inputs=False</span></code> by default. For the root FSDP instance,
its <code class="docutils literal notranslate"><span class="pre">cast_root_forward_inputs</span></code> takes precedence over its
<code class="docutils literal notranslate"><span class="pre">cast_forward_inputs</span></code>. For non-root FSDP instances, their
<code class="docutils literal notranslate"><span class="pre">cast_root_forward_inputs</span></code> values are ignored. The default setting is
sufficient for the typical case where each FSDP instance has the same
<code class="docutils literal notranslate"><span class="pre">MixedPrecision</span></code> configuration and only needs to cast inputs to the
<code class="docutils literal notranslate"><span class="pre">param_dtype</span></code> at the beginning of the model’s forward pass.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For nested FSDP instances with different <code class="docutils literal notranslate"><span class="pre">MixedPrecision</span></code>
configurations, we recommend setting individual <code class="docutils literal notranslate"><span class="pre">cast_forward_inputs</span></code>
values to configure casting inputs or not before each instance’s
forward. In such a case, since the casts happen before each FSDP
instance’s forward, a parent FSDP instance should have its non-FSDP
submodules run before its FSDP submodules to avoid the activation dtype
being changed due to a different <code class="docutils literal notranslate"><span class="pre">MixedPrecision</span></code> configuration.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">mixed_precision</span><span class="o">=</span><span class="n">MixedPrecision</span><span class="p">(</span><span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">cast_forward_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">mixed_precision</span><span class="o">=</span><span class="n">MixedPrecision</span><span class="p">(</span><span class="n">param_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">cast_forward_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
</pre></div>
</div>
<p>The above shows a working example. On the other hand, if <code class="docutils literal notranslate"><span class="pre">model[1]</span></code>
were replaced with <code class="docutils literal notranslate"><span class="pre">model[0]</span></code>, meaning that the submodule using
different <code class="docutils literal notranslate"><span class="pre">MixedPrecision</span></code> ran its forward first, then <code class="docutils literal notranslate"><span class="pre">model[1]</span></code>
would incorrectly see <code class="docutils literal notranslate"><span class="pre">float16</span></code> activations instead of <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>
ones.</p>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.fsdp.CPUOffload">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.fsdp.</span></span><span class="sig-name descname"><span class="pre">CPUOffload</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">offload_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/fsdp/api.html#CPUOffload"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.fsdp.CPUOffload" title="Permalink to this definition">¶</a></dt>
<dd><p>This configures CPU offloading.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>offload_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.11)"><em>bool</em></a>) – This specifies whether to offload parameters to
CPU when not involved in computation. If enabled, this implicitly
offloads gradients to CPU as well. This is to support the optimizer
step, which requires parameters and gradients to be on the same
device.</p>
</dd>
</dl>
</dd></dl>

</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="distributed.optim.html" class="btn btn-neutral float-right" title="Distributed Optimizers" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="elastic/kubernetes.html" class="btn btn-neutral" title="TorchElastic Kubernetes" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">FullyShardedDataParallel</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
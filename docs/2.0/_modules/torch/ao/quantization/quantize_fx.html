


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.ao.quantization.quantize_fx &mdash; PyTorch 2.0 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/ao/quantization/quantize_fx.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-dropdown.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/panels-bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />


  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  


  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>2.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/design.html">PyTorch Design Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance | Mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.func.html">Extending torch.func with autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/gradcheck.html">Gradcheck mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/hip.html">HIP (ROCm) semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/mps.html">MPS backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/numerical_accuracy.html">Numerical accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../dynamo/index.html">TorchDynamo Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dynamo/installation.html">Installing TorchDynamo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dynamo/get-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dynamo/guards-overview.html">Guards Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dynamo/custom-backends.html">Custom Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dynamo/deep-dive.html">TorchDynamo Deeper Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dynamo/troubleshooting.html">TorchDynamo Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dynamo/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ir.html">IRs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../_dynamo.html">torch._dynamo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx_diagnostics.html">torch.onnx diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../config_mod.html">torch.__config__</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../quantization.html">torch.ao.quantization</a> &gt;</li>
        
      <li>torch.ao.quantization.quantize_fx</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.ao.quantization.quantize_fx</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">GraphModule</span>
<span class="kn">from</span> <span class="nn">torch.fx.graph_module</span> <span class="kn">import</span> <span class="n">_USER_PRESERVED_ATTRIBUTES_KEY</span>
<span class="kn">from</span> <span class="nn">.fx.tracer</span> <span class="kn">import</span> <span class="n">QuantizationTracer</span>
<span class="kn">from</span> <span class="nn">.fx.tracer</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">Scope</span><span class="p">,</span>
    <span class="n">ScopeContextManager</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.fx</span> <span class="kn">import</span> <span class="n">fuse</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">.fx</span> <span class="kn">import</span> <span class="n">prepare</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">.fx.convert</span> <span class="kn">import</span> <span class="n">convert</span>
<span class="kn">from</span> <span class="nn">.backend_config</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># noqa: F401</span>
    <span class="n">BackendConfig</span><span class="p">,</span>
    <span class="n">get_tensorrt_backend_config</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.fx.graph_module</span> <span class="kn">import</span> <span class="n">ObservedGraphModule</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">.fx.custom_config</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ConvertCustomConfig</span><span class="p">,</span>
    <span class="n">FuseCustomConfig</span><span class="p">,</span>
    <span class="n">PrepareCustomConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.fx.utils</span> <span class="kn">import</span> <span class="n">get_custom_module_class_keys</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">.fx.utils</span> <span class="kn">import</span> <span class="n">get_skipped_module_name_and_classes</span>
<span class="kn">from</span> <span class="nn">.qconfig_mapping</span> <span class="kn">import</span> <span class="n">QConfigMapping</span>

<span class="k">def</span> <span class="nf">attach_preserved_attrs_to_model</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span> <span class="n">preserved_attrs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
    <span class="sd">&quot;&quot;&quot; Store preserved attributes to the model.meta so that it can be preserved during deepcopy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="n">_USER_PRESERVED_ATTRIBUTES_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">preserved_attrs</span><span class="p">)</span>  <span class="c1"># type: ignore[operator, index, assignment]</span>
    <span class="c1"># set the preserved attributes in the model so that user can call</span>
    <span class="c1"># model.attr as they do before calling fx graph mode quantization</span>
    <span class="k">for</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="n">_USER_PRESERVED_ATTRIBUTES_KEY</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>  <span class="c1"># type: ignore[index, union-attr]</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_is_graph_module</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">GraphModule</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;input model must be a GraphModule, &quot;</span>
            <span class="o">+</span> <span class="s2">&quot;Got type:&quot;</span>
            <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
            <span class="o">+</span> <span class="s2">&quot; Please make &quot;</span>
            <span class="o">+</span> <span class="s2">&quot;sure to follow the tutorials.&quot;</span>
        <span class="p">)</span>

<span class="k">def</span> <span class="nf">_attach_meta_to_node_if_not_exist</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Attach meta field to all nodes of the graph if it does not exist,</span>
<span class="sd">    meta field is a field stores some meta information about the node, such</span>
<span class="sd">    as dtype and shape information for output of the node, this only exists</span>
<span class="sd">    if the program is captured by make_fx (used in quantize_pt2e flow), if</span>
<span class="sd">    the program is captured by torch.fx symbolic tracing, this field may not exist,</span>
<span class="sd">    so we add it here to avoid checking this all over the places</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="s2">&quot;meta&quot;</span><span class="p">):</span>
            <span class="n">node</span><span class="o">.</span><span class="n">meta</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">def</span> <span class="nf">_swap_ff_with_fxff</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Swap FloatFunctional with FXFloatFunctional</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">modules_to_swap</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">FloatFunctional</span><span class="p">):</span>
            <span class="n">modules_to_swap</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_swap_ff_with_fxff</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">modules_to_swap</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ao</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">FXFloatFunctional</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_fuse_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">is_qat</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fuse_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">FuseCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Internal helper function to fuse modules in preparation for quantization</span>

<span class="sd">    Args:</span>
<span class="sd">        model: GraphModule object from symbolic tracing (torch.fx.symbolic_trace)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_graph_module</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fuse</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">is_qat</span><span class="p">,</span> <span class="n">fuse_custom_config</span><span class="p">,</span> <span class="n">backend_config</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

<span class="k">def</span> <span class="nf">_prepare_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">qconfig_mapping</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="n">is_qat</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">prepare_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PrepareCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">_equalization_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_standalone_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Internal helper function for prepare_fx</span>
<span class="sd">    Args:</span>
<span class="sd">      `model`, `qconfig_mapping`, `prepare_custom_config`, `_equalization_config`:</span>
<span class="sd">      see docs for :func:`~torch.ao.quantization.prepare_fx`</span>
<span class="sd">      `is_standalone_module`: a boolean flag indicates whether we are</span>
<span class="sd">      quantizing a standalone module or not, a standalone module</span>
<span class="sd">      is a submodule of the parent module that is not inlined in the</span>
<span class="sd">forward graph of the parent module,</span>
<span class="sd">      the way we quantize standalone module is described in:</span>
<span class="sd">      :func:`~torch.ao.quantization._prepare_standalone_module_fx`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prepare_custom_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prepare_custom_config</span> <span class="o">=</span> <span class="n">PrepareCustomConfig</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">_equalization_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_equalization_config</span> <span class="o">=</span> <span class="n">QConfigMapping</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prepare_custom_config</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Passing a prepare_custom_config_dict to prepare is deprecated and will not be supported &quot;</span>
            <span class="s2">&quot;in a future version. Please pass in a PrepareCustomConfig instead.&quot;</span><span class="p">)</span>
        <span class="n">prepare_custom_config</span> <span class="o">=</span> <span class="n">PrepareCustomConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">prepare_custom_config</span><span class="p">)</span>

    <span class="c1"># swap FloatFunctional with FXFloatFunctional</span>
    <span class="n">_swap_ff_with_fxff</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="n">skipped_module_names</span><span class="p">,</span> <span class="n">skipped_module_classes</span> <span class="o">=</span> \
        <span class="n">get_skipped_module_name_and_classes</span><span class="p">(</span><span class="n">prepare_custom_config</span><span class="p">,</span> <span class="n">is_standalone_module</span><span class="p">)</span>
    <span class="n">preserved_attr_names</span> <span class="o">=</span> <span class="n">prepare_custom_config</span><span class="o">.</span><span class="n">preserved_attributes</span>
    <span class="n">preserved_attrs</span> <span class="o">=</span> <span class="p">{</span><span class="n">attr</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">preserved_attr_names</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr</span><span class="p">)}</span>
    <span class="c1"># symbolically trace the model</span>
    <span class="n">tracer</span> <span class="o">=</span> <span class="n">QuantizationTracer</span><span class="p">(</span><span class="n">skipped_module_names</span><span class="p">,</span> <span class="n">skipped_module_classes</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
    <span class="n">graph_module</span> <span class="o">=</span> <span class="n">GraphModule</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tracer</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
    <span class="n">_attach_meta_to_node_if_not_exist</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>

    <span class="n">fuse_custom_config</span> <span class="o">=</span> <span class="n">FuseCustomConfig</span><span class="p">()</span><span class="o">.</span><span class="n">set_preserved_attributes</span><span class="p">(</span><span class="n">prepare_custom_config</span><span class="o">.</span><span class="n">preserved_attributes</span><span class="p">)</span>
    <span class="n">graph_module</span> <span class="o">=</span> <span class="n">_fuse_fx</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_qat</span><span class="p">,</span>
        <span class="n">fuse_custom_config</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="p">)</span>
    <span class="n">prepared</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">qconfig_mapping</span><span class="p">,</span>
        <span class="n">is_qat</span><span class="p">,</span>
        <span class="n">tracer</span><span class="o">.</span><span class="n">node_name_to_scope</span><span class="p">,</span>
        <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">,</span>
        <span class="n">prepare_custom_config</span><span class="o">=</span><span class="n">prepare_custom_config</span><span class="p">,</span>
        <span class="n">_equalization_config</span><span class="o">=</span><span class="n">_equalization_config</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">backend_config</span><span class="p">,</span>
        <span class="n">is_standalone_module</span><span class="o">=</span><span class="n">is_standalone_module</span><span class="p">,</span>
    <span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

    <span class="n">attach_preserved_attrs_to_model</span><span class="p">(</span><span class="n">prepared</span><span class="p">,</span> <span class="n">preserved_attrs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prepared</span>


<span class="k">def</span> <span class="nf">_prepare_standalone_module_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">qconfig_mapping</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="n">is_qat</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">prepare_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PrepareCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; [Internal use only] Prepare a standalone module, so that it can be used when quantizing the</span>
<span class="sd">    parent module.</span>
<span class="sd">    standalone_module means it a submodule that is not inlined in parent module,</span>
<span class="sd">    and will be quantized separately as one unit.</span>

<span class="sd">    How the standalone module is observed is specified by `input_quantized_idxs` and</span>
<span class="sd">    `output_quantized_idxs` in the prepare_custom_config for the standalone module</span>

<span class="sd">    Returns:</span>

<span class="sd">        * model(GraphModule): prepared standalone module. It has these attributes in</span>
<span class="sd">          model.meta:</span>

<span class="sd">            * `standalone_module_input_quantized_idxs(List[Int])`: a list of</span>
<span class="sd">              indexes for the graph input that is expected to be quantized,</span>
<span class="sd">              same as input_quantized_idxs configuration provided</span>
<span class="sd">              for the standalone module</span>
<span class="sd">            * `standalone_module_output_quantized_idxs(List[Int])`: a list of</span>
<span class="sd">              indexs for the graph output that is quantized</span>
<span class="sd">              same as input_quantized_idxs configuration provided</span>
<span class="sd">              for the standalone module</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_prepare_fx</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">qconfig_mapping</span><span class="p">,</span>
        <span class="n">is_qat</span><span class="p">,</span>
        <span class="n">example_inputs</span><span class="p">,</span>
        <span class="n">prepare_custom_config</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">backend_config</span><span class="p">,</span>
        <span class="n">is_standalone_module</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>


<div class="viewcode-block" id="fuse_fx"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx">[docs]</a><span class="k">def</span> <span class="nf">fuse_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">fuse_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">FuseCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.</span>
<span class="sd">    Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py</span>

<span class="sd">    Args:</span>

<span class="sd">        * `model` (torch.nn.Module): a torch.nn.Module model</span>
<span class="sd">        * `fuse_custom_config` (FuseCustomConfig): custom configurations for fuse_fx.</span>
<span class="sd">            See :class:`~torch.ao.quantization.fx.custom_config.FuseCustomConfig` for more details</span>
<span class="sd">    Example::</span>

<span class="sd">        from torch.ao.quantization import fuse_fx</span>
<span class="sd">        m = Model().eval()</span>
<span class="sd">        m = fuse_fx(m)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">fuse_custom_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fuse_custom_config</span> <span class="o">=</span> <span class="n">FuseCustomConfig</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fuse_custom_config</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Passing a fuse_custom_config_dict to fuse is deprecated and will not be supported &quot;</span>
            <span class="s2">&quot;in a future version. Please pass in a FuseCustomConfig instead.&quot;</span><span class="p">)</span>
        <span class="n">fuse_custom_config</span> <span class="o">=</span> <span class="n">FuseCustomConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">fuse_custom_config</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.fuse_fx&quot;</span><span class="p">)</span>
    <span class="n">preserved_attr_names</span> <span class="o">=</span> <span class="n">fuse_custom_config</span><span class="o">.</span><span class="n">preserved_attributes</span>
    <span class="n">preserved_attrs</span> <span class="o">=</span> <span class="p">{</span><span class="n">attr</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">preserved_attr_names</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">attr</span><span class="p">)}</span>

    <span class="n">graph_module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">symbolic_trace</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">_attach_meta_to_node_if_not_exist</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
    <span class="n">graph_module</span> <span class="o">=</span> <span class="n">_fuse_fx</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">fuse_custom_config</span><span class="p">,</span> <span class="n">backend_config</span><span class="p">)</span>

    <span class="n">attach_preserved_attrs_to_model</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">preserved_attrs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">graph_module</span></div>

<div class="viewcode-block" id="prepare_fx"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx">[docs]</a><span class="k">def</span> <span class="nf">prepare_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">qconfig_mapping</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">prepare_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PrepareCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">_equalization_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Prepare a model for post training static quantization</span>

<span class="sd">    Args:</span>
<span class="sd">      * `model` (torch.nn.Module): torch.nn.Module model</span>

<span class="sd">      * `qconfig_mapping` (QConfigMapping): QConfigMapping object to configure how a model is</span>
<span class="sd">         quantized, see :class:`~torch.ao.quantization.qconfig_mapping.QConfigMapping`</span>
<span class="sd">         for more details</span>

<span class="sd">      * `example_inputs` (Tuple[Any, ...]): Example inputs for forward function of the model,</span>
<span class="sd">         Tuple of positional args (keyword args can be passed as positional args as well)</span>

<span class="sd">      * `prepare_custom_config` (PrepareCustomConfig): customization configuration for quantization tool.</span>
<span class="sd">          See :class:`~torch.ao.quantization.fx.custom_config.PrepareCustomConfig` for more details</span>

<span class="sd">      * `_equalization_config`: config for specifying how to perform equalization on the model</span>

<span class="sd">      * `backend_config` (BackendConfig): config that specifies how operators are quantized</span>
<span class="sd">         in a backend, this includes how the operators are observed,</span>
<span class="sd">         supported fusion patterns, how quantize/dequantize ops are</span>
<span class="sd">         inserted, supported dtypes etc. See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details</span>

<span class="sd">    Return:</span>
<span class="sd">      A GraphModule with observer (configured by qconfig_mapping), ready for calibration</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        from torch.ao.quantization import get_default_qconfig_mapping</span>
<span class="sd">        from torch.ao.quantization import prepare_fx</span>

<span class="sd">        class Submodule(torch.nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super().__init__()</span>
<span class="sd">                self.linear = torch.nn.Linear(5, 5)</span>
<span class="sd">            def forward(self, x):</span>
<span class="sd">                x = self.linear(x)</span>
<span class="sd">                return x</span>

<span class="sd">        class M(torch.nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super().__init__()</span>
<span class="sd">                self.linear = torch.nn.Linear(5, 5)</span>
<span class="sd">                self.sub = Submodule()</span>

<span class="sd">            def forward(self, x):</span>
<span class="sd">                x = self.linear(x)</span>
<span class="sd">                x = self.sub(x) + x</span>
<span class="sd">                return x</span>

<span class="sd">        # initialize a floating point model</span>
<span class="sd">        float_model = M().eval()</span>

<span class="sd">        # define calibration function</span>
<span class="sd">        def calibrate(model, data_loader):</span>
<span class="sd">            model.eval()</span>
<span class="sd">            with torch.no_grad():</span>
<span class="sd">                for image, target in data_loader:</span>
<span class="sd">                    model(image)</span>

<span class="sd">        # qconfig is the configuration for how we insert observers for a particular</span>
<span class="sd">        # operator</span>
<span class="sd">        # qconfig = get_default_qconfig(&quot;fbgemm&quot;)</span>
<span class="sd">        # Example of customizing qconfig:</span>
<span class="sd">        # qconfig = torch.ao.quantization.QConfig(</span>
<span class="sd">        #    activation=MinMaxObserver.with_args(dtype=torch.qint8),</span>
<span class="sd">        #    weight=MinMaxObserver.with_args(dtype=torch.qint8))</span>
<span class="sd">        # `activation` and `weight` are constructors of observer module</span>

<span class="sd">        # qconfig_mapping is a collection of quantization configurations, user can</span>
<span class="sd">        # set the qconfig for each operator (torch op calls, functional calls, module calls)</span>
<span class="sd">        # in the model through qconfig_mapping</span>
<span class="sd">        # the following call will get the qconfig_mapping that works best for models</span>
<span class="sd">        # that target &quot;fbgemm&quot; backend</span>
<span class="sd">        qconfig_mapping = get_default_qconfig_mapping(&quot;fbgemm&quot;)</span>

<span class="sd">        # We can customize qconfig_mapping in different ways.</span>
<span class="sd">        # e.g. set the global qconfig, which means we will use the same qconfig for</span>
<span class="sd">        # all operators in the model, this can be overwritten by other settings</span>
<span class="sd">        # qconfig_mapping = QConfigMapping().set_global(qconfig)</span>
<span class="sd">        # e.g. quantize the linear submodule with a specific qconfig</span>
<span class="sd">        # qconfig_mapping = QConfigMapping().set_module_name(&quot;linear&quot;, qconfig)</span>
<span class="sd">        # e.g. quantize all nn.Linear modules with a specific qconfig</span>
<span class="sd">        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)</span>
<span class="sd">        # for a more complete list, please see the docstring for :class:`torch.ao.quantization.QConfigMapping`</span>
<span class="sd">        # argument</span>

<span class="sd">        # example_inputs is a tuple of inputs, that is used to infer the type of the</span>
<span class="sd">        # outputs in the model</span>
<span class="sd">        # currently it&#39;s not used, but please make sure model(*example_inputs) runs</span>
<span class="sd">        example_inputs = (torch.randn(1, 3, 224, 224),)</span>

<span class="sd">        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack</span>
<span class="sd">        # e.g. backend_config = get_default_backend_config(&quot;fbgemm&quot;)</span>
<span class="sd">        # `prepare_fx` inserts observers in the model based on qconfig_mapping and</span>
<span class="sd">        # backend_config. If the configuration for an operator in qconfig_mapping</span>
<span class="sd">        # is supported in the backend_config (meaning it&#39;s supported by the target</span>
<span class="sd">        # hardware), we&#39;ll insert observer modules according to the qconfig_mapping</span>
<span class="sd">        # otherwise the configuration in qconfig_mapping will be ignored</span>
<span class="sd">        #</span>
<span class="sd">        # Example:</span>
<span class="sd">        # in qconfig_mapping, user sets linear module to be quantized with quint8 for</span>
<span class="sd">        # activation and qint8 for weight:</span>
<span class="sd">        # qconfig = torch.ao.quantization.QConfig(</span>
<span class="sd">        #     observer=MinMaxObserver.with_args(dtype=torch.quint8),</span>
<span class="sd">        #     weight=MinMaxObserver.with-args(dtype=torch.qint8))</span>
<span class="sd">        # Note: current qconfig api does not support setting output observer, but</span>
<span class="sd">        # we may extend this to support these more fine grained control in the</span>
<span class="sd">        # future</span>
<span class="sd">        #</span>
<span class="sd">        # qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)</span>
<span class="sd">        # in backend config, linear module also supports in this configuration:</span>
<span class="sd">        # weighted_int8_dtype_config = DTypeConfig(</span>
<span class="sd">        #   input_dtype=torch.quint8,</span>
<span class="sd">        #   output_dtype=torch.quint8,</span>
<span class="sd">        #   weight_dtype=torch.qint8,</span>
<span class="sd">        #   bias_type=torch.float)</span>

<span class="sd">        # linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \</span>
<span class="sd">        #    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \</span>
<span class="sd">        #    .add_dtype_config(weighted_int8_dtype_config) \</span>
<span class="sd">        #    ...</span>

<span class="sd">        # backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)</span>
<span class="sd">        # `prepare_fx` will check that the setting requested by suer in qconfig_mapping</span>
<span class="sd">        # is supported by the backend_config and insert observers and fake quant modules</span>
<span class="sd">        # in the model</span>
<span class="sd">        prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)</span>
<span class="sd">        # Run calibration</span>
<span class="sd">        calibrate(prepared_model, sample_inference_data)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.prepare_fx&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_prepare_fx</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">qconfig_mapping</span><span class="p">,</span>
        <span class="kc">False</span><span class="p">,</span>  <span class="c1"># is_qat</span>
        <span class="n">example_inputs</span><span class="p">,</span>
        <span class="n">prepare_custom_config</span><span class="p">,</span>
        <span class="n">_equalization_config</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="p">,</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="prepare_qat_fx"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx">[docs]</a><span class="k">def</span> <span class="nf">prepare_qat_fx</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">qconfig_mapping</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="n">example_inputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">prepare_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">PrepareCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Prepare a model for quantization aware training</span>

<span class="sd">    Args:</span>
<span class="sd">      * `model` (torch.nn.Module): torch.nn.Module model</span>
<span class="sd">      * `qconfig_mapping` (QConfigMapping): see :func:`~torch.ao.quantization.prepare_fx`</span>
<span class="sd">      * `example_inputs` (Tuple[Any, ...]): see :func:`~torch.ao.quantization.prepare_fx`</span>
<span class="sd">      * `prepare_custom_config` (PrepareCustomConfig): see :func:`~torch.ao.quantization.prepare_fx`</span>
<span class="sd">      * `backend_config` (BackendConfig): see :func:`~torch.ao.quantization.prepare_fx`</span>

<span class="sd">    Return:</span>
<span class="sd">      A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for</span>
<span class="sd">      quantization aware training</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch</span>
<span class="sd">        from torch.ao.quantization import get_default_qat_qconfig_mapping</span>
<span class="sd">        from torch.ao.quantization import prepare_fx</span>

<span class="sd">        class Submodule(torch.nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super().__init__()</span>
<span class="sd">                self.linear = torch.nn.Linear(5, 5)</span>
<span class="sd">            def forward(self, x):</span>
<span class="sd">                x = self.linear(x)</span>
<span class="sd">                return x</span>

<span class="sd">        class M(torch.nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super().__init__()</span>
<span class="sd">                self.linear = torch.nn.Linear(5, 5)</span>
<span class="sd">                self.sub = Submodule()</span>

<span class="sd">            def forward(self, x):</span>
<span class="sd">                x = self.linear(x)</span>
<span class="sd">                x = self.sub(x) + x</span>
<span class="sd">                return x</span>

<span class="sd">        # initialize a floating point model</span>
<span class="sd">        float_model = M().train()</span>
<span class="sd">        # (optional, but preferred) load the weights from pretrained model</span>
<span class="sd">        # float_model.load_weights(...)</span>

<span class="sd">        # define the training loop for quantization aware training</span>
<span class="sd">        def train_loop(model, train_data):</span>
<span class="sd">            model.train()</span>
<span class="sd">            for image, target in data_loader:</span>
<span class="sd">                ...</span>

<span class="sd">        # qconfig is the configuration for how we insert observers for a particular</span>
<span class="sd">        # operator</span>
<span class="sd">        # qconfig = get_default_qconfig(&quot;fbgemm&quot;)</span>
<span class="sd">        # Example of customizing qconfig:</span>
<span class="sd">        # qconfig = torch.ao.quantization.QConfig(</span>
<span class="sd">        #    activation=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)),</span>
<span class="sd">        #    weight=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)))</span>
<span class="sd">        # `activation` and `weight` are constructors of observer module</span>

<span class="sd">        # qconfig_mapping is a collection of quantization configurations, user can</span>
<span class="sd">        # set the qconfig for each operator (torch op calls, functional calls, module calls)</span>
<span class="sd">        # in the model through qconfig_mapping</span>
<span class="sd">        # the following call will get the qconfig_mapping that works best for models</span>
<span class="sd">        # that target &quot;fbgemm&quot; backend</span>
<span class="sd">        qconfig_mapping = get_default_qat_qconfig(&quot;fbgemm&quot;)</span>

<span class="sd">        # We can customize qconfig_mapping in different ways, please take a look at</span>
<span class="sd">        # the docstring for :func:`~torch.ao.quantization.prepare_fx` for different ways</span>
<span class="sd">        # to configure this</span>

<span class="sd">        # example_inputs is a tuple of inputs, that is used to infer the type of the</span>
<span class="sd">        # outputs in the model</span>
<span class="sd">        # currently it&#39;s not used, but please make sure model(*example_inputs) runs</span>
<span class="sd">        example_inputs = (torch.randn(1, 3, 224, 224),)</span>

<span class="sd">        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack</span>
<span class="sd">        # e.g. backend_config = get_default_backend_config(&quot;fbgemm&quot;)</span>
<span class="sd">        # `prepare_qat_fx` inserts observers in the model based on qconfig_mapping and</span>
<span class="sd">        # backend_config, if the configuration for an operator in qconfig_mapping</span>
<span class="sd">        # is supported in the backend_config (meaning it&#39;s supported by the target</span>
<span class="sd">        # hardware), we&#39;ll insert fake_quantize modules according to the qconfig_mapping</span>
<span class="sd">        # otherwise the configuration in qconfig_mapping will be ignored</span>
<span class="sd">        # see :func:`~torch.ao.quantization.prepare_fx` for a detailed explanation of</span>
<span class="sd">        # how qconfig_mapping interacts with backend_config</span>
<span class="sd">        prepared_model = prepare_qat_fx(float_model, qconfig_mapping, example_inputs)</span>
<span class="sd">        # Run training</span>
<span class="sd">        train_loop(prepared_model, train_loop)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.prepare_qat_fx&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_prepare_fx</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">qconfig_mapping</span><span class="p">,</span>
        <span class="kc">True</span><span class="p">,</span>  <span class="c1"># is_qat</span>
        <span class="n">example_inputs</span><span class="p">,</span>
        <span class="n">prepare_custom_config</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">backend_config</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_convert_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">is_reference</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">convert_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvertCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_standalone_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">_remove_qconfig</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">qconfig_mapping</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_decomposed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot; `is_standalone_module`: see docs in :func:`~torch.ao.quantization.prepare_standalone_module_fx`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">convert_custom_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">convert_custom_config</span> <span class="o">=</span> <span class="n">ConvertCustomConfig</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">convert_custom_config</span><span class="p">,</span> <span class="n">Dict</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Passing a convert_custom_config_dict to convert is deprecated and will not be supported &quot;</span>
            <span class="s2">&quot;in a future version. Please pass in a ConvertCustomConfig instead.&quot;</span><span class="p">)</span>
        <span class="n">convert_custom_config</span> <span class="o">=</span> <span class="n">ConvertCustomConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">convert_custom_config</span><span class="p">)</span>

    <span class="n">_check_is_graph_module</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
    <span class="n">preserved_attr_names</span> <span class="o">=</span> <span class="n">convert_custom_config</span><span class="o">.</span><span class="n">preserved_attributes</span>
    <span class="n">preserved_attrs</span> <span class="o">=</span> <span class="p">{</span><span class="n">attr</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">preserved_attr_names</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">graph_module</span><span class="p">,</span> <span class="n">attr</span><span class="p">)}</span>

    <span class="n">quantized</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="p">,</span>
        <span class="n">convert_custom_config</span><span class="p">,</span>
        <span class="n">is_standalone_module</span><span class="p">,</span>
        <span class="n">_remove_qconfig_flag</span><span class="o">=</span><span class="n">_remove_qconfig</span><span class="p">,</span>
        <span class="n">qconfig_mapping</span><span class="o">=</span><span class="n">qconfig_mapping</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">backend_config</span><span class="p">,</span>
        <span class="n">is_decomposed</span><span class="o">=</span><span class="n">is_decomposed</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">attach_preserved_attrs_to_model</span><span class="p">(</span><span class="n">quantized</span><span class="p">,</span> <span class="n">preserved_attrs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">quantized</span>


<div class="viewcode-block" id="convert_fx"><a class="viewcode-back" href="../../../../generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx">[docs]</a><span class="k">def</span> <span class="nf">convert_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">convert_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvertCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">_remove_qconfig</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">qconfig_mapping</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Convert a calibrated or trained model to a quantized model</span>

<span class="sd">    Args:</span>
<span class="sd">        * `graph_module` (torch.fx.GraphModule): A prepared and calibrated/trained model (GraphModule)</span>

<span class="sd">        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.</span>
<span class="sd">            See :class:`~torch.ao.quantization.fx.custom_config.ConvertCustomConfig` for more details</span>

<span class="sd">        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.</span>

<span class="sd">        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.</span>

<span class="sd">           The keys must include the ones in the qconfig_mapping passed to `prepare_fx` or `prepare_qat_fx`,</span>
<span class="sd">           with the same values or `None`. Additional keys can be specified with values set to `None`.</span>

<span class="sd">          For each entry whose value is set to None, we skip quantizing that entry in the model::</span>

<span class="sd">            qconfig_mapping = QConfigMapping</span>
<span class="sd">                .set_global(qconfig_from_prepare)</span>
<span class="sd">                .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add</span>
<span class="sd">                .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)</span>
<span class="sd">                .set_module_name(&quot;foo.bar&quot;, None)  # skip quantizing module &quot;foo.bar&quot;</span>

<span class="sd">         * `backend_config` (BackendConfig): A configuration for the backend which describes how</span>
<span class="sd">            operators should be quantized in the backend, this includes quantization</span>
<span class="sd">            mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.),</span>
<span class="sd">            observer placement for each operators and fused operators.</span>
<span class="sd">            See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details</span>

<span class="sd">    Return:</span>
<span class="sd">        A quantized model (torch.nn.Module)</span>

<span class="sd">    Example::</span>

<span class="sd">        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training</span>
<span class="sd">        # convert_fx converts a calibrated/trained model to a quantized model for the</span>
<span class="sd">        # target hardware, this includes converting the model first to a reference</span>
<span class="sd">        # quantized model, and then lower the reference quantized model to a backend</span>
<span class="sd">        # Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and</span>
<span class="sd">        # they share the same set of quantized operators, so we are using the same</span>
<span class="sd">        # lowering procedure</span>
<span class="sd">        #</span>
<span class="sd">        # backend_config defines the corresponding reference quantized module for</span>
<span class="sd">        # the weighted modules in the model, e.g. nn.Linear</span>
<span class="sd">        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack</span>
<span class="sd">        # e.g. backend_config = get_default_backend_config(&quot;fbgemm&quot;)</span>
<span class="sd">        quantized_model = convert_fx(prepared_model)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.convert_fx&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_convert_fx</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">convert_custom_config</span><span class="o">=</span><span class="n">convert_custom_config</span><span class="p">,</span>
        <span class="n">_remove_qconfig</span><span class="o">=</span><span class="n">_remove_qconfig</span><span class="p">,</span>
        <span class="n">qconfig_mapping</span><span class="o">=</span><span class="n">qconfig_mapping</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">backend_config</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="k">def</span> <span class="nf">convert_to_reference_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">convert_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvertCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">_remove_qconfig</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">qconfig_mapping</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Convert a calibrated or trained model to a reference quantized model,</span>
<span class="sd">    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,</span>
<span class="sd">    reference quantzied model is a standard representation of a quantized model provided</span>
<span class="sd">    by FX Graph Mode Quantization, it can be further lowered to run on the target</span>
<span class="sd">    hardware, like accelerators</span>

<span class="sd">    Args:</span>
<span class="sd">        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)</span>

<span class="sd">        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.</span>
<span class="sd">            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.</span>

<span class="sd">        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.</span>

<span class="sd">        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.</span>
<span class="sd">            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.</span>

<span class="sd">         * `backend_config` (BackendConfig): A configuration for the backend which describes how</span>
<span class="sd">            operators should be quantized in the backend. See</span>
<span class="sd">            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.</span>

<span class="sd">    Return:</span>
<span class="sd">        A reference quantized model (GraphModule)</span>

<span class="sd">    Example::</span>

<span class="sd">        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training</span>
<span class="sd">        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack</span>
<span class="sd">        # e.g. backend_config = get_default_backend_config(&quot;fbgemm&quot;)</span>
<span class="sd">        reference_quantized_model = convert_to_reference_fx(prepared_model)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx.convert_to_reference_fx&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_convert_fx</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_custom_config</span><span class="o">=</span><span class="n">convert_custom_config</span><span class="p">,</span>
        <span class="n">_remove_qconfig</span><span class="o">=</span><span class="n">_remove_qconfig</span><span class="p">,</span>
        <span class="n">qconfig_mapping</span><span class="o">=</span><span class="n">qconfig_mapping</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">backend_config</span><span class="p">,</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">_convert_to_reference_decomposed_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">convert_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvertCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">_remove_qconfig</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">qconfig_mapping</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">QConfigMapping</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">backend_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BackendConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Convert a calibrated or trained model to a reference quantized model, with</span>
<span class="sd">    decomposed representation for quantized Tensor</span>
<span class="sd">    see https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md for more details,</span>
<span class="sd">    reference quantzied model is a standard representation of a quantized model provided</span>
<span class="sd">    by FX Graph Mode Quantization, it can be further lowered to run on the target</span>
<span class="sd">    hardware, like accelerators</span>

<span class="sd">    Note: this is not public API</span>

<span class="sd">    Args:</span>
<span class="sd">        * `graph_module` (GraphModule): A prepared and calibrated/trained model (GraphModule)</span>

<span class="sd">        * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function.</span>
<span class="sd">            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.</span>

<span class="sd">        * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert.</span>

<span class="sd">        * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization.</span>
<span class="sd">            See :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.</span>

<span class="sd">         * `backend_config` (BackendConfig): A configuration for the backend which describes how</span>
<span class="sd">            operators should be quantized in the backend. See</span>
<span class="sd">            :func:`~torch.ao.quantization.quantize_fx.convert_fx` for more details.</span>

<span class="sd">    Return:</span>
<span class="sd">        A reference quantized model (GraphModule) with operators working with decomposed quantized Tensor</span>

<span class="sd">    Example::</span>

<span class="sd">        # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training</span>
<span class="sd">        # TODO: add backend_config after we split the backend_config for fbgemm and qnnpack</span>
<span class="sd">        # e.g. backend_config = get_default_backend_config(&quot;fbgemm&quot;)</span>
<span class="sd">        reference_quantized_model = _convert_to_reference_decomposed_fx(prepared_model)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;quantization_api.quantize_fx._convert_to_reference_decomposed_fx&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_convert_fx</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_custom_config</span><span class="o">=</span><span class="n">convert_custom_config</span><span class="p">,</span>
        <span class="n">_remove_qconfig</span><span class="o">=</span><span class="n">_remove_qconfig</span><span class="p">,</span>
        <span class="n">qconfig_mapping</span><span class="o">=</span><span class="n">qconfig_mapping</span><span class="p">,</span>
        <span class="n">backend_config</span><span class="o">=</span><span class="n">backend_config</span><span class="p">,</span>
        <span class="n">is_decomposed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_convert_standalone_module_fx</span><span class="p">(</span>
    <span class="n">graph_module</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">is_reference</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">convert_custom_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ConvertCustomConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; [Internal use only] Convert a model produced by :func:`~torch.ao.quantization.prepare_standalone_module_fx`</span>
<span class="sd">    and convert it to a quantized model</span>

<span class="sd">    Returns a quantized standalone module, whether input/output is quantized is</span>
<span class="sd">    specified by prepare_custom_config, with</span>
<span class="sd">    input_quantized_idxs, output_quantized_idxs, please</span>
<span class="sd">    see docs for prepare_fx for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_convert_fx</span><span class="p">(</span>
        <span class="n">graph_module</span><span class="p">,</span>
        <span class="n">is_reference</span><span class="p">,</span>
        <span class="n">convert_custom_config</span><span class="p">,</span>
        <span class="n">is_standalone_module</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/clipboard.min.js"></script>
         <script src="../../../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Modules &mdash; PyTorch 1.8.0 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/notes/modules.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Multiprocessing best practices" href="multiprocessing.html" />
    <link rel="prev" title="Features for large-scale deployments" href="large_scale_deployments.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/elastic/">
                  <span class="dropdown-title">TorchElastic</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/docs/versions.html'>1.8.0 &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          


            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ddp_comm_hooks.html">DDP Communication Hooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pipeline.html">Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Modules</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/notes/modules.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="modules">
<span id="id1"></span><h1>Modules<a class="headerlink" href="#modules" title="Permalink to this headline">¶</a></h1>
<p>PyTorch uses modules to represent neural networks. Modules are:</p>
<ul class="simple">
<li><p><strong>Building blocks of stateful computation.</strong>
PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for
easy construction of elaborate, multi-layer neural networks.</p></li>
<li><p><strong>Tightly integrated with PyTorch’s</strong>
<a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">autograd</a>
<strong>system.</strong> Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.</p></li>
<li><p><strong>Easy to work with and transform.</strong> Modules are straightforward to save and restore, transfer between
CPU / GPU / TPU devices, prune, quantize, and more.</p></li>
</ul>
<p>This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,
many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents
are provided here as well.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#a-simple-custom-module" id="id3">A Simple Custom Module</a></p></li>
<li><p><a class="reference internal" href="#modules-as-building-blocks" id="id4">Modules as Building Blocks</a></p></li>
<li><p><a class="reference internal" href="#neural-network-training-with-modules" id="id5">Neural Network Training with Modules</a></p></li>
<li><p><a class="reference internal" href="#module-state" id="id6">Module State</a></p></li>
<li><p><a class="reference internal" href="#module-hooks" id="id7">Module Hooks</a></p></li>
<li><p><a class="reference internal" href="#advanced-features" id="id8">Advanced Features</a></p></li>
</ul>
</div>
<div class="section" id="a-simple-custom-module">
<h2><a class="toc-backref" href="#id3">A Simple Custom Module</a><a class="headerlink" href="#a-simple-custom-module" title="Permalink to this headline">¶</a></h2>
<p>To get started, let’s look at a simpler, custom version of PyTorch’s <a class="reference internal" href="../generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code></a> module.
This module applies an affine transformation to its input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MyLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">input</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</pre></div>
</div>
<p>This simple module has the following fundamental characteristics of modules:</p>
<ul class="simple">
<li><p><strong>It inherits from the base Module class.</strong>
All modules should subclass <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> for composability with other modules.</p></li>
<li><p><strong>It defines some “state” that is used in computation.</strong>
Here, the state consists of randomly-initialized <code class="docutils literal notranslate"><span class="pre">weight</span></code> and <code class="docutils literal notranslate"><span class="pre">bias</span></code> tensors that define the affine
transformation. Because each of these is defined as a <a class="reference internal" href="../generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>, they are
<em>registered</em> for the module and will automatically be tracked and returned from calls
to <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">parameters()</span></code></a>. Parameters can be
considered the “learnable” aspects of the module’s computation (more on this later). Note that modules
are not required to have state, and can also be stateless.</p></li>
<li><p><strong>It defines a forward() function that performs the computation.</strong> For this affine transformation module, the input
is matrix-multiplied with the <code class="docutils literal notranslate"><span class="pre">weight</span></code> parameter (using the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> short-hand notation) and added to the <code class="docutils literal notranslate"><span class="pre">bias</span></code>
parameter to produce the output. More generally, the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> implementation for a module can perform arbitrary
computation involving any number of inputs and outputs.</p></li>
</ul>
<p>This simple module demonstrates how modules package state and computation together. Instances of this module can be
constructed and called:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">m</span><span class="p">(</span><span class="n">sample_input</span><span class="p">)</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.3037</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0413</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2057</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the module itself is callable, and that calling it invokes its <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function.
This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.
The “forward pass” is responsible for applying the computation represented by the module
to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of
module outputs with respect to its inputs, which can be used for “training” parameters through gradient
descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it
is not required to manually implement a <code class="docutils literal notranslate"><span class="pre">backward()</span></code> function for each module. The process of training
module parameters through successive forward / backward passes is covered in detail in
<a class="reference internal" href="#neural-network-training-with-modules"><span class="std std-ref">Neural Network Training with Modules</span></a>.</p>
<p>The full set of parameters registered by the module can be iterated through via a call to
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">parameters()</span></code></a> or <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_parameters" title="torch.nn.Module.named_parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_parameters()</span></code></a>,
where the latter includes each parameter’s name:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>
<span class="p">:</span> <span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.0597</span><span class="p">,</span>  <span class="mf">1.1796</span><span class="p">,</span>  <span class="mf">0.8247</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.5080</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2635</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1045</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0593</span><span class="p">,</span>  <span class="mf">0.2469</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4299</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4926</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5457</span><span class="p">,</span>  <span class="mf">0.4793</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.3634</span><span class="p">,</span>  <span class="mf">0.2015</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8525</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<p>In general, the parameters registered by a module are aspects of the module’s computation that should be
“learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers.
Before we get to that, however, let’s first examine how modules can be composed with one another.</p>
</div>
<div class="section" id="modules-as-building-blocks">
<h2><a class="toc-backref" href="#id4">Modules as Building Blocks</a><a class="headerlink" href="#modules-as-building-blocks" title="Permalink to this headline">¶</a></h2>
<p>Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.
The simplest way to do this is using the <a class="reference internal" href="../generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a> module. It allows us to chain together
multiple modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
  <span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
  <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
  <span class="n">MyLinear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">net</span><span class="p">(</span><span class="n">sample_input</span><span class="p">)</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.6749</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <a class="reference internal" href="../generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a> automatically feeds the output of the first <code class="docutils literal notranslate"><span class="pre">MyLinear</span></code> module as input
into the <a class="reference internal" href="../generated/torch.nn.ReLU.html#torch.nn.ReLU" title="torch.nn.ReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code></a>, and the output of that as input into the second <code class="docutils literal notranslate"><span class="pre">MyLinear</span></code> module. As
shown, it is limited to in-order chaining of modules.</p>
<p>In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives
full flexibility on how submodules are used for a module’s computation.</p>
<p>For example, here’s a simple neural network implemented as a custom module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l0</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>This module is composed of two “children” or “submodules” (<code class="docutils literal notranslate"><span class="pre">l0</span></code> and <code class="docutils literal notranslate"><span class="pre">l1</span></code>) that define the layers of
the neural network and are utilized for computation within the module’s <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method. Immediate
children of a module can be iterated through via a call to <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.children" title="torch.nn.Module.children"><code class="xref py py-func docutils literal notranslate"><span class="pre">children()</span></code></a> or
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_children" title="torch.nn.Module.named_children"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_children()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
<span class="p">:</span> <span class="p">(</span><span class="s1">&#39;l0&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
<span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
</pre></div>
</div>
<p>To go deeper than just the immediate children, <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.modules" title="torch.nn.Module.modules"><code class="xref py py-func docutils literal notranslate"><span class="pre">modules()</span></code></a> and
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_modules" title="torch.nn.Module.named_modules"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_modules()</span></code></a> <em>recursively</em> iterate through a module and its child modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BigNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">big_net</span> <span class="o">=</span> <span class="n">BigNet</span><span class="p">()</span>
<span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">big_net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
<span class="p">:</span> <span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">BigNet</span><span class="p">(</span>
  <span class="p">(</span><span class="n">l1</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
  <span class="p">(</span><span class="n">net</span><span class="p">):</span> <span class="n">Net</span><span class="p">(</span>
    <span class="p">(</span><span class="n">l0</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
    <span class="p">(</span><span class="n">l1</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
  <span class="p">)</span>
<span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
<span class="p">(</span><span class="s1">&#39;net&#39;</span><span class="p">,</span> <span class="n">Net</span><span class="p">(</span>
  <span class="p">(</span><span class="n">l0</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
  <span class="p">(</span><span class="n">l1</span><span class="p">):</span> <span class="n">MyLinear</span><span class="p">()</span>
<span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;net.l0&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
<span class="p">(</span><span class="s1">&#39;net.l1&#39;</span><span class="p">,</span> <span class="n">MyLinear</span><span class="p">())</span>
</pre></div>
</div>
<p>Sometimes, it’s necessary for a module to dynamically define submodules.
The <a class="reference internal" href="../generated/torch.nn.ModuleList.html#torch.nn.ModuleList" title="torch.nn.ModuleList"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleList</span></code></a> and <a class="reference internal" href="../generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDict</span></code></a> modules are useful here; they
register submodules from a list or dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DynamicNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
      <span class="p">[</span><span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activations</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span>
      <span class="s1">&#39;relu&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
      <span class="s1">&#39;lrelu&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">()</span>
    <span class="p">})</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">final</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">act</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">linear</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activations</span><span class="p">[</span><span class="n">act</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">dynamic_net</span> <span class="o">=</span> <span class="n">DynamicNet</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">dynamic_net</span><span class="p">(</span><span class="n">sample_input</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.
This means that calls to <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">parameters()</span></code></a> and <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_parameters" title="torch.nn.Module.named_parameters"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_parameters()</span></code></a> will
recursively include child parameters, allowing for convenient optimization of all parameters within the network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">dynamic_net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>
<span class="p">:</span> <span class="p">(</span><span class="s1">&#39;linears.0.weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.2051</span><span class="p">,</span>  <span class="mf">0.7601</span><span class="p">,</span>  <span class="mf">1.1065</span><span class="p">,</span>  <span class="mf">0.1963</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">3.0592</span><span class="p">,</span>  <span class="mf">0.4354</span><span class="p">,</span>  <span class="mf">1.6598</span><span class="p">,</span>  <span class="mf">0.9828</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4446</span><span class="p">,</span>  <span class="mf">0.4628</span><span class="p">,</span>  <span class="mf">0.8774</span><span class="p">,</span>  <span class="mf">1.6848</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1222</span><span class="p">,</span>  <span class="mf">1.5458</span><span class="p">,</span>  <span class="mf">1.1729</span><span class="p">,</span>  <span class="mf">1.4647</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.0.bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.5310</span><span class="p">,</span>  <span class="mf">1.0609</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0940</span><span class="p">,</span>  <span class="mf">1.1266</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.1.weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.1113</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0623</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0806</span><span class="p">,</span>  <span class="mf">0.3508</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0550</span><span class="p">,</span>  <span class="mf">1.5317</span><span class="p">,</span>  <span class="mf">1.1064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5562</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.4028</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6942</span><span class="p">,</span>  <span class="mf">1.5793</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0140</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0329</span><span class="p">,</span>  <span class="mf">0.1160</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7183</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0434</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.1.bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.0361</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9768</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3889</span><span class="p">,</span>  <span class="mf">1.1613</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.2.weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6340</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3887</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9979</span><span class="p">,</span>  <span class="mf">0.0767</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3526</span><span class="p">,</span>  <span class="mf">0.8756</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5847</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6016</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.3269</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1608</span><span class="p">,</span>  <span class="mf">0.2897</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0829</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">2.6338</span><span class="p">,</span>  <span class="mf">0.9239</span><span class="p">,</span>  <span class="mf">0.6943</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5034</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;linears.2.bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.0268</span><span class="p">,</span>  <span class="mf">0.4489</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9403</span><span class="p">,</span>  <span class="mf">0.1571</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;final.weight&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.2509</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5052</span><span class="p">],</span> <span class="p">[</span> <span class="mf">0.3088</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.4951</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;final.bias&#39;</span><span class="p">,</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.3381</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<p>It’s also easy to move all parameters to a different device or change their precision using
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.to" title="torch.nn.Module.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Move all parameters to a CUDA device</span>
<span class="n">dynamic_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="c1"># Change precision of all parameters</span>
<span class="n">dynamic_net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">dynamic_net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>
<span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">6.5166</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>These examples show how elaborate neural networks can be formed through module composition. To allow for
quick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of
performant modules within the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code> namespace that perform computation commonly found within neural
networks, including pooling, convolutions, loss functions, etc.</p>
<p>In the next section, we give a full example of training a neural network.</p>
<p>For more information, check out:</p>
<ul class="simple">
<li><p>Recursively <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.apply" title="torch.nn.Module.apply"><code class="xref py py-func docutils literal notranslate"><span class="pre">apply()</span></code></a> a function to a module and its submodules</p></li>
<li><p>Library of PyTorch-provided modules: <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">torch.nn</a></p></li>
<li><p>Defining neural net modules: <a class="reference external" href="https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html">https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html</a></p></li>
</ul>
</div>
<div class="section" id="neural-network-training-with-modules">
<span id="id2"></span><h2><a class="toc-backref" href="#id5">Neural Network Training with Modules</a><a class="headerlink" href="#neural-network-training-with-modules" title="Permalink to this headline">¶</a></h2>
<p>Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s
Optimizers from <a class="reference internal" href="../optim.html#module-torch.optim" title="torch.optim"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.optim</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the network (from previous section) and optimizer</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Run a sample training loop that &quot;teaches&quot; the network</span>
<span class="c1"># to output the constant zero function</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
  <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
  <span class="n">net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according
to its absolute value by employing <a class="reference internal" href="../generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a> as a loss function. While this is not a very interesting task, the
key parts of training are present:</p>
<ul class="simple">
<li><p>A network is created.</p></li>
<li><p>An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s
parameters are associated with it.</p></li>
<li><dl class="simple">
<dt>A training loop…</dt><dd><ul>
<li><p>acquires an input,</p></li>
<li><p>runs the network,</p></li>
<li><p>computes a loss,</p></li>
<li><p>zeros the network’s parameters’ gradients,</p></li>
<li><p>calls loss.backward() to update the parameters’ gradients,</p></li>
<li><p>calls optimizer.step() to apply the gradients to the parameters.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the
value of <code class="docutils literal notranslate"><span class="pre">l1</span></code>’s <code class="docutils literal notranslate"><span class="pre">weight</span></code> parameter shows that its values are now much closer to 0 (as may be expected):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">l1</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="p">:</span> <span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0013</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0030</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0008</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Training neural networks can often be tricky. For more information, check out:</p>
<ul class="simple">
<li><p>Using Optimizers: <a class="reference external" href="https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html">https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html</a>.</p></li>
<li><p>Neural network training: <a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</a></p></li>
<li><p>Introduction to autograd: <a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html</a></p></li>
</ul>
</div>
<div class="section" id="module-state">
<h2><a class="toc-backref" href="#id6">Module State</a><a class="headerlink" href="#module-state" title="Permalink to this headline">¶</a></h2>
<p>In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation.
Now, if we want to save the trained model to disk, we can do so by saving its <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> (i.e. “state dictionary”):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the module</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;net.pt&#39;</span><span class="p">)</span>

<span class="o">...</span>

<span class="c1"># Load the module later on</span>
<span class="n">new_net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">new_net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;net.pt&#39;</span><span class="p">))</span>
<span class="p">:</span> <span class="o">&lt;</span><span class="n">All</span> <span class="n">keys</span> <span class="n">matched</span> <span class="n">successfully</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>A module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> contains state that affects its computation. This includes, but is not limited to, the
module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module
computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent”
and “non-persistent”. Following is an overview of the various types of state a module can have:</p>
<ul class="simple">
<li><p><strong>Parameters</strong>: learnable aspects of computation; contained within the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code></p></li>
<li><p><strong>Buffers</strong>: non-learnable aspects of computation</p>
<ul>
<li><p><strong>Persistent</strong> buffers: contained within the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> (i.e. serialized when saving &amp; loading)</p></li>
<li><p><strong>Non-persistent</strong> buffers: not contained within the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> (i.e. left out of serialization)</p></li>
</ul>
</li>
</ul>
<p>As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want
the current value of the running mean to be considered part of the module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> so that it will be
restored when loading a serialized form of the module, but we don’t want it to be learnable.
This snippet shows how to use <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_buffer" title="torch.nn.Module.register_buffer"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_buffer()</span></code></a> to accomplish this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RunningMean</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</pre></div>
</div>
<p>Now, the current value of the running mean is considered part of the module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>
and will be properly restored when loading the module from disk:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">RunningMean</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
  <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
<span class="p">:</span> <span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.1041</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1113</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0647</span><span class="p">,</span>  <span class="mf">0.1515</span><span class="p">]))]))</span>

<span class="c1"># Serialized form will contain the &#39;mean&#39; tensor</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;mean.pt&#39;</span><span class="p">)</span>

<span class="n">m_loaded</span> <span class="o">=</span> <span class="n">RunningMean</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">m_loaded</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mean.pt&#39;</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">mean</span> <span class="o">==</span> <span class="n">m_loaded</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span>
</pre></div>
</div>
<p>As mentioned previously, buffers can be left out of the module’s <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> by marking them as non-persistent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;unserialized_thing&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.to" title="torch.nn.Module.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Moves all module parameters and buffers to the specified device / dtype</span>
<span class="n">m</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
</div>
<p>Buffers of a module can be iterated over using <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.buffers" title="torch.nn.Module.buffers"><code class="xref py py-func docutils literal notranslate"><span class="pre">buffers()</span></code></a> or
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.named_buffers" title="torch.nn.Module.named_buffers"><code class="xref py py-func docutils literal notranslate"><span class="pre">named_buffers()</span></code></a>.</p>
<p>For more information, check out:</p>
<ul class="simple">
<li><p>Saving and loading: <a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">https://pytorch.org/tutorials/beginner/saving_loading_models.html</a></p></li>
<li><p>Serialization semantics: <a class="reference external" href="https://pytorch.org/docs/master/notes/serialization.html">https://pytorch.org/docs/master/notes/serialization.html</a></p></li>
<li><p>What is a state dict? <a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html">https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html</a></p></li>
</ul>
</div>
<div class="section" id="module-hooks">
<h2><a class="toc-backref" href="#id7">Module Hooks</a><a class="headerlink" href="#module-hooks" title="Permalink to this headline">¶</a></h2>
<p>In <a class="reference internal" href="#neural-network-training-with-modules"><span class="std std-ref">Neural Network Training with Modules</span></a>, we demonstrated the training process for a module, which iteratively
performs forward and backward passes, updating module parameters each iteration. For more control
over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward
pass, even modifying how the pass is done if desired. Some useful examples for this functionality include
debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules
you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules.</p>
<p>PyTorch provides two types of hooks for modules:</p>
<ul class="simple">
<li><p><strong>Forward hooks</strong> are called during the forward pass. They can be installed for a given module with
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook" title="torch.nn.Module.register_forward_pre_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_forward_pre_hook()</span></code></a> and <a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook" title="torch.nn.Module.register_forward_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_forward_hook()</span></code></a>.
These hooks will be called respectively just before the forward function is called and just after it is called.
Alternatively, these hooks can be installed globally for all modules with the analagous
<a class="reference internal" href="../generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook" title="torch.nn.modules.module.register_module_forward_pre_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_forward_pre_hook()</span></code></a> and
<a class="reference internal" href="../generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook" title="torch.nn.modules.module.register_module_forward_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_forward_hook()</span></code></a> functions.</p></li>
<li><p><strong>Backward hooks</strong> are called during the backward pass. They can be installed with
<a class="reference internal" href="../generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook" title="torch.nn.Module.register_full_backward_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code></a>. These hooks will be called when the backward for this
Module has been computed and will allow the user to access the gradients for both the inputs and outputs.
Alternatively, they can be installed globally for all modules with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_full_backward_hook()</span></code>.</p></li>
</ul>
<p>All hooks allow the user to return an updated value that will be used throughout the remaining computation.
Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or
modify some inputs/outputs without having to change the module’s <code class="docutils literal notranslate"><span class="pre">forward()</span></code> function.</p>
</div>
<div class="section" id="advanced-features">
<h2><a class="toc-backref" href="#id8">Advanced Features</a><a class="headerlink" href="#advanced-features" title="Permalink to this headline">¶</a></h2>
<p>PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities
are “inherited” when writing a new module. In-depth discussion of these features can be found in the links below.</p>
<p>For more information, check out:</p>
<ul class="simple">
<li><p>Profiling: <a class="reference external" href="https://pytorch.org/tutorials/beginner/profiler.html">https://pytorch.org/tutorials/beginner/profiler.html</a></p></li>
<li><p>Pruning: <a class="reference external" href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">https://pytorch.org/tutorials/intermediate/pruning_tutorial.html</a></p></li>
<li><p>Quantization: <a class="reference external" href="https://pytorch.org/tutorials/recipes/quantization.html">https://pytorch.org/tutorials/recipes/quantization.html</a></p></li>
<li><p>Exporting modules to TorchScript (e.g. for usage from C++):
<a class="reference external" href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html">https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html</a></p></li>
</ul>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="multiprocessing.html" class="btn btn-neutral float-right" title="Multiprocessing best practices" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="large_scale_deployments.html" class="btn btn-neutral" title="Features for large-scale deployments" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Modules</a><ul>
<li><a class="reference internal" href="#a-simple-custom-module">A Simple Custom Module</a></li>
<li><a class="reference internal" href="#modules-as-building-blocks">Modules as Building Blocks</a></li>
<li><a class="reference internal" href="#neural-network-training-with-modules">Neural Network Training with Modules</a></li>
<li><a class="reference internal" href="#module-state">Module State</a></li>
<li><a class="reference internal" href="#module-hooks">Module Hooks</a></li>
<li><a class="reference internal" href="#advanced-features">Advanced Features</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<script script type="text/javascript">
  var collapsedSections = ['Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
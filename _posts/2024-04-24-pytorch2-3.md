---
layout: blog_detail
title: "PyTorch 2.3 Release Blog"
---

We are excited to announce the release of PyTorchÂ® 2.3 ([release note](https://github.com/pytorch/pytorch/releases/tag/v2.3.0))! PyTorch 2.3 offers support for user-defined Triton kernels in torch.compile, allowing for users to migrate their own Triton kernels from eager without experiencing performance regressions or graph breaks. Tensor Parallelism improves the experience for training Large Language Models using native PyTorch functions, which has been validated on training runs for 100B parameter models. As well, semi-structured sparsity implements semi-structured sparsity as a Tensor subclass, with observed speedups of up to 1.6 over dense matrix multiplication.

This release is composed of 3393 commits and 426 contributors since PyTorch 2.2. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.3. More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.


<table class="table table-bordered">
  <tr>
   <td><strong>Beta</strong>
   </td>
   <td><strong>Prototype</strong>
   </td>
   <td><strong>Performance Improvements</strong>
   </td>
  </tr>
  <tr>
   <td>User-defined Triton kernels in torch.compile
   </td>
   <td>torch.export adds new API to specify dynamic_shapes
   </td>
   <td>Weight-Only-Quantization introduced into Inductor CPU backend
   </td>
  </tr>
  <tr>
   <td>Tensor parallelism within PyTorch Distributed
   </td>
   <td>Asynchronous checkpoint generation
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>Support for semi-structured sparsity
   </td>
   <td>
   </td>
   <td>
   </td>
  </tr>
</table>


*To see a full list of public feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).


## Beta Features


### [Beta] Support for User-defined Triton kernels in _torch.compile_

Allows for PyTorch code that contains triton kernels to be executed natively using torch.compile. This enables users to migrate code containing triton kernels from eager PyTorch to _torch.compile_ without running into performance regressions or graph breaks. Native support also creates an opportunity for Torch Inductor to precompile the user-defined Triton kernel as well as better organize code around the Triton kernel allowing for further optimizations.

You can find more information about how to utilize user defined Triton kernels in torch.compile within [this tutorial](https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html).


### [Beta] Tensor Parallelism introduces more efficient ways to train LLMs

The Tensor Parallel API facilitates various tensor manipulations across GPUs/hosts and integrates with FSDP for 2D Parallelism (Tensor parallelism across devices + Data Parallelism across hosts). It also offers a low-level API for constructing higher-level Tensor parallel APIs. This API has been validated to support the training of transformer models with over 100 billion parameters.

You can find more information on how to utilize this within your workflows within [this tutorial](https://pytorch.org/tutorials/intermediate/TP_tutorial.html).


### [Beta] Semi-structured sparsity provides users with a way to take advantage of accelerated sparse inference and memory savings

_torch.sparse.SparseSemiStructuredTensor_ implements semi-structured sparsity as a Tensor subclass, which have observed speedups of up to 1.6 over dense matrix multiplication.

In particular it adds:



* Additional support for quantization composability (mixed dtype, dequant fusion)
* Updated cuSPARSELt and CUTLASS kernels
* torch.compile support

You can find more information on how to take advantage of semi-structured sparsity [here](https://pytorch.org/tutorials/advanced/semi_structured_sparse.html). 


## Prototype Features


### [PROTOTYPE] _torch.export_ adds new API to specify _dynamic_shapes_

You can now use _torch.export.Dim_ to better represent dynamic shapes by enabling developers to specify ranges (min and max values) that can be reused across different input dimensions that are constrained to be equal.

To learn more about _torch.export.Dim_ as well as how it can be used to express more interesting relationships (such as linear arithmetic expressions) check out the tutorial [here](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes).


### [PROTOTYPE] Asynchronous checkpoint generation

Asynchronous checkpoint generation allows users to continue their training loops while checkpoints are being generated, essentially offloading much of the checkpointing cost.

You can find out how to utilize this within your own workflows with this [example](https://github.com/pytorch/pytorch/blob/release/2.3/torch/distributed/checkpoint/examples/async_checkpointing_example.py).


## Performance Improvements


### [PROTOTYPE] Weight-Only-Quantization introduced into Inductor CPU backend

PyTorch 2.3 enhances LLM inference performance on torch inductor CPU backend. The project [gpt-fast](https://github.com/pytorch-labs/gpt-fast) offers a simple and efficient PyTorch native acceleration for transformer text generation with _torch.compile_. Prior to 2.3 only CUDA devices were supported and this feature enables the CPU counterpart by providing highly optimized kernels for the int4 and int8 weight only quantization Linear.

For more information / how to utilize this feature please refer to the [gpt-fast README](https://github.com/pytorch-labs/gpt-fast#quantization).
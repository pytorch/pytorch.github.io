---
layout: blog_detail
title: "PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing"
author: Team PyTorch
---

We are excited to announce the release of PyTorch® 2.1 ([release note](https://github.com/pytorch/pytorch/releases/tag/v2.1.0))! PyTorch 2.1 offers automatic dynamic shape support in _torch.compile_, _torch.distributed.checkpoint_ for saving/loading distributed training jobs on multiple ranks in parallel, and _torch.compile_ support for the NumPy API.

In addition, this release offers numerous performance improvements (e.g. CPU inductor improvements, AVX512 support, scaled-dot-product-attention support) as well as a prototype release of _torch.export_, a sound full-graph capture mechanism, and _torch.export_-based quantization.

Along with 2.1, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog. 

This release is composed of 6,682 commits and 784 contributors since 2.0. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.1.  More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.

Summary: 
- _torch.compile_ now includes automatic support for detecting and minimizing recompilations due to tensor shape changes using _automatic dynamic shapes._
- _torch.distributed.checkpoint_ enables saving and loading models from multiple ranks in parallel, as well as resharding due to changes in cluster topology.
- _torch.compile_ can now compile NumPy operations via translating them into PyTorch-equivalent operations.
- _torch.compile_ now includes improved support for Python 3.11.
- New CPU performance features include inductor improvements (e.g. bfloat16 support and dynamic shapes), AVX512 kernel support, and scaled-dot-product-attention kernels.
- _torch.export_, a sound full-graph capture mechanism is introduced as a prototype feature, as well as _torch.export_-based quantization.
- _torch.sparse_ now includes prototype support for semi-structured (2:4) sparsity on NVIDIA® GPUs.
 
 
 | **Stable** | **Beta**                                      | **Prototype**                   | **Performance Improvements**                              |
|------------|-----------------------------------------------|---------------------------------|-----------------------------------------------------------|
|            | Automatic Dynamic Shapes                      | _torch.export()_                | AVX512 kernel support                                     |
|            | _torch.distributed.checkpoint_                | Torch.export-based Quantization | CPU optimizations for scaled-dot-product-attention (SPDA) |
|            | _torch.compile_ + NumPy                       | semi-structed (2:4) sparsity    | CPU optimizations for bfloat16                            |
|            | _torch.compile_ + Python 3.11                 | _cpp_wrapper_ for torchinductor |                                                           |
|            | _torch.compile + autograd.Function_           |                                 |                                                           |
|            | third-party device integration: _PrivateUse1_ |                                 |                                                           |
 
\*To see a full list of public 2.1, 2.0, and 1.13 feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).

## **Beta Features**
 
 **(Beta) Automatic Dynamic Shapes**
 
 Dynamic shapes is functionality built into _torch.compile_ that can minimize recompilations by tracking and generating code based on the symbolic shape of a tensor rather than the static shape (e.g. _\[B, 128, 4]_ rather than _\[64, 128, 4]_). This allows _torch.compile_ to generate a single kernel that can work for many sizes, at only a modest cost to efficiency. Dynamic shapes has been greatly stabilized in PyTorch 2.1, and is now automatically enabled if _torch.compile_ notices recompilation due to varying input shapes. You can disable automatic dynamic by passing _dynamic=False_ to torch.compile, or by setting _torch.\_dynamo.config.automatic\_dynamic\_shapes = False_.
 
 In PyTorch 2.1, we have shown good performance with dynamic shapes enabled on a variety of model types, including large language models, on both CUDA and CPU.
 
 For more information on dynamic shapes, see [this documentation](https://pytorch.org/docs/2.1/torch.compiler_dynamic_shapes.html).
 
 **\[Beta] _torch.distributed.checkpoint_**
 
 _torch.distributed.checkpoint_ enables saving and loading models from multiple ranks in parallel. In addition, checkpointing automatically handles fully-qualified-name (FQN) mappings across models and optimizers, enabling load-time resharding across differing cluster topologies.
 
 For more information, see _torch.distributed.checkpoint_ [documentation](https://pytorch.org/docs/2.1/distributed.checkpoint.html) and [tutorial](https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html).
 
 **\[Beta] _torch.compile_ + _NumPy_**
 
 _torch.compile_ now understands how to compile NumPy operations via translating them into PyTorch-equivalent operations.  Because this integration operates in a device-agnostic manner, you can now GPU-accelerate NumPy programs – or even mixed NumPy/PyTorch programs – just by using _torch.compile_.
 
 Please see [this section](https://pytorch.org/docs/2.1/torch.compiler_faq.html#does-numpy-work-with-torch-compile) in the _torch.compile_ FAQ for more information about _torch.compile + NumPy interaction_, and follow the [PyTorch Blog](https://pytorch.org/blog/) for a forthcoming blog about this feature.
 
 **\[Beta] _torch.compile_ + Python 3.11**
 
 _torch.compile_ previously only supported Python versions 3.8-3.10. Users can now optimize models with _torch.compile_ in Python 3.11.
 
 **\[Beta] _torch.compile_ + _autograd.Function_**
 
 _torch.compile_ can now trace and optimize the backward function of user-defined [autograd Functions](https://pytorch.org/docs/stable/autograd.html#function), which unlocks training optimizations for models that make heavier use of extensions mechanisms.
 
 **\[Beta] Improved third-party device support: _PrivateUse1_**
 
 Third-party device types can now be registered to PyTorch using the privateuse1 dispatch key.  This allows device extensions to register new kernels to PyTorch and to associate them with the new key, allowing user code to work equivalently to built-in device types.  For example, to register _“my\_hardware\_device_”, one can do the following:    
 
```
torch.rename_privateuse1_backend("my_hardware_device")
torch.utils.generate_methods_for_privateuse1_backend()
x = torch.randn((2, 3), device='my_hardware_device')
y = x + x # run add kernel on 'my_hardware_device'
```

To validate this feature, the OSS team from _Ascend NPU_ has successfully integrated [**torch\_npu**](https://github.com/Ascend/pytorch) into pytorch as a plug-in through the _PrivateUse1_ functionality.

For more information, please see the PrivateUse1 tutorial [here](https://pytorch.org/tutorials/advanced/privateuseone.html).

## **Prototype Features**

**\[Prototype] _torch.export()_**

_torch.export()_ provides a sound tracing mechanism to capture a full graph from a PyTorch program based on new technologies provided by PT2.0.

Users can extract a clean representation (Export IR) of a PyTorch program in the form of a dataflow graph, consisting of mostly straight-line calls to PyTorch operators. Export IR can then be transformed, serialized, saved to file, transferred, loaded back for execution in an environment with or without Python.

For more information, please see the tutorial [here](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html).

**\[Prototype] _torch.export_-based Quantization**

_torch.ao.quantization_ now supports post-training static quantization on PyTorch2-based _torch.export_ flows.  This includes support for built-in _XNNPACK_ and _X64Inductor_ _Quantizer_, as well as the ability to specify one’s own _Quantizer_.

For an explanation on post-training static quantization with torch.export, see [this tutorial](https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html), for quantization-aware training for static quantization with torch.export, see [this tutorial](https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html).

For an explanation on how to write one’s own Quantizer, see [this tutorial](https://pytorch.org/tutorials/prototype/pt2e_quantizer.html).

**\[Prototype] semi-structured (2:4) sparsity for NVIDIA® GPUs**

_torch.sparse_ now supports creating and accelerating compute over semi-structured sparse (2:4) tensors.  For more information on the format, see [this](https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/) blog from NVIDIA.A minimal example introducing semi-structured sparsity is as follows:

```
from torch.sparse import to_sparse_semi_structured
 
x = torch.rand(64, 64).half().cuda()
mask = torch.tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()
linear = nn.Linear(64, 64).half().cuda()

linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))
linear(x)
```

To learn more, please see the [documentation](https://pytorch.org/docs/2.1/sparse.html#sparse-semi-structured-tensors) and accompanying [tutorial](https://pytorch.org/tutorials/prototype/semi_structured_sparse.html).

**\[Prototype] _cpp\_wrapper_ for _torchinductor_**

_cpp\_wrapper_ can reduce the Python overhead for invoking kernels in torchinductor by generating the kernel wrapper code in C++. This feature is still in the prototype phase; it does not support all programs that successfully compile in PT2 today. Please file issues if you discover limitations for your use case to help us prioritize.

The API to turn this feature on is:
```
import torch
import torch._inductor.config as config
config.cpp_wrapper = True
```

For more information, please see the [tutorial](https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html).

## **Performance Improvements**

**AVX512 kernel support**

In PyTorch 2.0, AVX2 kernels would be used even if the CPU supported AVX512 instructions.  Now, PyTorch defaults to using AVX512 CPU kernels if the CPU supports those instructions, equivalent to setting _ATEN\_CPU\_CAPABILITY=avx512_ in previous releases.  The previous behavior can be enabled by setting _ATEN\_CPU\_CAPABILITY=avx2._

**CPU optimizations for scaled-dot-product-attention (SDPA)**

Previous versions of PyTorch provided optimized CUDA implementations for transformer primitives via _torch.nn.functiona.scaled\_dot\_product\_attention_.  PyTorch 2.1 includes optimized FlashAttention-based CPU routines.

See the documentation [here](https://pytorch.org/docs/2.1/generated/torch.nn.functional.scaled_dot_product_attention.html).

**CPU optimizations for bfloat16**

PyTorch 2.1 includes CPU optimizations for bfloat16, including improved vectorization support and _torchinductor_ codegen.

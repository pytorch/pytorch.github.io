---
layout: blog_detail
title: 'New PyTorch Library Releases in PyTorch 1.9, including TorchVision, TorchAudio, and more'
author: Team PyTorch 
---

Today, we are announcing updates to a number of PyTorch libraries, alongside the [PyTorch 1.9 release](https://pytorch.org/blog/pytorch-1.9-released/). The updates include new releases for the domain libraries including TorchVision, TorchText and TorchAudio. These releases, along with the PyTorch 1.9 release, include a number of new features and improvements that will provide a broad set of updates for the PyTorch community.

Some highlights include:

* **TorchVision** - Added new SSD and SSDLite models, quantized kernels for object detection, GPU Jpeg decoding, and iOS support. See [release notes](https://github.com/pytorch/vision/releases) here.
* **TorchAudio** - Added wav2vec 2.0 model deployable in non-Python environments (including C++, Android, and iOS). Many performance improvements in lfilter, spectral operations, resampling. Added options for quality control in sampling (i.e. Kaiser window support). Initiated the migration of complex tensors operations. Improved autograd support. See [release notes](https://github.com/pytorch/audio/releases) here.
* **TorchText** - Added a new high-performance Vocab module that provides common functional APIs for NLP workflows. See [release notes](https://github.com/pytorch/text/releases) here.

We’d like to thank the community for their support and work on this latest release.

Features in PyTorch releases are classified as Stable, Beta, and Prototype. You can learn more about the definitions in [this blog post](https://pytorch.org/blog/pytorch-feature-classification-changes/). 

# TorchVision 0.10

### (Stable) Quantized kernels for object detection 
The forward pass of the nms and roi_align operators now support tensors with a quantized dtype, which can help lower the memory footprint of object detection models, particularly on mobile environments. For more details, refer to [the documentation](https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_align). 

### (Stable) Speed optimizations for Tensor transforms 
The resize and flip transforms have been optimized and its runtime improved by up to 5x on the CPU. 

### (Stable) Documentation improvements 
Significant improvements were made to the documentation. In particular, a new gallery of examples is available. These examples visually illustrate how each transform acts on an image, and also properly documents and illustrates the output of the segmentation models.

The example gallery will be extended in the future to provide more comprehensive examples and serve as a reference for common torchvision tasks. For more details, refer to [the documentation](https://pytorch.org/vision/stable/auto_examples/index.html).

### (Beta) New models for detection 
[SSD](https://arxiv.org/abs/1512.02325) and [SSDlite](https://arxiv.org/abs/1801.04381) are two popular object detection architectures that are efficient in terms of speed and provide good results for low resolution pictures. In this release, we provide implementations for the original SSD model with VGG16 backbone and for its mobile-friendly variant SSDlite with MobileNetV3-Large backbone.

The models were pre-trained on COCO train2017 and can be used as follows:

```python
import torch
import torchvision

# Original SSD variant
x = [torch.rand(3, 300, 300), torch.rand(3, 500, 400)]
m_detector = torchvision.models.detection.ssd300_vgg16(pretrained=True)
m_detector.eval()
predictions = m_detector(x)

# Mobile-friendly SSDlite variant
x = [torch.rand(3, 320, 320), torch.rand(3, 500, 400)]
m_detector = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)
m_detector.eval()
predictions = m_detector(x)
```

The following accuracies can be obtained on COCO val2017 (full results available in [#3403](https://github.com/pytorch/vision/pull/3403) and [#3757](https://github.com/pytorch/vision/pull/3757)):


 {:.table.table-striped.table-bordered}
| Model | mAP | mAP@50 | mAP@75 |
| ------------- | ------------- |  ------------- |  ------------- |
| SSD300 VGG16 | 25.1 | 41.5 | 26.2 | 
| SSDlite320 MobileNetV3-Large | 21.3 | 34.3 | 22.1 |


For more details, refer to [the documentation](https://pytorch.org/vision/stable/models.html#id37).

### (Beta) JPEG decoding on the GPU 
Decoding jpegs is now possible on GPUs with the use of [nvjpeg](https://developer.nvidia.com/nvjpeg), which should be readily available in your CUDA setup. The decoding time of a single image should be about 2 to 3 times faster than with libjpeg on CPU. While the resulting tensor will be stored on the GPU device, the input raw tensor still needs to reside on the host (CPU), because the first stages of the decoding process take place on the host:
from torchvision.io.image import read_file, decode_jpeg

```python
data = read_file('path_to_image.jpg')  # raw data is on CPU
img = decode_jpeg(data, device='cuda')  # decoded image in on GPU
```
For more details, see [the documentation](https://pytorch.org/vision/stable/io.html#torchvision.io.decode_jpeg).

### (Beta) iOS support 
TorchVision 0.10 now provides pre-compiled iOS binaries for its C++ operators, which means you can run Faster R-CNN and Mask R-CNN on iOS. An example app on how to build a program leveraging those ops can be found [here](https://github.com/pytorch/vision/tree/master/ios/VisionTestApp). 

# TorchAudio 0.9.0

### (Stable) Complex Tensor Migration 
TorchAudio has functions that handle complex-valued tensors. These functions follow a convention to use an extra dimension to represent real and imaginary parts. In PyTorch 1.6, the native complex type was introduced. As its API is getting stable, torchaudio has started to migrate to the native complex type. 

In this release, we added support for native complex tensors, and you can opt-in to use them. Using the native complex types, we have verified that affected functions continue to support autograd and TorchScript, moreover, switching to native complex types improves their performance. For more details, refer to [pytorch/audio#1337](https://github.com/pytorch/audio/issues/1337). 

### (Stable) Filtering Improvement 
In release 0.8, we added the C++ implementation of the core part of ```lfilter``` for CPU, which improved the performance. In this release, we optimized some internal operations of the CPU implementation for further performance improvement. We also added autograd support to both CPU and GPU. Now ```lfilter``` and all the ```biquad``` filters (```biquad```, ```band_biquad```, ```bass_biquad```, ```treble_biquad```, ```allpass_biquad```, ```lowpass_biquad```, ```highpass_biquad```, ```bandpass_biquad```, ```equalizer_biquad``` and ```bandrefect_biquad```) benefit from the performance improvement and support autograd. We also moved the implementation of overdrive to C++ for performance improvement. For more details, refer to [the documentation](https://pytorch.org/audio/0.9.0/functional.html#lfilter).
 
### (Stable) Improved Autograd Support 
Along with the work of Complex Tensor Migration and Filtering Improvement, we also added autograd tests to transforms. `lfilter`, `biquad` and its variants, and most transforms are now guaranteed to support autograd. For more details, refer to [the release note](https://github.com/pytorch/audio/releases).

### (Stable) Improved Windows Support 
Torchaudio implements some operations in C++ for reasons such as performance and integration with third-party libraries. These C++ components were only available on Linux and macOS. In this release, we have added support to Windows. With this, the efficient filtering implementation mentioned above is also available on Windows.

However, please note that not all the C++ components are available for Windows. “sox_io” backend and ```torchaudio.functional.compute_kaldi_pitch``` are not supported. 

### (Stable) I/O Functions Migration 
Since the 0.6 release, we have continuously improved I/O functionality. Specifically, in 0.8 we changed the default backend from “sox” to “sox_io” and applied the same switch to API of the “soundfile” backend. The 0.9 release concludes this migration by removing the deprecated backends. For more details, please refer to [#903](https://github.com/pytorch/audio/issues/903). 

### (Beta) Wav2Vec2.0 Model
We have added the model architectures from [Wav2Vec2.0](https://arxiv.org/abs/2006.11477). You can import fine-tuned models parameters published on [fairseq](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec) and [Hugging Face Hub](https://huggingface.co/models?filter=wav2vec2). Our model definition supports TorchScript, and it is possible to deploy the model to non-Python environments, such as C++, [Android](https://github.com/pytorch/android-demo-app/tree/master/SpeechRecognition) and [iOS](https://github.com/pytorch/ios-demo-app/tree/master/SpeechRecognition). 

The following code snippet illustrates such a use case. Please check out our [c++ example directory](https://github.com/pytorch/audio/tree/master/examples/libtorchaudio) for the complete example. Currently, it is designed for running inference. If you would like more support for training, please file a feature request.

```python
# Import fine-tuned model from Hugging Face Hub
import transformers
from torchaudio.models.wav2vec2.utils import import_huggingface_model

original = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
imported = import_huggingface_model(original)
```

```python
# Import fine-tuned model from fairseq
import fairseq
from torchaudio.models.wav2vec2.utils import import_fairseq_model

original, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task(
    ["wav2vec_small_960h.pt"], arg_overrides={'data': "<data_dir>"})
imported = import_fairseq_model(original[0].w2v_encoder)
```

```python
# Build uninitialized model and load state dict
from torchaudio.models import wav2vec2_base

model = wav2vec2_base(num_out=32)
model.load_state_dict(imported.state_dict())

# Quantize / script / optimize for mobile
quantized_model = torch.quantization.quantize_dynamic(
    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)
scripted_model = torch.jit.script(quantized_model)
optimized_model = optimize_for_mobile(scripted_model)
optimized_model.save("model_for_deployment.pt")
```

For more details, see [the documentation](https://pytorch.org/audio/0.9.0/models.html#wav2vec2-0). 

### (Beta) Resampling Improvement 
In release 0.8, we vectorized the operation in ```torchaudio.compliance.kaldi.resample_waveform```, which improved the performance of ```resample_waveform``` and ```torchaudio.transforms.Resample```. In this release, we have further revised the way the resampling algorithm is implemented. 

We have:
*  Added Kaiser Window support for a wider range of resampling quality.
*  Added ```rolloff``` parameter for anti-aliasing control.
*  Added the mechanism to precompute the kernel and cache it in ```torchaudio.transforms.Resample``` for even faster operation.
*  Moved the implementation from ```torchaudio.compliance.kaldi.resample_waveform``` to ```torchaudio.functional.resample``` and deprecated ```torchaudio.compliance.kaldi.resample_waveform```.

For more details, see [the documentation](https://pytorch.org/audio/0.9.0/transforms.html#resample). 

### (Prototype) RNN Transducer Loss 
The RNN transducer loss is used in training RNN transducer models, which is a popular architecture for speech recognition tasks. The prototype loss in torchaudio currently supports autograd, torchscript, float16 and float32, and can also be run on both CPU and CUDA. For more details, please refer to [the documentation](https://pytorch.org/audio/stable/index.html).

# TorchText 0.10.0

### (Beta) New Vocab Module 
In this release, we introduce a new Vocab module that replaces the current Vocab class. The new Vocab provides common functional APIs for NLP workflows. This module is backed by an efficient C++ implementation that reduces batch look-up time by up-to ~85% (refer to summary of [#1248](https://github.com/pytorch/text/pull/1248) and [#1290](https://github.com/pytorch/text/pull/1290) for further information on benchmarks), and provides support for TorchScript. We provide accompanying factory functions that can be used to build the Vocab object either through a python ordered dictionary or an Iterator that yields lists of tokens.

```python
#creating Vocab from text file
import io
from torchtext.vocab import build_vocab_from_iterator
#generator that yield list of tokens
def yield_tokens(file_path):
   with io.open(file_path, encoding = 'utf-8') as f:
      for line in f:
          yield line.strip().split()
#get Vocab object
vocab_obj = build_vocab_from_iterator(yield_tokens(file_path), specials=["<unk>"])

#creating Vocab through ordered dict
from torchtext.vocab import vocab
from collections import Counter, OrderedDict
counter = Counter(["a", "a", "b", "b", "b"])
sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)
ordered_dict = OrderedDict(sorted_by_freq_tuples)
vocab_obj = vocab(ordered_dict)

#common API usage

#look-up index
vocab_obj["a"]

#batch look-up indices
vocab_obj.looup_indices(["a","b"])
#support forward API of PyTorch nn Modules
vocab_obj(["a","b"])

#batch look-up tokens
vocab_obj.lookup_tokens([0,1])

#set default index to return when token not found
vocab_obj.set_default_index(0)
vocab_obj["out_of_vocabulary"] #prints 0
```

For more details, refer to [the documentation](https://pytorch.org/text/stable/vocab.html). 

Thanks for reading. If you’re interested in these updates and want to join the PyTorch community, we encourage you to join [the discussion](https://discuss.pytorch.org/) forums and [open GitHub issues](https://github.com/pytorch/pytorch/issues). To get the latest news from PyTorch, follow us on [Facebook](https://www.facebook.com/pytorch/), [Twitter](https://twitter.com/PyTorch), [Medium](https://medium.com/pytorch), [YouTube](https://www.youtube.com/pytorch) or [LinkedIn](https://www.linkedin.com/company/pytorch).  

Cheers!

-Team PyTorch

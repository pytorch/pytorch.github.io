---
layout: blog_detail
title: "PyTorch 2.2: FlashAttention-v2 integration, AOTInductor"
---

We are excited to announce the release of PyTorchÂ® 2.2 ([release note](https://github.com/pytorch/pytorch/releases/tag/v2.2.0))!  PyTorch 2.2 offers ~2x performance improvements to _[scaled_dot_product_attention](https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html)_ via [FlashAttention-v2](https://arxiv.org/abs/2307.08691) integration, as well as _AOTInductor_, a new ahead-of-time compilation and deployment tool built for  non-python server-side deployments.

This release also includes improved _torch.compile_ support for Optimizers, a number of new inductor optimizations, and a new logging mechanism called TORCH_LOGS.

Please note that we are [deprecating macOS x86 support](https://github.com/pytorch/pytorch/issues/114602), and PyTorch 2.2.x will be the last version that supports macOS x64.

Along with 2.2, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog. 

This release is composed of 3,628 commits and 521 contributors since PyTorch 2.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.2.  More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.

Summary: 

* _[scaled_dot_product_attention](https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html)_ (SDPA) now supports _[FlashAttention-2](https://arxiv.org/abs/2307.08691)_, yielding around 2x speedups compared to previous versions.
* PyTorch 2.2 introduces a new ahead-of-time extension of [TorchInductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747) called _[AOTInductor](https://pytorch.org/docs/main/torch.compiler_aot_inductor.html)_, designed to compile and deploy PyTorch programs for non-python server-side.
* _torch.distributed_ supports a new abstraction for initializing and representing ProcessGroups called _[device_mesh](https://pytorch.org/tutorials/recipes/distributed_device_mesh.html)_.
* PyTorch 2.2 ships a standardized, configurable logging mechanism called [TORCH_LOGS](https://pytorch.org/tutorials/recipes/torch_logs.html).
* A number of _torch.compile_ improvements are included in PyTorch 2.2, including improved support for compiling Optimizers and improved TorchInductor fusion and layout optimizations.
* Please note that we are [deprecating macOS x86 support](https://github.com/pytorch/pytorch/issues/114602), and PyTorch 2.2.x will be the last version that supports macOS x64.

<table class="table table-bordered">
  <tr>
   <td style="width:25%">
<strong>Stable</strong>
   </td>
   <td><strong>Beta</strong>
   </td>
   <td><strong>Performance Improvements</strong>
   </td>
  </tr>
  <tr>
   <td> 
   </td>
   <td><a href="#bookmark=id.ok7v7pq0igzw">FlashAttention-2 Integration</a>
   </td>
   <td><a href="#bookmark=id.rk3gf4pgy5m9">Inductor optimizations</a>
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><a href="#bookmark=id.3qfc7y6r1dog">AOTInductor</a>
   </td>
   <td><a href="#bookmark=id.gfep1ccb8bvk">aarch64 optimizations</a>
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><a href="#bookmark=id.n2lkw22a8l2m">TORCH_LOGS</a>
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><em><a href="#bookmark=id.h50nybtt0fdm">device_mesh</a></em>
   </td>
   <td>
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td><a href="#bookmark=id.1lx0dkeu5zqt">Optimizer compilation</a>
   </td>
   <td>
   </td>
  </tr>
</table>


*To see a full list of public feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).


## Beta Features

### [Beta] FlashAttention-2 support in _torch.nn.functional.scaled_dot_product_attention_

_[torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html)_ (SDPA) now supports FlashAttention-2, yielding around 2x speedups (compared to the previous version) and reaching ~50-73% of theoretical maximum FLOPs/s on A100 GPUs.

More information is available on FlashAttention-2 in [this paper](https://arxiv.org/abs/2307.08691).

For a tutorial on how to use SDPA please see [this tutorial](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html).  

### [Beta] AOTInductor: ahead-of-time compilation and deployment for torch.export-ed programs

AOTInductor is an extension of [TorchInductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747), designed to process exported PyTorch models, optimize them, and produce shared libraries as well as other relevant artifacts. These compiled artifacts can be deployed in non-Python environments, which are frequently employed for inference on the server-side.  Note that AOTInductor supports the same backends as Inductor, including CUDA, ROCm, and CPU.

For more information please see the [AOTInductor tutorial](https://pytorch.org/docs/main/torch.compiler_aot_inductor.html).

### [Beta] Fine-grained configurable logging via TORCH_LOGS

PyTorch now ships a standardized, configurable logging mechanism that can be used to analyze the status of various subsystems such as compilation and distributed operations.

Logs can be enabled via the TORCH_LOGS environment variable.  For example, to set the log level of TorchDynamo to logging.ERROR and the log level of TorchInductor to logging.DEBUG pass _TORCH_LOGS="-dynamo,+inductor"_ to PyTorch.

For more information, please see the logging [documentation](https://pytorch.org/docs/2.2/logging.html) and [tutorial](https://pytorch.org/tutorials/recipes/torch_logs.html).

### [Beta] torch.distributed.device_mesh

PyTorch 2.2 introduces a new abstraction for representing the ProcessGroups involved in distributed parallelisms called _torch.distributed.device_mesh_. This abstraction allows users to represent inter-node and intra-node process groups via an N-dimensional array where, for example, one dimension can data parallelism in FSDP while another could represent tensor parallelism within FSDP.

For more information, see the [device_mesh tutorial](https://pytorch.org/tutorials/recipes/distributed_device_mesh.html).

### [Beta] Improvements to _torch.compile_-ing Optimizers

A number of improvements have been made to torch.compile-ing Optimizers including less overhead and support for cuda graphs.

More technical details of the improvements are available on [dev-discuss](https://dev-discuss.pytorch.org/t/compiling-the-optimizer-with-pt2/1669), and a recipe for _torch.compile_-ing optimizers is available [here](https://pytorch.org/tutorials/recipes/compiling_optimizer.html).


## Performance Improvements

### Inductor Performance Optimizations

A number of performance optimizations have been added to TorchInductor including [horizontal fusion support for torch.concat](https://github.com/pytorch/pytorch/pull/111437), [improved convolution layout optimizations](https://github.com/pytorch/pytorch/pull/114600), and improved _scaled_dot_product_attention_ [pattern](https://github.com/pytorch/pytorch/pull/109156) [matching](https://github.com/pytorch/pytorch/pull/110001).

For a complete list of inductor optimizations, please see the [Release Notes](https://github.com/pytorch/pytorch/tree/v2.2.0).

### aarch64 Performance Optimizations

PyTorch 2.2 includes a number of performance enhancements for aarch64 including support for [mkldnn weight pre-packing](https://github.com/pytorch/pytorch/pull/115037/files), improved [ideep](https://github.com/intel/ideep) [primitive caching](https://github.com/intel/ideep/pull/261), and improved inference speed via [fixed format kernel improvements](https://github.com/oneapi-src/oneDNN/pull/1590) to [OneDNN](https://github.com/oneapi-src/oneDNN/).

For a complete list of aarch64 optimizations, please see the [Release Notes](https://github.com/pytorch/pytorch/tree/v2.2.0).
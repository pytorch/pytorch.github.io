<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Accelerating Generative AI with PyTorch: Segment Anything, Fast | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="This post is the first part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples of how these features can be combined to see how far we can push PyTorch native performance.

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Accelerating Generative AI with PyTorch: Segment Anything, Fast" />
<meta property="og:description" content="This post is the first part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples of how these features can be combined to see how far we can push PyTorch native performance.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Accelerating Generative AI with PyTorch: Segment Anything, Fast" />
<meta name="twitter:description" content="This post is the first part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples of how these features can be combined to see how far we can push PyTorch native performance.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/ptc/2022">
            <span class="dropdown-title">PyTorch Conference - 2022</span>
            <p>See the posters presented at PyTorch Conference - 2022</p>
          </a>
          <a class="nav-dropdown-item" href="https://events.linuxfoundation.org/pytorch-conference/">
            <span class="dropdown-title">PyTorch Conference - 2023</span>
            <p>October 16-17 in San Francisco</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          PyTorch Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torcharrow">
            <span class="dropdown-title docs-title">torcharrow</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class=dropdown-title>Community stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">November 16, 2023</p>
            <h1>
                <a class="blog-title">Accelerating Generative AI with PyTorch: Segment Anything, Fast</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <p>This post is the first part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples of how these features can be combined to see how far we can push PyTorch native performance.</p>

<p>As announced during the <a href="https://www.youtube.com/watch?v=IWpM_9AsC-U">PyTorch Developer Conference 2023</a>, the PyTorch team <a href="https://github.com/facebookresearch/segment-anything">rewrote Meta’s Segment Anything (“SAM”) Model</a> <strong>resulting in 8x faster code</strong> than <a href="https://github.com/facebookresearch/segment-anything">the original implementation</a>, with no loss of accuracy, all using native PyTorch optimizations. We leverage a breadth of new PyTorch features:</p>

<ul>
  <li><a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">Torch.compile</a>: A compiler for PyTorch models</li>
  <li><a href="https://github.com/pytorch-labs/ao/tree/main#torchao">GPU quantization</a>: Accelerate models with reduced precision operations</li>
  <li><a href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html">Scaled Dot Product Attention (SDPA)</a>: Memory efficient attention implementations</li>
  <li><a href="https://pytorch.org/tutorials/prototype/semi_structured_sparse.html">Semi-Structured (2:4) Sparsity:</a> A GPU optimized sparse memory format</li>
  <li><a href="https://pytorch.org/tutorials/prototype/nestedtensor.html">Nested Tensor:</a> Batch together non-uniformly sized data into a single Tensor, such as images of different sizes.</li>
  <li><strong>Custom operators with Triton:</strong> Write GPU operations using Triton Python DSL and easily integrate it into PyTorch’s various components with custom operator registration.</li>
</ul>

<p>We encourage readers to copy-paste code from <a href="https://github.com/pytorch-labs/segment-anything-fast">our implementation of SAM on Github</a> and <a href="https://github.com/pytorch-labs/segment-anything-fast/issues">ask us questions</a> on Github.</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_7.png" alt="A quick glimpse of increasing throughput and decreasing memory overhead" style="width:100%;" /></p>

<p><em>A quick glimpse of increasing throughput and decreasing memory overhead with our newly released, PyTorch native, features. Benchmarks run on p4d.24xlarge instance (8x A100s).</em></p>

<h2 id="segmentanything-model">SegmentAnything Model</h2>

<p><a href="https://github.com/facebookresearch/segment-anything">SAM</a> is a zero-shot vision model for generating promptable image masks.</p>

<p><img src="/assets/images/accelerating-generative-ai/intro_image.jpg" alt="sam image masks" style="width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;" /></p>

<p>The SAM architecture [described<a href="https://arxiv.org/abs/2304.02643"> in its paper</a>] includes multiple prompt and image encoders based on the Transformer architecture. Of this, we measured performance across the smallest and largest vision transformer backbones: <a href="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth">ViT-B</a> and <a href="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth">ViT-H</a>. And for simplicity, we only show traces for the ViT-B model.</p>

<h2 id="optimizations">Optimizations</h2>

<p>Below we tell the story of optimizing SAM: profiling, identifying bottlenecks, and building new features into PyTorch that solve these problems. Throughout, we showcase our new PyTorch features: <strong>torch.compile, SDPA, Triton kernels, Nested Tensor and semi-structured sparsity.</strong> The following sections are progressively built upon each other, ending with our SAM-fast, now <a href="https://github.com/pytorch-labs/segment-anything-fast">available on Github</a>. We motivate each feature using real kernel and memory traces, using fully PyTorch native tooling, and visualize these traces with <a href="https://perfetto.dev/">Perfetto UI</a>.</p>

<h3 id="baseline">Baseline</h3>

<p>Our SAM baseline is Facebook Research’s <a href="https://github.com/facebookresearch/segment-anything">unmodified model</a>, using float32 dtype and a batch size of 1. After some initial warmup, we can look at a kernel trace using the <a href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler</a>:</p>

<p><img src="/assets/images/accelerating-generative-ai/baseline_trace.jpg" alt="kernel trace" style="width:100%;" /></p>

<p>We notice two areas ripe for optimization.</p>

<p>The first is long calls to aten::index, the underlying call resulting from a Tensor index operation (e.g., []). While the actual GPU time spent on aten::index is relatively low. aten::index is launching two kernels, and a blocking cudaStreamSynchronize is happening in between. This means the CPU is waiting for the GPU to finish processing until it launches the second kernel. To optimize SAM, we should aim to remove blocking GPU syncs causing idle time.</p>

<p>The second is significant time spent on GPU in matrix multiplication (dark green on stream 7 7 above). This is common in Transformers. We can significantly speed up SAM if we can reduce the amount of GPU time spent on matrix multiplication.</p>

<p>We can measure the throughput (img/s) and memory overhead (GiB) from out of the box SAM to establish a baseline:</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_0.png" alt="throughput (img/s) and memory overhead (GiB) from out of the box SAM" style="width:100%;" /></p>

<h3 id="bfloat16-half-precision-gpu-syncs-and-batching">Bfloat16 Half precision (+GPU syncs and batching)</h3>

<p>To address the first issue of less time spent in matrix multiplication, we can turn to <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a>. Bfloat16 is a commonly used half-precision type. Through less precision per parameter and activations, we can save significant time and memory in computation. With reducing precision of parameters, it’s critical to validate end to end model accuracy.</p>

<p><img src="/assets/images/accelerating-generative-ai/bfloat16_snippet.jpg" alt="replacing padding dtypes with half precision, bfloat16" style="width:100%;" /></p>

<p><em>Shown here is an example of replacing padding dtypes with half precision, bfloat16. <a href="https://github.com/pytorch-labs/segment-anything-fast/blame/main/segment_anything_fast/modeling/prompt_encoder.py#L86">Code is here</a>.</em></p>

<p>Next to simply setting <code class="language-plaintext highlighter-rouge">model.to(torch.bfloat16)</code> we have to change a few small places that assume the default dtype.</p>

<p>Now, in order to remove GPU syncs we need to audit operations that cause them. We can find these pieces of code by searching the GPU traces for calls to <code class="language-plaintext highlighter-rouge">cudaStreamSynchronize</code>. In fact, we found two locations that we were able to rewrite to be sync-free.</p>

<p><img src="/assets/images/accelerating-generative-ai/code1.jpg" alt="code sample 1" style="width:100%;" /></p>

<p><img src="/assets/images/accelerating-generative-ai/bfloat16_snippet2.jpg" alt="replacing padding dtypes with half precision, bfloat16" style="width:100%;" /></p>

<p>Specifically, we see that within SAM’s image encoder, there are variables acting as coordinate scalers, q_coords and k_coords. These are both allocated and processed on the CPU. However, once these variables are used to index in rel_pos_resized, the index operation automatically moves these variables to the GPU. This copy over causes the GPU sync we’ve observed above. We notice a second call to index in SAM’s prompt encoder: We can use torch.where to rewrite this as shown above.</p>

<p><strong>Kernel trace</strong></p>

<p>After applying these changes, we begin to see significant time between individual kernel calls. This is typically observed with small batch sizes (1 here) due to the GPU overhead of launching kernels. To get a closer look at practical areas for optimization, we can start to profile SAM inference with batch size 8:</p>

<p><img src="/assets/images/accelerating-generative-ai/bfloat16_trace.jpg" alt="profile SAM inference with batch size 8" style="width:100%;" /></p>

<p>Looking at the time spent per-kernel, we obverse most of SAM’s GPU time spent on elementwise kernels and softmax operation. With this we now see that matrix multiplications have become a much smaller relative overhead.</p>

<p><img src="/assets/images/accelerating-generative-ai/bfloat16_kernels.jpg" alt="matrix multiplications have become a much smaller relative overhead" style="width:100%;" /></p>

<p>Taken the GPU sync and bfloat16 optimizations together, we have now pushed SAM performance by up to 3x</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_1.png" alt="SAM performance by up to 3x" style="width:100%;" /></p>

<h3 id="torchcompile-graph-breaks-and-cuda-graphs">Torch.compile (+graph breaks and CUDA graphs)</h3>

<p>When observing a large number of small operations, such as the elementwise kernels profiled above, turning to a compiler to fuse operations can have strong benefits. PyTorch’s recently released <strong>torch.compile</strong> does a great job optimizing by:</p>

<ol>
  <li>Fusing together sequences of operations such as nn.LayerNorm or nn.GELU into a single GPU kernel that is called and</li>
  <li>Epilogues: fusing operations that immediately follow matrix multiplication kernels to reduce the number of GPU kernel calls.</li>
</ol>

<p>Through these optimizations, we reduce the number of GPU global memory roundtrips, thus speeding up inference. We can now try torch.compile on SAM’s <a href="https://github.com/pytorch-labs/segment-anything-fast/blob/3bd74614fe7285de4de3d763d8ec2e951c4c589c/experiments/eval_combo.py#L196-L201">image encoder</a>. To maximize performance we use a few advanced compile techniques such as:</p>

<ul>
  <li>using torch.compile’s max-autotune mode enables <a href="https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/">CUDA graphs</a> and shape-specific kernels with custom epilogues</li>
  <li>By setting TORCH_LOGS=”graph_breaks,recompiles” we can manually verify that we are not running into <a href="https://pytorch.org/docs/main/torch.compiler_faq.html#graph-breaks">graph breaks</a> or recompiles.</li>
  <li>Padding the batch of images input to the encoder with zeros ensures compile accepts static shapes thus being able to always use shape-specific optimized kernels with custom epilogues without recompilations.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>predictor.model.image_encoder = \
    torch.compile(predictor.model.image_encoder, mode=use_compile)
</code></pre></div></div>

<p><strong>Kernel trace</strong></p>

<p><img src="/assets/images/accelerating-generative-ai/compile_trace.jpg" alt="Kernel trace" style="width:100%;" /></p>

<p>torch.compile is working beautifully. We launch a single CUDA graph, which makes up a significant portion of GPU time within the timed region. Let’s run our profile again and look at the percentage of GPU time spent in specific kernels:</p>

<p><img src="/assets/images/accelerating-generative-ai/compile_kernels.jpg" alt="the percentage of GPU time spent in specific kernels" style="width:100%;" /></p>

<p>We now see softmax makes up a significant portion of the time followed by various GEMM variants. In summary we observe the following measurements for batch size 8 and above changes.</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_2.png" alt="measurements for batch size 8 and above" style="width:100%;" /></p>

<h3 id="sdpa-scaled_dot_product_attention">SDPA: scaled_dot_product_attention</h3>

<p>Next up, we can tackle one of the most common areas for transformer performance overhead: the attention mechanism. Naive attention implementations scale quadratically in time and memory with sequence length. PyTorch’s <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product_attention#torch.nn.functional.scaled_dot_product_attention">scaled_dot_product_attention</a> operation built upon the principles of <a href="https://arxiv.org/pdf/2205.14135.pdf">Flash Attention</a>, <a href="https://github.com/Dao-AILab/flash-attention">FlashAttentionV2</a> and <a href="https://github.com/facebookresearch/xformers">xFormer’s memory efficient attention</a> can significantly speed up GPU attention. Combined with torch.compile, this operation allows us to express and fuse a common pattern within variants of MultiheadAttention. After <a href="https://github.com/facebookresearch/segment-anything/compare/50cb459d080bcd783a4b481d3bde4150d35ac497...7dc75fdf283693f73606f2fe7fdcb693afcb16b9">a small set of changes</a> we can adapt the model to use scaled_dot_product_attention.</p>

<p><img src="/assets/images/accelerating-generative-ai/sdpa_snippet.jpg" alt="PyTorch native attention implementation" style="width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;" /></p>

<p><em>PyTorch native attention implementation, <a href="https://github.com/pytorch-labs/segment-anything-fast/blob/main/segment_anything_fast/modeling/image_encoder.py#L236">see code here</a>.</em></p>

<p><strong>Kernel trace</strong></p>

<p>We can now see that in particular the memory efficient attention kernel is taking up a large amount of computational time on the GPU:</p>

<p><img src="/assets/images/accelerating-generative-ai/sdpa_kernels.jpg" alt="memory efficient attention kernel is taking up a large amount of computational time on the GPU" style="width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;" /></p>

<p>Using PyTorch’s native scaled_dot_product_attention, we can significantly increase the batch size. We now observe the following measurements for batch size 32 and above changes.</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_3.png" alt="batch size 32 and above" style="width:100%;" /></p>

<h3 id="triton-custom-sdpa-for-fused-relative-positional-encoding">Triton: Custom SDPA for fused relative positional encoding</h3>

<p>Transitioning away from inference throughput for a moment, we started profiling overall SAM memory. Within the image encoder, we saw significant spikes in memory allocation:</p>

<p><img src="/assets/images/accelerating-generative-ai/triton_trace.png" alt="spikes in memory allocation" style="width:100%;" /></p>

<p>Zooming in, we see this allocation happens within add_decomposed_rel_pos, <a href="https://github.com/pytorch-labs/segment-anything-fast/blob/main/segment_anything_fast/modeling/image_encoder.py#L373">on the following line:</a></p>

<p><img src="/assets/images/accelerating-generative-ai/triton_snippet.jpg" alt="we see this allocation happens within add_decomposed_rel_pos" style="width:100%;display: block;max-width:500px; margin-left:auto; margin-right:auto;" /></p>

<p>The attn variable here is the addition of two smaller tensors: rel_h of shape (B, q_h, q_w, k_h, 1) and rel_w of shape (B, q_h, q_w, 1, k_w).</p>

<p>It’s not surprising that the memory efficient attention kernel (used via SDPA) is taking a long time with an attention bias size over 3.0GiB. If instead of allocating this large attn tensor, we thread into SDPA the two smaller rel_h and rel_w tensors, and only construct attn as needed, we’d anticipate significant performance gain.</p>

<p>Unfortunately this is not a trivial modification; SDPA kernels are highly optimized and written in CUDA. We can turn to Triton, with their easy to understand and use <a href="https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html">tutorial on a FlashAttention implementation</a>. After some significant digging and in close collaboration with xFormer’s Daniel Haziza we found one case of input shapes where it is relatively straightforward to implement a fused version of the kernel. The <a href="https://github.com/pytorch-labs/segment-anything-fast/blob/main/segment_anything_fast/flash_4.py">details have been added to the repository</a>. Surprisingly this can be done in under 350 lines of code for the inference case.</p>

<p>This is a great example of extending PyTorch with a new kernel, straightforwardly built with Triton code.</p>

<p><strong>Kernel trace</strong></p>

<p><img src="/assets/images/accelerating-generative-ai/triton_kernels.jpg" alt="kernel trace" style="width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;" /></p>

<p>With our custom positional Triton kernel we observe the following measurements for batch size 32.</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_4.png" alt="we observe the following measurements for batch size 32" style="width:100%;" /></p>

<h3 id="nt-nestedtensor-and-batching-predict_torch">NT: NestedTensor and batching predict_torch</h3>

<p>We have spent a lot of time on the image encoder. This makes sense, since it takes up the most amount of computational time. At this point however it is fairly well optimized and the operator that takes the most time would require significant additional investment to be improved.</p>

<p>We discovered an interesting observation with the <a href="https://github.com/pytorch-labs/segment-anything-fast/blob/7cd6ba3cea451602acb7d36d176da06c70ac68f1/experiments/eval_combo.py#L137-L157">mask prediction pipeline</a>: for each image we have there is an associated size, coords, and fg_labels Tensor. Each of these tensors are of different batch sizes. Each image itself is also of a different size. This representation of data looks like <a href="https://en.wikipedia.org/wiki/Jagged_array">Jagged Data</a>. With PyTorch’s recently released <a href="https://pytorch.org/tutorials/prototype/nestedtensor.html">NestedTensor</a>, we can modify our data pipeline batch coords and fg_labels Tensors into a single NestedTensor. This can have significant performance benefits for the prompt encoder and mask decoder that follow the image encoder. Invoking:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.nested.nested_tensor(data, dtype=dtype, layout=torch.jagged)
</code></pre></div></div>

<p><strong>Kernel trace</strong></p>

<p><img src="/assets/images/accelerating-generative-ai/trace1.jpg" alt="Kernel trace" style="width:100%;" /></p>

<p><img src="/assets/images/accelerating-generative-ai/nt_kernel.jpg" alt="we can launch kernels much faster from the CPU than the GPU can process" style="width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;" /></p>

<p>We can see now that we can launch kernels much faster from the CPU than the GPU can process and that it spends a long time waiting at the end of our timed region for the GPU to finish (cudaDeviceSynchronize). We also don’t see any more idle time (white space) between kernels on the GPU.</p>

<p>With Nested Tensor, we observe the following measurements for batch size 32 and above changes.</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_5.png" alt="batch size 32 and above changes" style="width:100%;" /></p>

<h3 id="int8-quantization-and-approximating-matmul">int8: quantization and approximating matmul</h3>

<p>We notice in the above trace, that significant time is now spent in GEMM kernels. We’ve optimized enough that we now see matrix multiplication account for more time in inference than scaled dot product attention.</p>

<p>Building on earlier learnings going from fp32 to bfloat16, let’s go a step further, emulating even lower precision with int8 quantization. Looking at quantization methods, we focus on <a href="https://pytorch.org/tutorials/recipes/quantization.html">Dynamic quantization</a> wherein our model observes the range of possible inputs and weights of a layer, and subdivides the expressible int8 range to uniformly “spread out” observed values. Ultimately each float input will be mapped to a single integer in the range [-128, 127]. For more information see PyTorch’s <a href="https://pytorch.org/tutorials/recipes/quantization.html">tutorial on quantization</a></p>

<p>Reducing precision can immediately lead to peak memory savings, but to realize inference speedups, we have to make full use of int8 through SAM’s operations. This requires building an efficient int8@int8 matrix multiplication kernel, as well as casting logic to translate from high to low precision (quantization) as well as reversing back from low to high (dequantization). Utilizing the power of torch.compile, we can compile and fuse together these quantization and dequantization routines into efficient single kernels and epilogues of our matrix multiplication. The resulting implementation is <a href="https://github.com/pytorch-labs/segment-anything-fast/blob/21b0208ae46eefc5659f7f200a2bf447add8765b/segment_anything_fast/dynamic_quant.py">fairly short and less than 250 lines of code</a>. For more information on the APIs and usage, see <a href="https://github.com/pytorch-labs/ao/tree/main#torchao">pytorch-labs/ao</a>.</p>

<p>While it’s common to see some accuracy regression when quantizing models at inference time, SAM has been particularly robust to lower precision inference with minimal loss of accuracy. With quantization added, we now observe the following measurements for <strong>batch size 32</strong> and above changes.</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_6.png" alt="batch size 32 and above changes" style="width:100%;" /></p>

<h3 id="sparse-semi-structured-24-sparsity">sparse: Semi-structured (2:4) sparsity</h3>

<p>Matrix multiplications are still our bottleneck. We can turn to the model acceleration playbook with another classic method to approximate matrix multiplication: sparsification. By sparsifying our matrices (i.e., zeroing out values), we could theoretically use fewer bits to store weight and activation tensors. The process by which we decide which weights in the tensor to set to zero is called pruning. The idea behind pruning is that small weights in a weight tensor contribute little to the net output of a layer, typically the product of weights with activations. Pruning away small weights can potentially reduce model size without significant loss of accuracy.</p>

<p>Methods for pruning are varied, from completely unstructured, wherein weights are greedily pruned to highly structured, wherein large sub-components of a tensor are pruned a time. Choice of method is not trivial. While unstructured pruning may have the theoretically least impact on accuracy, GPUs are also highly efficient with multiplying large, dense matrices and may suffer significant performance degradation in sparse regimes. One recent pruning method supported in PyTorch seeks to strike a balance, called semi-structured (or 2:4) sparsity. This sparse storage reduces the original tensor by a significant 50%, while simultaneously resulting in a dense tensor output that can leverage highly performant, 2:4 GPU kernels. See the following picture for an illustration.</p>

<p><img src="/assets/images/accelerating-generative-ai/sparse_image.png" alt="dense tensor output that can leverage highly performant, 2:4 GPU kernels" style="width:100%;display: block;max-width:600px; margin-left:auto; margin-right:auto;" /></p>

<p>From <a href="https://developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt">developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt</a></p>

<p>In order to use this sparse storage format and the associated fast kernels we need to prune our weights such that they adhere to the constraints for the format. We pick the two smallest weights to prune in a 1 by 4 region, measuring the performance vs accuracy tradeoff. It is easy to change a weight from its default PyTorch (“strided”) layout to this new, semi-structured sparse layout. To implement <code class="language-plaintext highlighter-rouge">apply_sparse(model)</code> we only require 32 lines of Python code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch
from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor

# Sparsity helper functions
def apply_fake_sparsity(model):
    """
    This function simulates 2:4 sparsity on all linear layers in a model.
    It uses the torch.ao.pruning flow.
    """
    # torch.ao.pruning flow
    from torch.ao.pruning import WeightNormSparsifier
    sparse_config = []
    for name, mod in model.named_modules():
        if isinstance(mod, torch.nn.Linear):
            sparse_config.append({"tensor_fqn": f"{name}.weight"})

    sparsifier = WeightNormSparsifier(sparsity_level=1.0,
                                      sparse_block_shape=(1,4),
                                      zeros_per_block=2)
    sparsifier.prepare(model, sparse_config)
    sparsifier.step()

    sparsifier.step()
    sparsifier.squash_mask()


def apply_sparse(model):
    apply_fake_sparsity(model)
    for name, mod in model.named_modules():
        if isinstance(mod, torch.nn.Linear):
            mod.weight = torch.nn.Parameter(to_sparse_semi_structured(mod.weight))
</code></pre></div></div>

<p>With 2:4 sparsity, we observe peak performance on SAM with vit_b and batch size 32:</p>

<p><img src="/assets/images/accelerating-generative-ai/bar_chart_7.png" alt="With 2:4 sparsity, we observe peak performance on SAM with vit_b and batch size 32" style="width:100%;" /></p>

<h3 id="conclusion">Conclusion</h3>

<p>Wrapping up, we are excited to have<a href="https://www.youtube.com/watch?v=IWpM_9AsC-U"> announced</a> our fastest implementation of <a href="https://github.com/facebookresearch/segment-anything">Segment Anything</a> to date. We rewrote Meta’s original SAM in pure PyTorch with no loss of accuracy using a breadth of newly released features:</p>

<ul>
  <li><strong>Torch.compile</strong> PyTorch’s native JIT compiler, providing fast, automated fusion of PyTorch operations [<a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">tutorial</a>]</li>
  <li><strong>GPU quantization</strong> accelerate models with reduced precision operations [<a href="https://github.com/pytorch-labs/ao/tree/main#torchao">api</a>]</li>
  <li><strong>Scaled Dot Product Attention (SDPA)</strong> a new, memory efficient implementation of Attention [<a href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html">tutorial</a>]</li>
  <li><strong>Semi-Structured (2:4) Sparsity</strong> accelerate models with fewer bits to store weights and activations [<a href="https://pytorch.org/tutorials/prototype/semi_structured_sparse.html">tutorial</a>]</li>
  <li><strong>Nested Tensor</strong> Highly optimized, ragged array handling for non-uniform batch and image sizes [<a href="https://pytorch.org/tutorials/prototype/nestedtensor.html">tutorial</a>]</li>
  <li><strong>Triton kernels.</strong> Custom GPU operations, easily built and optimized via Triton</li>
</ul>

<p>For more details on how to reproduce the data presented in this blog post, check out <a href="https://github.com/pytorch-labs/segment-anything-fast/tree/main/experiments">the experiments folder of segment-anything-fast</a>. Please don’t hesitate to contact us or <a href="https://github.com/pytorch-labs/segment-anything-fast/issues/new">open an issue</a> if you run into any technical issues.</p>

<p>In our next post, we are excited to share similar performance gains with our PyTorch natively authored LLM!</p>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>We would like to thank Meta’s <a href="https://github.com/facebookresearch/xformers">xFormers</a> team including Daniel Haziza and Francisco Massa for authoring SDPA kernels and helping us design our custom one-off Triton kernel.</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          <li><a href="https://github.com/pytorch/pytorch/security/policy" target="_blank">Security</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank">Mastodon</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
      </ul>
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/ptc/2022">PyTorch Conference - 2022</a>
          </li>
          <li>
            <a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference - 2023</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>PyTorch Edge </a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/torcharrow">torcharrow</a>
          </li>

          <li class="">
            <a href="/data">TorchData</a>
          </li> 

          <li class="">
            <a href="/torchrec">TorchRec</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          
          <li>
            <a href="/#community-module">Community</a>
          </li>
          
          <li class="">
            <a href="/community-stories">Community stories</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

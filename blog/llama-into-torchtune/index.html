<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Distilling Llama3.1 8B into 1B in torchtune | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="In this blog, we present a case study on distilling a Llama 3.1 8B model into Llama 3.2 1B using torchtune’s knowledge distillation recipe. We demonstrate how knowledge distillation (KD) can be used in post-training to improve instruction-following task performance and showcase how users can leverage the recipe.

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Distilling Llama3.1 8B into 1B in torchtune" />
<meta property="og:description" content="In this blog, we present a case study on distilling a Llama 3.1 8B model into Llama 3.2 1B using torchtune’s knowledge distillation recipe. We demonstrate how knowledge distillation (KD) can be used in post-training to improve instruction-following task performance and showcase how users can leverage the recipe.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Distilling Llama3.1 8B into 1B in torchtune" />
<meta name="twitter:description" content="In this blog, we present a case study on distilling a Llama 3.1 8B model into Llama 3.2 1B using torchtune’s knowledge distillation recipe. We demonstrate how knowledge distillation (KD) can be used in post-training to improve instruction-following task performance and showcase how users can leverage the recipe.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="https://github.com/pytorch-fdn/ecosystem" target="_blank">
            <span class="dropdown-title">Join the Ecosystem</span>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2024">
            <span class="dropdown-title">Contributor Awards - 2024</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
          <a class="nav-dropdown-item" target="_blank" href="https://pytorch.org/executorch/stable/index.html">
            <span class="dropdown-title">ExecuTorch Documentation</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/newsletter">
            <span class=dropdown-title>Newsletter</span>
            <p>Stay up-to-date with the latest updates</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
          <a class="nav-dropdown-item" href="/credits">
            <span class=dropdown-title>Cloud Credit Program</span>
          </a>
          <a class="nav-dropdown-item" href="/tac">
            <span class=dropdown-title>Technical Advisory Council</span>
          </a>
          <a class="nav-dropdown-item" href="/staff">
            <span class=dropdown-title>Staff</span>
          </a>
          <a class="nav-dropdown-item" href="/contact-us">
            <span class=dropdown-title>Contact Us</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">November 18, 2024</p>
            <h1>
                <a class="blog-title">Distilling Llama3.1 8B into 1B in torchtune</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Linda Wang, Evan Smothers, Kartikay Khandelwal
                      
                    </p>
                    <p>In this blog, we present a case study on distilling a Llama 3.1 8B model into Llama 3.2 1B using torchtune’s knowledge distillation recipe. We demonstrate how knowledge distillation (KD) can be used in post-training to improve instruction-following task performance and showcase how users can leverage the recipe.</p>

<h2 id="what-is-knowledge-distillation">What is Knowledge Distillation?</h2>

<p><a href="https://arxiv.org/pdf/1503.02531">Knowledge Distillation</a> is a widely used compression technique that transfers knowledge from a larger (teacher) model to a smaller (student) model. Larger models have more parameters and capacity for knowledge, however, this larger capacity is also more computationally expensive to deploy. Knowledge distillation can be used to compress the knowledge of a larger model into a smaller model. The idea is that performance of smaller models can be improved by learning from larger model’s outputs.</p>

<h2 id="how-does-knowledge-distillation-work">How does Knowledge Distillation work?</h2>

<p>Knowledge is transferred from the teacher to student model by training on a transfer set where the student is trained to imitate the token-level probability distributions of the teacher. The assumption is that the teacher model distribution is similar to the transfer dataset. The diagram below is a simplified representation of how KD works.</p>

<p><img src="/assets/images/llama-into-torchtune/fg1.png" alt="Figure 1: Simplified representation of knowledge transfer from teacher to student model" style="width:100%" /></p>

<p><strong>Figure 1: Simplified representation of knowledge transfer from teacher to student model</strong></p>

<p>As knowledge distillation for LLMs is an active area of research, there are papers, such as <a href="https://arxiv.org/pdf/2306.08543">MiniLLM</a>, <a href="https://arxiv.org/pdf/2402.03898">DistiLLM</a>, <a href="https://arxiv.org/pdf/2404.02657">AKL</a>, and <a href="https://arxiv.org/pdf/2306.13649">Generalized KD</a>, investigating different loss approaches. In this case study, we focus on the standard cross-entropy (CE) loss with the forward <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler (KL) divergence</a> loss as the baseline. Forward KL divergence aims to minimize the difference by forcing the student’s distribution to align with all of the teacher’s distributions.</p>

<h2 id="why-is-knowledge-distillation-useful">Why is Knowledge Distillation useful?</h2>

<p>The idea of knowledge distillation is that a smaller model can achieve better performance using a teacher model’s outputs as an additional signal than it could training from scratch or with supervised fine-tuning. For instance, <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama 3.2 lightweight 1B and 3B text models</a> incorporated logits from Llama 3.1 8B and 70B to recover performance after pruning. In addition, for fine-tuning on instruction-following tasks, research in LLM distillation demonstrates that knowledge distillation methods can outperform supervised fine-tuning (SFT) alone.</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="2"><strong>Model</strong>
   </td>
   <td rowspan="2"><strong>Method</strong>
   </td>
   <td><strong>DollyEval</strong>
   </td>
   <td><strong>Self-Inst</strong>
   </td>
   <td><strong>S-NI</strong>
   </td>
  </tr>
  <tr>
   <td>GPT-4 Eval
   </td>
   <td>GPT-4 Eval
   </td>
   <td>Rouge-L
   </td>
  </tr>
  <tr>
   <td rowspan="3">Llama 7B
   </td>
   <td>SFT
   </td>
   <td>73.0
   </td>
   <td>69.2
   </td>
   <td>32.4
   </td>
  </tr>
  <tr>
   <td>KD
   </td>
   <td>73.7
   </td>
   <td>70.5
   </td>
   <td>33.7
   </td>
  </tr>
  <tr>
   <td>MiniLLM
   </td>
   <td>76.4
   </td>
   <td>73.1
   </td>
   <td>35.5
   </td>
  </tr>
  <tr>
   <td rowspan="3">Llama 1.1B
   </td>
   <td>SFT
   </td>
   <td>22.1
   </td>
   <td>-
   </td>
   <td>27.8
   </td>
  </tr>
  <tr>
   <td>KD
   </td>
   <td>22.2
   </td>
   <td>-
   </td>
   <td>28.1
   </td>
  </tr>
  <tr>
   <td>AKL
   </td>
   <td>24.4
   </td>
   <td>-
   </td>
   <td>31.4
   </td>
  </tr>
  <tr>
   <td rowspan="4">OpenLlama 3B
   </td>
   <td>SFT
   </td>
   <td>47.3
   </td>
   <td>41.7
   </td>
   <td>29.3
   </td>
  </tr>
  <tr>
   <td>KD
   </td>
   <td>44.9
   </td>
   <td>42.1
   </td>
   <td>27.9
   </td>
  </tr>
  <tr>
   <td>SeqKD
   </td>
   <td>48.1
   </td>
   <td>46.0
   </td>
   <td>29.1
   </td>
  </tr>
  <tr>
   <td>DistiLLM
   </td>
   <td>59.9
   </td>
   <td>53.3
   </td>
   <td>37.6
   </td>
  </tr>
</table>

<p><strong>Table 1: Comparison of knowledge distillation approaches to supervised fine-tuning</strong></p>

<p>Below is a simplified example of how knowledge distillation differs from supervised fine-tuning.</p>

<table class="table table-bordered">
  <tr>
   <th>Supervised fine-tuning
   </th>
   <th>Knowledge distillation
   </th>
  </tr>
  <tr>
   <td>
   <pre class="highlight">
   <code>
model = llama3_2_1b()
ce_loss = CrossEntropyLoss()
kd_loss = ForwardKLLoss()

tokens, labels = batch["tokens"], batch["labels"]
logits = model(tokens, ...)

loss = ce_loss(logits, labels)
loss.backward()

   </code>
   </pre>
   </td>
   <td>
   <pre class="highlight">
   <code>
model = llama3_2_1b()
teacher_model = llama3_1_8b()
ce_loss = CrossEntropyLoss()
kd_loss = ForwardKLLoss()

tokens, labels = batch["tokens"], batch["labels"]
logits = model(tokens, ...)
teacher_logits = teacher_model(tokens, ...)
loss = ce_loss(logits, labels) + kd_loss(logits, teacher_logits, labels)
loss.backward()
   </code>
   </pre>
   </td>
  </tr>
</table>

<h2 id="kd-recipe-in-torchtune">KD recipe in torchtune</h2>

<p>With torchtune, we can easily apply knowledge distillation to Llama3, as well as other LLM model families, using torchtune’s <a href="https://github.com/pytorch/torchtune/blob/4234b78b914af23384ce0348f564e2119d107a96/recipes/knowledge_distillation_single_device.py">KD recipe</a>. The objective for this recipe is to fine-tune Llama3.2-1B on the Alpaca instruction-following dataset by distilling from Llama3.1-8B. This recipe focuses on post-training and assumes the teacher and student models have already been pre-trained.</p>

<p>First, we have to download the model weights. To be consistent with other torchtune fine-tuning configs, we will use the instruction tuned models of Llama3.1-8B as teacher and Llama3.2-1B as student.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth" --hf_token &lt;HF_TOKEN&gt;

tune download meta-llama/Llama-3.2-1B-Instruct --output-dir /tmp/Llama-3.2-1B-Instruct --ignore-patterns "original/consolidated.00.pth" --hf_token &lt;HF_TOKEN&gt;
</code></pre></div></div>

<p>In order for the teacher model distribution to be similar to the Alpaca dataset, we will fine-tune the teacher model using LoRA. Based on our experiments, shown in the next section, we’ve found that KD performs better when the teacher model is already fine-tuned on the target dataset.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device
</code></pre></div></div>

<p>Finally, we can run the following command to distill the fine-tuned 8B model into the 1B model on a single GPU. For this case study, we used a single A100 80GB GPU. We also have a <a href="https://github.com/pytorch/torchtune/blob/09c2619f713e771b4159f7b83bac8971c7053bd3/recipes/knowledge_distillation_distributed.py">distributed recipe</a> for running on multiple devices.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tune run knowledge_distillation_single_device --config llama3_2/knowledge_distillation_single_device
</code></pre></div></div>

<h2 id="ablation-studies">Ablation studies</h2>

<p>In this section, we demonstrate how changing configurations and hyperparameters can affect performance. By default, our configuration uses the LoRA fine-tuned 8B teacher model,  downloaded 1B student model, learning rate of 3e<sup>-4</sup> and KD loss ratio of 0.5. For this case study, we fine-tuned on the <a href="https://pytorch.org/torchtune/main/generated/torchtune.datasets.alpaca_cleaned_dataset.html#torchtune.datasets.alpaca_cleaned_dataset">alpaca_cleaned_dataset</a> and evaluated the models on <a href="https://github.com/EleutherAI/lm-evaluation-harness/tree/feff1b55c57993c4d42c8f913a22eeec395cd690/lm_eval/tasks/truthfulqa">truthfulqa_mc2</a>, <a href="https://github.com/EleutherAI/lm-evaluation-harness/tree/517aadc/lm_eval/tasks/hellaswagd">hellaswag</a> and <a href="https://github.com/EleutherAI/lm-evaluation-harness/tree/b62b9bd/lm_eval/tasks/commonsense_qa">commonsense_qa</a> tasks through the EleutherAI <a href="https://github.com/EleutherAI/lm-evaluation-harness/tree/main">LM evaluation harness</a>. Let’s take a look at the effects of:</p>

<ol>
  <li>Using a fine-tuned teacher model</li>
  <li>Using a fine-tuned student model</li>
  <li>Hyperparameter tuning of KD loss ratio and learning rate</li>
</ol>

<h3 id="using-a-fine-tuned-teacher-model">Using a fine-tuned teacher model</h3>

<p>The default settings in the config uses the fine-tuned teacher model. Now, let’s take a look at the effects of not fine-tuning the teacher model first.</p>

<p>Taking a loss at the losses, using the baseline 8B as teacher results in a higher loss than using the fine-tuned teacher model. The KD loss also remains relatively constant, suggesting that the teacher model should have the same distributions as the transfer dataset.</p>

<p><img src="/assets/images/llama-into-torchtune/fg2.png" alt="Figure 2: (left to right) KD loss from forward KL divergence, class loss from cross entropy, total loss: even combination of KD and class loss." style="width:100%" /></p>

<p><strong>Figure 2: (left to right) KD loss from forward KL divergence, class loss from cross entropy, total loss: even combination of KD and class loss.</strong></p>

<p>In our benchmarks, we can see that supervised fine-tuning of the 1B model achieves better accuracy than the baseline 1B model. By using the fine-tuned 8B teacher model, we see comparable results for truthfulqa and improvement for hellaswag and commonsense. When using the baseline 8B as a teacher, we see improvement across all metrics, but lower than the other configurations.</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="2"><strong>Model</strong>
   </td>
   <td><strong>TruthfulQA</strong>
   </td>
   <td colspan="2"><strong>hellaswag</strong>
   </td>
   <td><strong>commonsense</strong>
   </td>
  </tr>
  <tr>
   <td>mc2
   </td>
   <td>acc
   </td>
   <td>acc_norm
   </td>
   <td>acc
   </td>
  </tr>
  <tr>
   <td>Baseline Llama 3.1 8B
   </td>
   <td>0.5401
   </td>
   <td>0.5911
   </td>
   <td>0.7915
   </td>
   <td>0.7707
   </td>
  </tr>
  <tr>
   <td>Fine-tuned Llama 3.1 8B using LoRA
   </td>
   <td>0.5475
   </td>
   <td>0.6031
   </td>
   <td>0.7951
   </td>
   <td>0.7789
   </td>
  </tr>
  <tr>
   <td>Baseline Llama 3.2 1B
   </td>
   <td>0.4384
   </td>
   <td>0.4517
   </td>
   <td>0.6064
   </td>
   <td>0.5536
   </td>
  </tr>
  <tr>
   <td>Fine-tuned Llama 3.2 1B using LoRA
   </td>
   <td><strong>0.4492</strong>
   </td>
   <td>0.4595
   </td>
   <td>0.6132
   </td>
   <td>0.5528
   </td>
  </tr>
  <tr>
   <td>KD using baseline 8B as teacher
   </td>
   <td>0.444
   </td>
   <td>0.4576
   </td>
   <td>0.6123
   </td>
   <td>0.5561
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B as teacher
   </td>
   <td>0.4481
   </td>
   <td><strong>0.4603</strong>
   </td>
   <td><strong>0.6157</strong>
   </td>
   <td><strong>0.5569</strong>
   </td>
  </tr>
</table>

<p><strong>Table 2: Comparison between using baseline and fine-tuned 8B as teacher model</strong></p>

<h3 id="using-a-fine-tuned-student-model">Using a fine-tuned student model</h3>

<p>For these experiments, we look at the effects of KD when the student model is already fine-tuned. We analyze the effects using different combinations of baseline and fine-tuned 8B and 1B models.</p>

<p>Based on the loss graphs, using a fine-tuned teacher model results in a lower loss irrespective of whether the student model is fine-tuned or not. It’s also interesting to note that the class loss starts to increase when using a fine-tuned student model.</p>

<p><img src="/assets/images/llama-into-torchtune/fg3.png" alt="Figure 3: Comparing losses of different teacher and student model initializations" style="width:100%" /></p>

<p><strong>Figure 3: Comparing losses of different teacher and student model initializations</strong></p>

<p>Using the fine-tuned student model boosts accuracy even further for truthfulqa, but the accuracy drops for hellaswag and commonsense. Using a fine-tuned teacher model and baseline student model achieved the best results on hellaswag and commonsense dataset. Based on these findings, the best configuration will change depending on which evaluation dataset and metric you are optimizing for.</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="2"><strong>Model</strong>
   </td>
   <td><strong>TruthfulQA</strong>
   </td>
   <td colspan="2"><strong>hellaswag</strong>
   </td>
   <td><strong>commonsense</strong>
   </td>
  </tr>
  <tr>
   <td>mc2
   </td>
   <td>acc
   </td>
   <td>acc_norm
   </td>
   <td>acc
   </td>
  </tr>
  <tr>
   <td>Baseline Llama 3.1 8B
   </td>
   <td>0.5401
   </td>
   <td>0.5911
   </td>
   <td>0.7915
   </td>
   <td>0.7707
   </td>
  </tr>
  <tr>
   <td>Fine-tuned Llama 3.1 8B using LoRA
   </td>
   <td>0.5475
   </td>
   <td>0.6031
   </td>
   <td>0.7951
   </td>
   <td>0.7789
   </td>
  </tr>
  <tr>
   <td>Baseline Llama 3.2 1B
   </td>
   <td>0.4384
   </td>
   <td>0.4517
   </td>
   <td>0.6064
   </td>
   <td>0.5536
   </td>
  </tr>
  <tr>
   <td>Fine-tuned Llama 3.2 1B using LoRA
   </td>
   <td>0.4492
   </td>
   <td>0.4595
   </td>
   <td>0.6132
   </td>
   <td>0.5528
   </td>
  </tr>
  <tr>
   <td>KD using baseline 8B and baseline 1B
   </td>
   <td>0.444
   </td>
   <td>0.4576
   </td>
   <td>0.6123
   </td>
   <td>0.5561
   </td>
  </tr>
  <tr>
   <td>KD using baseline 8B and fine-tuned 1B
   </td>
   <td>0.4508
   </td>
   <td>0.448
   </td>
   <td>0.6004
   </td>
   <td>0.5274
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>0.4481
   </td>
   <td><strong>0.4603</strong>
   </td>
   <td><strong>0.6157</strong>
   </td>
   <td><strong>0.5569</strong>
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and fine-tuned 1B
   </td>
   <td><strong>0.4713</strong>
   </td>
   <td>0.4512
   </td>
   <td>0.599
   </td>
   <td>0.5233
   </td>
  </tr>
</table>

<p><strong>Table 3: Comparison using baseline and fine-tuned teacher and student models</strong></p>

<h3 id="hyperparameter-tuning-learning-rate">Hyperparameter tuning: learning rate</h3>

<p>By default, the recipe has a learning rate of 3e-4. For these experiments, we changed the learning rate from as high as 1e-3 to as low as 1e-5.</p>

<p>Based on the loss graphs, all learning rates result in similar losses except for 1e-5, which has a higher KD and class loss.</p>

<p><img src="/assets/images/llama-into-torchtune/fg4.png" alt="Figure 4: Comparing losses of different learning rates" style="width:100%" /></p>

<p><strong>Figure 4: Comparing losses of different learning rates</strong></p>

<p>Based on our benchmarks, the optimal learning rate changes depending on which metric and tasks you are optimizing for.</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="2"><strong>Model</strong>
   </td>
   <td rowspan="2"><strong>learning rate</strong>
   </td>
   <td><strong>TruthfulQA</strong>
   </td>
   <td colspan="2"><strong>hellaswag</strong>
   </td>
   <td><strong>commonsense</strong>
   </td>
  </tr>
  <tr>
   <td>mc2
   </td>
   <td>acc
   </td>
   <td>acc_norm
   </td>
   <td>acc
   </td>
  </tr>
  <tr>
   <td>Baseline Llama 3.1 8B
   </td>
   <td>-
   </td>
   <td>0.5401
   </td>
   <td>0.5911
   </td>
   <td>0.7915
   </td>
   <td>0.7707
   </td>
  </tr>
  <tr>
   <td>Fine-tuned Llama 3.1 8B using LoRA
   </td>
   <td>-
   </td>
   <td>0.5475
   </td>
   <td>0.6031
   </td>
   <td>0.7951
   </td>
   <td>0.7789
   </td>
  </tr>
  <tr>
   <td>Baseline Llama 3.2 1B
   </td>
   <td>-
   </td>
   <td>0.4384
   </td>
   <td>0.4517
   </td>
   <td>0.6064
   </td>
   <td>0.5536
   </td>
  </tr>
  <tr>
   <td>Fine-tuned Llama 3.2 1B using LoRA
   </td>
   <td>-
   </td>
   <td>0.4492
   </td>
   <td>0.4595
   </td>
   <td>0.6132
   </td>
   <td>0.5528
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>3e-4
   </td>
   <td>0.4481
   </td>
   <td>0.4603
   </td>
   <td><strong>0.6157</strong>
   </td>
   <td>0.5569
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>1e-3
   </td>
   <td>0.4453
   </td>
   <td>0.4535
   </td>
   <td>0.6071
   </td>
   <td>0.5258
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>1e-4
   </td>
   <td>0.4489
   </td>
   <td><strong>0.4606</strong>
   </td>
   <td>0.6156
   </td>
   <td><strong>0.5586</strong>
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>1e-5
   </td>
   <td><strong>0.4547</strong>
   </td>
   <td>0.4548
   </td>
   <td>0.6114
   </td>
   <td>0.5487
   </td>
  </tr>
</table>

<p><strong>Table 4: Effects of tuning learning rate</strong></p>

<h3 id="hyperparameter-tuning-kd-ratio">Hyperparameter tuning: KD ratio</h3>

<p>By default, the KD ratio is set to 0.5, which gives even weighting to both the class and KD loss. In these experiments, we look at the effects of different KD ratios, where 0 only uses the class loss and 1 only uses the KD loss.</p>

<p>Overall, the benchmark results show that for these tasks and metrics, higher KD ratios perform slightly better.</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="2"><strong>Model</strong>
   </td>
   <td rowspan="2"><strong>kd_ratio (lr=3e-4)</strong>
   </td>
   <td><strong>TruthfulQA</strong>
   </td>
   <td colspan="2"><strong>hellaswag</strong>
   </td>
   <td><strong>commonsense</strong>
   </td>
  </tr>
  <tr>
   <td>mc2
   </td>
   <td>acc
   </td>
   <td>acc_norm
   </td>
   <td>acc
   </td>
  </tr>
  <tr>
   <td>Baseline Llama 3.1 8B
   </td>
   <td>-
   </td>
   <td>0.5401
   </td>
   <td>0.5911
   </td>
   <td>0.7915
   </td>
   <td>0.7707
   </td>
  </tr>
  <tr>
   <td>Fine-tuned Llama 3.1 8B using LoRA
   </td>
   <td>-
   </td>
   <td>0.5475
   </td>
   <td>0.6031
   </td>
   <td>0.7951
   </td>
   <td>0.7789
   </td>
  </tr>
  <tr>
   <td>Baseline Llama 3.2 1B
   </td>
   <td>-
   </td>
   <td>0.4384
   </td>
   <td>0.4517
   </td>
   <td>0.6064
   </td>
   <td>0.5536
   </td>
  </tr>
  <tr>
   <td>Fine-tuned Llama 3.2 1B using LoRA
   </td>
   <td>-
   </td>
   <td>0.4492
   </td>
   <td>0.4595
   </td>
   <td>0.6132
   </td>
   <td>0.5528
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>0.25
   </td>
   <td>0.4485
   </td>
   <td>0.4595
   </td>
   <td>0.6155
   </td>
   <td>0.5602
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>0.5
   </td>
   <td>0.4481
   </td>
   <td>0.4603
   </td>
   <td>0.6157
   </td>
   <td>0.5569
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>0.75
   </td>
   <td><strong>0.4543</strong>
   </td>
   <td>0.463
   </td>
   <td><strong>0.6189</strong>
   </td>
   <td>0.5643
   </td>
  </tr>
  <tr>
   <td>KD using fine-tuned 8B and baseline 1B
   </td>
   <td>1.0
   </td>
   <td>0.4537
   </td>
   <td><strong>0.4641</strong>
   </td>
   <td>0.6177
   </td>
   <td><strong>0.5717</strong>
   </td>
  </tr>
</table>

<p><strong>Table 5: Effects of tuning KD ratio</strong></p>

<h2 id="looking-ahead">Looking Ahead</h2>

<p>In this blog, we presented a study on how to distill LLMs through torchtune using the forward KL divergence loss on Llama 3.1 8B and Llama 3.2 1B logits. There are many directions for future exploration to further improve performance and offer more flexibility in distillation methods.</p>

<ul>
  <li><strong>Expand KD loss offerings</strong>. The KD recipe uses the forward KL divergence loss. However, aligning the student distribution to the whole teacher distribution may not be effective, as mentioned above. There are multiple papers, such as <a href="https://arxiv.org/pdf/2306.08543">MiniLLM</a>, <a href="https://arxiv.org/pdf/2402.03898">DistiLLM</a>, and <a href="https://arxiv.org/pdf/2306.13649">Generalized KD</a>, that introduce new KD losses and policies to address the limitation and have shown to outperform the standard use of cross entropy with forward KL divergence loss. For instance, MiniLLM uses reverse KL divergence to prevent the student from over-estimating low-probability regions of the teacher. DistiLLM introduces a skewed KL loss and an adaptive training policy.</li>
  <li><strong>Enable cross-tokenizer distillation</strong>. The current recipe requires the teacher and student model to use the same tokenizer, which limits the ability to distill across different LLM families. There has been research on cross-tokenizer approaches (e.g. <a href="https://arxiv.org/pdf/2402.12030">Universal Logit Distillation</a>) that we could explore.</li>
  <li><strong>Expand distillation to multimodal LLMs and encoder models</strong>. A natural extension of the KD recipe is to expand to multimodal LLMs. Similar to deploying more efficient LLMs, there’s also a need to deploy smaller and more efficient multimodal LLMs. In addition, there has been work in demonstrating LLMs as encoder models (e.g. <a href="https://arxiv.org/pdf/2404.05961">LLM2Vec</a>). Distillation from LLMs as encoders to smaller encoder models may also be a promising direction to explore.</li>
</ul>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p
        class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
    
    
        <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
        <script>
          hbspt.forms.create({
            region: "na1",
            portalId: "8112310",
            formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
          });
        </script>
        
    
      <p
        class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
        
    </div>
    


    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://join.slack.com/t/pytorch/shared_invite/zt-2j2la612p-miUinTTaxXczKOJw48poHA" target="_blank" title="PyTorch Slack">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack"><path fill="currentColor" d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z"></path></svg>
        </a></li>
        <li><a href="/wechat" title="PyTorch on WeChat">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat"><path fill="currentColor" d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z"></path><path fill="currentColor" d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z"></path></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Tools</a>
          </li>
          <li>
            <a href="https://github.com/pytorch-fdn/ecosystem">Join the Ecosystem</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2024">Contributor Awards - 2024</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
          <li>
            <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
          <li>
            <a href="/newsletter">Newsletter</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="/credits">Cloud Credit Program</a>
          </li>
          <li>          
            <a href="/tac">Technical Advisory Council</a>
          </li>
          <li>
            <a href="/staff">Staff</a>
          </li>
          <li>
            <a href="/contact-us">Contact Us</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

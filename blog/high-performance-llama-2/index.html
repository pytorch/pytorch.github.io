<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="In a landscape where AI innovation is accelerating at an unprecedented pace, Meta’s Llama family of open sourced large language models (LLMs) stands out as a notable breakthrough. Llama marked a significant step forward for LLMs, demonstrating the power of pre-trained architectures for a wide range of applications. Llama 2 further pushed the boundaries of scale and capabilities, inspiring advancements in language understanding, generation, and beyond.

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs" />
<meta property="og:description" content="In a landscape where AI innovation is accelerating at an unprecedented pace, Meta’s Llama family of open sourced large language models (LLMs) stands out as a notable breakthrough. Llama marked a significant step forward for LLMs, demonstrating the power of pre-trained architectures for a wide range of applications. Llama 2 further pushed the boundaries of scale and capabilities, inspiring advancements in language understanding, generation, and beyond.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs" />
<meta name="twitter:description" content="In a landscape where AI innovation is accelerating at an unprecedented pace, Meta’s Llama family of open sourced large language models (LLMs) stands out as a notable breakthrough. Llama marked a significant step forward for LLMs, demonstrating the power of pre-trained architectures for a wide range of applications. Llama 2 further pushed the boundaries of scale and capabilities, inspiring advancements in language understanding, generation, and beyond.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/ptc/2022">
            <span class="dropdown-title">PyTorch Conference - 2022</span>
            <p>See the posters presented at PyTorch Conference - 2022</p>
          </a>
          <a class="nav-dropdown-item" href="https://events.linuxfoundation.org/pytorch-conference/">
            <span class="dropdown-title">PyTorch Conference - 2023</span>
            <p>October 16-17 in San Francisco</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torcharrow">
            <span class="dropdown-title docs-title">torcharrow</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class=dropdown-title>Community stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">November 06, 2023</p>
            <h1>
                <a class="blog-title">High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Jiewen Tan, Jon Bolin, Yeounoh Chung, Liyang Lu, Siyuan Liu, Wonjoo Lee, Manfei Bai, Meghan Cowan, Jack Cao, Milad Mohammadi, Shauheen Zahirazami, Alex Spiridonov
                      
                    </p>
                    <p>In a landscape where AI innovation is accelerating at an unprecedented pace, Meta’s <a href="https://ai.meta.com/llama/">Llama</a> family of open sourced large language models (LLMs) stands out as a notable breakthrough. <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">Llama</a> marked a significant step forward for LLMs, demonstrating the power of pre-trained architectures for a wide range of applications. <a href="https://about.fb.com/news/2023/07/llama-2/">Llama 2</a> further pushed the boundaries of scale and capabilities, inspiring advancements in language understanding, generation, and beyond.</p>

<p>Shortly after the announcement of Llama, we published a <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/">blog post</a> showcasing ultra-low inference latency for Llama using PyTorch/XLA on Cloud TPU v4. Building on these results, today, we are proud to share Llama 2 training and inference performance using <a href="https://github.com/pytorch/xla">PyTorch/XLA</a> on Cloud TPU v4 and our newest AI supercomputer, <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga">Cloud TPU v5e</a>.</p>

<p>In this blog post, we use Llama 2 as an example model to demonstrate the power of PyTorch/XLA on Cloud TPUs for LLM training and inference. We discuss the computation techniques and optimizations used to improve inference throughput and training model FLOPs utilization (MFU). <strong>For Llama 2 70B parameters, we deliver 53% training MFU, 17 ms/token inference latency, 42 tokens/s/chip throughput powered by PyTorch/XLA on Google Cloud TPU.</strong> We offer a <a href="https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/SPMD_USER_GUIDE.md">training user guide</a> and an <a href="https://github.com/pytorch-tpu/llama/blob/llama2-google-next-inference/TORCH_XLA_USER_GUIDE.md">inference user guide</a> for reproducing the results in this article. Additionally, you may find our <a href="https://www.youtube.com/watch?v=PSpmRtWuMs8">Google Next 2023 presentation here</a>.</p>

<h2 id="model-overview">Model Overview</h2>

<p>Llama 2 comes in various sizes, ranging from 7B to 70B parameters, catering to different needs, computational resources, and training / inference budgets. Whether it’s small-scale projects or large-scale deployments, Llama models offer versatility and scalability to accommodate a wide range of applications.</p>

<p>Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The largest, 70B model, uses grouped-query attention, which speeds up inference without sacrificing quality. <a href="https://arxiv.org/pdf/2307.09288.pdf">Llama 2 is trained on 2 trillion tokens</a> (40% more data than Llama) and has the context length of 4,096 tokens for inference (double the context length of Llama), which enables more accuracy, fluency, and creativity for the model.</p>

<p>Llama 2 is a state-of-the-art LLM that outperforms many other open source language models on many benchmarks, including reasoning, coding, proficiency, and knowledge tests. The model’s scale and complexity place many demands on AI accelerators, making it an ideal benchmark for LLM training and inference performance of PyTorch/XLA on Cloud TPUs.</p>

<h2 id="performance-challenge-of-llms">Performance Challenge of LLMs</h2>

<p>Large-scale distributed training for LLMs such as Llama 2 introduces technical challenges that require practical solutions to make the most efficient use of TPUs. Llama’s size can strain both memory and processing resources of TPUs. To address this, we use model sharding, which involves breaking down the model into smaller segments, each fitting within the capacity of a single TPU core. This enables parallelism across multiple TPUs, improving training speed while reducing communication overhead.</p>

<p>Another challenge is managing the large datasets required for training Llama 2 efficiently, which requires effective data distribution and synchronization methods. Additionally, optimizing factors like learning rate schedules, gradient aggregation, and weight synchronization across distributed TPUs is crucial for achieving convergence.</p>

<p>After pretraining or fine-tuning Llama 2, running inference on the model checkpoint creates additional technical challenges. All of the challenges discussed in our <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/">previous blog post</a>, such as autoregressive decoding, variable input prompt lengths, and the need for model sharding and quantization still apply for Llama 2. In addition, Llama 2 introduced two new capabilities: grouped-query attention and early stopping. We discuss how PyTorch/XLA handles these challenges to enable high-performance, cost-efficient training and inference of Llama 2 on Cloud TPU v4 and v5e.</p>

<h2 id="large-scale-distributed-training">Large-Scale Distributed Training</h2>

<p>PyTorch/XLA offers two major ways of doing large-scale distributed training: <a href="https://pytorch.org/blog/pytorch-xla-spmd/">SPMD</a>, which utilizes the XLA compiler to transform and partition a single-device program into a multi-device distributed program; and <a href="https://pytorch.org/blog/large-scale-training-hugging-face/">FSDP</a>, which implements the widely-adopted <a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">Fully Sharded Data Parallel</a> algorithm.</p>

<p>In this blog post, we show how to use the SPMD API to annotate the <a href="https://huggingface.co/blog/llama2">HuggingFace (HF) Llama 2</a> implementation to maximize performance. For comparison, we also show our FSDP results with the same configurations; read about <a href="https://github.com/pytorch/xla/blob/master/docs/fsdp.md">PyTorch/XLA FSDP API here</a>.</p>

<h3 id="spmd-overview">SPMD Overview</h3>

<p>Let’s briefly review the fundamentals of SPMD. For details, please refer to our <a href="https://pytorch.org/blog/pytorch-xla-spmd/">blog post</a> and <a href="https://github.com/pytorch/xla/blob/master/docs/spmd.md">user guide</a>.</p>

<h4 id="mesh">Mesh</h4>

<p>A multidimensional array that describes the logical topology of the TPU devices:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Assuming you are running on a TPU host that has 8 devices attached
num_devices = xr.global_runtime_device_count()
# mesh shape will be (4,2) in this example
mesh_shape = (num_devices // 2, 2)
device_ids = np.array(range(num_devices))
# axis_names 'x' and 'y' are optional
mesh = Mesh(device_ids, mesh_shape, ('x', 'y'))
</code></pre></div></div>

<h4 id="partition-spec">Partition Spec</h4>

<p>A tuple that describes how the corresponding tensor’s dimensions are sharded across the mesh:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>partition_spec = ('x', 'y')
</code></pre></div></div>

<h4 id="mark-sharding">Mark Sharding</h4>

<p>An API that takes a mesh and a partition_spec, and then generates a sharding annotation for the XLA compiler.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor = torch.randn(4, 4).to('xla')
# Let's resue the above mesh and partition_spec.
# It means the tensor's 0th dim is sharded 4 way and 1th dim is sharded 2 way.
xs.mark_sharding(tensor, mesh, partition_spec)
</code></pre></div></div>

<h3 id="2d-sharding-with-spmd">2D Sharding with SPMD</h3>

<p>In our <a href="https://pytorch.org/blog/pytorch-xla-spmd/">SPMD blog post</a>, we demonstrated using 1D FSDP style sharding. Here, we introduce a more powerful sharding strategy, called <a href="https://arxiv.org/pdf/2105.04663.pdf">2D sharding</a>, where both the parameters and activations are sharded. This new sharding strategy not only allows fitting a larger model but also boosts the MFU to up to <strong>54.3%</strong>. For more details, read the Benchmarks section.</p>

<p>This section introduces a set of general rules that applies to most LLMs, and for convenience we directly reference the variable names and configuration names from <a href="https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/modeling_llama.py">HF Llama</a>.</p>

<p>First, let’s create a 2D Mesh with corresponding axis names: data and model. The data axis is usually where we distribute the input data, and the model axis is where we further distribute the model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mesh = Mesh(device_ids, mesh_shape, ('data', 'model'))
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">mesh_shape</code> can be a hyper-parameter that is tuned for different model sizes and hardware configurations. The same mesh will be reused in all following sharding annotations. In the next few sections, we will cover how to use the mesh to shard parameters, activations and input data.</p>

<h4 id="parameter-sharding">Parameter Sharding</h4>

<p>Below is a table that summarizes all parameters of HF Llama 2 and corresponding partition specifications. Example HF code can be found <a href="https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/examples/pytorch/language-modeling/run_clm.py#L572">here</a>.</p>

<table class="table table-bordered">
  <tr>
   <td><strong>Parameter Name</strong>
   </td>
   <td><strong>Explanation</strong>
   </td>
   <td><strong>Parameter Shape</strong>
   </td>
   <td><strong>Partition Spec</strong>
   </td>
  </tr>
  <tr>
   <td><code>embed_tokens</code>
   </td>
   <td>embedding layer
   </td>
   <td>(<code>vocab_size</code>, <code>hidden_size</code>)
   </td>
   <td>(model, data)
   </td>
  </tr>
  <tr>
   <td><code>q_proj</code>
   </td>
   <td>attention weights
   </td>
   <td>(<code>num_heads</code> <code>x</code> <code>head_dim</code>, <code>hidden_size</code>)
   </td>
   <td>(data, model)
   </td>
  </tr>
  <tr>
   <td><code>k_proj / v_proj</code>
   </td>
   <td>attention weights
   </td>
   <td>(<code>num_key_value_heads</code> <code>x</code> <code>head_dim</code>, <code>hidden_size</code>)
   </td>
   <td>(data, model)
   </td>
  </tr>
  <tr>
   <td><code>o_proj</code>
   </td>
   <td>attention weights
   </td>
   <td>(<code>hidden_size</code>, <code>num_heads x head_dim</code>)
   </td>
   <td>(model, data)
   </td>
  </tr>
  <tr>
   <td><code>gate_proj / up_proj</code>
   </td>
   <td>MLP weights
   </td>
   <td>(<code>intermediate_size</code>, <code>hidden_size</code>)
   </td>
   <td>(model, data)
   </td>
  </tr>
  <tr>
   <td><code>down_proj</code>
   </td>
   <td>MLP weights
   </td>
   <td>(<code>hidden_size</code>, <code>intermediate_size</code>)
   </td>
   <td>(data, model)
   </td>
  </tr>
  <tr>
   <td><code>lm_head</code>
   </td>
   <td>HF output embedding 
   </td>
   <td>(<code>vocab_size</code>, <code>hidden_size</code>)
   </td>
   <td>(model, data)
   </td>
  </tr>
</table>

<p><strong>Table 1: SPMD 2D Sharding Parameter Partition Spec</strong></p>

<p>The rule is to shard the <code class="language-plaintext highlighter-rouge">hidden_size</code> dim of any weights except QKVO projections according to the <code class="language-plaintext highlighter-rouge">data</code> axis of the mesh, then shard the other dim with the remaining <code class="language-plaintext highlighter-rouge">model</code> axis. For QKVO, do the opposite. This model-data axis rotation methodology is similar to that of <a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM</a> to reduce communication overhead. For <code class="language-plaintext highlighter-rouge">layernorm</code> weights, we implicitly mark them as replicated across different devices given they are 1D tensors.</p>

<h4 id="activation-sharding">Activation Sharding</h4>

<p>In order to better utilize the device memory, very often we need to annotate the output of some memory bound ops. That way the compiler is forced to only keep partial output on devices instead of the full output. In Llama 2, we explicitly annotate all <code class="language-plaintext highlighter-rouge">torch.matmul</code> and <code class="language-plaintext highlighter-rouge">nn.Linear</code> outputs. Table 2 summarizes the corresponding annotations; the example HF code can be found <a href="https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/modeling_llama.py#L235">here</a>.</p>

<table class="table table-bordered">
  <tr>
   <td><strong>Output Name</strong>
   </td>
   <td><strong>Explanation</strong>
   </td>
   <td><strong>Output Shape</strong>
   </td>
   <td><strong>Partition Spec</strong>
   </td>
  </tr>
  <tr>
   <td><code>inputs_embeds</code>
   </td>
   <td>embedding layer output
   </td>
   <td>(<code>batch_size</code>, <code>sequence_length</code>, <code>hidden_size</code>)
   </td>
   <td>(data, None, model)
   </td>
  </tr>
  <tr>
   <td><code>query_states</code>
   </td>
   <td>attention nn.Linear output
   </td>
   <td>(<code>batch_size</code>, <code>sequence_length</code>, <code>num_heads x head_dim</code>)
   </td>
   <td>(data, None, model)
   </td>
  </tr>
  <tr>
   <td><code>key_states / value_states</code>
   </td>
   <td>attention nn.Linear output
   </td>
   <td>(<code>batch_size</code>, <code>sequence_length</code>, <code>num_key_value_heads x head_dim</code>)
   </td>
   <td>(data, None, model)
   </td>
  </tr>
  <tr>
   <td><code>attn_weights</code>
   </td>
   <td>attention weights
   </td>
   <td>(<code>batch_size</code>, <code>num_attention_heads</code>, <code>sequence_length</code>, <code>sequence_length</code>)
   </td>
   <td>(data, model, None, None)
   </td>
  </tr>
  <tr>
   <td><code>attn_output</code>
   </td>
   <td>attention layer output
   </td>
   <td>(<code>batch_size</code>, <code>sequence_length</code>, <code>hidden_size</code>)
   </td>
   <td>(data, None, model)
   </td>
  </tr>
  <tr>
   <td><code>up_proj / gate_proj / down_proj</code>
   </td>
   <td>MLP <code>nn.Linear</code> outputs
   </td>
   <td>(<code>batch_size</code>, <code>sequence_length</code>, <code>intermediate_size</code>)
   </td>
   <td>(data, None, model)
   </td>
  </tr>
  <tr>
   <td><code>logits</code>
   </td>
   <td>HF output embedding output 
   </td>
   <td>(<code>batch_size</code>, <code>sequence_length</code>, <code>hidden_size</code>)
   </td>
   <td>(data, None, model)
   </td>
  </tr>
</table>

<p><strong>Table 2: SPMD 2D Sharding Activation Partition Spec</strong></p>

<p>The rule is to shard the <code class="language-plaintext highlighter-rouge">batch_size</code> dim of any outputs according to the <code class="language-plaintext highlighter-rouge">data</code> axis of the mesh, then replicate the length dims of any outputs, and finally shard the last dim along the <code class="language-plaintext highlighter-rouge">model</code> axis.</p>

<h4 id="input-sharding">Input Sharding</h4>

<p>For input sharding, the rule is to shard the batch dim along the <code class="language-plaintext highlighter-rouge">data</code> axis of the mesh, and replicate the <code class="language-plaintext highlighter-rouge">sequence_length</code> dim. Below is the example code, and the corresponding HF change may be found <a href="https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/trainer.py#L1456">here</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>partition_spec = ('data', None)
sharding_spec = xs.ShardingSpec(mesh, partition_spec)
# MpDeviceLoader will shard the input data before sending to the device.
pl.MpDeviceLoader(dataloader, self.args.device, input_sharding=sharding_spec, ...)
</code></pre></div></div>

<p>Now, all the data and model tensors that require sharding are covered!</p>

<h4 id="optimizer-states--gradients">Optimizer States &amp; Gradients</h4>

<p>You may be wondering whether it is necessary to shard the optimizer states and gradients as well. Great news: the sharding propagation feature of the XLA compiler automates the sharding annotation in these two scenarios, without needing more hints to improve performance.</p>

<p>It is important to note that optimizer states are typically initialized within the first iteration of the training loop. From the standpoint of the XLA compiler, the optimizer states are the outputs of the first graph, and therefore have the sharding annotation propagated. For subsequent iterations, the optimizer states become inputs to the second graph, with the sharding annotation propagated from the first one. This is also why PyTorch/XLA typically produces two graphs for the training loops. If the optimizer states are somehow initialized before the first iteration, users will have to manually annotate them, just like the model weights.</p>

<p>Again, all concrete examples of the above sharding annotation can be found in our fork of HF Transformers <a href="https://github.com/pytorch-tpu/transformers/tree/llama2-google-next-training">here</a>. The repo also contains code for our experimental feature <a href="https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads">MultiSlice</a>, including <code class="language-plaintext highlighter-rouge">HybridMesh</code> and <code class="language-plaintext highlighter-rouge">dcn</code> axis, which follows the same principles mentioned above.</p>

<h3 id="caveats">Caveats</h3>

<p>While using SPMD for training, there are a few important things to pay attention to:</p>

<ul>
  <li>Use <code class="language-plaintext highlighter-rouge">torch.einsum</code> instead of <code class="language-plaintext highlighter-rouge">torch.matmul</code>; <code class="language-plaintext highlighter-rouge">torch.matmul</code> usually flattens tensors and does a <code class="language-plaintext highlighter-rouge">torch.mm</code> at the end, and that’s bad for SPMD when the combined axes are sharded. The XLA compiler will have a hard time determining how to propagate the sharding.</li>
  <li>PyTorch/XLA provides patched <code>[nn.Linear](https://github.com/pytorch/xla/blob/master/torch_xla/experimental/xla_sharding.py#L570)</code> to overcome the above constraint:</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch_xla.experimental.xla_sharding as xs
from torch_xla.distributed.fsdp.utils import apply_xla_patch_to_nn_linear

 model = apply_xla_patch_to_nn_linear(model, xs.xla_patched_nn_linear_forward)
</code></pre></div></div>

<ul>
  <li>Always reuse the same mesh across all shardings</li>
  <li>Always specify <code>--dataloader_drop_last yes</code>. The last smaller data is hard to annotate.</li>
  <li>Large models which are initialized on the host can induce host-side OOM. One way to avoid this issue is to initialize parameters on the <a href="https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/examples/pytorch/language-modeling/run_clm.py#L501">meta device</a>, then create and shard real tensors layer-by-layer.</li>
</ul>

<h3 id="infrastructure-improvements">Infrastructure Improvements</h3>

<p>Besides the above modeling techniques, we have developed additional features and improvements to maximize performance, including:</p>

<ul>
  <li>We enable asynchronous collective communication. This requires enhancements on the XLA compiler’s latency hiding scheduler to better optimize for the Llama 2 PyTorch code.</li>
  <li>We now allow sharding annotations in the middle of the IR graph, just like JAX’s <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html">jax.lax.with_sharding_constraint</a>. Previously, only graph inputs were annotated.</li>
  <li>We also propagate replicated sharding spec from the compiler to the graph outputs. This allows us to shard the optimizer states automatically.</li>
</ul>

<h2 id="inference-optimizations">Inference Optimizations</h2>

<p>All the PyTorch/XLA <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/">optimizations</a> implemented for Llama inference are applied to Llama 2 as well. That includes <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/#fairscale-sharding">Tensor Parallelism + Dynamo (torch.compile) using torch-xla collective ops</a>, <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/#autoregressive-decoding-on-pytorchxla">autoregressive decoding logic improvement to avoid recompilation</a>, <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/#input-prompt-optimization">bucketized prompt length</a>, <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/#kv-cache-optimization">KV-cache with compilation friendly index ops</a>. Llama 2 introduces two new changes: Grouped Query Attention, and Early Stopping when eos is reached for all prompts. We applied corresponding changes to promote better performance and flexibility with PyTorch/XLA.</p>

<h3 id="grouped-query-attention">Grouped Query Attention</h3>

<p>Llama 2 enables <a href="https://arxiv.org/pdf/2305.13245.pdf">Grouped Query Attention</a> for the 70B models. It allows the number of Key and Value heads to be smaller than the number of Query heads, while still supporting KV-cache sharding up to the number of KV heads. For the 70B models, the <code class="language-plaintext highlighter-rouge">n_kv_heads</code> is 8, which limits the tensor parallelism to be less or equal to 8. In order to shard the model checkpoint to run on more devices, the K, V projection weights need to be replicated first, and then split into multiple pieces. For example, to shard the 70B model checkpoint from 8 pieces to 16 pieces, the K, V projection weights are duplicated and split into 2 pieces for each shard. We provide a <a href="https://github.com/pytorch-tpu/llama/blob/llama2-google-next-inference/reshard_checkpoints.py">reshard_checkpoints.py</a> script to handle that, and to make sure the sharded checkpoint performs mathematically identical to the original checkpoint.</p>

<h3 id="eos-early-stopping">EOS Early Stopping</h3>

<p>The Llama 2 generation code added <a href="https://github.com/facebookresearch/llama/blob/ea9f33d6d3ea8ed7d560d270986407fd6c2e52b7/llama/generation.py#L159">the early stopping logic</a>. A <code class="language-plaintext highlighter-rouge">eos_reached</code> tensor is used to track the completion of all the prompt generations, and if the <code class="language-plaintext highlighter-rouge">eos</code> token is reached for all the prompts in the batch, the generation would stop early. The similar change is incorporated in the PyTorch/XLA optimized version as well, with some minor tweaks.</p>

<p>In PyTorch/XLA, checking the value of a tensor like <code class="language-plaintext highlighter-rouge">eos_reached</code> as part of the control flow condition would invoke a blocking device-to-host transfer. The tensor would be transferred from device memory to CPU memory to evaluate its value, while all other logics are waiting. This introduced a delay on the scale of ms after every new token generation. As a trade-off, we reduce the rate of checking the <code class="language-plaintext highlighter-rouge">eos_reached</code> value to be <a href="https://github.com/pytorch-tpu/llama/blob/b89dd0f2351c42fef367670d9d2c5b65cd0ae932/llama/generation.py#L268C13-L270C26">once every 10 new token generations</a>. With this change, the impact of the blocking device-to-host transfer would be reduced by 10x, while the early stopping would still be effective, and at most 9 unnecessary tokens would be generated after each sequence reaches the <code class="language-plaintext highlighter-rouge">eos</code> token.</p>

<h3 id="model-serving">Model Serving</h3>

<p>PyTorch/XLA is working on a serving strategy to enable the PyTorch community to serve their deep learning applications via <a href="https://pytorch.org/docs/stable/export.html">Torch.Export</a>, <a href="https://github.com/openxla/stablehlo">StableHLO</a>, and <a href="https://www.tensorflow.org/guide/saved_model">SavedModel</a>. PyTorch/XLA Serving is an experimental feature in <a href="https://github.com/pytorch/xla/releases">PyTorch/XLA 2.1 release</a>; for details visit our <a href="https://github.com/pytorch/xla/blob/r2.1/docs/stablehlo.md#convert-saved-stablehlo-for-serving">serving user guide</a>. Users can take advantage of TorchServe to run their single-host workloads.</p>

<h2 id="benchmarks">Benchmarks</h2>

<h3 id="metrics">Metrics</h3>

<p>To measure training performance, we use the industry-standard metric: <a href="https://arxiv.org/abs/2204.02311">Model FLOPS Utilization (MFU)</a>. Model FLOPS are the floating point operations required to perform a single forward and backward pass. Model FLOPs are hardware and implementation independent and only depend on the underlying model. MFU measures how effectively the model is using the actual hardware during training. Achieving 100% MFU means that the model is using the hardware perfectly.</p>

<p>To measure inference performance, we use the industry-standard metric of throughput. First, we measure latency per token when the model has been compiled and loaded. Then, we calculate throughput by dividing batch size (BS) over latency per chip. As a result, throughput measures how the model is performing in production environments regardless of how many chips are used.</p>

<h3 id="results">Results</h3>

<h4 id="training-evaluation">Training Evaluation</h4>

<p>Figure 1 shows Llama 2 SPMD 2D sharding training results on a range of Google TPU v4 hardware with <a href="https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/">PyTorch/XLA FSDP</a> as the baseline. We increased MFU by <strong>28%</strong> across all sizes of Llama 2 compared to FSDP running on the same hardware configuration. This performance improvement is largely due to: 1) 2D Sharding has less communication overhead than FSDP, and 2) asynchronous collective communication is enabled in SPMD which allows communication and computation overlapping. Also note that as the model size scales, we maintain the high MFU. Table 3 shows all the hardware configurations plus some hyperparameters used in the training benchmarks.</p>

<p><img src="/assets/images/high-performance-llama-2/fig1.jpg" alt="Figure 1. Llama 2 Training MFU on TPU v4 Hardware" style="width:100%;" /></p>

<p style="line-height: 1.05"><small><em><strong>Fig. 1</strong>: Llama 2 Training MFU on TPU v4 Hardware</em></small></p>

<p>The results in Figure 1 are produced with sequence length 1,024. Figure 2 shows how the performance behaves with larger sequence lengths. It shows our performance also scales linearly with sequence lengths. The MFU is expected to decrease a little as a smaller per device batch size is needed to accommodate the additional memory pressure introduced by the larger sequence length since the sequence length axis is not sharded in 2D sharding. And TPU is very sensitive to batch size. For Llama 2, 70B parameters, the performance decrease is as low as <strong>4%</strong>. At the time of preparing these results, <a href="https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/src/transformers/models/llama/tokenization_llama.py#L48">Hugging Face Llama 2 tokenizer</a> limits the max model input to 2,048, preventing us from evaluating larger sequence lengths.</p>

<p><img src="/assets/images/high-performance-llama-2/fig2.jpg" alt="Figure 2. Llama 2 SPMD Training MFU on TPU v4 with Different Sequence Lengths" style="width:100%;" /></p>

<p style="line-height: 1.05"><small><em><strong>Fig. 2</strong>: Llama 2 SPMD Training MFU on TPU v4 with Different Sequence Lengths</em></small></p>

<table class="table table-bordered">
  <tr>
   <td><strong>Model Size</strong>
   </td>
   <td colspan="2"><strong>7B</strong>
   </td>
   <td colspan="2"><strong>13B</strong>
   </td>
   <td colspan="2"><strong>70B</strong>
   </td>
  </tr>
  <tr>
   <td><strong>TPU NumCores</strong>
   </td>
   <td colspan="2">V4-32
   </td>
   <td colspan="2">V4-64
   </td>
   <td colspan="2">V4-256
   </td>
  </tr>
  <tr>
   <td><strong>Mesh Shape</strong>
   </td>
   <td colspan="2">(16, 1)
   </td>
   <td colspan="2">(32, 1)
   </td>
   <td colspan="2">(32, 4)
   </td>
  </tr>
  <tr>
   <td><strong>Seq Len</strong>
   </td>
   <td>1,024
   </td>
   <td>2,048
   </td>
   <td>1,024
   </td>
   <td>2,048
   </td>
   <td>1,024
   </td>
   <td>2,048
   </td>
  </tr>
  <tr>
   <td><strong>Global Batch</strong>
   </td>
   <td>256
   </td>
   <td>128
   </td>
   <td>256
   </td>
   <td>128
   </td>
   <td>512
   </td>
   <td>256
   </td>
  </tr>
  <tr>
   <td><strong>Per Device Batch</strong>
   </td>
   <td>16
   </td>
   <td>8
   </td>
   <td>8
   </td>
   <td>4
   </td>
   <td>16
   </td>
   <td>8
   </td>
  </tr>
</table>

<p><strong>Table 3: Llama 2 SPMD Training Benchmark TPU Configurations and Hyperparameters</strong></p>

<p>One last thing to call out is that we use <a href="https://arxiv.org/abs/1804.04235">adafactor</a> as the optimizer for better memory utilization. And once again, here is the <a href="https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/SPMD_USER_GUIDE.md">user guide</a> to reproduce the benchmark results listed above.</p>

<h4 id="inference-evaluation">Inference Evaluation</h4>

<p>In this section, we extend our <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/">previous evaluation of Llama on Cloud v4 TPU</a>. Here, we demonstrate the performance properties of TPU v5e for inference applications.</p>

<p>We define inference throughput as the number of tokens produced by a model per second per TPU chip. Figure 3 shows Llama 2 70B throughput on a v5e-16 TPU node. Given Llama is a memory bound application, we see that applying weight-only quantization unblocks extending the model batch size to 32. Higher throughput results would be possible on larger TPU v5e hardware up to the point where the ICI network bandwidth between chips throttle the TPU slice from delivering higher throughput. Exploring the upper bound limits of TPU v5e on Llama 2 was outside of the scope of this work. Notice, to make the Llama 2 70B model run on v5e-16, we replicated the attention heads to have one head per chip as discussed in the Inference section above. As discussed <a href="https://pytorch.org/blog/path-achieve-low-inference-latency/">previously</a>, with increasing model batch size, per-token latency grows proportionally; quantization improves overall latency by reducing memory I/O demand.</p>

<p><img src="/assets/images/high-performance-llama-2/fig3.jpg" alt="Figure 3. Llama 2 70B Inference Per-Chip Throughput on TPU v5e vs. Batch Size" style="width:100%;" /></p>

<p style="line-height: 1.05"><small><em><strong>Fig. 3</strong>: Llama 2 70B Inference Per-Chip Throughput on TPU v5e vs. Batch Size</em></small></p>

<p>Figure 4 shows inference throughput results across different model sizes. These results highlight the largest throughput given the hardware configuration when using <code class="language-plaintext highlighter-rouge">bf16</code> precision. With weight only quantization, this throughput reaches 42 on the 70B model. As mentioned above, increasing hardware resources may lead to performance gains.</p>

<p><img src="/assets/images/high-performance-llama-2/fig4.jpg" alt="Figure 4. Llama 2 Inference Per-Chip Throughput on TPU v5e" style="width:100%;" /></p>

<p style="line-height: 1.05"><small><em><strong>Fig. 4</strong>: Llama 2 Inference Per-Chip Throughput on TPU v5e</em></small></p>

<p>Figure 5 shows the cost of serving Llama 2 models (from Figure 4) on Cloud TPU v5e. We report the TPU v5e per-chip cost based on the 3-year commitment (reserved) price in the <code class="language-plaintext highlighter-rouge">us-west4</code> region. All model sizes use maximum sequence length of 2,048 and maximum generation length of 1,000 tokens. Note that with quantization, the cost for the 70B model drops to <strong>$0.0036 per 1,000 tokens</strong>.</p>

<p><img src="/assets/images/high-performance-llama-2/fig5.jpg" alt="Figure 5. Llama 2 Inference Per-Chip Cost on TPU v5e" style="width:100%;" /></p>

<p style="line-height: 1.05"><small><em><strong>Fig. 5</strong>: Llama 2 Inference Per-Chip Cost on TPU v5e</em></small></p>

<p>Figure 6 summarizes our best Llama 2 inference latency results on TPU v5e. Llama 2 7B results are obtained from our non-quantized configuration (BF16 Weight, BF16 Activation) while the 13B and 70B results are from the quantized (INT8 Weight, BF16 Activation) configuration. We attribute this observation to the inherent memory saving vs. compute overhead tradeoff of quantization; as a result, for smaller models, quantization may not lead to lower inference latency.</p>

<p>Additionally, prompt length has a strong effect on the memory requirements of LLMs. For instance, we observe a latency of 1.2ms / token (i.e. 201 tokens / second / chip) when <code class="language-plaintext highlighter-rouge">max_seq_len=256</code> at batch size of 1 with no quantization on v5e-4 running Llama2 7B.</p>

<p><img src="/assets/images/high-performance-llama-2/fig6.jpg" alt="Figure 6. Llama 2 Inference Latency on TPU v5e" style="width:100%;" /></p>

<p style="line-height: 1.05"><small><em><strong>Fig. 6</strong>: Llama 2 Inference Latency on TPU v5e</em></small></p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>The recent wave of AI innovation has been nothing short of transformative, with breakthroughs in LLMs at the forefront. Meta’s Llama and Llama 2 models stand as notable milestones in this wave of progress. PyTorch/XLA uniquely enables high-performance, cost-efficient training and inference for Llama 2 and other LLMs and generative AI models on Cloud TPUs, including the new Cloud TPU v5e. Looking forward, PyTorch/XLA will continue to push the performance limits on Cloud TPUs in both throughput and scalability and at the same time maintain the same PyTorch user experience.</p>

<p>We are ecstatic about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source. So, please file issues, submit pull requests, and send RFCs to <a href="https://github.com/pytorch/xla">GitHub</a> so that we can openly collaborate. You can also <a href="https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb">try out</a> PyTorch/XLA for yourself on various XLA devices including TPUs and GPUs.</p>

<p>We would like to extend our special thanks to Marcello Maggioni, Tongfei Guo, Andy Davis, Berkin Ilbeyi for their support and collaboration in this effort.</p>

<p>Cheers,<br />
The PyTorch/XLA Team at Google</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          <li><a href="https://github.com/pytorch/pytorch/security/policy" target="_blank">Security</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank">Mastodon</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
      </ul>
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/ptc/2022">PyTorch Conference - 2022</a>
          </li>
          <li>
            <a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference - 2023</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/torcharrow">torcharrow</a>
          </li>

          <li class="">
            <a href="/data">TorchData</a>
          </li> 

          <li class="">
            <a href="/torchrec">TorchRec</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          
          <li>
            <a href="/#community-module">Community</a>
          </li>
          
          <li class="">
            <a href="/community-stories">Community stories</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

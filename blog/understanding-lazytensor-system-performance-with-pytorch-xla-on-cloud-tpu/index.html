<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Understanding LazyTensor System Performance with PyTorch/XLA on Cloud TPU | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="Introduction

" />

  <meta property="og:image" content="https://pytorch.org/" />
  <meta name="twitter:image" content="https://pytorch.org/" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Understanding LazyTensor System Performance with PyTorch/XLA on Cloud TPU" />
<meta property="og:description" content="Introduction

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Understanding LazyTensor System Performance with PyTorch/XLA on Cloud TPU" />
<meta name="twitter:description" content="Introduction

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/ptc/2022">
            <span class="dropdown-title">PyTorch Conference - 2022</span>
            <p>See the posters presented at PyTorch Conference - 2022</p>
          </a>
          <a class="nav-dropdown-item" href="https://events.linuxfoundation.org/pytorch-conference/">
            <span class="dropdown-title">PyTorch Conference - 2023</span>
            <p>October 16-17 in San Francisco</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torcharrow">
            <span class="dropdown-title docs-title">torcharrow</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class=dropdown-title>Community stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">March 02, 2022</p>
            <h1>
                <a class="blog-title">Understanding LazyTensor System Performance with PyTorch/XLA on Cloud TPU</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Vaibhav Singh
                      
                    </p>
                    <h2 id="introduction">Introduction</h2>

<p>Ease of use, expressivity, and debuggability are among the core principles of PyTorch. One of the key drivers for the ease of use is that PyTorch execution is by default “eager, i.e. op by op execution preserves the imperative nature of the program. However, eager execution does not offer the compiler based optimization, for example, the optimizations when the computation can be expressed as a graph.</p>

<p>LazyTensor [<a href="https://arxiv.org/pdf/2102.13267.pdf">1</a>], first introduced with PyTorch/XLA, helps combine these seemingly disparate approaches. While PyTorch eager execution is widely used, intuitive, and well understood, lazy execution is not as prevalent yet.</p>

<p>In this post we will explore some of the basic concepts of the LazyTensor System with the goal of applying these concepts to understand and debug performance of LazyTensor based implementations in PyTorch. Although we will use PyTorch/XLA on Cloud TPU as the vehicle for exploring these concepts, we hope that these ideas will be useful to understand other system(s) built on LazyTensors.</p>

<h2 id="lazytensor">LazyTensor</h2>

<p>Any operation performed on a PyTorch tensor is by default dispatched as a kernel or a composition of kernels to the underlying hardware. These kernels are executed asynchronously on the underlying hardware. The program execution is not blocked until the value of a tensor is fetched. This approach scales extremely well with massively parallel programmed hardware such as GPUs.</p>

<p>The starting point of a LazyTensor system is a custom tensor type. In PyTorch/XLA, this type is called XLA tensor. In contrast to PyTorch’s native tensor type, operations performed on XLA tensors are recorded into an IR graph. Let’s examine an example that sums the product of two tensors:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="n">xm</span>

<span class="n">dev</span> <span class="o">=</span> <span class="n">xm</span><span class="p">.</span><span class="n">xla_device</span><span class="p">()</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>

<span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'bs,st-&gt;bt'</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch_xla</span><span class="p">.</span><span class="n">_XLAC</span><span class="p">.</span><span class="n">_get_xla_tensors_text</span><span class="p">([</span><span class="n">y1</span><span class="p">]))</span>
</code></pre></div></div>

<p>You can execute <a href="https://github.com/ultrons/xla/blob/lazy-tensor-post/contrib/colab/LazyTensor_Basics.ipynb">this</a> colab notebook to examine the resulting graph for y1. Notice that no computation has been performed yet.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y1</span> <span class="o">=</span> <span class="n">y1</span> <span class="o">+</span> <span class="n">x2</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch_xla</span><span class="p">.</span><span class="n">_XLAC</span><span class="p">.</span><span class="n">_get_xla_tensors_text</span><span class="p">([</span><span class="n">y1</span><span class="p">]))</span>
</code></pre></div></div>

<p>The operations will continue until PyTorch/XLA encounters a barrier. This barrier can either be a <a href="https://github.com/pytorch/xla/blob/ff079bb48744e5aa6696201ccf34057f15fc7cac/torch_xla/core/xla_model.py#L751">mark step()</a> api call or any other event which forces the execution of the graph recorded so far.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xm</span><span class="p">.</span><span class="n">mark_step</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">torch_xla</span><span class="p">.</span><span class="n">_XLAC</span><span class="p">.</span><span class="n">_get_xla_tensors_text</span><span class="p">([</span><span class="n">y1</span><span class="p">]))</span>
</code></pre></div></div>

<p>Once the mark_step() is called, the graph is compiled and then executed on TPU, i.e. the tensors have been materialized. Therefore, the graph is now reduced to a single line y1 tensor which holds the result of the computation.</p>

<h3 id="compile-once-execute-often">Compile Once, Execute Often</h3>

<p>XLA compilation passes offer optimizations (e.g. op-fusion, which reduces HBM pressure by using scratch-pad memory for multiple ops, <a href="https://arxiv.org/pdf/2004.13336.pdf">ref</a> ) and leverages lower level XLA infrastructure to optimally use the underlying hardware. However, there is one caveat, compilation passes are expensive, i.e. can add to the training step time. Therefore, this approach scales well if and only if we can <strong>compile once and execute often</strong> (compilation cache helps, such that the same graph is not compiled more than once).</p>

<p>In the following example, we create a small computation graph and time the execution:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dummy_step</span><span class="p">()</span> <span class="p">:</span>
  <span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'bs,st-&gt;bt'</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">xm</span><span class="p">.</span><span class="n">mark_step</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">y1</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">timeit</span> <span class="n">dummy_step</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">The</span> <span class="n">slowest</span> <span class="n">run</span> <span class="n">took</span> <span class="mf">29.74</span> <span class="n">times</span> <span class="n">longer</span> <span class="n">than</span> <span class="n">the</span> <span class="n">fastest</span><span class="p">.</span> <span class="n">This</span> <span class="n">could</span> <span class="n">mean</span> <span class="n">that</span> <span class="n">an</span> <span class="n">intermediate</span> <span class="n">result</span> <span class="ow">is</span> <span class="n">being</span> <span class="n">cached</span><span class="p">.</span>
<span class="mi">10000000</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">5</span><span class="p">:</span> <span class="mf">34.2</span> <span class="n">ns</span> <span class="n">per</span> <span class="n">loop</span>
</code></pre></div></div>

<p>You notice that the slowest step is quite longer than the fastest. This is because of the graph compilation overhead which is incurred only once for a given shape of graph, input shape, and output shape. Subsequent steps are faster because no graph compilation is necessary.</p>

<p>This also implies that we expect to see performance cliffs when the “compile once and execute often” assumption breaks. Understanding when this assumption breaks is the key to understanding and optimizing the performance of a LazyTensor system. Let’s examine what triggers the compilation.</p>

<h3 id="graph-compilation-and-execution-and-lazytensor-barrier">Graph Compilation and Execution and LazyTensor Barrier</h3>

<p>We saw that the computation graph is compiled and executed when a LazyTensor barrier is encountered. There are three scenarios when the LazyTensor barrier is automatically or manually introduced. The first is the explicit call of mark_step() api as shown in the preceding example. mark_step() is also called implicitly at every step when you wrap your dataloader with MpDeviceLoader (highly recommended to overlap compute and data upload to TPU device). The <a href="https://github.com/pytorch/xla/blob/master/torch_xla/core/xla_model.py#L804">Optimizer step</a> method of xla_model also allows to implicitly call mark_step (when you set barrier=True).</p>

<p>The second scenario where a barrier is introduced is when PyTorch/XLA finds an op with no mapping (lowering) to equivalent XLA HLO ops. PyTorch has <a href="https://dev-discuss.pytorch.org/t/where-do-the-2000-pytorch-operators-come-from-more-than-you-wanted-to-know/373">2000+</a> operations. Although most of these operations are composite (i.e. can be expressed in terms of other fundamental operations), some of these operations do not have corresponding lowering in XLA.</p>

<p align="center">
  <img src="/assets/images/unlowered-op.jpg" />
</p>

<p>What happens when an op with no XLA lowering is used? PyTorch XLA stops the operation recording and cuts the graph(s) leading to the input(s) of the unlowered op. This cut graph is then compiled and dispatched for execution. The results (materialized tensor) of execution are sent back from device to host, the unlowered op is then executed on the host (cpu), and then downstream LazyTensor operations creating a new graph(s) until a barrier is encountered again.</p>

<p>The third and final scenario which results in a LazyTensor barrier is when there is a control structure/statement or another method which requires the value of a tensor. This statement would at the minimum cause the execution of the computation graph leading to the tensor (if the graph has already been seen) or cause compilation and execution of both.</p>

<p>Other examples of such methods include .item(), isEqual(). In general, any operation that maps Tensor -&gt; Scalar will cause this behavior.</p>

<h3 id="dynamic-graph">Dynamic Graph</h3>

<p>As illustrated in the preceding section, graph compilation cost is amortized if the same shape of the graph is executed many times. It’s because the compiled graph is cached with a hash derived from the graph shape, input shape, and the output shape. If these shapes change it will trigger compilation, and too frequent compilation will result in training time degradation.</p>

<p>Let’s consider the following example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dummy_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'bs,st-&gt;bt'</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">step_loss</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
  <span class="k">if</span> <span class="n">acc</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">loss</span><span class="p">,</span> <span class="n">step_loss</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">step_loss</span>
  <span class="n">xm</span><span class="p">.</span><span class="n">mark_step</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">loss</span>


<span class="kn">import</span> <span class="nn">time</span>
<span class="k">def</span> <span class="nf">measure_time</span><span class="p">(</span><span class="n">acc</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
  <span class="n">exec_times</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">iter_count</span> <span class="o">=</span> <span class="mi">100</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">512</span><span class="p">,</span> <span class="mi">8</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iter_count</span><span class="p">):</span>
    <span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">dummy_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">acc</span><span class="p">)</span>
    <span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">exec_times</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">exec_times</span>

<span class="n">dyn</span> <span class="o">=</span> <span class="n">measure_time</span><span class="p">(</span><span class="n">acc</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># acc= True Results in dynamic graph
</span><span class="n">st</span> <span class="o">=</span> <span class="n">measure_time</span><span class="p">(</span><span class="n">acc</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># Static graph, computation shape, inputs and output shapes don't change
</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">st</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'static graph'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dyn</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'dynamic graph'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Execution time in seconds'</span><span class="p">)</span>
</code></pre></div></div>

<p align="center">
  <img src="/assets/images/executiontimegraph.jpg" width="60%" />
</p>

<p>Note that static and dynamic cases have the same computation but dynamic graph compiles every time, leading to the higher overall run-time. In practice, the training step with recompilation can sometimes be an order of magnitude or slower. In the next section we discuss some of the PyTorch/XLA tools to debug training degradation.</p>

<h3 id="profiling-training-performance-with-pytorchxla">Profiling Training Performance with PyTorch/XLA</h3>

<p>PyTorch/XLA profiling consists of two major components. First is the client side profiling. This feature is turned on by simply setting the environment variable PT_XLA_DEBUG to 1. Client side profiling points to unlowered ops or device-to-host transfer in your source code. Client side profiling also reports if there are too frequent compilations happening during the training. You can explore some metrics and counters provided by PyTorch/XLA in conjunction with the profiler in <a href="https://github.com/ultrons/xla/blob/lazy-tensor-post/contrib/colab/Exploring_LazyTensor_with_Debug_Metrics.ipynb">this</a> notebook.</p>

<p>The second component offered by PyTorch/XLA profiler is the inline trace annotation. For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch_xla.debug.profiler</span> <span class="k">as</span> <span class="n">xp</span>

<span class="k">def</span> <span class="nf">train_imagenet</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'==&gt; Preparing data..'</span><span class="p">)</span>
  <span class="n">img_dim</span> <span class="o">=</span> <span class="n">get_model_property</span><span class="p">(</span><span class="s">'img_dim'</span><span class="p">)</span>
  <span class="p">....</span>
  <span class="n">server</span> <span class="o">=</span> <span class="n">xp</span><span class="p">.</span><span class="n">start_server</span><span class="p">(</span><span class="mi">3294</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">train_loop_fn</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="p">....</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">xp</span><span class="p">.</span><span class="n">StepTrace</span><span class="p">(</span><span class="s">'Train_Step'</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="n">step</span><span class="p">):</span>
        <span class="p">....</span>
        <span class="k">if</span> <span class="n">FLAGS</span><span class="p">.</span><span class="n">amp</span><span class="p">:</span>
        <span class="p">....</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">xp</span><span class="p">.</span><span class="n">Trace</span><span class="p">(</span><span class="s">'build_graph'</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
          <span class="n">xm</span><span class="p">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
</code></pre></div></div>

<p>Notice the start_server api call. The port number that you have used here is the same port number you will use with the tensorboard profiler in order to view the op trace similar to:</p>

<p align="center">
  <img src="/assets/images/port-numbers.png" width="100%" />
</p>

<p>Op trace along with the client-side debugging function is a powerful set of tools to debug and optimize your training performance with PyTorch/XLA. For more detailed instructions on the profiler usage, the reader is encouraged to explore blogs <a href="https://cloud.google.com/blog/topics/developers-practitioners/pytorchxla-performance-debugging-tpu-vm-part-1">part-1</a>, <a href="https://cloud.google.com/blog/topics/developers-practitioners/pytorchxla-performance-debugging-cloud-tpu-vm-part-ii">part-2</a>, and <a href="https://cloud.google.com/blog/topics/developers-practitioners/pytorchxla-performance-debugging-cloud-tpu-vm-part-iii">part-3</a> of the blog series on PyTorch/XLA performance debugging.</p>

<h3 id="summary">Summary</h3>

<p>In this article we have reviewed the fundamentals of the LazyTensor system. We built on those fundamentals with PyTorch/XLA to understand the potential causes of training performance degradation. We discussed why “compile once and execute often” helps to get the best performance on LazyTensor systems, and why training slows down when this assumption breaks.</p>

<p>We hope that PyTorch users will find these insights helpful for their novel works with LazyTensor systems.</p>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>A big thank you to my outstanding colleagues Jack Cao, Milad Mohammedi, Karl Weinmeister, Rajesh Thallam, Jordan Tottan (Google) and Geeta Chauhan (Meta) for their meticulous reviews and feedback. And thanks to the extended PyTorch/XLA development team from Google, Meta, and the open source community to make PyTorch possible on TPUs. And finally, thanks to the authors of the <a href="https://arxiv.org/pdf/2102.13267.pdf">LazyTensor paper</a> not only for developing LazyTensor but also for writing such an accessible paper.</p>

<h2 id="refrences">Refrences</h2>

<p>[<a href="https://arxiv.org/pdf/2102.13267.pdf">1</a>] LazyTensor: combining eager execution with domain-specific compilers</p>


                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          <li><a href="https://github.com/pytorch/pytorch/security/policy" target="_blank">Security</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank">Mastodon</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
      </ul>
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/ptc/2022">PyTorch Conference - 2022</a>
          </li>
          <li>
            <a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference - 2023</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/torcharrow">torcharrow</a>
          </li>

          <li class="">
            <a href="/data">TorchData</a>
          </li> 

          <li class="">
            <a href="/torchrec">TorchRec</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          
          <li>
            <a href="/#community-module">Community</a>
          </li>
          
          <li class="">
            <a href="/community-stories">Community stories</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

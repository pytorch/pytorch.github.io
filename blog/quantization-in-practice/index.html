<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-52DXT37');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Practical Quantization in PyTorch | PyTorch
    
  </title>
  
  <meta property="og:title" content="Practical Quantization in PyTorch" />
  <meta property="og:description" content="Quantization is a cheap and easy way to make your DNN run faster and with lower memory requirements. PyTorch offers a few different approaches to quantize your model. In this blog post, we’ll lay a (quick) foundation of quantization in deep learning, and then take a look at how each technique looks like in practice. Finally we’ll end with recommendations from the literature for using quantization in your workflows.

" />
  <meta property="og:image" content="https://pytorch.org/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Practical Quantization in PyTorch">
  <meta name="twitter:description" content="Quantization is a cheap and easy way to make your DNN run faster and with lower memory requirements. PyTorch offers a few different approaches to quantize your model. In this blog post, we’ll lay a (quick) foundation of quantization in deep learning, and then take a look at how each technique looks like in practice. Finally we’ll end with recommendations from the literature for using quantization in your workflows.

" />


<meta property="og:type" content="website" />
<meta name="robots" content="index, follow" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-52DXT37"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow" href="/ecosystem">
          Ecosystem
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/pted/2021">
            <span class="dropdown-title">Ecosystem Day - 2021</span>
            <p>See the posters presented at ecosystem day 2021</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/ptdd/2021">
            <span class="dropdown-title">Developer Day - 2021</span>
            <p>See the posters presented at developer day 2021</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="doc-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">February 08, 2022</p>
            <h1>
                <a class="blog-title">Practical Quantization in PyTorch</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Suraj Subramanian, Mark Saroufim, Jerry Zhang
                      
                    </p>
                    <p>Quantization is a cheap and easy way to make your DNN run faster and with lower memory requirements. PyTorch offers a few different approaches to quantize your model. In this blog post, we’ll lay a (quick) foundation of quantization in deep learning, and then take a look at how each technique looks like in practice. Finally we’ll end with recommendations from the literature for using quantization in your workflows.</p>

<p align="center">
  <img src="/assets/images/quantization-practice/hero.gif" width="60%" />
  <br />
  Fig 1. PyTorch &lt;3 Quantization
</p>

<p><strong>Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#fundamentals-of-quantization" id="markdown-toc-fundamentals-of-quantization">Fundamentals of Quantization</a>    <ul>
      <li><a href="#mapping-function" id="markdown-toc-mapping-function">Mapping function</a></li>
      <li><a href="#quantization-parameters" id="markdown-toc-quantization-parameters">Quantization Parameters</a></li>
      <li><a href="#calibration" id="markdown-toc-calibration">Calibration</a></li>
      <li><a href="#affine-and-symmetric-quantization-schemes" id="markdown-toc-affine-and-symmetric-quantization-schemes">Affine and Symmetric Quantization Schemes</a></li>
      <li><a href="#per-tensor-and-per-channel-quantization-schemes" id="markdown-toc-per-tensor-and-per-channel-quantization-schemes">Per-Tensor and Per-Channel Quantization Schemes</a></li>
      <li><a href="#backend-engine" id="markdown-toc-backend-engine">Backend Engine</a></li>
      <li><a href="#qconfig" id="markdown-toc-qconfig">QConfig</a></li>
    </ul>
  </li>
  <li><a href="#in-pytorch" id="markdown-toc-in-pytorch">In PyTorch</a>    <ul>
      <li><a href="#post-training-dynamicweight-only-quantization" id="markdown-toc-post-training-dynamicweight-only-quantization">Post-Training Dynamic/Weight-only Quantization</a></li>
      <li><a href="#post-training-static-quantization-ptq" id="markdown-toc-post-training-static-quantization-ptq">Post-Training Static Quantization (PTQ)</a></li>
      <li><a href="#quantization-aware-training-qat" id="markdown-toc-quantization-aware-training-qat">Quantization-aware Training (QAT)</a></li>
    </ul>
  </li>
  <li><a href="#sensitivity-analysis" id="markdown-toc-sensitivity-analysis">Sensitivity Analysis</a></li>
  <li><a href="#recommendations-for-your-workflow" id="markdown-toc-recommendations-for-your-workflow">Recommendations for your workflow</a>    <ul>
      <li><a href="#points-to-note" id="markdown-toc-points-to-note">Points to note</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>
<h2 id="fundamentals-of-quantization">Fundamentals of Quantization</h2>

<blockquote>
  <p>If someone asks you what time it is, you don’t respond “10:14:34:430705”, but you might say “a quarter past 10”.</p>
</blockquote>

<p>Quantization has roots in information compression; in deep networks it refers to reducing the numerical precision of its weights and/or activations.</p>

<p>Overparameterized DNNs have more degrees of freedom and this makes them good candidates for information compression [<a href="https://arxiv.org/pdf/2103.13630.pdf">1</a>]. When you quantize a model, two things generally happen - the model gets smaller and runs with better efficiency. Hardware vendors explicitly allow for faster processing of 8-bit data (than 32-bit data) resulting in higher throughput. A smaller model has lower memory footprint and power consumption [<a href="https://arxiv.org/pdf/1806.08342.pdf">2</a>], crucial for deployment at the edge.</p>

<h3 id="mapping-function">Mapping function</h3>
<p>The mapping function is what you might guess - a function that maps values from floating-point to integer space. A commonly used mapping function is a linear transformation given by <img src="https://latex.codecogs.com/gif.latex?Q(r) = round(r/S + Z)" />, where <img src="https://latex.codecogs.com/gif.latex?r" /> is the input and <img src="https://latex.codecogs.com/gif.latex?S, Z" /> are <strong>quantization parameters</strong>.</p>

<p>To reconvert to floating point space, the inverse function is given by <img src="https://latex.codecogs.com/gif.latex?\tilde r = (Q(r) - Z) \cdot S" />.</p>

<p><img src="https://latex.codecogs.com/gif.latex?\tilde r \neq r" />, and their difference constitutes the <em>quantization error</em>.</p>

<h3 id="quantization-parameters">Quantization Parameters</h3>
<p>The mapping function is parameterized by the <strong>scaling factor</strong> <img src="https://latex.codecogs.com/gif.latex?S" /> and <strong>zero-point</strong> <img src="https://latex.codecogs.com/gif.latex?Z" />.</p>

<p><img src="https://latex.codecogs.com/gif.latex?S" /> is simply the ratio of the input range to the output range 
<img src="https://latex.codecogs.com/gif.latex?S = \frac{\beta - \alpha}{\beta_q - \alpha_q}" /></p>

<p>where [<img src="https://latex.codecogs.com/gif.latex?\alpha, \beta" />] is the clipping range of the input, i.e. the boundaries of permissible inputs. [<img src="https://latex.codecogs.com/gif.latex?\alpha_q, \beta_q" />] is the range in quantized output space that it is mapped to. For 8-bit quantization, the output range <img src="https://latex.codecogs.com/gif.latex?\beta_q - \alpha_q &lt;= (2^8 - 1)" />.</p>

<p><img src="https://latex.codecogs.com/gif.latex?Z" /> acts as a bias to ensure that a 0 in the input space maps perfectly to a 0 in the quantized space. <img src="https://latex.codecogs.com/gif.latex?Z = -(\frac{\alpha}{S} - \alpha_q)" /></p>

<h3 id="calibration">Calibration</h3>
<p>The process of choosing the input clipping range is known as <strong>calibration</strong>. The simplest technique (also the default in PyTorch) is to record the running mininmum and maximum values and assign them to <img src="https://latex.codecogs.com/gif.latex?\alpha" /> and <img src="https://latex.codecogs.com/gif.latex?\beta" />. <a href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/calib.html">TensorRT</a> also uses entropy minimization (KL divergence), mean-square-error minimization, or percentiles of the input range.</p>

<p>In PyTorch, <code class="language-plaintext highlighter-rouge">Observer</code> modules (<a href="https://PyTorch.org/docs/stable/torch.quantization.html?highlight=observer#observers">docs</a>, <a href="https://github.com/PyTorch/PyTorch/blob/748d9d24940cd17938df963456c90fa1a13f3932/torch/ao/quantization/observer.py#L88">code</a>) collect statistics on the input values and calculate the qparams <img src="https://latex.codecogs.com/gif.latex?S, Z" />. Different calibration schemes result in different quantized outputs, and it’s best to empirically verify which scheme works best for your application and architecture (more on that later).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.quantization.observer</span> <span class="kn">import</span> <span class="n">MinMaxObserver</span><span class="p">,</span> <span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span> <span class="n">HistogramObserver</span>
<span class="n">C</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">normal</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">normal</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">normal</span><span class="p">.</span><span class="n">sample</span><span class="p">((</span><span class="n">C</span><span class="p">,</span> <span class="n">L</span><span class="p">)),</span> <span class="n">normal</span><span class="p">.</span><span class="n">sample</span><span class="p">((</span><span class="n">C</span><span class="p">,</span> <span class="n">L</span><span class="p">))]</span>
<span class="k">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># &gt;&gt;&gt;&gt;&gt;
# [tensor([[-0.0590,  1.1674,  0.7119, -1.1270],
#          [-1.3974,  0.5077, -0.5601,  0.0683],
#          [-0.0929,  0.9473,  0.7159, -0.4574]]]),
</span>
<span class="c1"># tensor([[-0.0236, -0.7599,  1.0290,  0.8914],
#          [-1.1727, -1.2556, -0.2271,  0.9568],
#          [-0.2500,  1.4579,  1.4707,  0.4043]])]
</span>
<span class="n">observers</span> <span class="o">=</span> <span class="p">[</span><span class="n">MinMaxObserver</span><span class="p">(),</span> <span class="n">MovingAverageMinMaxObserver</span><span class="p">(),</span> <span class="n">HistogramObserver</span><span class="p">()]</span>
<span class="k">for</span> <span class="n">obs</span> <span class="ow">in</span> <span class="n">observers</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">obs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
  <span class="k">print</span><span class="p">(</span><span class="n">obs</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span> <span class="n">obs</span><span class="p">.</span><span class="n">calculate_qparams</span><span class="p">())</span>

<span class="c1"># &gt;&gt;&gt;&gt;&gt;
# MinMaxObserver (tensor([0.0112]), tensor([124], dtype=torch.int32))
# MovingAverageMinMaxObserver (tensor([0.0101]), tensor([139], dtype=torch.int32))
# HistogramObserver (tensor([0.0100]), tensor([106], dtype=torch.int32))
</span></code></pre></div></div>

<h3 id="affine-and-symmetric-quantization-schemes">Affine and Symmetric Quantization Schemes</h3>
<p><strong>Affine or asymmetric quantization</strong> schemes assign the input range to the min and max observed values. Affine schemes generally offer tighter clipping ranges and are useful for quantizing non-negative activations (you don’t need the input range to contain negative values if your input tensors are never negative). The range is calculated as 
<img src="https://latex.codecogs.com/gif.latex?\alpha = min(r), \beta = max(r)" />. Affine quantization leads to more computationally expensive inference when used for weight tensors [<a href="https://arxiv.org/abs/2004.09602">3</a>].</p>

<p><strong>Symmetric quantization</strong> schemes center the input range around 0, eliminating the need to calculate a zero-point offset. The range is calculated as 
<img src="https://latex.codecogs.com/gif.latex?-\alpha = \beta = max(|max(r)|,|min(r)|)" />. For skewed signals (like non-negative activations) this can result in bad quantization resolution because the clipping range includes values that never show up in the input (see the pyplot below).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">act</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">pareto</span><span class="p">.</span><span class="n">Pareto</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">).</span><span class="n">sample</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1024</span><span class="p">))</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">normal</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">).</span><span class="n">sample</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)).</span><span class="n">flatten</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">get_symmetric_range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span> <span class="n">x</span><span class="p">.</span><span class="nb">min</span><span class="p">().</span><span class="nb">abs</span><span class="p">())</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">beta</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">beta</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">get_affine_range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nb">min</span><span class="p">().</span><span class="n">item</span><span class="p">(),</span> <span class="n">x</span><span class="p">.</span><span class="nb">max</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">plt</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">scheme</span><span class="p">):</span>
  <span class="n">boundaries</span> <span class="o">=</span> <span class="n">get_affine_range</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">if</span> <span class="n">scheme</span> <span class="o">==</span> <span class="s">'affine'</span> <span class="k">else</span> <span class="n">get_symmetric_range</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">a</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">a</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">boundaries</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s">'purple'</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">ymax</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">act</span><span class="p">,</span> <span class="s">'affine'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Activation, Affine-Quantized"</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">act</span><span class="p">,</span> <span class="s">'symmetric'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Activation, Symmetric-Quantized"</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">weights</span><span class="p">,</span> <span class="s">'affine'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Weights, Affine-Quantized"</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">weights</span><span class="p">,</span> <span class="s">'symmetric'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Weights, Symmetric-Quantized"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p align="center">
  <img src="/assets/images/quantization-practice/affine-symmetric.png" width="100%" />
  <br /> Fig 2. Clipping ranges (in purple) for affine and symmetric schemes
</p>

<p>In PyTorch, you can specify affine or symmetric schemes while initializing the Observer. Note that not all observers support both schemes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">qscheme</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">per_tensor_affine</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">per_tensor_symmetric</span><span class="p">]:</span>
  <span class="n">obs</span> <span class="o">=</span> <span class="n">MovingAverageMinMaxObserver</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">qscheme</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">obs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Qscheme: </span><span class="si">{</span><span class="n">qscheme</span><span class="si">}</span><span class="s"> | </span><span class="si">{</span><span class="n">obs</span><span class="p">.</span><span class="n">calculate_qparams</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># &gt;&gt;&gt;&gt;&gt;
# Qscheme: torch.per_tensor_affine | (tensor([0.0101]), tensor([139], dtype=torch.int32))
# Qscheme: torch.per_tensor_symmetric | (tensor([0.0109]), tensor([128]))
</span></code></pre></div></div>

<h3 id="per-tensor-and-per-channel-quantization-schemes">Per-Tensor and Per-Channel Quantization Schemes</h3>
<p>Quantization parameters can be calculated for the layer’s entire weight tensor as a whole, or separately for each channel. In per-tensor, the same clipping range is applied to all the channels in a layer</p>

<p align="center">
  <img src="/assets/images/quantization-practice/per-channel-tensor.svg" width="60%" />
  <br /> Fig 3. Per-Channel uses one set of qparams for each channel. Per-tensor uses the same qparams for the entire tensor.
</p>

<p>For weights quantization, symmetric-per-channel quantization provides better accuracies; per-tensor quantization performs poorly, possibly due to high variance in conv weights across channels from batchnorm folding [<a href="https://arxiv.org/abs/2004.09602">3</a>].</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.quantization.observer</span> <span class="kn">import</span> <span class="n">MovingAveragePerChannelMinMaxObserver</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">MovingAveragePerChannelMinMaxObserver</span><span class="p">(</span><span class="n">ch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># calculate qparams for all `C` channels separately
</span><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">obs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">obs</span><span class="p">.</span><span class="n">calculate_qparams</span><span class="p">())</span>

<span class="c1"># &gt;&gt;&gt;&gt;&gt;
# (tensor([0.0090, 0.0075, 0.0055]), tensor([125, 187,  82], dtype=torch.int32))
</span></code></pre></div></div>

<h3 id="backend-engine">Backend Engine</h3>
<p>Currently, quantized operators run on x86 machines via the <a href="https://github.com/pytorch/FBGEMM">FBGEMM backend</a>, or use <a href="https://github.com/pytorch/QNNPACK">QNNPACK</a> primitives on ARM machines. Backend support for server GPUs (via TensorRT and cuDNN) is coming soon. Learn more about extending quantization to custom backends: <a href="https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md">RFC-0019</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">backend</span> <span class="o">=</span> <span class="s">'fbgemm'</span> <span class="k">if</span> <span class="n">x86</span> <span class="k">else</span> <span class="s">'qnnpack'</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>  
<span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">quantized</span><span class="p">.</span><span class="n">engine</span> <span class="o">=</span> <span class="n">backend</span>
</code></pre></div></div>

<h3 id="qconfig">QConfig</h3>

<p>The <code class="language-plaintext highlighter-rouge">QConfig</code> (<a href="https://github.com/PyTorch/PyTorch/blob/d6b15bfcbdaff8eb73fa750ee47cef4ccee1cd92/torch/ao/quantization/qconfig.py#L165">code</a>, <a href="https://pytorch.org/docs/stable/torch.quantization.html?highlight=qconfig#torch.quantization.QConfig">docs</a>) NamedTuple stores the Observers and the quantization schemes used to quantize activations and weights.</p>

<p>Be sure to pass the Observer class (not the instance), or a callable that can return Observer instances. Use <code class="language-plaintext highlighter-rouge">with_args()</code> to override the default arguments.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">QConfig</span><span class="p">(</span>
  <span class="n">activation</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="p">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">per_tensor_affine</span><span class="p">),</span>
  <span class="n">weight</span><span class="o">=</span><span class="n">MovingAveragePerChannelMinMaxObserver</span><span class="p">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">qint8</span><span class="p">)</span>
<span class="p">)</span>
<span class="c1"># &gt;&gt;&gt;&gt;&gt;
# QConfig(activation=functools.partial(&lt;class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'&gt;, qscheme=torch.per_tensor_affine){}, weight=functools.partial(&lt;class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'&gt;, qscheme=torch.qint8){})
</span></code></pre></div></div>

<h2 id="in-pytorch">In PyTorch</h2>

<p>PyTorch allows you a few different ways to quantize your model depending on</p>
<ul>
  <li>if you prefer a flexible but manual, or a restricted automagic process (<em>Eager Mode</em> v/s <em>FX Graph Mode</em>)</li>
  <li>if qparams for quantizing activations (layer outputs) are precomputed for all inputs, or calculated afresh with each input (<em>static</em> v/s <em>dynamic</em>),</li>
  <li>if qparams are computed with or without retraining (<em>quantization-aware training</em> v/s <em>post-training quantization</em>)</li>
</ul>

<p>FX Graph Mode automatically fuses eligible modules, inserts Quant/DeQuant stubs, calibrates the model and returns a quantized module - all in two method calls - but only for networks that are <a href="https://PyTorch.org/docs/stable/fx.html#torch.fx.symbolic_trace">symbolic traceable</a>. The examples below contain the calls using Eager Mode and FX Graph Mode for comparison.</p>

<p>In DNNs, eligible candidates for quantization are the FP32 weights (layer parameters) and activations (layer outputs). Quantizing weights reduces the model size. Quantized activations typically result in faster inference.</p>

<p>As an example, the 50-layer ResNet network has ~26 million weight parameters and computes ~16 million activations in the forward pass.</p>

<h3 id="post-training-dynamicweight-only-quantization">Post-Training Dynamic/Weight-only Quantization</h3>
<p>Here the model’s weights are pre-quantized; the activations are quantized on-the-fly (“dynamic”) during inference. The simplest of all approaches, it has a one line API call in <code class="language-plaintext highlighter-rouge">torch.quantization.quantize_dynamic</code>. Currently only Linear and Recurrent (<code class="language-plaintext highlighter-rouge">LSTM</code>, <code class="language-plaintext highlighter-rouge">GRU</code>, <code class="language-plaintext highlighter-rouge">RNN</code>) layers are supported for dynamic quantization.</p>

<p><strong>(+)</strong> Can result in higher accuracies since the clipping range is exactly calibrated for each input [<a href="https://arxiv.org/pdf/2103.13630.pdf">1</a>].</p>

<p><strong>(+)</strong> Dynamic quantization is preferred for models like LSTMs and Transformers where writing/retrieving the model’s weights from memory dominate bandwidths [<a href="https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization">4</a>].</p>

<p><strong>(-)</strong> Calibrating and quantizing the activations at each layer during runtime can add to the compute overhead.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># toy model
</span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
  <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">8</span><span class="p">,)),</span>
  <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
  <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span>
  <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">m</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1">## EAGER MODE
</span><span class="kn">from</span> <span class="nn">torch.quantization</span> <span class="kn">import</span> <span class="n">quantize_dynamic</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">qconfig_spec</span><span class="o">=</span><span class="p">{</span><span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">qint8</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="c1">## FX MODE
</span><span class="kn">from</span> <span class="nn">torch.quantization</span> <span class="kn">import</span> <span class="n">quantize_fx</span>
<span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">""</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">default_dynamic_qconfig</span><span class="p">}</span>  <span class="c1"># An empty key denotes the default applied to all modules
</span><span class="n">model_prepared</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="p">.</span><span class="n">prepare_fx</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="p">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="post-training-static-quantization-ptq">Post-Training Static Quantization (PTQ)</h3>
<p>PTQ also pre-quantizes model weights but instead of calibrating activations on-the-fly, the clipping range is pre-calibrated and fixed (“static”) using validation data. Activations stay in quantized precision between operations during inference. About 100 mini-batches of representative data are sufficient to calibrate the observers [<a href="https://arxiv.org/pdf/1806.08342.pdf">2</a>]. The examples below use random data in calibration for convenience - using that in your application will result in bad qparams.</p>

<p align="center">
  <img src="/assets/images/quantization-practice/ptq-flowchart.svg" alt="PTQ flowchart" width="60%" />
  <br />
  Fig 4. Steps in Post-Training Static Quantization
</p>

<p><a href="https://pytorch.org/tutorials/recipes/fuse.html">Module fusion</a> combines multiple sequential modules (eg: <code class="language-plaintext highlighter-rouge">[Conv2d, BatchNorm, ReLU]</code>) into one. Fusing modules means the compiler needs to only run one kernel instead of many; this speeds things up and improves accuracy by reducing quantization error.</p>

<p><strong>(+)</strong> Static quantization has faster inference than dynamic quantization because it eliminates the float&lt;-&gt;int conversion costs between layers.</p>

<p><strong>(-)</strong> Static quantized models may need regular re-calibration to stay robust against distribution-drift.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Static quantization of a model consists of the following steps:
</span>
<span class="c1">#     Fuse modules
#     Insert Quant/DeQuant Stubs
#     Prepare the fused module (insert observers before and after layers)
#     Calibrate the prepared module (pass it representative data)
#     Convert the calibrated module (replace with quantized version)
</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">backend</span> <span class="o">=</span> <span class="s">"fbgemm"</span>  <span class="c1"># running on a x86 CPU. Use "qnnpack" if running on ARM.
</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
     <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span>
     <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
     <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
     <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1">## EAGER MODE
</span><span class="s">"""Fuse
- Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules
"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[</span><span class="s">'0'</span><span class="p">,</span><span class="s">'1'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># fuse first Conv-ReLU pair
</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[</span><span class="s">'2'</span><span class="p">,</span><span class="s">'3'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># fuse second Conv-ReLU pair
</span>
<span class="s">"""Insert stubs"""</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">QuantStub</span><span class="p">(),</span> 
                  <span class="o">*</span><span class="n">m</span><span class="p">,</span> 
                  <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">DeQuantStub</span><span class="p">())</span>

<span class="s">"""Prepare"""</span>
<span class="n">m</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="s">"""Calibrate
- This example uses random data for convenience. Use representative (validation) data instead.
"""</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">inference_mode</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="s">"""Convert"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="s">"""Check"""</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">[[</span><span class="mi">1</span><span class="p">]].</span><span class="n">weight</span><span class="p">().</span><span class="n">element_size</span><span class="p">())</span> <span class="c1"># 1 byte instead of 4 bytes for FP32
</span>

<span class="c1">## FX GRAPH
</span><span class="kn">from</span> <span class="nn">torch.quantization</span> <span class="kn">import</span> <span class="n">quantize_fx</span>
<span class="n">m</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">""</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)}</span>
<span class="c1"># Prepare
</span><span class="n">model_prepared</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="p">.</span><span class="n">prepare_fx</span><span class="p">(</span><span class="n">model_to_quantize</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
<span class="c1"># Calibrate - Use representative (validation) data.
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">inference_mode</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">model_prepared</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># quantize
</span><span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="p">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="quantization-aware-training-qat">Quantization-aware Training (QAT)</h3>
<p align="center">
  <img src="/assets/images/quantization-practice/qat-flowchart.svg" alt="QAT flowchart" width="60%" />
  <br />
  Fig 5. Steps in Quantization-Aware Training
</p>

<p>The PTQ approach is great for large models, but accuracy suffers in smaller models [[6]]. This is of course due to the loss in numerical precision when adapting a model from FP32 to the INT8 realm <em>(Figure 6(a))</em>. QAT tackles this by including this quantization error in the training loss, thereby training an INT8-first model.</p>

<p align="center">
  <img src="/assets/images/quantization-practice/ptq_vs_qat.png" alt="Fig. 6: Comparison of PTQ and QAT" width="100%" />
  <br />
  Fig 6. Comparison of PTQ and QAT convergence [3]
</p>

<p>All weights and biases are stored in FP32, and backpropagation happens as usual. However in the forward pass, quantization is internally simulated via <code class="language-plaintext highlighter-rouge">FakeQuantize</code> modules. They are called fake because they quantize and immediately dequantize the data, adding quantization noise similar to what might be encountered during quantized inference. The final loss thus accounts for any expected quantization errors. Optimizing on this allows the model to identify a wider region in the loss function <em>(Figure 6(b))</em>, and identify FP32 parameters such that quantizing them to INT8 does not significantly affect accuracy.</p>

<p align="center">
  <img src="/assets/images/quantization-practice/qat-fake-quantization.png" alt="Fake Quantization in the forward and backward pass" width="100%" />
  <br /> Fig 7. Fake Quantization in the forward and backward pass 
  <br /> Image source: https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt
</p>

<p><strong>(+)</strong> QAT yields higher accuracies than PTQ.</p>

<p><strong>(+)</strong> Qparams can be learned during model training for more fine-grained accuracy (see <a href="https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/_learnable_fake_quantize.py">LearnableFakeQuantize</a>)</p>

<p><strong>(-)</strong> Computational cost of retraining a model in QAT can be several hundred epochs [<a href="https://arxiv.org/pdf/2103.13630.pdf">1</a>]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># QAT follows the same steps as PTQ, with the exception of the training loop before you actually convert the model to its quantized version
</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">backend</span> <span class="o">=</span> <span class="s">"fbgemm"</span>  <span class="c1"># running on a x86 CPU. Use "qnnpack" if running on ARM.
</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
     <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span>
     <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
     <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
     <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="p">)</span>

<span class="s">"""Fuse"""</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[</span><span class="s">'0'</span><span class="p">,</span><span class="s">'1'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># fuse first Conv-ReLU pair
</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[</span><span class="s">'2'</span><span class="p">,</span><span class="s">'3'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># fuse second Conv-ReLU pair
</span>
<span class="s">"""Insert stubs"""</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">QuantStub</span><span class="p">(),</span> 
                  <span class="o">*</span><span class="n">m</span><span class="p">,</span> 
                  <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">DeQuantStub</span><span class="p">())</span>

<span class="s">"""Prepare"""</span>
<span class="n">m</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">m</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">prepare_qat</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="s">"""Training Loop"""</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">out</span><span class="p">,</span> <span class="n">tgt</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">tgt</span><span class="o">-</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">24</span><span class="p">)</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
  <span class="n">opt</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="n">opt</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="s">"""Convert"""</span>
<span class="n">m</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="sensitivity-analysis">Sensitivity Analysis</h2>
<p>Not all layers respond to quantization equally, some are more sensitive to precision drops than others. Identifying the optimal combination of layers that minimizes accuracy drop is time-consuming, so [<a href="https://arxiv.org/abs/2004.09602">3</a>] suggest a one-at-a-time sensitivity analysis to identify which layers are most sensitive, and retaining FP32 precision on those. In their experiments, skipping just 2 conv layers (out of a total 28 in MobileNet v1) give them near-FP32 accuracy. Using FX Graph Mode, we can create custom qconfigs to do this easily:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ONE-AT-A-TIME SENSITIVITY ANALYSIS 
</span>
<span class="k">for</span> <span class="n">quantized_layer</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">named_modules</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Only quantizing layer: "</span><span class="p">,</span> <span class="n">quantized_layer</span><span class="p">)</span>

  <span class="c1"># The module_name key allows module-specific qconfigs. 
</span>  <span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">""</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span> 
  <span class="s">"module_name"</span><span class="p">:[(</span><span class="n">quantized_layer</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">))]}</span>

  <span class="n">model_prepared</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="p">.</span><span class="n">prepare_fx</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
  <span class="c1"># calibrate
</span>  <span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="p">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>
  <span class="c1"># evaluate(model)
</span></code></pre></div></div>

<p>Another approach is to compare statistics of the FP32 and INT8 layers; commonly used metrics for these are SQNR (Signal to Quantized Noise Ratio) and Mean-Squre-Error. Such a comparative analysis may also help in guiding further optimizations.</p>

<p align="center">
  <img src="/assets/images/quantization-practice/compare_output_ns.png" alt="Fig 8. Comparing model weights and activations" width="100%" align="center" />
  <br />
  Fig 8. Comparing model weights and activations
</p>

<p>PyTorch provides tools to help with this analysis under the Numeric Suite. Learn more about using Numeric Suite from the <a href="https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html">full tutorial</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># extract from https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html
</span><span class="kn">import</span> <span class="nn">torch.quantization._numeric_suite</span> <span class="k">as</span> <span class="n">ns</span>

<span class="k">def</span> <span class="nf">SQNR</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Higher is better
</span>    <span class="n">Ps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">Pn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">20</span><span class="o">*</span><span class="n">torch</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">Ps</span><span class="o">/</span><span class="n">Pn</span><span class="p">)</span>

<span class="n">wt_compare_dict</span> <span class="o">=</span> <span class="n">ns</span><span class="p">.</span><span class="n">compare_weights</span><span class="p">(</span><span class="n">fp32_model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">int8_model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">())</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">wt_compare_dict</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">compute_error</span><span class="p">(</span><span class="n">wt_compare_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s">'float'</span><span class="p">],</span> <span class="n">wt_compare_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s">'quantized'</span><span class="p">].</span><span class="n">dequantize</span><span class="p">()))</span>

<span class="n">act_compare_dict</span> <span class="o">=</span> <span class="n">ns</span><span class="p">.</span><span class="n">compare_model_outputs</span><span class="p">(</span><span class="n">fp32_model</span><span class="p">,</span> <span class="n">int8_model</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">act_compare_dict</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">compute_error</span><span class="p">(</span><span class="n">act_compare_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s">'float'</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">act_compare_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s">'quantized'</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">dequantize</span><span class="p">()))</span>

</code></pre></div></div>

<h2 id="recommendations-for-your-workflow">Recommendations for your workflow</h2>
<p align="center">
  <img src="/assets/images/quantization-practice/quantization-flowchart2.png" alt="Suggested quantization workflow" width="100%" align="center" />
  <br />
  Fig 9. Suggested quantization workflow
</p>
<p><a href="/assets/images/quantization-practice/quantization-flowchart2.png" target="_blank" align="center"> Click for larger image </a></p>

<h3 id="points-to-note">Points to note</h3>
<ul>
  <li>Large (10M+ parameters) models are more robust to quantization error. [<a href="https://arxiv.org/pdf/1806.08342.pdf">2</a>]</li>
  <li>Quantizing a model from a FP32 checkpoint provides better accuracy than training an INT8 model from scratch.[<a href="https://arxiv.org/pdf/1806.08342.pdf">2</a>]</li>
  <li>Profiling the model runtime is optional but it can help identify layers that bottleneck inference.</li>
  <li>Dynamic Quantization is an easy first step, especially if your model has many Linear or Recurrent layers.</li>
  <li>Use symmetric-per-channel quantization with <code class="language-plaintext highlighter-rouge">MinMax</code> observers for quantizing weights. Use affine-per-tensor quantization with <code class="language-plaintext highlighter-rouge">MovingAverageMinMax</code> observers for quantizing activations[<a href="https://arxiv.org/pdf/1806.08342.pdf">2</a>, <a href="https://arxiv.org/abs/2004.09602">3</a>]</li>
  <li>Use metrics like SQNR to identify which layers are most suscpetible to quantization error. Turn off quantization on these layers.</li>
  <li>Use QAT to fine-tune for around 10% of the original training schedule with an annealing learning rate schedule starting at 1% of the initial training learning rate. [<a href="https://arxiv.org/abs/2004.09602">3</a>]</li>
  <li>If the above workflow didn’t work for you, we want to know more. Post a thread with details of your code (model architecture, accuracy metric, techniques tried). Feel free to cc me <a href="https://discuss.pytorch.org/u/suraj.pt/">@suraj.pt</a>.</li>
</ul>

<p>That was a lot to digest, congratulations for sticking with it! Next, we’ll take a look at quantizing a “real-world” model that uses dynamic control structures (if-else, loops). These elements disallow symbolic tracing a model, which makes it a bit tricky to directly quantize the model out of the box. In the next post of this series, we’ll get our hands dirty on a model that is chock full of loops and if-else blocks, and even uses third-party libraries in the <code class="language-plaintext highlighter-rouge">forward</code> call.</p>

<p>We’ll also cover a cool new feature in PyTorch Quantization called Define-by-Run, that tries to ease this constraint by needing only subsets of the model’s computational graph to be free of dynamic flow. Check out the <a href="https://s3.amazonaws.com/assets.pytorch.org/ptdd2021/posters/C8.png">Define-by-Run poster at PTDD’21</a> for a preview.</p>

<h2 id="references">References</h2>
<p>[<a href="https://arxiv.org/pdf/2103.13630.pdf">1</a>] Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., &amp; Keutzer, K. (2021). A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630.</p>

<p>[<a href="https://arxiv.org/pdf/1806.08342.pdf">2</a>] Krishnamoorthi, R. (2018). Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342.</p>

<p>[<a href="https://arxiv.org/abs/2004.09602">3</a>] Wu, H., Judd, P., Zhang, X., Isaev, M., &amp; Micikevicius, P. (2020). Integer quantization for deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602.</p>

<p>[<a href="https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization">4</a>] PyTorch Quantization Docs</p>


                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-tos-privacy-policy.pdf" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-oss-privacy-policy.pdf" target="_blank">Privacy</a></li>
      </ul>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/ecosystem">Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/pted/2021">Ecosystem Day 2021</a>
          </li>
          <li>
            <a href="/ecosystem/ptdd/2021">Developer Day 2021</a>
          </li>
        </ul>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/docs/stable/torchvision">torchvision</a>
          </li>

          <li class="">
            <a href="/elastic">TorchElastic</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/#community-module">Community</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

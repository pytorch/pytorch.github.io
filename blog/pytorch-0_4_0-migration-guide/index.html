<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      PyTorch 0.4.0 Migration Guide | PyTorch
    
  </title>
  <meta property="og:title" content="PyTorch"/>
<meta property="og:description" content="An open source deep learning platform that provides a seamless path from research prototyping to production deployment."/>
<meta property="og:url" content="https://www.pytorch.org"/>
<meta property="og:type" content="website"/>
<meta property="og:image" content="https://pytorch.org/assets/images/pytorch-logo.png"/>

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
  </script>
  <noscript>
    <img height="1" width="1"
    src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
    &noscript=1"/>
  </noscript>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>

  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');

</script>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
</head>


<body class="blog">
    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item ">
      <a href="/features">Features</a>
    </li>

    <li class="main-menu-item ">
      <a href="/ecosystem">Ecosystem</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <a href="/docs">Docs</a>
    </li>

    <li class="main-menu-item ">
      <a href="/resources">Resources</a>
    </li>

    <li class="main-menu-item">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">April 22, 2018</p>
            <h1>
                <a class="blog-title">PyTorch 0.4.0 Migration Guide</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        The PyTorch Team
                      
                    </p>
                    <p>Welcome to the migration guide for PyTorch 0.4.0. In this release we introduced <a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0">many exciting new features and critical bug fixes</a>, with the goal of providing users a better and cleaner interface. In this guide, we will cover the most important changes in migrating existing code from previous versions:</p>

<ul>
  <li><code class="highlighter-rouge">Tensors</code> and <code class="highlighter-rouge">Variables</code> have merged</li>
  <li>Support for 0-dimensional (scalar) <code class="highlighter-rouge">Tensors</code></li>
  <li>Deprecation of the <code class="highlighter-rouge">volatile</code> flag</li>
  <li><code class="highlighter-rouge">dtypes</code>, <code class="highlighter-rouge">devices</code>, and Numpy-style <code class="highlighter-rouge">Tensor</code> creation functions</li>
  <li>Writing device-agnostic code</li>
  <li>New edge-case constraints on names of submodules, parameters, and buffers in <code class="highlighter-rouge">nn.Module</code></li>
</ul>

<h2 id="merging-tensor-and-variable-and-classes">Merging <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">Tensor</code></a> and <code class="highlighter-rouge">Variable</code> and classes</h2>

<p><a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">torch.Tensor</code></a> and <code class="highlighter-rouge">torch.autograd.Variable</code> are now the same class. More precisely, <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">torch.Tensor</code></a> is capable of tracking history and behaves like the old <code class="highlighter-rouge">Variable</code>; <code class="highlighter-rouge">Variable</code> wrapping continues to work as before but returns an object of type <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">torch.Tensor</code></a>. This means that you don’t need the <code class="highlighter-rouge">Variable</code> wrapper everywhere in your code anymore.</p>

<h3 id="the-type-of-a-tensor-has-changed">The <code class="highlighter-rouge">type()</code> of a <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">Tensor</code></a> has changed</h3>

<p>Note also that the <code class="highlighter-rouge">type()</code> of a Tensor no longer reflects the data type. Use <code class="highlighter-rouge">isinstance()</code> or <code class="highlighter-rouge">x.type()</code>instead:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># was torch.DoubleTensor</span>
<span class="s">"&lt;class 'torch.Tensor'&gt;"</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="nb">type</span><span class="p">())</span>  <span class="c"># OK: 'torch.DoubleTensor'</span>
<span class="s">'torch.DoubleTensor'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">))</span>  <span class="c"># OK: True</span>
<span class="bp">True</span>
</code></pre></div></div>

<h3 id="when-does-autograd-start-tracking-history-now">When does <a href="https://pytorch.org/docs/0.4.0/autograd.html"><code class="highlighter-rouge">autograd</code></a> start tracking history now?</h3>

<p><code class="highlighter-rouge">requires_grad</code>, the central flag for <a href="https://pytorch.org/docs/0.4.0/autograd.html"><code class="highlighter-rouge">autograd</code></a>, is now an attribute on <code class="highlighter-rouge">Tensors</code>. The same rules previously used for <code class="highlighter-rouge">Variables</code> applies to <code class="highlighter-rouge">Tensors</code>; <a href="https://pytorch.org/docs/0.4.0/autograd.html"><code class="highlighter-rouge">autograd</code></a> starts tracking history when any input <code class="highlighter-rouge">Tensor</code> of an operation has <code class="highlighter-rouge">requires_grad=True</code>. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c"># create a tensor with requires_grad=False (default)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c"># another tensor with requires_grad=False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># both inputs have requires_grad=False. so does the output</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># then autograd won't track this computation. let's verify!</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">element</span> <span class="mi">0</span> <span class="n">of</span> <span class="n">tensors</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">require</span> <span class="n">grad</span> <span class="ow">and</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">have</span> <span class="n">a</span> <span class="n">grad_fn</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># now create a tensor with requires_grad=True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># add to the previous result that has require_grad=False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">total</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">z</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># the total sum now requires grad!</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">total</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># autograd can compute the gradients as well</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">total</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># and no computation is wasted to compute gradients for x, y and z, which don't require grad</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="bp">None</span>
<span class="bp">True</span>
</code></pre></div></div>

<h4 id="manipulating-requires_grad-flag">Manipulating <code class="highlighter-rouge">requires_grad</code> flag</h4>

<p>Other than directly setting the attribute, you can change this flag <code class="highlighter-rouge">in-place</code> using <a href="https://pytorch.org/docs/0.4.0/tensors.html#torch.Tensor.requires_grad_"><code class="highlighter-rouge">my_tensor.requires_grad_()</code></a>, or, as in the above example, at creation time by passing it in as an argument (default is <code class="highlighter-rouge">False</code>), e.g.,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">existing_tensor</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">existing_tensor</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">my_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">my_tensor</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
</code></pre></div></div>

<h3 id="what-about-data">What about <code class="highlighter-rouge">.data?</code></h3>

<p><code class="highlighter-rouge">.data</code> was the primary way to get the underlying <code class="highlighter-rouge">Tensor</code> from a <code class="highlighter-rouge">Variable</code>. After this merge, calling <code class="highlighter-rouge">y = x.data</code> still has similar semantics. So <code class="highlighter-rouge">y</code> will be a <code class="highlighter-rouge">Tensor</code> that shares the same data with <code class="highlighter-rouge">x</code>, is unrelated with the computation history of <code class="highlighter-rouge">x</code>, and has <code class="highlighter-rouge">requires_grad=False</code>.</p>

<p>However, <code class="highlighter-rouge">.data</code> can be unsafe in some cases. Any changes on <code class="highlighter-rouge">x.data</code> wouldn’t be tracked by <code class="highlighter-rouge">autograd</code>, and the computed gradients would be incorrect if <code class="highlighter-rouge">x</code> is needed in a backward pass. A safer alternative is to use <a href="https://pytorch.org/docs/master/autograd.html#torch.Tensor.detach"><code class="highlighter-rouge">x.detach()</code></a>, which also returns a <code class="highlighter-rouge">Tensor</code> that shares data with <code class="highlighter-rouge">requires_grad=False</code>, but will have its in-place changes reported by <code class="highlighter-rouge">autograd</code> if <code class="highlighter-rouge">x</code> is needed in backward.</p>

<p>Here is an example of the difference between <code class="highlighter-rouge">.data</code> and <code class="highlighter-rouge">x.detach()</code> (and why we recommend using <code class="highlighter-rouge">detach</code> in general).</p>

<p>If you use <code class="highlighter-rouge">Tensor.detach()</code>, the gradient computation is guaranteed to be correct.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span>  <span class="c"># modified by c.zero_() !!</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c"># Requires the original value of out, but that was overwritten by c.zero_()</span>
<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">variables</span> <span class="n">needed</span> <span class="k">for</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">has</span> <span class="n">been</span> <span class="n">modified</span> <span class="n">by</span> <span class="n">an</span>
</code></pre></div></div>

<p>However, using <code class="highlighter-rouge">Tensor.data</code> can be unsafe and can easily result in incorrect gradients when a tensor is required for gradient computation but modified in-place.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">data</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span>  <span class="c"># out  was modified by c.zero_()</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>  <span class="c"># The result is very, very wrong because `out` changed!</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="support-for-0-dimensional-scalar-tensors">Support for 0-dimensional (scalar) Tensors</h2>

<p>Previously, indexing into a <code class="highlighter-rouge">Tensor</code> vector (1-dimensional tensor) gave a Python number but indexing into a <code class="highlighter-rouge">Variable</code> vector gave (inconsistently!) a vector of size <code class="highlighter-rouge">(1,)</code>! Similar behavior existed with reduction functions, e.g. <code class="highlighter-rouge">tensor.sum()</code> would return a Python number, but <code class="highlighter-rouge">variable.sum()</code> would return a vector of size <code class="highlighter-rouge">(1,)</code>.</p>

<p>Fortunately, this release introduces proper scalar (0-dimensional tensor) support in PyTorch! Scalars can be created using the new <code class="highlighter-rouge">torch.tensor</code> function (which will be explained in more detail later; for now just think of it as the PyTorch equivalent of <code class="highlighter-rouge">numpy.array</code>). Now you can do things like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">)</span>         <span class="c"># create a scalar directly</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c"># scalar is 0-dimensional</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>     <span class="c"># compare to a vector of size 1</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>  <span class="c"># this is a vector</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>                    <span class="c"># indexing into a vector gives a scalar</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>             <span class="c"># .item() gives the value as a Python number</span>
<span class="mf">5.0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([])</span>
</code></pre></div></div>

<h3 id="accumulating-losses">Accumulating losses</h3>

<p>Consider the widely used pattern <code class="highlighter-rouge">total_loss += loss.data[0]</code>. Before 0.4.0. <code class="highlighter-rouge">loss</code> was a <code class="highlighter-rouge">Variable</code> wrapping a tensor of size <code class="highlighter-rouge">(1,)</code>, but in 0.4.0 <code class="highlighter-rouge">loss</code> is now a scalar and has <code class="highlighter-rouge">0</code> dimensions. Indexing into a scalar doesn’t make sense (it gives a warning now, but will be a hard error in 0.5.0). Use <code class="highlighter-rouge">loss.item()</code> to get the Python number from a scalar.</p>

<p>Note that if you don’t convert to a Python number when accumulating losses, you may find increased memory usage in your program. This is because the right-hand-side of the above expression used to be a Python float, while it is now a zero-dim Tensor. The total loss is thus accumulating Tensors and their gradient history, which may keep around large autograd graphs for much longer than necessary.</p>

<h2 id="deprecation-of-volatile-flag">Deprecation of volatile flag</h2>

<p>The <code class="highlighter-rouge">volatile</code> flag is now deprecated and has no effect. Previously, any computation that involves a <code class="highlighter-rouge">Variable</code> with <code class="highlighter-rouge">volatile=True</code> wouldn’t be tracked by <code class="highlighter-rouge">autograd</code>. This has now been replaced by a <a href="https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation">set of more flexible context managers</a> including <code class="highlighter-rouge">torch.no_grad()</code>, <code class="highlighter-rouge">torch.set_grad_enabled(grad_mode)</code>, and others.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="o">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">is_train</span> <span class="o">=</span> <span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
<span class="o">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># this can also be used as a function</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
</code></pre></div></div>

<h2 id="dtypes-devices-and-numpy-style-creation-functions"><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="highlighter-rouge">dtypes</code></a>, <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="highlighter-rouge">devices</code></a> and NumPy-style creation functions</h2>

<p>In previous versions of PyTorch, we used to specify data type (e.g. float vs double), device type (cpu vs cuda) and layout (dense vs sparse) together as a “tensor type”. For example, <code class="highlighter-rouge">torch.cuda.sparse.DoubleTensor</code> was the <code class="highlighter-rouge">Tensor</code> type representing the <code class="highlighter-rouge">double</code> data type, living on CUDA devices, and with <a href="https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)">COO sparse tensor</a> layout.</p>

<p>In this release, we introduce <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="highlighter-rouge">torch.dtype</code></a>, <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="highlighter-rouge">torch.device</code></a> and <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="highlighter-rouge">torch.layout</code></a> classes to allow better management of these properties via NumPy-style creation functions.</p>

<h3 id="torchdtype"><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="highlighter-rouge">torch.dtype</code></a></h3>

<p>Below is a complete list of available <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="highlighter-rouge">torch.dtype</code></a>s (data types) and their corresponding tensor types.</p>

<table>
  <thead>
    <tr>
      <th>Data</th>
      <th><code class="highlighter-rouge">type torch.dtype</code></th>
      <th>Tensor types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>32-bit floating point</td>
      <td><code class="highlighter-rouge">torch.float32</code> or <code class="highlighter-rouge">torch.float</code></td>
      <td><code class="highlighter-rouge">torch.*.FloatTensor</code></td>
    </tr>
    <tr>
      <td>64-bit floating point</td>
      <td><code class="highlighter-rouge">torch.float64</code> or <code class="highlighter-rouge">torch.double</code></td>
      <td><code class="highlighter-rouge">torch.*.DoubleTensor</code></td>
    </tr>
    <tr>
      <td>16-bit floating point</td>
      <td><code class="highlighter-rouge">torch.float16</code> or <code class="highlighter-rouge">torch.half</code></td>
      <td><code class="highlighter-rouge">torch.*.HalfTensor</code></td>
    </tr>
    <tr>
      <td>8-bit integer (unsigned)</td>
      <td><code class="highlighter-rouge">torch.uint8</code></td>
      <td><code class="highlighter-rouge">torch.*.ByteTensor</code></td>
    </tr>
    <tr>
      <td>8-bit integer (signed)</td>
      <td><code class="highlighter-rouge">torch.int8</code></td>
      <td><code class="highlighter-rouge">torch.*.CharTensor</code></td>
    </tr>
    <tr>
      <td>16-bit integer (signed)</td>
      <td><code class="highlighter-rouge">torch.int16</code> or <code class="highlighter-rouge">torch.short</code></td>
      <td><code class="highlighter-rouge">torch.*.ShortTensor</code></td>
    </tr>
    <tr>
      <td>32-bit integer (signed)</td>
      <td><code class="highlighter-rouge">torch.int32</code> or <code class="highlighter-rouge">torch.int</code></td>
      <td><code class="highlighter-rouge">torch.*.IntTensor</code></td>
    </tr>
    <tr>
      <td>64-bit integer (signed)</td>
      <td><code class="highlighter-rouge">torch.int64</code> or <code class="highlighter-rouge">torch.long</code></td>
      <td><code class="highlighter-rouge">torch.*.LongTensor</code></td>
    </tr>
  </tbody>
</table>

<p>The dtype of a tensor can be access via its <code class="highlighter-rouge">dtype</code> attribute.</p>

<h3 id="torchdevice"><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="highlighter-rouge">torch.device</code></a></h3>

<p>A <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="highlighter-rouge">torch.device</code></a> contains a device type (<code class="highlighter-rouge">'cpu'</code> or <code class="highlighter-rouge">'cuda'</code>) and optional device ordinal (id) for the device type. It can be initialized with <code class="highlighter-rouge">torch.device('{device_type}')</code> or <code class="highlighter-rouge">torch.device('{device_type}:{device_ordinal}')</code>.</p>

<p>If the device ordinal is not present, this represents the current device for the device type; e.g., <code class="highlighter-rouge">torch.device('cuda')</code> is equivalent to <code class="highlighter-rouge">torch.device('cuda:X')</code> where <code class="highlighter-rouge">X</code> is the result of <code class="highlighter-rouge">torch.cuda.current_device()</code>.</p>

<p>The device of a tensor can be accessed via its <code class="highlighter-rouge">device</code> attribute.</p>

<h3 id="torchlayout"><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="highlighter-rouge">torch.layout</code></a></h3>

<p><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="highlighter-rouge">torch.layout</code></a> represents the data layout of a <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">Tensor</code></a>. Currently <code class="highlighter-rouge">torch.strided</code> (dense tensors, the default) and <code class="highlighter-rouge">torch.sparse_coo</code> (sparse tensors with COO format) are supported.</p>

<p>The layout of a tensor can be access via its <code class="highlighter-rouge">layout</code> attribute.</p>

<h3 id="creating-tensors">Creating Tensors</h3>

<p><a href="https://pytorch.org/docs/0.4.0/torch.html#creation-ops">Methods that create a</a> <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">Tensor</code></a> now also take in <code class="highlighter-rouge">dtype</code>, <code class="highlighter-rouge">device</code>, <code class="highlighter-rouge">layout</code>, and <code class="highlighter-rouge">requires_grad</code> options to specify the desired attributes on the returned <code class="highlighter-rouge">Tensor</code>. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:1"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.6344</span><span class="p">,</span>  <span class="mf">0.8562</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2758</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.8414</span><span class="p">,</span>  <span class="mf">1.7962</span><span class="p">,</span>  <span class="mf">1.0589</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1369</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0462</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4373</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda:1'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>  <span class="c"># default is False</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
</code></pre></div></div>

<h5 id="torchtensordata-"><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="highlighter-rouge">torch.tensor(data, ...)</code></a></h5>

<p><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="highlighter-rouge">torch.tensor</code></a> is one of the newly added <a href="https://pytorch.org/docs/0.4.0/torch.html#creation-ops">tensor creation methods</a>. It takes in array-like data of all kinds and copies the contained values into a new <code class="highlighter-rouge">Tensor</code>. As mentioned earlier, <a href="https://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="highlighter-rouge">torch.tensor</code></a> is the PyTorch equivalent of NumPy’s <code class="highlighter-rouge">numpy.array</code>constructor. Unlike the <code class="highlighter-rouge">torch.*Tensor</code> methods, you can also create zero-dimensional <code class="highlighter-rouge">Tensor</code>s (aka scalars) this way (a single python number is treated as a Size in the <code class="highlighter-rouge">torch.*Tensor methods</code>). Moreover, if a <code class="highlighter-rouge">dtype</code> argument isn’t given, it will infer the suitable <code class="highlighter-rouge">dtype</code> given the data. It is the recommended way to create a tensor from existing data like a Python list. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>               <span class="c"># scalar</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span>  <span class="c"># type inferece</span>
<span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span>    <span class="c"># type inferece</span>
<span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
</code></pre></div></div>

<p>We’ve also added more tensor creation methods. Some of them have <code class="highlighter-rouge">torch.*_like</code> and/or <code class="highlighter-rouge">tensor.new_*</code> variants.</p>

<ul>
  <li>
    <p><code class="highlighter-rouge">torch.*_like</code> takes in an input <code class="highlighter-rouge">Tensor</code> instead of a shape. It returns a <code class="highlighter-rouge">Tensor</code> with same attributes as the input <code class="highlighter-rouge">Tensor</code> by default unless otherwise specified:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">tensor.new_*</code> can also create <code class="highlighter-rouge">Tensors</code> with same attributes as <code class="highlighter-rouge">tensor</code>, but it always takes in a shape argument:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>To specify the desired shape, you can either use a tuple (e.g., <code class="highlighter-rouge">torch.zeros((2, 3))</code>) or variable arguments (e.g., <code class="highlighter-rouge">torch.zeros(2, 3)</code>) in most cases.</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Returned <code class="highlighter-rouge">Tensor</code></th>
      <th><code class="highlighter-rouge">torch.*_like</code> variant</th>
      <th><code class="highlighter-rouge">tensor.new_*</code> variant</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.empty"><code class="highlighter-rouge">torch.empty</code></a></td>
      <td>uninitialized memory</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.zeros"><code class="highlighter-rouge">torch.zeros</code></a></td>
      <td>all zeros</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.ones"><code class="highlighter-rouge">torch.ones</code></a></td>
      <td>all ones</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.full"><code class="highlighter-rouge">torch.full</code></a></td>
      <td>filled with a given value</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.rand"><code class="highlighter-rouge">torch.rand</code></a></td>
      <td>i.i.d. continuous Uniform[0, 1)</td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.randn"><code class="highlighter-rouge">torch.randn</code></a></td>
      <td>i.i.d. <code class="highlighter-rouge">Normal(0, 1)</code></td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.randint"><code class="highlighter-rouge">torch.randint</code></a></td>
      <td>i.i.d. discrete Uniform in given range</td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.randperm"><code class="highlighter-rouge">torch.randperm</code></a></td>
      <td>random permutation of <code class="highlighter-rouge">{0, 1, ..., n - 1}</code></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="highlighter-rouge">torch.tensor</code></a></td>
      <td>copied from existing data (list, NumPy ndarray, etc.)</td>
      <td> </td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy"><code class="highlighter-rouge">torch.from_numpy</code>*</a></td>
      <td>from NumPy <code class="highlighter-rouge">ndarray</code> (sharing storage without copying)</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.arange"><code class="highlighter-rouge">torch.arange</code></a>, <a href="https://pytorch.org/docs/0.4.0/torch.html#torch.range"><code class="highlighter-rouge">torch.range</code></a>, and <a href="https://pytorch.org/docs/0.4.0/torch.html#torch.linspace"><code class="highlighter-rouge">torch.linspace</code></a></td>
      <td>uniformly spaced values in a given range</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.logspace"><code class="highlighter-rouge">torch.logspace</code></a></td>
      <td>logarithmically spaced values in a given range</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.eye"><code class="highlighter-rouge">torch.eye</code></a></td>
      <td>identity matrix</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>*: <a href="https://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy"><code class="highlighter-rouge">torch.from_numpy</code></a> only takes in a NumPy <code class="highlighter-rouge">ndarray</code> as its input argument.</p>

<h2 id="writing-device-agnostic-code">Writing device-agnostic code</h2>

<p>Previous versions of PyTorch made it difficult to write code that was device agnostic (i.e. that could run on both CUDA-enabled and CPU-only machines without modification).</p>

<p>PyTorch 0.4.0 makes this easier in two ways:</p>

<ul>
  <li>The <code class="highlighter-rouge">device</code> attribute of a Tensor gives the <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device">torch.device</a> for all Tensors (<code class="highlighter-rouge">get_device</code> only works for CUDA tensors)</li>
  <li>The <code class="highlighter-rouge">to</code> method of <code class="highlighter-rouge">Tensors</code> and <code class="highlighter-rouge">Modules</code> can be used to easily move objects to different devices (instead of having to call <code class="highlighter-rouge">cpu()</code> or <code class="highlighter-rouge">cuda()</code> based on the context)</li>
</ul>

<p>We recommend the following pattern:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># at beginning of the script</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="o">...</span>

<span class="c"># then whenever you get a new Tensor or Module</span>
<span class="c"># this won't copy if they are already on the desired device</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="new-edge-case-constraints-on-names-of-submodules-parameters-and-buffers-in-nnmodule">New edge-case constraints on names of submodules, parameters, and buffers in <code class="highlighter-rouge">nn.Module</code></h2>

<p><code class="highlighter-rouge">name</code> that is an empty string or contains <code class="highlighter-rouge">"."</code> is no longer permitted in <code class="highlighter-rouge">module.add_module(name, value)</code>, <code class="highlighter-rouge">module.add_parameter(name, value)</code> or <code class="highlighter-rouge">module.add_buffer(name, value)</code> because such names may cause lost data in the <code class="highlighter-rouge">state_dict</code>. If you are loading a checkpoint for modules containing such names, please update the module definition and patch the <code class="highlighter-rouge">state_dict</code> before loading it.</p>

<h2 id="code-samples-putting-it-all-together">Code Samples (Putting it all together)</h2>

<p>To get a flavor of the overall recommended changes in 0.4.0, let’s look at a quick example for a common code pattern in both 0.3.1 and 0.4.0:</p>

<ul>
  <li>0.3.1 (old):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MyRNN</span><span class="p">()</span>
<span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c"># train</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">h_shape</span><span class="p">))</span>  <span class="c"># init hidden</span>
    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">hidden</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="o">...</span>  <span class="c"># get loss and optimize</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c"># evaluate</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">volatile</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
        <span class="o">...</span>
    <span class="o">...</span>
</code></pre></div>    </div>
  </li>
  <li>0.4.0 (new):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># torch.device object used throughout this script</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyRNN</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c"># train</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="o">*</span><span class="n">h_shape</span><span class="p">)</span>  <span class="c"># has the same device &amp; dtype as `input`</span>
    <span class="o">...</span>  <span class="c"># get loss and optimize</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>           <span class="c"># get Python number from 1-element Tensor</span>

<span class="c"># evaluate</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>                   <span class="c"># operations inside don't track history</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="o">...</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>Thank you for reading! Please refer to our <a href="https://pytorch.org/docs/0.4.0/index.html">documentation</a> and <a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0">release notes</a> for more details.</p>

<p>Happy PyTorch-ing!</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="https://goo.gl/forms/PP1AGvNHpSaJP8to1" target="_blank">Slack</a></li>
        </ul>
      </div>

      <div class="footer-links-col follow-us-col">
        <ul>
          <li class="list-title">Stay Connected</li>
          <li>
            
            <div id="mc_embed_signup">
  <form
    action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
    method="post"
    id="mc-embedded-subscribe-form"
    name="mc-embedded-subscribe-form"
    class="email-subscribe-form validate"
    target="_blank"
    novalidate>
    <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
      <div class="mc-field-group">
        <label for="mce-EMAIL" style="display:none;">Email Address</label>
        <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
      </div>

      <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
      </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

      <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

      <div class="clear">
        <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
      </div>
    </div>
  </form>
</div>

          </li>
        </ul>

        <div class="footer-social-icons">
          <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
          <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
        </div>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="http://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="">
          <a href="/features">Features</a>
        </li>

        <li class="">
          <a href="/ecosystem">Ecosystem</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li>
          <a href="/docs">Docs</a>
        </li>

        <li class="">
          <a href="/resources">Resources</a>
        </li>

        <li>
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>
<script src="/assets/search-bar.js"></script>
<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top === 0) {
        $(".header-holder").css({"backgroundColor": "transparent"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>



</body>

</html>

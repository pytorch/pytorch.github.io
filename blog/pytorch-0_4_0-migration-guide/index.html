<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      PyTorch 0.4.0 Migration Guide | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="Welcome to the migration guide for PyTorch 0.4.0. In this release we introduced many exciting new features and critical bug fixes, with the goal of providing users a better and cleaner interface. In this guide, we will cover the most important changes in migrating existing code from previous versions:

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="PyTorch 0.4.0 Migration Guide" />
<meta property="og:description" content="Welcome to the migration guide for PyTorch 0.4.0. In this release we introduced many exciting new features and critical bug fixes, with the goal of providing users a better and cleaner interface. In this guide, we will cover the most important changes in migrating existing code from previous versions:

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="PyTorch 0.4.0 Migration Guide" />
<meta name="twitter:description" content="Welcome to the migration guide for PyTorch 0.4.0. In this release we introduced many exciting new features and critical bug fixes, with the goal of providing users a better and cleaner interface. In this guide, we will cover the most important changes in migrating existing code from previous versions:

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">April 22, 2018</p>
            <h1>
                <a class="blog-title">PyTorch 0.4.0 Migration Guide</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <p>Welcome to the migration guide for PyTorch 0.4.0. In this release we introduced <a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0">many exciting new features and critical bug fixes</a>, with the goal of providing users a better and cleaner interface. In this guide, we will cover the most important changes in migrating existing code from previous versions:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Tensors</code> and <code class="language-plaintext highlighter-rouge">Variables</code> have merged</li>
  <li>Support for 0-dimensional (scalar) <code class="language-plaintext highlighter-rouge">Tensors</code></li>
  <li>Deprecation of the <code class="language-plaintext highlighter-rouge">volatile</code> flag</li>
  <li><code class="language-plaintext highlighter-rouge">dtypes</code>, <code class="language-plaintext highlighter-rouge">devices</code>, and Numpy-style <code class="language-plaintext highlighter-rouge">Tensor</code> creation functions</li>
  <li>Writing device-agnostic code</li>
  <li>New edge-case constraints on names of submodules, parameters, and buffers in <code class="language-plaintext highlighter-rouge">nn.Module</code></li>
</ul>

<h2 id="merging-tensor-and-variable-and-classes">Merging <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="language-plaintext highlighter-rouge">Tensor</code></a> and <code class="language-plaintext highlighter-rouge">Variable</code> and classes</h2>

<p><a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="language-plaintext highlighter-rouge">torch.Tensor</code></a> and <code class="language-plaintext highlighter-rouge">torch.autograd.Variable</code> are now the same class. More precisely, <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="language-plaintext highlighter-rouge">torch.Tensor</code></a> is capable of tracking history and behaves like the old <code class="language-plaintext highlighter-rouge">Variable</code>; <code class="language-plaintext highlighter-rouge">Variable</code> wrapping continues to work as before but returns an object of type <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="language-plaintext highlighter-rouge">torch.Tensor</code></a>. This means that you don’t need the <code class="language-plaintext highlighter-rouge">Variable</code> wrapper everywhere in your code anymore.</p>

<h3 id="the-type-of-a-tensor-has-changed">The <code class="language-plaintext highlighter-rouge">type()</code> of a <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="language-plaintext highlighter-rouge">Tensor</code></a> has changed</h3>

<p>Note also that the <code class="language-plaintext highlighter-rouge">type()</code> of a Tensor no longer reflects the data type. Use <code class="language-plaintext highlighter-rouge">isinstance()</code> or <code class="language-plaintext highlighter-rouge">x.type()</code>instead:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">DoubleTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># was torch.DoubleTensor
</span><span class="s">"&lt;class 'torch.Tensor'&gt;"</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nb">type</span><span class="p">())</span>  <span class="c1"># OK: 'torch.DoubleTensor'
</span><span class="s">'torch.DoubleTensor'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">DoubleTensor</span><span class="p">))</span>  <span class="c1"># OK: True
</span><span class="bp">True</span>
</code></pre></div></div>

<h3 id="when-does-autograd-start-tracking-history-now">When does <a href="https://pytorch.org/docs/0.4.0/autograd.html"><code class="language-plaintext highlighter-rouge">autograd</code></a> start tracking history now?</h3>

<p><code class="language-plaintext highlighter-rouge">requires_grad</code>, the central flag for <a href="https://pytorch.org/docs/0.4.0/autograd.html"><code class="language-plaintext highlighter-rouge">autograd</code></a>, is now an attribute on <code class="language-plaintext highlighter-rouge">Tensors</code>. The same rules previously used for <code class="language-plaintext highlighter-rouge">Variables</code> applies to <code class="language-plaintext highlighter-rouge">Tensors</code>; <a href="https://pytorch.org/docs/0.4.0/autograd.html"><code class="language-plaintext highlighter-rouge">autograd</code></a> starts tracking history when any input <code class="language-plaintext highlighter-rouge">Tensor</code> of an operation has <code class="language-plaintext highlighter-rouge">requires_grad=True</code>. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># create a tensor with requires_grad=False (default)
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># another tensor with requires_grad=False
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># both inputs have requires_grad=False. so does the output
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># then autograd won't track this computation. let's verify!
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">element</span> <span class="mi">0</span> <span class="n">of</span> <span class="n">tensors</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">require</span> <span class="n">grad</span> <span class="ow">and</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">have</span> <span class="n">a</span> <span class="n">grad_fn</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># now create a tensor with requires_grad=True
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># add to the previous result that has require_grad=False
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">total</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">z</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># the total sum now requires grad!
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">total</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># autograd can compute the gradients as well
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">total</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span><span class="p">.</span><span class="n">grad</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># and no computation is wasted to compute gradients for x, y and z, which don't require grad
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="p">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">y</span><span class="p">.</span><span class="n">grad</span> <span class="o">==</span> <span class="bp">None</span>
<span class="bp">True</span>
</code></pre></div></div>

<h4 id="manipulating-requires_grad-flag">Manipulating <code class="language-plaintext highlighter-rouge">requires_grad</code> flag</h4>

<p>Other than directly setting the attribute, you can change this flag <code class="language-plaintext highlighter-rouge">in-place</code> using <a href="https://pytorch.org/docs/0.4.0/tensors.html#torch.Tensor.requires_grad_"><code class="language-plaintext highlighter-rouge">my_tensor.requires_grad_()</code></a>, or, as in the above example, at creation time by passing it in as an argument (default is <code class="language-plaintext highlighter-rouge">False</code>), e.g.,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">existing_tensor</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">existing_tensor</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">my_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">my_tensor</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
</code></pre></div></div>

<h3 id="what-about-data">What about <code class="language-plaintext highlighter-rouge">.data?</code></h3>

<p><code class="language-plaintext highlighter-rouge">.data</code> was the primary way to get the underlying <code class="language-plaintext highlighter-rouge">Tensor</code> from a <code class="language-plaintext highlighter-rouge">Variable</code>. After this merge, calling <code class="language-plaintext highlighter-rouge">y = x.data</code> still has similar semantics. So <code class="language-plaintext highlighter-rouge">y</code> will be a <code class="language-plaintext highlighter-rouge">Tensor</code> that shares the same data with <code class="language-plaintext highlighter-rouge">x</code>, is unrelated with the computation history of <code class="language-plaintext highlighter-rouge">x</code>, and has <code class="language-plaintext highlighter-rouge">requires_grad=False</code>.</p>

<p>However, <code class="language-plaintext highlighter-rouge">.data</code> can be unsafe in some cases. Any changes on <code class="language-plaintext highlighter-rouge">x.data</code> wouldn’t be tracked by <code class="language-plaintext highlighter-rouge">autograd</code>, and the computed gradients would be incorrect if <code class="language-plaintext highlighter-rouge">x</code> is needed in a backward pass. A safer alternative is to use <a href="https://pytorch.org/docs/master/autograd.html#torch.Tensor.detach"><code class="language-plaintext highlighter-rouge">x.detach()</code></a>, which also returns a <code class="language-plaintext highlighter-rouge">Tensor</code> that shares data with <code class="language-plaintext highlighter-rouge">requires_grad=False</code>, but will have its in-place changes reported by <code class="language-plaintext highlighter-rouge">autograd</code> if <code class="language-plaintext highlighter-rouge">x</code> is needed in backward.</p>

<p>Here is an example of the difference between <code class="language-plaintext highlighter-rouge">.data</code> and <code class="language-plaintext highlighter-rouge">x.detach()</code> (and why we recommend using <code class="language-plaintext highlighter-rouge">detach</code> in general).</p>

<p>If you use <code class="language-plaintext highlighter-rouge">Tensor.detach()</code>, the gradient computation is guaranteed to be correct.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span>  <span class="c1"># modified by c.zero_() !!
</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Requires the original value of out, but that was overwritten by c.zero_()
</span><span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">variables</span> <span class="n">needed</span> <span class="k">for</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">has</span> <span class="n">been</span> <span class="n">modified</span> <span class="n">by</span> <span class="n">an</span>
</code></pre></div></div>

<p>However, using <code class="language-plaintext highlighter-rouge">Tensor.data</code> can be unsafe and can easily result in incorrect gradients when a tensor is required for gradient computation but modified in-place.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">data</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span>  <span class="c1"># out  was modified by c.zero_()
</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">backward</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="p">.</span><span class="n">grad</span>  <span class="c1"># The result is very, very wrong because `out` changed!
</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="support-for-0-dimensional-scalar-tensors">Support for 0-dimensional (scalar) Tensors</h2>

<p>Previously, indexing into a <code class="language-plaintext highlighter-rouge">Tensor</code> vector (1-dimensional tensor) gave a Python number but indexing into a <code class="language-plaintext highlighter-rouge">Variable</code> vector gave (inconsistently!) a vector of size <code class="language-plaintext highlighter-rouge">(1,)</code>! Similar behavior existed with reduction functions, e.g. <code class="language-plaintext highlighter-rouge">tensor.sum()</code> would return a Python number, but <code class="language-plaintext highlighter-rouge">variable.sum()</code> would return a vector of size <code class="language-plaintext highlighter-rouge">(1,)</code>.</p>

<p>Fortunately, this release introduces proper scalar (0-dimensional tensor) support in PyTorch! Scalars can be created using the new <code class="language-plaintext highlighter-rouge">torch.tensor</code> function (which will be explained in more detail later; for now just think of it as the PyTorch equivalent of <code class="language-plaintext highlighter-rouge">numpy.array</code>). Now you can do things like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">)</span>         <span class="c1"># create a scalar directly
</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">).</span><span class="n">size</span><span class="p">()</span>  <span class="c1"># scalar is 0-dimensional
</span><span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">]).</span><span class="n">size</span><span class="p">()</span>     <span class="c1"># compare to a vector of size 1
</span><span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>  <span class="c1"># this is a vector
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>                    <span class="c1"># indexing into a vector gives a scalar
</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>             <span class="c1"># .item() gives the value as a Python number
</span><span class="mf">5.0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]).</span><span class="nb">sum</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([])</span>
</code></pre></div></div>

<h3 id="accumulating-losses">Accumulating losses</h3>

<p>Consider the widely used pattern <code class="language-plaintext highlighter-rouge">total_loss += loss.data[0]</code>. Before 0.4.0. <code class="language-plaintext highlighter-rouge">loss</code> was a <code class="language-plaintext highlighter-rouge">Variable</code> wrapping a tensor of size <code class="language-plaintext highlighter-rouge">(1,)</code>, but in 0.4.0 <code class="language-plaintext highlighter-rouge">loss</code> is now a scalar and has <code class="language-plaintext highlighter-rouge">0</code> dimensions. Indexing into a scalar doesn’t make sense (it gives a warning now, but will be a hard error in 0.5.0). Use <code class="language-plaintext highlighter-rouge">loss.item()</code> to get the Python number from a scalar.</p>

<p>Note that if you don’t convert to a Python number when accumulating losses, you may find increased memory usage in your program. This is because the right-hand-side of the above expression used to be a Python float, while it is now a zero-dim Tensor. The total loss is thus accumulating Tensors and their gradient history, which may keep around large autograd graphs for much longer than necessary.</p>

<h2 id="deprecation-of-volatile-flag">Deprecation of volatile flag</h2>

<p>The <code class="language-plaintext highlighter-rouge">volatile</code> flag is now deprecated and has no effect. Previously, any computation that involves a <code class="language-plaintext highlighter-rouge">Variable</code> with <code class="language-plaintext highlighter-rouge">volatile=True</code> wouldn’t be tracked by <code class="language-plaintext highlighter-rouge">autograd</code>. This has now been replaced by a <a href="https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation">set of more flexible context managers</a> including <code class="language-plaintext highlighter-rouge">torch.no_grad()</code>, <code class="language-plaintext highlighter-rouge">torch.set_grad_enabled(grad_mode)</code>, and others.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="p">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">is_train</span> <span class="o">=</span> <span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
<span class="p">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># this can also be used as a function
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
</code></pre></div></div>

<h2 id="dtypes-devices-and-numpy-style-creation-functions"><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="language-plaintext highlighter-rouge">dtypes</code></a>, <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="language-plaintext highlighter-rouge">devices</code></a> and NumPy-style creation functions</h2>

<p>In previous versions of PyTorch, we used to specify data type (e.g. float vs double), device type (cpu vs cuda) and layout (dense vs sparse) together as a “tensor type”. For example, <code class="language-plaintext highlighter-rouge">torch.cuda.sparse.DoubleTensor</code> was the <code class="language-plaintext highlighter-rouge">Tensor</code> type representing the <code class="language-plaintext highlighter-rouge">double</code> data type, living on CUDA devices, and with <a href="https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)">COO sparse tensor</a> layout.</p>

<p>In this release, we introduce <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="language-plaintext highlighter-rouge">torch.dtype</code></a>, <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="language-plaintext highlighter-rouge">torch.device</code></a> and <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="language-plaintext highlighter-rouge">torch.layout</code></a> classes to allow better management of these properties via NumPy-style creation functions.</p>

<h3 id="torchdtype"><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="language-plaintext highlighter-rouge">torch.dtype</code></a></h3>

<p>Below is a complete list of available <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="language-plaintext highlighter-rouge">torch.dtype</code></a>s (data types) and their corresponding tensor types.</p>

<table>
  <thead>
    <tr>
      <th>Data</th>
      <th><code class="language-plaintext highlighter-rouge">type torch.dtype</code></th>
      <th>Tensor types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>32-bit floating point</td>
      <td><code class="language-plaintext highlighter-rouge">torch.float32</code> or <code class="language-plaintext highlighter-rouge">torch.float</code></td>
      <td><code class="language-plaintext highlighter-rouge">torch.*.FloatTensor</code></td>
    </tr>
    <tr>
      <td>64-bit floating point</td>
      <td><code class="language-plaintext highlighter-rouge">torch.float64</code> or <code class="language-plaintext highlighter-rouge">torch.double</code></td>
      <td><code class="language-plaintext highlighter-rouge">torch.*.DoubleTensor</code></td>
    </tr>
    <tr>
      <td>16-bit floating point</td>
      <td><code class="language-plaintext highlighter-rouge">torch.float16</code> or <code class="language-plaintext highlighter-rouge">torch.half</code></td>
      <td><code class="language-plaintext highlighter-rouge">torch.*.HalfTensor</code></td>
    </tr>
    <tr>
      <td>8-bit integer (unsigned)</td>
      <td><code class="language-plaintext highlighter-rouge">torch.uint8</code></td>
      <td><code class="language-plaintext highlighter-rouge">torch.*.ByteTensor</code></td>
    </tr>
    <tr>
      <td>8-bit integer (signed)</td>
      <td><code class="language-plaintext highlighter-rouge">torch.int8</code></td>
      <td><code class="language-plaintext highlighter-rouge">torch.*.CharTensor</code></td>
    </tr>
    <tr>
      <td>16-bit integer (signed)</td>
      <td><code class="language-plaintext highlighter-rouge">torch.int16</code> or <code class="language-plaintext highlighter-rouge">torch.short</code></td>
      <td><code class="language-plaintext highlighter-rouge">torch.*.ShortTensor</code></td>
    </tr>
    <tr>
      <td>32-bit integer (signed)</td>
      <td><code class="language-plaintext highlighter-rouge">torch.int32</code> or <code class="language-plaintext highlighter-rouge">torch.int</code></td>
      <td><code class="language-plaintext highlighter-rouge">torch.*.IntTensor</code></td>
    </tr>
    <tr>
      <td>64-bit integer (signed)</td>
      <td><code class="language-plaintext highlighter-rouge">torch.int64</code> or <code class="language-plaintext highlighter-rouge">torch.long</code></td>
      <td><code class="language-plaintext highlighter-rouge">torch.*.LongTensor</code></td>
    </tr>
  </tbody>
</table>

<p>The dtype of a tensor can be access via its <code class="language-plaintext highlighter-rouge">dtype</code> attribute.</p>

<h3 id="torchdevice"><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="language-plaintext highlighter-rouge">torch.device</code></a></h3>

<p>A <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="language-plaintext highlighter-rouge">torch.device</code></a> contains a device type (<code class="language-plaintext highlighter-rouge">'cpu'</code> or <code class="language-plaintext highlighter-rouge">'cuda'</code>) and optional device ordinal (id) for the device type. It can be initialized with <code class="language-plaintext highlighter-rouge">torch.device('{device_type}')</code> or <code class="language-plaintext highlighter-rouge">torch.device('{device_type}:{device_ordinal}')</code>.</p>

<p>If the device ordinal is not present, this represents the current device for the device type; e.g., <code class="language-plaintext highlighter-rouge">torch.device('cuda')</code> is equivalent to <code class="language-plaintext highlighter-rouge">torch.device('cuda:X')</code> where <code class="language-plaintext highlighter-rouge">X</code> is the result of <code class="language-plaintext highlighter-rouge">torch.cuda.current_device()</code>.</p>

<p>The device of a tensor can be accessed via its <code class="language-plaintext highlighter-rouge">device</code> attribute.</p>

<h3 id="torchlayout"><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="language-plaintext highlighter-rouge">torch.layout</code></a></h3>

<p><a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="language-plaintext highlighter-rouge">torch.layout</code></a> represents the data layout of a <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="language-plaintext highlighter-rouge">Tensor</code></a>. Currently <code class="language-plaintext highlighter-rouge">torch.strided</code> (dense tensors, the default) and <code class="language-plaintext highlighter-rouge">torch.sparse_coo</code> (sparse tensors with COO format) are supported.</p>

<p>The layout of a tensor can be access via its <code class="language-plaintext highlighter-rouge">layout</code> attribute.</p>

<h3 id="creating-tensors">Creating Tensors</h3>

<p><a href="https://pytorch.org/docs/0.4.0/torch.html#creation-ops">Methods that create a</a> <a href="https://pytorch.org/docs/0.4.0/tensors.html"><code class="language-plaintext highlighter-rouge">Tensor</code></a> now also take in <code class="language-plaintext highlighter-rouge">dtype</code>, <code class="language-plaintext highlighter-rouge">device</code>, <code class="language-plaintext highlighter-rouge">layout</code>, and <code class="language-plaintext highlighter-rouge">requires_grad</code> options to specify the desired attributes on the returned <code class="language-plaintext highlighter-rouge">Tensor</code>. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:1"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.6344</span><span class="p">,</span>  <span class="mf">0.8562</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2758</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.8414</span><span class="p">,</span>  <span class="mf">1.7962</span><span class="p">,</span>  <span class="mf">1.0589</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1369</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0462</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4373</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda:1'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span>  <span class="c1"># default is False
</span><span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
</code></pre></div></div>

<h5 id="torchtensordata-"><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="language-plaintext highlighter-rouge">torch.tensor(data, ...)</code></a></h5>

<p><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="language-plaintext highlighter-rouge">torch.tensor</code></a> is one of the newly added <a href="https://pytorch.org/docs/0.4.0/torch.html#creation-ops">tensor creation methods</a>. It takes in array-like data of all kinds and copies the contained values into a new <code class="language-plaintext highlighter-rouge">Tensor</code>. As mentioned earlier, <a href="https://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="language-plaintext highlighter-rouge">torch.tensor</code></a> is the PyTorch equivalent of NumPy’s <code class="language-plaintext highlighter-rouge">numpy.array</code>constructor. Unlike the <code class="language-plaintext highlighter-rouge">torch.*Tensor</code> methods, you can also create zero-dimensional <code class="language-plaintext highlighter-rouge">Tensor</code>s (aka scalars) this way (a single python number is treated as a Size in the <code class="language-plaintext highlighter-rouge">torch.*Tensor methods</code>). Moreover, if a <code class="language-plaintext highlighter-rouge">dtype</code> argument isn’t given, it will infer the suitable <code class="language-plaintext highlighter-rouge">dtype</code> given the data. It is the recommended way to create a tensor from existing data like a Python list. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">half</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>               <span class="c1"># scalar
</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">]).</span><span class="n">dtype</span>  <span class="c1"># type inferece
</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]).</span><span class="n">dtype</span>    <span class="c1"># type inferece
</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span>
</code></pre></div></div>

<p>We’ve also added more tensor creation methods. Some of them have <code class="language-plaintext highlighter-rouge">torch.*_like</code> and/or <code class="language-plaintext highlighter-rouge">tensor.new_*</code> variants.</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">torch.*_like</code> takes in an input <code class="language-plaintext highlighter-rouge">Tensor</code> instead of a shape. It returns a <code class="language-plaintext highlighter-rouge">Tensor</code> with same attributes as the input <code class="language-plaintext highlighter-rouge">Tensor</code> by default unless otherwise specified:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">int</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">tensor.new_*</code> can also create <code class="language-plaintext highlighter-rouge">Tensors</code> with same attributes as <code class="language-plaintext highlighter-rouge">tensor</code>, but it always takes in a shape argument:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">int</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>To specify the desired shape, you can either use a tuple (e.g., <code class="language-plaintext highlighter-rouge">torch.zeros((2, 3))</code>) or variable arguments (e.g., <code class="language-plaintext highlighter-rouge">torch.zeros(2, 3)</code>) in most cases.</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Returned <code class="language-plaintext highlighter-rouge">Tensor</code></th>
      <th><code class="language-plaintext highlighter-rouge">torch.*_like</code> variant</th>
      <th><code class="language-plaintext highlighter-rouge">tensor.new_*</code> variant</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.empty"><code class="language-plaintext highlighter-rouge">torch.empty</code></a></td>
      <td>uninitialized memory</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.zeros"><code class="language-plaintext highlighter-rouge">torch.zeros</code></a></td>
      <td>all zeros</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.ones"><code class="language-plaintext highlighter-rouge">torch.ones</code></a></td>
      <td>all ones</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.full"><code class="language-plaintext highlighter-rouge">torch.full</code></a></td>
      <td>filled with a given value</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.rand"><code class="language-plaintext highlighter-rouge">torch.rand</code></a></td>
      <td>i.i.d. continuous Uniform[0, 1)</td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.randn"><code class="language-plaintext highlighter-rouge">torch.randn</code></a></td>
      <td>i.i.d. <code class="language-plaintext highlighter-rouge">Normal(0, 1)</code></td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.randint"><code class="language-plaintext highlighter-rouge">torch.randint</code></a></td>
      <td>i.i.d. discrete Uniform in given range</td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.randperm"><code class="language-plaintext highlighter-rouge">torch.randperm</code></a></td>
      <td>random permutation of <code class="language-plaintext highlighter-rouge">{0, 1, ..., n - 1}</code></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="language-plaintext highlighter-rouge">torch.tensor</code></a></td>
      <td>copied from existing data (list, NumPy ndarray, etc.)</td>
      <td> </td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy"><code class="language-plaintext highlighter-rouge">torch.from_numpy</code>*</a></td>
      <td>from NumPy <code class="language-plaintext highlighter-rouge">ndarray</code> (sharing storage without copying)</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.arange"><code class="language-plaintext highlighter-rouge">torch.arange</code></a>, <a href="https://pytorch.org/docs/0.4.0/torch.html#torch.range"><code class="language-plaintext highlighter-rouge">torch.range</code></a>, and <a href="https://pytorch.org/docs/0.4.0/torch.html#torch.linspace"><code class="language-plaintext highlighter-rouge">torch.linspace</code></a></td>
      <td>uniformly spaced values in a given range</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.logspace"><code class="language-plaintext highlighter-rouge">torch.logspace</code></a></td>
      <td>logarithmically spaced values in a given range</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://pytorch.org/docs/0.4.0/torch.html#torch.eye"><code class="language-plaintext highlighter-rouge">torch.eye</code></a></td>
      <td>identity matrix</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>*: <a href="https://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy"><code class="language-plaintext highlighter-rouge">torch.from_numpy</code></a> only takes in a NumPy <code class="language-plaintext highlighter-rouge">ndarray</code> as its input argument.</p>

<h2 id="writing-device-agnostic-code">Writing device-agnostic code</h2>

<p>Previous versions of PyTorch made it difficult to write code that was device agnostic (i.e. that could run on both CUDA-enabled and CPU-only machines without modification).</p>

<p>PyTorch 0.4.0 makes this easier in two ways:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">device</code> attribute of a Tensor gives the <a href="https://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device">torch.device</a> for all Tensors (<code class="language-plaintext highlighter-rouge">get_device</code> only works for CUDA tensors)</li>
  <li>The <code class="language-plaintext highlighter-rouge">to</code> method of <code class="language-plaintext highlighter-rouge">Tensors</code> and <code class="language-plaintext highlighter-rouge">Modules</code> can be used to easily move objects to different devices (instead of having to call <code class="language-plaintext highlighter-rouge">cpu()</code> or <code class="language-plaintext highlighter-rouge">cuda()</code> based on the context)</li>
</ul>

<p>We recommend the following pattern:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># at beginning of the script
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="p">...</span>

<span class="c1"># then whenever you get a new Tensor or Module
# this won't copy if they are already on the desired device
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">(...).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="new-edge-case-constraints-on-names-of-submodules-parameters-and-buffers-in-nnmodule">New edge-case constraints on names of submodules, parameters, and buffers in <code class="language-plaintext highlighter-rouge">nn.Module</code></h2>

<p><code class="language-plaintext highlighter-rouge">name</code> that is an empty string or contains <code class="language-plaintext highlighter-rouge">"."</code> is no longer permitted in <code class="language-plaintext highlighter-rouge">module.add_module(name, value)</code>, <code class="language-plaintext highlighter-rouge">module.add_parameter(name, value)</code> or <code class="language-plaintext highlighter-rouge">module.add_buffer(name, value)</code> because such names may cause lost data in the <code class="language-plaintext highlighter-rouge">state_dict</code>. If you are loading a checkpoint for modules containing such names, please update the module definition and patch the <code class="language-plaintext highlighter-rouge">state_dict</code> before loading it.</p>

<h2 id="code-samples-putting-it-all-together">Code Samples (Putting it all together)</h2>

<p>To get a flavor of the overall recommended changes in 0.4.0, let’s look at a quick example for a common code pattern in both 0.3.1 and 0.4.0:</p>

<ul>
  <li>0.3.1 (old):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MyRNN</span><span class="p">()</span>
<span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># train
</span><span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">h_shape</span><span class="p">))</span>  <span class="c1"># init hidden
</span>    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="p">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">hidden</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="p">...</span>  <span class="c1"># get loss and optimize
</span>    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># evaluate
</span><span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">volatile</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
        <span class="p">...</span>
    <span class="p">...</span>
</code></pre></div>    </div>
  </li>
  <li>0.4.0 (new):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.device object used throughout this script
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyRNN</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># train
</span><span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="o">*</span><span class="n">h_shape</span><span class="p">)</span>  <span class="c1"># has the same device &amp; dtype as `input`
</span>    <span class="p">...</span>  <span class="c1"># get loss and optimize
</span>    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>           <span class="c1"># get Python number from 1-element Tensor
</span>
<span class="c1"># evaluate
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>                   <span class="c1"># operations inside don't track history
</span>    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="p">...</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>Thank you for reading! Please refer to our <a href="https://pytorch.org/docs/0.4.0/index.html">documentation</a> and <a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0">release notes</a> for more details.</p>

<p>Happy PyTorch-ing!</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      PyTorch 0.4.0 Migration Guide | PyTorch
    
  </title>
  <meta property="og:title" content="PyTorch"/>
<meta property="og:description" content="An open source deep learning platform that provides a seamless path from research prototyping to production deployment."/>
<meta property="og:url" content="https://www.pytorch.org"/>
<meta property="og:type" content="website"/>
<meta property="og:image" content="https://pytorch.org/assets/images/pytorch-logo.png"/>

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA‌-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
  </script>
  <noscript>
    <img height="1" width="1"
    src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
    &noscript=1"/>
  </noscript>

  
</head>


<body class="blog">
    <div class="main-background "></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="">
      <a href="/features">Features</a>
    </li>

    <li class="">
      <a href="/ecosystem">Ecosystem</a>
    </li>

    <li class="active">
      <a href="/blog">Blog</a>
    </li>

    <li>
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li>
      <a href="/docs">Docs</a>
    </li>

    <li class="">
      <a href="/resources">Resources</a>
    </li>

    <li>
      <a href="https://github.com/pytorch/pytorch">Github</a>
    </li>
  </ul>
</div>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid">
        <div class="container blog-detail-container">
            <p class="blog-date">April 22, 2018</p>
            <h1>
                <a class="blog-title">PyTorch 0.4.0 Migration Guide</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper">
        <div class="main-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        The PyTorch Team
                      
                    </p>
                    <p>Welcome to the migration guide for PyTorch 0.4.0. In this release we introduced <a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0">many exciting new features and critical bug fixes</a>, with the goal of providing users a better and cleaner interface. In this guide, we will cover the most important changes in migrating existing code from previous versions:</p>

<ul>
  <li><code class="highlighter-rouge">Tensors</code> and <code class="highlighter-rouge">Variables</code> have merged</li>
  <li>Support for 0-dimensional (scalar) <code class="highlighter-rouge">Tensors</code></li>
  <li>Deprecation of the <code class="highlighter-rouge">volatile</code> flag</li>
  <li><code class="highlighter-rouge">dtypes</code>, <code class="highlighter-rouge">devices</code>, and Numpy-style <code class="highlighter-rouge">Tensor</code> creation functions</li>
  <li>Writing device-agnostic code</li>
  <li>New edge-case constraints on names of submodules, parameters, and buffers in <code class="highlighter-rouge">nn.Module</code></li>
</ul>

<h2 id="merging-tensor-and-variable-and-classes">Merging <a href="http://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">Tensor</code></a> and <code class="highlighter-rouge">Variable</code> and classes</h2>

<p><a href="http://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">torch.Tensor</code></a> and <code class="highlighter-rouge">torch.autograd.Variable</code> are now the same class. More precisely, <a href="http://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">torch.Tensor</code></a> is capable of tracking history and behaves like the old <code class="highlighter-rouge">Variable</code>; <code class="highlighter-rouge">Variable</code> wrapping continues to work as before but returns an object of type <a href="http://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">torch.Tensor</code></a>. This means that you don’t need the <code class="highlighter-rouge">Variable</code> wrapper everywhere in your code anymore.</p>

<h3 id="the-type-of-a-tensor-has-changed">The <code class="highlighter-rouge">type()</code> of a <a href="http://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">Tensor</code></a> has changed</h3>

<p>Note also that the <code class="highlighter-rouge">type()</code> of a Tensor no longer reflects the data type. Use <code class="highlighter-rouge">isinstance()</code> or <code class="highlighter-rouge">x.type()</code>instead:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c"># was torch.DoubleTensor</span>
<span class="s">"&lt;class 'torch.Tensor'&gt;"</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="nb">type</span><span class="p">())</span>  <span class="c"># OK: 'torch.DoubleTensor'</span>
<span class="s">'torch.DoubleTensor'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">))</span>  <span class="c"># OK: True</span>
<span class="bp">True</span>
</code></pre></div></div>

<h3 id="when-does-autograd-start-tracking-history-now">When does <a href="http://pytorch.org/docs/0.4.0/autograd.html"><code class="highlighter-rouge">autograd</code></a> start tracking history now?</h3>

<p><code class="highlighter-rouge">requires_grad</code>, the central flag for <a href="http://pytorch.org/docs/0.4.0/autograd.html"><code class="highlighter-rouge">autograd</code></a>, is now an attribute on <code class="highlighter-rouge">Tensors</code>. The same rules previously used for <code class="highlighter-rouge">Variables</code> applies to <code class="highlighter-rouge">Tensors</code>; <a href="http://pytorch.org/docs/0.4.0/autograd.html"><code class="highlighter-rouge">autograd</code></a> starts tracking history when any input <code class="highlighter-rouge">Tensor</code> of an operation has <code class="highlighter-rouge">requires_grad=True</code>. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c"># create a tensor with requires_grad=False (default)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c"># another tensor with requires_grad=False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># both inputs have requires_grad=False. so does the output</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># then autograd won't track this computation. let's verify!</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">element</span> <span class="mi">0</span> <span class="n">of</span> <span class="n">tensors</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">require</span> <span class="n">grad</span> <span class="ow">and</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">have</span> <span class="n">a</span> <span class="n">grad_fn</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># now create a tensor with requires_grad=True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># add to the previous result that has require_grad=False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">total</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">z</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># the total sum now requires grad!</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">total</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># autograd can compute the gradients as well</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">total</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># and no computation is wasted to compute gradients for x, y and z, which don't require grad</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span> <span class="o">==</span> <span class="bp">None</span>
<span class="bp">True</span>
</code></pre></div></div>

<h4 id="manipulating-requires_grad-flag">Manipulating <code class="highlighter-rouge">requires_grad</code> flag</h4>

<p>Other than directly setting the attribute, you can change this flag <code class="highlighter-rouge">in-place</code> using <a href="http://pytorch.org/docs/0.4.0/tensors.html#torch.Tensor.requires_grad_"><code class="highlighter-rouge">my_tensor.requires_grad_()</code></a>, or, as in the above example, at creation time by passing it in as an argument (default is <code class="highlighter-rouge">False</code>), e.g.,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">existing_tensor</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">existing_tensor</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">my_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">my_tensor</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
</code></pre></div></div>

<h3 id="what-about-data">What about <code class="highlighter-rouge">.data?</code></h3>

<p><code class="highlighter-rouge">.data</code> was the primary way to get the underlying <code class="highlighter-rouge">Tensor</code> from a <code class="highlighter-rouge">Variable</code>. After this merge, calling <code class="highlighter-rouge">y = x.data</code> still has similar semantics. So <code class="highlighter-rouge">y</code> will be a <code class="highlighter-rouge">Tensor</code> that shares the same data with <code class="highlighter-rouge">x</code>, is unrelated with the computation history of <code class="highlighter-rouge">x</code>, and has <code class="highlighter-rouge">requires_grad=False</code>.</p>

<p>However, <code class="highlighter-rouge">.data</code> can be unsafe in some cases. Any changes on <code class="highlighter-rouge">x.data</code> wouldn’t be tracked by <code class="highlighter-rouge">autograd</code>, and the computed gradients would be incorrect if <code class="highlighter-rouge">x</code> is needed in a backward pass. A safer alternative is to use <a href="http://pytorch.org/docs/master/autograd.html#torch.Tensor.detach"><code class="highlighter-rouge">x.detach()</code></a>, which also returns a <code class="highlighter-rouge">Tensor</code> that shares data with <code class="highlighter-rouge">requires_grad=False</code>, but will have its in-place changes reported by <code class="highlighter-rouge">autograd</code> if <code class="highlighter-rouge">x</code> is needed in backward.</p>

<p>Here is an example of the difference between <code class="highlighter-rouge">.data</code> and <code class="highlighter-rouge">x.detach()</code> (and why we recommend using <code class="highlighter-rouge">detach</code> in general).</p>

<p>If you use <code class="highlighter-rouge">Tensor.detach()</code>, the gradient computation is guaranteed to be correct.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span>  <span class="c"># modified by c.zero_() !!</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c"># Requires the original value of out, but that was overwritten by c.zero_()</span>
<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">variables</span> <span class="n">needed</span> <span class="k">for</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">has</span> <span class="n">been</span> <span class="n">modified</span> <span class="n">by</span> <span class="n">an</span>
</code></pre></div></div>

<p>However, using <code class="highlighter-rouge">Tensor.data</code> can be unsafe and can easly result in incorrect gradients when a tensor is required for gradient computation but modified in-place.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">data</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span>  <span class="c"># out  was modified by c.zero_()</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>  <span class="c"># The result is very, very wrong because `out` changed!</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="support-for-0-dimensional-scalar-tensors">Support for 0-dimensional (scalar) Tensors</h2>

<p>Previously, indexing into a <code class="highlighter-rouge">Tensor</code> vector (1-dimensional tensor) gave a Python number but indexing into a <code class="highlighter-rouge">Variable</code> vector gave (incosistently!) a vector of size <code class="highlighter-rouge">(1,)</code>! Similar behavior existed with reduction functions, e.g. <code class="highlighter-rouge">tensor.sum()</code> would return a Python number, but <code class="highlighter-rouge">variable.sum()</code> would return a vector of size <code class="highlighter-rouge">(1,)</code>.</p>

<p>Fortunately, this release introduces proper scalar (0-dimensional tensor) support in PyTorch! Scalars can be created using the new <code class="highlighter-rouge">torch.tensor</code> function (which will be explained in more detail later; for now just think of it as the PyTorch equivalent of <code class="highlighter-rouge">numpy.array</code>). Now you can do things like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">)</span>         <span class="c"># create a scalar directly</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.1416</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>  <span class="c"># scalar is 0-dimensional</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>     <span class="c"># compare to a vector of size 1</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>  <span class="c"># this is a vector</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>                    <span class="c"># indexing into a vector gives a scalar</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vector</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>             <span class="c"># .item() gives the value as a Python number</span>
<span class="mf">5.0</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mysum</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([])</span>
</code></pre></div></div>

<h3 id="accumulating-losses">Accumulating losses</h3>

<p>Consider the widely used pattern <code class="highlighter-rouge">total_loss += loss.data[0]</code>. Before 0.4.0. <code class="highlighter-rouge">loss</code> was a <code class="highlighter-rouge">Variable</code> wrapping a tensor of size <code class="highlighter-rouge">(1,)</code>, but in 0.4.0 <code class="highlighter-rouge">loss</code> is now a scalar and has <code class="highlighter-rouge">0</code> dimensions. Indexing into a scalar doesn’t make sense (it gives a warning now, but will be a hard error in 0.5.0). Use <code class="highlighter-rouge">loss.item()</code> to get the Python number from a scalar.</p>

<p>Note that if you don’t convert to a Python number when accumulating losses, you may find increased memory usage in your program. This is because the right-hand-side of the above expression used to be a Python float, while it is now a zero-dim Tensor. The total loss is thus accumulating Tensors and their gradient history, which may keep around large autograd graphs for much longer than necessary.</p>

<h2 id="deprecation-of-volatile-flag">Deprecation of volatile flag</h2>

<p>The <code class="highlighter-rouge">volatile</code> flag is now deprecated and has no effect. Previously, any computation that involves a <code class="highlighter-rouge">Variable</code> with <code class="highlighter-rouge">volatile=True</code> wouldn’t be tracked by <code class="highlighter-rouge">autograd</code>. This has now been replaced by a <a href="http://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation">set of more flexible context managers</a> including <code class="highlighter-rouge">torch.no_grad()</code>, <code class="highlighter-rouge">torch.set_grad_enabled(grad_mode)</code>, and others.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="o">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">is_train</span> <span class="o">=</span> <span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
<span class="o">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># this can also be used as a function</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
</code></pre></div></div>

<h2 id="dtypes-devices-and-numpy-style-creation-functions"><a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="highlighter-rouge">dtypes</code></a>, <a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="highlighter-rouge">devices</code></a> and NumPy-style creation functions</h2>

<p>In previous versions of PyTorch, we used to specify data type (e.g. float vs double), device type (cpu vs cuda) and layout (dense vs sparse) together as a “tensor type”. For example, <code class="highlighter-rouge">torch.cuda.sparse.DoubleTensor</code> was the <code class="highlighter-rouge">Tensor</code> type respresenting the <code class="highlighter-rouge">double</code> data type, living on CUDA devices, and with <a href="https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)">COO sparse tensor</a> layout.</p>

<p>In this release, we introduce <a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="highlighter-rouge">torch.dtype</code></a>, <a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="highlighter-rouge">torch.device</code></a> and <a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="highlighter-rouge">torch.layout</code></a> classes to allow better management of these properties via NumPy-style creation functions.</p>

<h3 id="torchdtype"><a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="highlighter-rouge">torch.dtype</code></a></h3>

<p>Below is a complete list of available <a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype"><code class="highlighter-rouge">torch.dtype</code></a>s (data types) and their corresponding tensor types.</p>

<table>
  <thead>
    <tr>
      <th>Data</th>
      <th><code class="highlighter-rouge">type torch.dtype</code></th>
      <th>Tensor types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>32-bit floating point</td>
      <td><code class="highlighter-rouge">torch.float32</code> or <code class="highlighter-rouge">torch.float</code></td>
      <td><code class="highlighter-rouge">torch.*.FloatTensor</code></td>
    </tr>
    <tr>
      <td>64-bit floating point</td>
      <td><code class="highlighter-rouge">torch.float64</code> or <code class="highlighter-rouge">torch.double</code></td>
      <td><code class="highlighter-rouge">torch.*.DoubleTensor</code></td>
    </tr>
    <tr>
      <td>16-bit floating point</td>
      <td><code class="highlighter-rouge">torch.float16</code> or <code class="highlighter-rouge">torch.half</code></td>
      <td><code class="highlighter-rouge">torch.*.HalfTensor</code></td>
    </tr>
    <tr>
      <td>8-bit integer (unsigned)</td>
      <td><code class="highlighter-rouge">torch.uint8</code></td>
      <td><code class="highlighter-rouge">torch.*.ByteTensor</code></td>
    </tr>
    <tr>
      <td>8-bit integer (signed)</td>
      <td><code class="highlighter-rouge">torch.int8</code></td>
      <td><code class="highlighter-rouge">torch.*.CharTensor</code></td>
    </tr>
    <tr>
      <td>16-bit integer (signed)</td>
      <td><code class="highlighter-rouge">torch.int16</code> or <code class="highlighter-rouge">torch.short</code></td>
      <td><code class="highlighter-rouge">torch.*.ShortTensor</code></td>
    </tr>
    <tr>
      <td>32-bit integer (signed)</td>
      <td><code class="highlighter-rouge">torch.int32</code> or <code class="highlighter-rouge">torch.int</code></td>
      <td><code class="highlighter-rouge">torch.*.IntTensor</code></td>
    </tr>
    <tr>
      <td>64-bit integer (signed)</td>
      <td><code class="highlighter-rouge">torch.int64</code> or <code class="highlighter-rouge">torch.long</code></td>
      <td><code class="highlighter-rouge">torch.*.LongTensor</code></td>
    </tr>
  </tbody>
</table>

<p>The dtype of a tensor can be access via its <code class="highlighter-rouge">dtype</code> attribute.</p>

<h3 id="torchdevice"><a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="highlighter-rouge">torch.device</code></a></h3>

<p>A <a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device"><code class="highlighter-rouge">torch.device</code></a> contains a device type (<code class="highlighter-rouge">'cpu'</code> or <code class="highlighter-rouge">'cuda'</code>) and optional device ordinal (id) for the device type. It can be initilized with <code class="highlighter-rouge">torch.device('{device_type}')</code> or <code class="highlighter-rouge">torch.device('{device_type}:{device_ordinal}')</code>.</p>

<p>If the device ordinal is not present, this represents the current device for the device type; e.g., <code class="highlighter-rouge">torch.device('cuda')</code> is equivalent to <code class="highlighter-rouge">torch.device('cuda:X')</code> where <code class="highlighter-rouge">X</code> is the result of <code class="highlighter-rouge">torch.cuda.current_device()</code>.</p>

<p>The device of a tensor can be accessed via its <code class="highlighter-rouge">device</code> attribute.</p>

<h3 id="torchlayout"><a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="highlighter-rouge">torch.layout</code></a></h3>

<p><a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout"><code class="highlighter-rouge">torch.layout</code></a> represents the data layout of a <a href="http://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">Tensor</code></a>. Currently <code class="highlighter-rouge">torch.strided</code> (dense tensors, the default) and <code class="highlighter-rouge">torch.sparse_coo</code> (sparse tensors with COO format) are supported.</p>

<p>The layout of a tensor can be access via its <code class="highlighter-rouge">layout</code> attribute.</p>

<h3 id="creating-tensors">Creating Tensors</h3>

<p><a href="http://pytorch.org/docs/0.4.0/torch.html#creation-ops">Methods that create a</a> <a href="http://pytorch.org/docs/0.4.0/tensors.html"><code class="highlighter-rouge">Tensor</code></a> now also take in <code class="highlighter-rouge">dtype</code>, <code class="highlighter-rouge">device</code>, <code class="highlighter-rouge">layout</code>, and <code class="highlighter-rouge">requires_grad</code> options to specify the desired attributes on the returned <code class="highlighter-rouge">Tensor</code>. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:1"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.6344</span><span class="p">,</span>  <span class="mf">0.8562</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2758</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.8414</span><span class="p">,</span>  <span class="mf">1.7962</span><span class="p">,</span>  <span class="mf">1.0589</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1369</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0462</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4373</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda:1'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>  <span class="c"># default is False</span>
<span class="bp">False</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">True</span>
</code></pre></div></div>

<h5 id="torchtensordata-"><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="highlighter-rouge">torch.tensor(data, ...)</code></a></h5>

<p><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="highlighter-rouge">torch.tensor</code></a> is one of the newly added <a href="http://pytorch.org/docs/0.4.0/torch.html#creation-ops">tensor creation methods</a>. It takes in array-like data of all kinds and copies the contained values into a new <code class="highlighter-rouge">Tensor</code>. As mentioned earlier, <a href="http://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="highlighter-rouge">torch.tensor</code></a> is the PyTorch equivalent of NumPy’s <code class="highlighter-rouge">numpy.array</code>constructor. Unlike the <code class="highlighter-rouge">torch.*Tensor</code> methods, you can also create zero-dimensional <code class="highlighter-rouge">Tensor</code>s (aka scalars) this way (a single python number is treated as a Size in the <code class="highlighter-rouge">torch.*Tensor methods</code>). Moreover, if a <code class="highlighter-rouge">dtype</code> argument isn’t given, it will infer the suitable <code class="highlighter-rouge">dtype</code> given the data. It is the recommended way to create a tensor from existing data like a Python list. For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s">'cuda:0'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>               <span class="c"># scalar</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span>  <span class="c"># type inferece</span>
<span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span>    <span class="c"># type inferece</span>
<span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
</code></pre></div></div>

<p>We’ve also added more tensor creation methods. Some of them have <code class="highlighter-rouge">torch.*_like</code> and/or <code class="highlighter-rouge">tensor.new_*</code> variants.</p>

<ul>
  <li>
    <p><code class="highlighter-rouge">torch.*_like</code> takes in an input <code class="highlighter-rouge">Tensor</code> instead of a shape. It returns a <code class="highlighter-rouge">Tensor</code> with same attributes as the input <code class="highlighter-rouge">Tensor</code> by default unless otherwise specified:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><code class="highlighter-rouge">tensor.new_*</code> can also create <code class="highlighter-rouge">Tensors</code> with same attributes as <code class="highlighter-rouge">tensor</code>, but it always takes in a shape argument:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span>
 <span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>To specify the desired shape, you can either use a tuple (e.g., <code class="highlighter-rouge">torch.zeros((2, 3))</code>) or variable arguments (e.g., <code class="highlighter-rouge">torch.zeros(2, 3)</code>) in most cases.</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Returned <code class="highlighter-rouge">Tensor</code></th>
      <th><code class="highlighter-rouge">torch.*_like</code> variant</th>
      <th><code class="highlighter-rouge">tensor.new_*</code> variant</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.empty"><code class="highlighter-rouge">torch.empty</code></a></td>
      <td>unintialized memory</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.zeros"><code class="highlighter-rouge">torch.zeros</code></a></td>
      <td>all zeros</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.ones"><code class="highlighter-rouge">torch.ones</code></a></td>
      <td>all ones</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.full"><code class="highlighter-rouge">torch.full</code></a></td>
      <td>filled with a given value</td>
      <td>✔</td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.rand"><code class="highlighter-rouge">torch.rand</code></a></td>
      <td>i.i.d. continuous Uniform[0, 1)</td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.randn"><code class="highlighter-rouge">torch.randn</code></a></td>
      <td>i.i.d. <code class="highlighter-rouge">Normal(0, 1)</code></td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.randint"><code class="highlighter-rouge">torch.randint</code></a></td>
      <td>i.i.d. discrete Uniform in given range</td>
      <td>✔</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.randperm"><code class="highlighter-rouge">torch.randperm</code></a></td>
      <td>random permutation of <code class="highlighter-rouge">{0, 1, ..., n - 1}</code></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.tensor"><code class="highlighter-rouge">torch.tensor</code></a></td>
      <td>copied from existing data (list, NumPy ndarray, etc.)</td>
      <td> </td>
      <td>✔</td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy"><code class="highlighter-rouge">torch.from_numpy</code>*</a></td>
      <td>from NumPy <code class="highlighter-rouge">ndarray</code> (sharing storage without copying)</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.arange"><code class="highlighter-rouge">torch.arange</code></a>, <a href="http://pytorch.org/docs/0.4.0/torch.html#torch.range"><code class="highlighter-rouge">torch.range</code></a>, and <a href="http://pytorch.org/docs/0.4.0/torch.html#torch.linspace"><code class="highlighter-rouge">torch.linspace</code></a></td>
      <td>uniformly spaced values in a given range</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.logspace"><code class="highlighter-rouge">torch.logspace</code></a></td>
      <td>logarithmically spaced values in a given range</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://pytorch.org/docs/0.4.0/torch.html#torch.eye"><code class="highlighter-rouge">torch.eye</code></a></td>
      <td>identity matrix</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>*: <a href="http://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy"><code class="highlighter-rouge">torch.from_numpy</code></a> only takes in a NumPy <code class="highlighter-rouge">ndarray</code> as its input argument.</p>

<h2 id="writing-device-agnostic-code">Writing device-agnostic code</h2>

<p>Previous versions of PyTorch made it difficult to write code that was device agnostic (i.e. that could run on both CUDA-enabled and CPU-only machines without modification).</p>

<p>PyTorch 0.4.0 makes this easier in two ways:</p>

<ul>
  <li>The <code class="highlighter-rouge">device</code> attribute of a Tensor gives the <a href="http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device">torch.device</a> for all Tensors (<code class="highlighter-rouge">get_device</code> only works for CUDA tensors)</li>
  <li>The <code class="highlighter-rouge">to</code> method of <code class="highlighter-rouge">Tensors</code> and <code class="highlighter-rouge">Modules</code> can be used to easily move objects to different devices (instead of having to call <code class="highlighter-rouge">cpu()</code> or <code class="highlighter-rouge">cuda()</code> based on the context)</li>
</ul>

<p>We recommend the following pattern:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># at beginning of the script</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="o">...</span>

<span class="c"># then whenever you get a new Tensor or Module</span>
<span class="c"># this won't copy if they are already on the desired device</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="new-edge-case-constraints-on-names-of-submodules-parameters-and-buffers-in-nnmodule">New edge-case constraints on names of submodules, parameters, and buffers in <code class="highlighter-rouge">nn.Module</code></h2>

<p><code class="highlighter-rouge">name</code> that is an empty string or contains <code class="highlighter-rouge">"."</code> is no longer permitted in <code class="highlighter-rouge">module.add_module(name, value)</code>, <code class="highlighter-rouge">module.add_parameter(name, value)</code> or <code class="highlighter-rouge">module.add_buffer(name, value)</code> because such names may cause lost data in the <code class="highlighter-rouge">state_dict</code>. If you are loading a checkpoint for modules containing such names, please update the module definition and patch the <code class="highlighter-rouge">state_dict</code> before loading it.</p>

<h2 id="code-samples-putting-it-all-together">Code Samples (Putting it all together)</h2>

<p>To get a flavor of the overall recommended changes in 0.4.0, let’s look at a quick example for a common code pattern in both 0.3.1 and 0.4.0:</p>

<ul>
  <li>0.3.1 (old):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MyRNN</span><span class="p">()</span>
<span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c"># train</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">h_shape</span><span class="p">))</span>  <span class="c"># init hidden</span>
    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">hidden</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="o">...</span>  <span class="c"># get loss and optimize</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c"># evaluate</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">volatile</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
        <span class="o">...</span>
    <span class="o">...</span>
</code></pre></div>    </div>
  </li>
  <li>0.4.0 (new):
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># torch.device object used throughout this script</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyRNN</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c"># train</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="o">*</span><span class="n">h_shape</span><span class="p">)</span>  <span class="c"># has the same device &amp; dtype as `input`</span>
    <span class="o">...</span>  <span class="c"># get loss and optimize</span>
    <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>           <span class="c"># get Python number from 1-element Tensor</span>

<span class="c"># evaluate</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>                   <span class="c"># operations inside don't track history</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="o">...</span>
</code></pre></div>    </div>
  </li>
</ul>

<p>Thank you for reading! Please refer to our <a href="http://pytorch.org/docs/0.4.0/index.html">documentation</a> and <a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0">release notes</a> for more details.</p>

<p>Happy PyTorch-ing!</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
        </ul>
      </div>

      <div class="footer-links-col follow-us-col">
        <ul>
          <li class="list-title">Stay Connected</li>
          <li>
            
            <div id="mc_embed_signup">
  <form
    action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
    method="post"
    id="mc-embedded-subscribe-form"
    name="mc-embedded-subscribe-form"
    class="email-subscribe-form validate"
    target="_blank"
    novalidate>
    <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
      <div class="mc-field-group">
        <label for="mce-EMAIL" style="display:none;">Email Address</label>
        <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
      </div>

      <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
      </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

      <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

      <div class="clear">
        <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
      </div>
    </div>
  </form>
</div>

          </li>
        </ul>

        <div class="footer-social-icons">
          <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
          <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
        </div>
      </div>
    </div>
  </div>
</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
  <ul>
    <li class="">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="">
      <a href="/features">Features</a>
    </li>

    <li class="">
      <a href="/ecosystem">Ecosystem</a>
    </li>

    <li class="active">
      <a href="/blog">Blog</a>
    </li>

    <li>
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li>
      <a href="/docs">Docs</a>
    </li>

    <li class="">
      <a href="/resources">Resources</a>
    </li>

    <li>
      <a href="https://github.com/pytorch/pytorch">Github</a>
    </li>
  </ul>
</div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top === 0) {
        $(".header-holder").css({"backgroundColor": "transparent"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>



</body>

</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      How Computational Graphs are Executed in PyTorch | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="Welcome to the last entry into understanding the autograd engine of PyTorch series!
If you haven’t read parts 1 &amp; 2 check them now to understand how PyTorch creates the computational graph for the backward pass!

" />

  <meta property="og:image" content="https://pytorch.org/" />
  <meta name="twitter:image" content="https://pytorch.org/" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="How Computational Graphs are Executed in PyTorch" />
<meta property="og:description" content="Welcome to the last entry into understanding the autograd engine of PyTorch series!
If you haven’t read parts 1 &amp; 2 check them now to understand how PyTorch creates the computational graph for the backward pass!

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="How Computational Graphs are Executed in PyTorch" />
<meta name="twitter:description" content="Welcome to the last entry into understanding the autograd engine of PyTorch series!
If you haven’t read parts 1 &amp; 2 check them now to understand how PyTorch creates the computational graph for the backward pass!

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">June 27, 2022</p>
            <h1>
                <a class="blog-title">How Computational Graphs are Executed in PyTorch</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Preferred Networks
                      
                    </p>
                    <p>Welcome to the last entry into understanding the autograd engine of PyTorch series!
If you haven’t read parts <a href="https://pytorch.org/blog/overview-of-pytorch-autograd-engine/">1</a> &amp; <a href="https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/">2</a> check them now to understand how PyTorch creates the computational graph for the backward pass!</p>

<p>This post is based on PyTorch v1.11, so some highlighted parts may differ across versions.</p>

<h1 id="pytorch-autograd-graph-execution">PyTorch autograd graph execution</h1>

<p>The last post showed how PyTorch constructs the graph to calculate the outputs’ derivatives w.r.t. the inputs when executing the forward pass. Now we will see how the execution of the backward pass is coordinated and done by looking at the whole process, starting from Python down to the lower C++ level internals.</p>

<h1 id="what-happens-when-calling-backwardgrad-from-python">What Happens when Calling <code class="language-plaintext highlighter-rouge">backward()</code>/<code class="language-plaintext highlighter-rouge">grad()</code> from Python</h1>
<h2 id="using-variablebackward">Using <code class="language-plaintext highlighter-rouge">variable.backward()</code></h2>

<p>After doing all our calculations with an input set to require the gradient, we call <code class="language-plaintext highlighter-rouge">.backward()</code> on the result to initiate the backward pass execution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>Calling <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/_tensor.py#L307-L363"><code class="language-plaintext highlighter-rouge">.backward()</code></a> on a tensor results in a call to <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/autograd/__init__.py#L85-L175"><code class="language-plaintext highlighter-rouge">torch.autograd.backward()</code></a>.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch/_tensor.py
</span>
<span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="err">…</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>

</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">torch.autograd.backward()</code> checks the arguments and calls the autograd engine in the C++ layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
    <span class="n">tensors</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
    <span class="n">grad_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">retain_graph</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">create_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">grad_variables</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="err">…</span>

    <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">RuntimeError</span><span class="p">(</span><span class="s">"'inputs' argument to backward() cannot be empty."</span><span class="p">)</span>

    <span class="n">tensors</span> <span class="o">=</span> <span class="p">(</span><span class="n">tensors</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> \
        <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">()</span>

    <span class="n">grad_tensors_</span> <span class="o">=</span> <span class="n">_tensor_or_tensors_to_tuple</span><span class="p">(</span><span class="n">grad_tensors</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">))</span>
    <span class="n">grad_tensors_</span> <span class="o">=</span> <span class="n">_make_grads</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">grad_tensors_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">retain_graph</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">retain_graph</span> <span class="o">=</span> <span class="n">create_graph</span>

    <span class="n">Variable</span><span class="p">.</span><span class="n">_execution_engine</span><span class="p">.</span><span class="n">run_backward</span><span class="p">(</span>
        <span class="n">tensors</span><span class="p">,</span> <span class="n">grad_tensors_</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span>
        <span class="n">allow_unreachable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">accumulate_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># allow_unreachable flag
</span>
</code></pre></div></div>
<p>First, whether the <code class="language-plaintext highlighter-rouge">grad_tensors</code> argument was specified or not, there is a call to the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/autograd/__init__.py#L30-L74"><code class="language-plaintext highlighter-rouge">_make_grads</code></a> function. This is used to check the provided <code class="language-plaintext highlighter-rouge">grad_tensors</code> or to specify the default value for them by looking at the <code class="language-plaintext highlighter-rouge">tensors</code> argument values’ shapes. Check the first blog post for details on the default value for the <code class="language-plaintext highlighter-rouge">grad_tensors</code> of the backward pass. This function just provides the vector of the vector jacobian product if it was not initially specified.</p>

<p>In the above code, <code class="language-plaintext highlighter-rouge">Variable</code> has an <code class="language-plaintext highlighter-rouge">_execution_engine</code> attribute that is defined in <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/autograd/variable.py#L14"><code class="language-plaintext highlighter-rouge">torch.autograd.variable</code></a> to be of type <code class="language-plaintext highlighter-rouge">ImperativeEngine</code>; the C++ engine exported to python and declared in <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/python_engine.cpp#L384"><code class="language-plaintext highlighter-rouge">torch/csrc/autograd/python_engine.cpp</code></a>. In the following sections, we explain in detail how this object executes the backward pass.</p>

<p>Note that the <code class="language-plaintext highlighter-rouge">torch.autograd.backward</code> function has an <code class="language-plaintext highlighter-rouge">inputs</code> optional argument. This argument is used when we want to calculate the <code class="language-plaintext highlighter-rouge">.grad</code> field of only a subset of input tensors in the forward pass.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">backward</span><span class="p">([</span><span class="n">z</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.1051</span><span class="p">,</span> <span class="mf">1.7676</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">.</span><span class="n">grad</span>  <span class="c1"># None
</span><span class="o">&gt;&gt;&gt;</span>

</code></pre></div></div>
<h2 id="using-torchautogradgrad">Using <code class="language-plaintext highlighter-rouge">torch.autograd.grad</code></h2>

<p>An alternative to <code class="language-plaintext highlighter-rouge">backward()</code> is to use <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/autograd/__init__.py#L177-L277"><code class="language-plaintext highlighter-rouge">torch.autograd.grad()</code></a>. The main difference to <code class="language-plaintext highlighter-rouge">backward()</code> is that <code class="language-plaintext highlighter-rouge">grad()</code> returns a tuple of tensors with the gradients of the <code class="language-plaintext highlighter-rouge">outputs</code> w.r.t. the <code class="language-plaintext highlighter-rouge">inputs</code> kwargs instead of storing them in the <code class="language-plaintext highlighter-rouge">.grad</code> field of the tensors. As you can see, the <code class="language-plaintext highlighter-rouge">grad()</code> code shown below is very similar to backward.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">grad</span><span class="p">(</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">_TensorOrTensors</span><span class="p">,</span>
    <span class="n">grad_outputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_TensorOrTensors</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">retain_graph</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">create_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">only_inputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">allow_unused</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
   <span class="n">is_grads_batched</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="p">...]:</span>
   
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">outputs</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">overridable_args</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="n">inputs</span>
    <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">overridable_args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
            <span class="n">grad</span><span class="p">,</span>
            <span class="n">overridable_args</span><span class="p">,</span>
            <span class="n">outputs</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_outputs</span><span class="p">,</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
            <span class="n">only_inputs</span><span class="o">=</span><span class="n">only_inputs</span><span class="p">,</span>
            <span class="n">allow_unused</span><span class="o">=</span><span class="n">allow_unused</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">grad_outputs_</span> <span class="o">=</span> <span class="n">_tensor_or_tensors_to_tuple</span><span class="p">(</span><span class="n">grad_outputs</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span>
    <span class="n">grad_outputs_</span> <span class="o">=</span> <span class="n">_make_grads</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">grad_outputs_</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">retain_graph</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">retain_graph</span> <span class="o">=</span> <span class="n">create_graph</span>

    <span class="k">if</span> <span class="n">is_grads_batched</span><span class="p">:</span>
        <span class="c1"># …. It will not be covered here
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Variable</span><span class="p">.</span><span class="n">_execution_engine</span><span class="p">.</span><span class="n">run_backward</span><span class="p">(</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">grad_outputs_</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span>
            <span class="n">allow_unused</span><span class="p">,</span> <span class="n">accumulate_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># Calls into the C++ engine to run the backward pass
</span>
</code></pre></div></div>

<p>Figure 1 shows the computational graph with the <code class="language-plaintext highlighter-rouge">backward()</code> and <code class="language-plaintext highlighter-rouge">grad()</code> arguments highlighted in red and blue, respectively:</p>

<p align="center">
  <img src="/assets/images/backward-grad-fig-1.png" width="100%" />
</p>

<p align="center">
Fgiure 1: Correspondence of `backward`/`grad` arguments in the graphs.
</p>

<h1 id="going-inside-the-autograd-engine">Going Inside the Autograd Engine</h1>

<h2 id="refreshing-concepts-nodes-and-edges">Refreshing Concepts: Nodes and Edges</h2>

<p>As we saw in <a href="https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/">2</a>
The computational graph comprises <code class="language-plaintext highlighter-rouge">Node</code> and <code class="language-plaintext highlighter-rouge">Edge</code> objects. Please read that post if you haven’t done it yet.</p>

<h3 id="nodes">Nodes</h3>

<p><code class="language-plaintext highlighter-rouge">Node</code> objects are defined in <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/function.h#L105-L176"><code class="language-plaintext highlighter-rouge">torch/csrc/autograd/function.h</code></a>, and they provide an overload of <code class="language-plaintext highlighter-rouge">operator()</code> for the associated function and a list of edges to do the graph traversal. Note that <code class="language-plaintext highlighter-rouge">Node</code> is a base class that autograd functions inherit from and override the <code class="language-plaintext highlighter-rouge">apply</code> method to execute the backward function.</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">TORCH_API</span> <span class="n">Node</span> <span class="o">:</span> <span class="n">std</span><span class="o">::</span><span class="n">enable_shared_from_this</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="p">{</span>
 <span class="p">...</span>
 <span class="c1">/// Evaluates the function on the given inputs and returns the result of the</span>
  <span class="c1">/// function call.</span>
  <span class="n">variable_list</span> <span class="k">operator</span><span class="p">()(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span> <span class="n">inputs</span><span class="p">)</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="p">}</span>

<span class="nl">protected:</span>
  <span class="c1">/// Performs the `Node`'s actual operation.</span>
  <span class="k">virtual</span> <span class="n">variable_list</span> <span class="n">apply</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="err">…</span>
  <span class="n">edge_list</span> <span class="n">next_edges_</span><span class="p">;</span>
  <span class="kt">uint64_t</span> <span class="n">topological_nr_</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="err">…</span>

</code></pre></div></div>

<p>There is an attribute called <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/function.h#L481"><code class="language-plaintext highlighter-rouge">topological_nr_</code></a> in every node object. This number is used to optimize the graph execution as it allows to discard of graph branches under certain conditions. The topological number is the longest distance between this node and any leaf node and it is shown in Figure 2. Its main property is that for any pair of nodes <code class="language-plaintext highlighter-rouge">x</code>, <code class="language-plaintext highlighter-rouge">y</code> in a directed graph <code class="language-plaintext highlighter-rouge">topo_nr(x) &lt; topo_nr(y)</code> means that there is no path from <code class="language-plaintext highlighter-rouge">x</code> to <code class="language-plaintext highlighter-rouge">y</code>. So this allows for reducing the number of paths in the graph in need of traversal. Check the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/function.h#L314-L343">topological_nr</a>
) method comment for further details.</p>

<p align="center">
  <img src="/assets/images/topological-number-fig-2.png" width="100%" />
</p>

<p align="center">
Figure 2: Example of the Topological Number calculation
</p>

<h3 id="edges">Edges</h3>

<p>The <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/edge.h#L14-L39"><code class="language-plaintext highlighter-rouge">Edge</code></a> object links <code class="language-plaintext highlighter-rouge">Node</code>s together, and its implementation is straightforward.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">Edge</span> <span class="p">{</span>
  <span class="p">...</span>
  <span class="c1">/// The function this `Edge` points to.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">function</span><span class="p">;</span>
  <span class="c1">/// The identifier of a particular input to the function.</span>
  <span class="kt">uint32_t</span> <span class="n">input_nr</span><span class="p">;</span>
<span class="p">};</span>

</code></pre></div></div>

<p>It only requires a function pointer to the <code class="language-plaintext highlighter-rouge">Node</code> and an input number that is the index of the output from the forward function this edge points to. When preparing the set of gradients before calling “function”, we know that what is flowing from this edge should be accumulated in the “input_nr”th argument. Note that the input/output name is flipped here and this is the input to the backward function.
 <code class="language-plaintext highlighter-rouge">Edge</code> objects are constructed using the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/variable.cpp#L221-L233"><code class="language-plaintext highlighter-rouge">gradient_edge</code></a>  function method.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">Edge</span> <span class="nf">gradient_edge</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">gradient</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">())</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">output_nr</span><span class="p">());</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">grad_accumulator</span><span class="p">(</span><span class="n">self</span><span class="p">),</span> <span class="mi">0</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

</code></pre></div></div>
<h2 id="entering-the-c-realm">Entering the C++ Realm</h2>

<p>Once that <code class="language-plaintext highlighter-rouge">torch.autograd.backward()</code> has been invoked, the
<a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/python_engine.cpp#L152-L286"><code class="language-plaintext highlighter-rouge">THPEngine_run_backward</code></a> routine starts the graph traversal. Following is a schema of the function body:</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PyObject</span> <span class="o">*</span><span class="nf">THPEngine_run_backward</span><span class="p">(</span><span class="n">PyObject</span> <span class="o">*</span><span class="n">self</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">PyObject</span> <span class="o">*</span><span class="n">kwargs</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">HANDLE_TH_ERRORS</span>
  <span class="n">PyObject</span> <span class="o">*</span><span class="n">tensors</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="n">PyObject</span> <span class="o">*</span><span class="n">grad_tensors</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
  <span class="kt">unsigned</span> <span class="kt">char</span> <span class="n">keep_graph</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="kt">unsigned</span> <span class="kt">char</span> <span class="n">create_graph</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">PyObject</span> <span class="o">*</span><span class="n">inputs</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
  
  <span class="c1">// Convert the python arguments to C++ objects</span>
  <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">accepted_kwargs</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="c1">// NOLINT</span>
      <span class="s">"tensors"</span><span class="p">,</span> <span class="s">"grad_tensors"</span><span class="p">,</span> <span class="s">"keep_graph"</span><span class="p">,</span> <span class="s">"create_graph"</span><span class="p">,</span> <span class="s">"inputs"</span><span class="p">,</span>
      <span class="s">"allow_unreachable"</span><span class="p">,</span> <span class="s">"accumulate_grad"</span><span class="p">,</span> <span class="nb">nullptr</span>
  <span class="p">};</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">PyArg_ParseTupleAndKeywords</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="s">"OObb|Obb"</span><span class="p">,</span> <span class="p">(</span><span class="kt">char</span><span class="o">**</span><span class="p">)</span><span class="n">accepted_kwargs</span><span class="p">,</span>
        <span class="o">&amp;</span><span class="n">tensors</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">grad_tensors</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">keep_graph</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">create_graph</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">allow_unreachable</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">accumulate_grad</span><span class="p">))</span>

 <span class="c1">// Prepare arguments</span>
 <span class="k">for</span><span class="p">(</span><span class="k">const</span> <span class="k">auto</span> <span class="n">i</span> <span class="o">:</span> <span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">num_tensors</span><span class="p">))</span> <span class="p">{</span>
   <span class="c1">// Check that the tensors require gradients</span>
  <span class="p">}</span>

  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Edge</span><span class="o">&gt;</span> <span class="n">output_edges</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">!=</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
     <span class="c1">// Prepare outputs</span>
  <span class="p">}</span>

  <span class="p">{</span>
      <span class="c1">// Calls the actual autograd engine</span>
    <span class="n">pybind11</span><span class="o">::</span><span class="n">gil_scoped_release</span> <span class="n">no_gil</span><span class="p">;</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="n">roots</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">keep_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">accumulate_grad</span><span class="p">,</span> <span class="n">output_edges</span><span class="p">);</span>
  <span class="p">}</span>
    <span class="c1">// Clean up and finish</span>
<span class="p">}</span>

</code></pre></div></div>

<p>First, we prepare the input arguments after converting the <code class="language-plaintext highlighter-rouge">PyObject</code> arguments to actual C++ objects. The <code class="language-plaintext highlighter-rouge">tensors</code> list contains the tensors from which we start the backward pass. These tensors are converted to edges using <code class="language-plaintext highlighter-rouge">torch::autograd::impl::gradient_edge</code> and added to a list called <code class="language-plaintext highlighter-rouge">roots</code> where the graph traversal starts.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">edge_list</span> <span class="n">roots</span><span class="p">;</span>
  <span class="n">roots</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">num_tensors</span><span class="p">);</span>
  <span class="n">variable_list</span> <span class="n">grads</span><span class="p">;</span>
  <span class="n">grads</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">num_tensors</span><span class="p">);</span>
  <span class="k">for</span><span class="p">(</span><span class="k">const</span> <span class="k">auto</span> <span class="n">i</span> <span class="o">:</span> <span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">num_tensors</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">PyObject</span> <span class="o">*</span><span class="n">_tensor</span> <span class="o">=</span> <span class="n">PyTuple_GET_ITEM</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
       <span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">variable</span> <span class="o">=</span> <span class="n">THPVariable_Unpack</span><span class="p">(</span><span class="n">_tensor</span><span class="p">);</span>
       <span class="k">auto</span> <span class="n">gradient_edge</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">gradient_edge</span><span class="p">(</span><span class="n">variable</span><span class="p">);</span>
     <span class="n">roots</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">));</span>

    <span class="n">PyObject</span> <span class="o">*</span><span class="n">grad</span> <span class="o">=</span> <span class="n">PyTuple_GET_ITEM</span><span class="p">(</span><span class="n">grad_tensors</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">THPVariable_Check</span><span class="p">(</span><span class="n">grad</span><span class="p">))</span> <span class="p">{</span>
      <span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">grad_var</span> <span class="o">=</span> <span class="n">THPVariable_Unpack</span><span class="p">(</span><span class="n">grad</span><span class="p">);</span>
      <span class="n">grads</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">grad_var</span><span class="p">);</span>
    <span class="p">}</span> 
  <span class="p">}</span>

</code></pre></div></div>

<p>Now, if the <code class="language-plaintext highlighter-rouge">inputs</code> argument was specified in <code class="language-plaintext highlighter-rouge">backward</code> or we used the <code class="language-plaintext highlighter-rouge">torch.autograd.grad</code> api, the following code creates a list of edges to accumulate the gradients in the specified tensors at the end of the computation. The engine uses this later to optimize the execution as it doesn’t add the gradients in all the leaf nodes, just the specified ones.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Edge</span><span class="o">&gt;</span> <span class="n">output_edges</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">!=</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">PyTuple_GET_SIZE</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
    <span class="n">output_edges</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span> <span class="n">i</span> <span class="o">:</span> <span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">))</span> <span class="p">{</span>
      <span class="n">PyObject</span> <span class="o">*</span><span class="n">input</span> <span class="o">=</span> <span class="n">PyTuple_GET_ITEM</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
      <span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">tensor</span> <span class="o">=</span> <span class="n">THPVariable_Unpack</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
      <span class="k">const</span> <span class="k">auto</span> <span class="n">output_nr</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">output_nr</span><span class="p">();</span>
      <span class="k">auto</span> <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">();</span>
      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">grad_fn</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">try_get_grad_accumulator</span><span class="p">(</span><span class="n">tensor</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">accumulate_grad</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">tensor</span><span class="p">.</span><span class="n">retain_grad</span><span class="p">();</span>
      <span class="p">}</span>
      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">grad_fn</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">output_edges</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Identity</span><span class="o">&gt;</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">output_edges</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">output_nr</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>

</code></pre></div></div>

<p>The next step is the actual graph traversal and node function execution, and finally, the cleanup and return.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="p">{</span>
    <span class="c1">// Calls the actual autograd engine</span>
    <span class="n">pybind11</span><span class="o">::</span><span class="n">gil_scoped_release</span> <span class="n">no_gil</span><span class="p">;</span>
    <span class="k">auto</span><span class="o">&amp;</span> <span class="n">engine</span> <span class="o">=</span> <span class="n">python</span><span class="o">::</span><span class="n">PythonEngine</span><span class="o">::</span><span class="n">get_python_engine</span><span class="p">();</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">engine</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="n">roots</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">keep_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">accumulate_grad</span><span class="p">,</span> <span class="n">output_edges</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="c1">// Clean up and finish</span>
<span class="err">}</span>

</code></pre></div></div>

<h1 id="starting-the-real-execution">Starting the Real Execution</h1>

<p><code class="language-plaintext highlighter-rouge">engine.execute</code>is present in <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L969-L1044">torch/csrc/autograd/engine.cpp</a></p>

<p>There are two differentiated steps here:</p>

<p>Analyze the graph to find dependencies between functions
Create worker threads that traverse the graph</p>

<h2 id="data-structures-used-for-the-execution">Data Structures Used for the Execution</h2>

<h3 id="graphtask">GraphTask</h3>

<p>All the execution metadata is managed by the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.h#L51-L196"><code class="language-plaintext highlighter-rouge">GraphTask</code></a> class in <a href="https://github.com/pytorch/pytorch/blob/release/1.11/torch/csrc/autograd/engine.h">torch/csrc/autograd/engine.h</a></p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">GraphTask</span><span class="o">:</span> <span class="n">std</span><span class="o">::</span><span class="n">enable_shared_from_this</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">uint64_t</span><span class="o">&gt;</span> <span class="n">outstanding_tasks_</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
  <span class="c1">//  … </span>
  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">*</span><span class="p">,</span> <span class="n">InputBuffer</span><span class="o">&gt;</span> <span class="n">not_ready_</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">dependencies_</span><span class="p">;</span>

  <span class="k">struct</span> <span class="nc">ExecInfo</span> <span class="p">{</span>
     <span class="c1">// …</span>
  <span class="p">};</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">*</span><span class="p">,</span> <span class="n">ExecInfo</span><span class="o">&gt;</span> <span class="n">exec_info_</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Variable</span><span class="o">&gt;</span> <span class="n">captured_vars_</span><span class="p">;</span>
  <span class="c1">// …</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ReadyQueue</span><span class="o">&gt;</span> <span class="n">cpu_ready_queue_</span><span class="p">;</span>
<span class="p">};</span>

</code></pre></div></div>

<p>Here we see a series of variables dedicated to maintaining the execution state.
<code class="language-plaintext highlighter-rouge">outstanding_tasks_</code> tracks the number of tasks left to be executed for the backward pass to complete. <code class="language-plaintext highlighter-rouge">not_ready_</code> holds the input arguments for the <code class="language-plaintext highlighter-rouge">Node</code>s that are not ready to be executed. <code class="language-plaintext highlighter-rouge">dependencies_</code> track the number of predecessors that a <code class="language-plaintext highlighter-rouge">Node</code> has. As the count reaches <code class="language-plaintext highlighter-rouge">0</code>,  the <code class="language-plaintext highlighter-rouge">Node</code> is ready for execution; it is placed in a ready queue to be retrieved and executed later.</p>

<p><code class="language-plaintext highlighter-rouge">exec_info_</code> and the associated <code class="language-plaintext highlighter-rouge">ExecInfo</code> struct are used only when the <code class="language-plaintext highlighter-rouge">inputs</code> argument is specified or it is a call to <code class="language-plaintext highlighter-rouge">autograd.grad()</code>. They allow filter paths on the graph that are not needeed since only the gradients are calculated only for the variables in the <code class="language-plaintext highlighter-rouge">inputs</code> list.</p>

<p><code class="language-plaintext highlighter-rouge">captured_vars_</code> is where the results of the graph execution are temporarily stored if we used the <code class="language-plaintext highlighter-rouge">torch.autograd.grad()</code> api instead of <code class="language-plaintext highlighter-rouge">torch.autograd.backward()</code> since <code class="language-plaintext highlighter-rouge">grad()</code> returns the gradients as tensors instead of just filling the <code class="language-plaintext highlighter-rouge">.grad</code> field of the inputs.</p>

<h3 id="nodetask">NodeTask</h3>

<p>The <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.h#L210-L242"><code class="language-plaintext highlighter-rouge">NodeTask</code></a> struct is a basic class that holds an <code class="language-plaintext highlighter-rouge">fn_</code> pointer to the node to execute, and an <code class="language-plaintext highlighter-rouge">inputs_</code> buffer to store the input arguments to this function. Note that the functions executed by the backward pass are the derivatives specified in the <code class="language-plaintext highlighter-rouge">derivatives.yaml</code> file. or the user provided backward function when using custom functions as described in the second blog post.</p>

<p>The <code class="language-plaintext highlighter-rouge">inputs_</code> buffer is also where the output gradients of the previously executed functions are aggregated, and it is defined as a <a href="https://github.com/pytorch/pytorch/blob/release/1.10/torch/csrc/autograd/input_buffer.h"><code class="language-plaintext highlighter-rouge">std::vector&lt;Variable&gt;</code> container</a> with facilities to accumulate values at a given position.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">NodeTask</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;</span> <span class="n">base_</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">fn_</span><span class="p">;</span>
  <span class="c1">// This buffer serves as an implicit "addition" node for all of the</span>
  <span class="c1">// gradients flowing here.  Once all the dependencies are finished, we</span>
  <span class="c1">// use the contents of this buffer to run the function.</span>
  <span class="n">InputBuffer</span> <span class="n">inputs_</span><span class="p">;</span>
<span class="p">};</span>

</code></pre></div></div>
<h3 id="graphroot">GraphRoot</h3>

<p>The <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/functions/basic_ops.h#L72-L89"><code class="language-plaintext highlighter-rouge">GraphRoot</code></a> is a special function used to hold multiple input variables in a single place. The code is pretty simple as it only acts as a container of variables.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">TORCH_API</span> <span class="n">GraphRoot</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Node</span> <span class="p">{</span>
  <span class="n">GraphRoot</span><span class="p">(</span><span class="n">edge_list</span> <span class="n">functions</span><span class="p">,</span> <span class="n">variable_list</span> <span class="n">inputs</span><span class="p">)</span>
      <span class="o">:</span> <span class="n">Node</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">functions</span><span class="p">)),</span>
      <span class="n">outputs</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">t</span> <span class="o">:</span> <span class="n">outputs</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">add_input_metadata</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="n">variable_list</span> <span class="n">apply</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span> <span class="n">inputs</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="p">;</span>
  <span class="p">}</span>

</code></pre></div></div>

<h3 id="accumulategrad">AccumulateGrad</h3>

<p>This function is set during the graph creation in <code class="language-plaintext highlighter-rouge">gradient_edge</code> when the <code class="language-plaintext highlighter-rouge">Variable</code> object doesn’t have a <code class="language-plaintext highlighter-rouge">grad_fn</code>. This is, it is a leaf node.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">if</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">gradient</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">())</span> <span class="p">{</span>
      <span class="c1">// …</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="k">return</span> <span class="n">Edge</span><span class="p">(</span><span class="n">grad_accumulator</span><span class="p">(</span><span class="n">self</span><span class="p">),</span> <span class="mi">0</span><span class="p">);</span>
    <span class="p">}</span>

</code></pre></div></div>

<p>The function body is defined in <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/functions/accumulate_grad.cpp#L25-L63"><code class="language-plaintext highlighter-rouge">torch/csrc/autograd/functions/accumulate_grad.cpp</code></a> and it essentially accumulates the input grads in the object’s <code class="language-plaintext highlighter-rouge">.grad</code> attribute.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">AccumulateGrad</span><span class="o">::</span><span class="n">apply</span><span class="p">(</span><span class="n">variable_list</span><span class="o">&amp;&amp;</span> <span class="n">grads</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">variable_list</span> <span class="p">{</span>
  <span class="n">check_input_variables</span><span class="p">(</span><span class="s">"AccumulateGrad"</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="err">…</span>

  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">new_grad</span> <span class="o">=</span> <span class="n">callHooks</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
  <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>

  <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">variable</span><span class="p">.</span><span class="n">mutable_grad</span><span class="p">();</span>
  <span class="n">accumulateGrad</span><span class="p">(</span>
      <span class="n">variable</span><span class="p">,</span>
      <span class="n">grad</span><span class="p">,</span>
      <span class="n">new_grad</span><span class="p">,</span>
      <span class="mi">1</span> <span class="o">+</span> <span class="o">!</span><span class="n">post_hooks</span><span class="p">().</span><span class="n">empty</span><span class="p">()</span> <span class="cm">/* num_expected_refs */</span><span class="p">,</span>
      <span class="p">[</span><span class="o">&amp;</span><span class="n">grad</span><span class="p">](</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;&amp;</span> <span class="n">grad_update</span><span class="p">)</span> <span class="p">{</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">grad_update</span><span class="p">);</span> <span class="p">});</span>
  <span class="k">return</span> <span class="n">variable_list</span><span class="p">();</span>
<span class="p">}</span>
<span class="err">}}</span> <span class="c1">// namespace torch::autograd</span>



</code></pre></div></div>

<p><a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/functions/accumulate_grad.h#L100"><code class="language-plaintext highlighter-rouge">accumulateGrad</code></a>
does several checks on the tensors format and eventually performs the <code class="language-plaintext highlighter-rouge">variable_grad += new_grad;</code> accumulation.</p>

<h2 id="preparing-the-graph-for-execution">Preparing the graph for execution</h2>

<p>Now, let’s walk through <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L969-L1126"><code class="language-plaintext highlighter-rouge">Engine::execute</code></a>. The first thing to do besides arguments consistency checks is to create the actual <code class="language-plaintext highlighter-rouge">GraphTask</code> object we described above. This object keeps all the metadata of the graph execution.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">Engine</span><span class="o">::</span><span class="n">execute</span><span class="p">(</span><span class="k">const</span> <span class="n">edge_list</span><span class="o">&amp;</span> <span class="n">roots</span><span class="p">,</span>
                     <span class="k">const</span> <span class="n">variable_list</span><span class="o">&amp;</span> <span class="n">inputs</span><span class="p">,</span>
                     <span class="kt">bool</span> <span class="n">keep_graph</span><span class="p">,</span>
                     <span class="kt">bool</span> <span class="n">create_graph</span><span class="p">,</span>
                     <span class="kt">bool</span> <span class="n">accumulate_grad</span><span class="p">,</span>
                     <span class="k">const</span> <span class="n">edge_list</span><span class="o">&amp;</span> <span class="n">outputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">variable_list</span> <span class="p">{</span>

  <span class="n">validate_outputs</span><span class="p">(</span><span class="n">roots</span><span class="p">,</span> <span class="k">const_cast</span><span class="o">&lt;</span><span class="n">variable_list</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="p">[](</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">msg</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">msg</span><span class="p">;</span>
  <span class="p">});</span>

  <span class="c1">// Checks</span>

  <span class="k">auto</span> <span class="n">graph_task</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;</span><span class="p">(</span>
      <span class="cm">/* keep_graph */</span> <span class="n">keep_graph</span><span class="p">,</span>
      <span class="cm">/* create_graph */</span> <span class="n">create_graph</span><span class="p">,</span>
      <span class="cm">/* depth */</span> <span class="n">not_reentrant_backward_call</span> <span class="o">?</span> <span class="mi">0</span> <span class="o">:</span> <span class="n">total_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
      <span class="cm">/* cpu_ready_queue */</span> <span class="n">local_ready_queue</span><span class="p">);</span>

  <span class="c1">// If we receive a single root, skip creating extra root node</span>
  <span class="c1">// …</span>
  <span class="c1">// Prepare graph by computing dependencies</span>
  <span class="c1">// …</span>
  <span class="c1">// Queue the root </span>
  <span class="c1">// …</span>
  <span class="c1">// launch execution</span>
  <span class="c1">// …</span>
<span class="p">}</span>

</code></pre></div></div>

<p>After creating the <code class="language-plaintext highlighter-rouge">GraphTask</code>, we use its associated function if we only have one root node. If we have multiple root nodes, we create a special <code class="language-plaintext highlighter-rouge">GraphRoot</code> object as described before.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kt">bool</span> <span class="n">skip_dummy_node</span> <span class="o">=</span> <span class="n">roots</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">;</span>
  <span class="k">auto</span> <span class="n">graph_root</span> <span class="o">=</span> <span class="n">skip_dummy_node</span> <span class="o">?</span>
    <span class="n">roots</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">function</span> <span class="o">:</span>
    <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">GraphRoot</span><span class="o">&gt;</span><span class="p">(</span><span class="n">roots</span><span class="p">,</span> <span class="n">inputs</span><span class="p">);</span>

</code></pre></div></div>

<p>The next step is to fill the <code class="language-plaintext highlighter-rouge">dependencies_</code> map in the <code class="language-plaintext highlighter-rouge">GraphTask</code> object since the engine must know when it can execute a task. The <code class="language-plaintext highlighter-rouge">outputs</code> here is the <code class="language-plaintext highlighter-rouge">inputs</code> argument passed to the <code class="language-plaintext highlighter-rouge">torch.autograd.backward()</code> call in Python. But here, we have reversed the names since the gradients w.r.t. the inputs of the forward pass are now the outputs of the backward pass. And from now on, there is no concept of forward/backward, but only graph traversal and execution.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">auto</span> <span class="n">min_topo_nr</span> <span class="o">=</span> <span class="n">compute_min_topological_nr</span><span class="p">(</span><span class="n">outputs</span><span class="p">);</span>
  <span class="c1">// Now compute the dependencies for all executable functions</span>
  <span class="n">compute_dependencies</span><span class="p">(</span><span class="n">graph_root</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="o">*</span><span class="n">graph_task</span><span class="p">,</span> <span class="n">min_topo_nr</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">outputs</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">init_to_execute</span><span class="p">(</span><span class="o">*</span><span class="n">graph_root</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">accumulate_grad</span><span class="p">,</span> <span class="n">min_topo_nr</span><span class="p">);</span>
  <span class="p">}</span>

</code></pre></div></div>

<p>Here we preprocess the graph for the execution of the nodes. First, <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L922-L933"><code class="language-plaintext highlighter-rouge">compute_min_topological_nr</code></a> is called to to obtain the minimum topological number of the tensors specified in <code class="language-plaintext highlighter-rouge">outputs</code> (0 if no <code class="language-plaintext highlighter-rouge">inputs</code> kwarg was supplied to <code class="language-plaintext highlighter-rouge">.backward</code> or <code class="language-plaintext highlighter-rouge">input</code> for <code class="language-plaintext highlighter-rouge">.grad</code>). This computation prunes paths in the graph that lead to input variables of which we don’t want/need to calculate the grads.</p>

<p>Second, is the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L935-L967"><code class="language-plaintext highlighter-rouge">compute_dependencies</code></a> call. This function is a very simple graph traversal that starts with the root <code class="language-plaintext highlighter-rouge">Node</code>, and for each of the edges in <code class="language-plaintext highlighter-rouge">node.next_edges()</code> it increments the counter in <code class="language-plaintext highlighter-rouge">dependencies_</code>. Figure 3 shows the result of the dependencies calculation for the example graph. Note that the number of dependencies of any node is just the number of edges arriving at it.</p>

<p align="center">
  <img src="/assets/images/node-fig-3.png" width="100%" />
</p>

<p align="center">
Figure 3:  Number of dependencies for each node
</p>

<p>Finally, the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L1281-L1383"><code class="language-plaintext highlighter-rouge">init_to_execute</code></a> call, this is the one that populates the <code class="language-plaintext highlighter-rouge">GraphTask::exec_info_</code> map in case that <code class="language-plaintext highlighter-rouge">inputs</code> were specified in the python <code class="language-plaintext highlighter-rouge">backward</code> call. It iterates the graph again, starting from the root, and records in the <code class="language-plaintext highlighter-rouge">exec_info_</code> map the intermediate nodes needed to calculate only the given <code class="language-plaintext highlighter-rouge">inputs</code> gradients.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">// Queue the root</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">skip_dummy_node</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">InputBuffer</span> <span class="n">input_buffer</span><span class="p">(</span><span class="n">roots</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">function</span><span class="o">-&gt;</span><span class="n">num_inputs</span><span class="p">());</span>
    <span class="k">auto</span> <span class="n">input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>


    <span class="n">input_buffer</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">roots</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">input_nr</span><span class="p">,</span>
                      <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input</span><span class="p">),</span>
                      <span class="n">input_stream</span><span class="p">,</span>
                      <span class="n">opt_next_stream</span><span class="p">);</span>

    <span class="n">execute_with_graph_task</span><span class="p">(</span><span class="n">graph_task</span><span class="p">,</span> <span class="n">graph_root</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_buffer</span><span class="p">));</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">execute_with_graph_task</span><span class="p">(</span><span class="n">graph_task</span><span class="p">,</span> <span class="n">graph_root</span><span class="p">,</span> <span class="n">InputBuffer</span><span class="p">(</span><span class="n">variable_list</span><span class="p">()));</span>
  <span class="p">}</span>
  <span class="c1">// Avoid a refcount bump for the Future, since we check for refcount in</span>
  <span class="c1">// DistEngine (see TORCH_INTERNAL_ASSERT(futureGrads.use_count() == 1)</span>
  <span class="c1">// in dist_engine.cpp).</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">fut</span> <span class="o">=</span> <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">future_result_</span><span class="p">;</span>
  <span class="n">fut</span><span class="o">-&gt;</span><span class="n">wait</span><span class="p">();</span>
  <span class="k">return</span> <span class="n">fut</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">().</span><span class="n">toTensorVector</span><span class="p">();</span>
<span class="err">}</span>

</code></pre></div></div>

<p>And now, we are ready to start the actual execution by creating the <code class="language-plaintext highlighter-rouge">InputBuffer</code>. In case we only have one root variable, we begin by copying the value of the <code class="language-plaintext highlighter-rouge">inputs</code> tensor (this is the <code class="language-plaintext highlighter-rouge">gradients</code> passed to python <code class="language-plaintext highlighter-rouge">backward</code>) in position 0 of the input_buffer. This is a small optimization that avoids running the <code class="language-plaintext highlighter-rouge">RootNode</code> for no reason. Also, if the rest of the graph is not on the cpu, we directly start on that worker while the <code class="language-plaintext highlighter-rouge">RootNode</code> is always placed on the cpu ready queue. Details of the workers and ready queues are explained in the section below.</p>

<p>On the other hand, if we have multiple roots, the <code class="language-plaintext highlighter-rouge">GraphRoot</code> object also holds the inputs, so it is enough to pass it an empty <code class="language-plaintext highlighter-rouge">InputBuffer</code>.</p>

<h2 id="graph-traversal-and-node-execution">Graph Traversal and Node Execution</h2>
<h3 id="devices-threads-and-queues">Devices, Threads and Queues</h3>

<p>Before diving into the actual execution, we need to see how the engine is structured.</p>

<p>First of all, the engine is multithreaded with one thread per device. For example, the caller thread is associated with the CPU while additional threads are created and associated with each GPU or other devices available in the system. Each thread tracks its device using thread-local storage in the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L69"><code class="language-plaintext highlighter-rouge">worker_device</code></a> variable. In addition, the threads have a queue of tasks to be executed also located in thread-local storage, the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L103-L104"><code class="language-plaintext highlighter-rouge">local_ready_queue</code></a>. This is where work is queued for this thread to execute in the <code class="language-plaintext highlighter-rouge">thread_main</code> function that is explained later.
You will wonder how the device where a task should be executed is decided. The <code class="language-plaintext highlighter-rouge">InputBuffer</code> class has a <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/input_buffer.cpp#L173-L189"><code class="language-plaintext highlighter-rouge">device()</code></a> function that returns the first non-cpu device of all its tensors.
This function is used together with <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L1181-L1190"><code class="language-plaintext highlighter-rouge">Engine::ready_queue</code></a> to select the queue to queue a task.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">Engine</span><span class="o">::</span><span class="n">ready_queue</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ReadyQueue</span><span class="o">&gt;</span> <span class="n">cpu_ready_queue</span><span class="p">,</span> <span class="n">at</span><span class="o">::</span><span class="n">Device</span> <span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ReadyQueue</span><span class="o">&gt;</span><span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="p">.</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">at</span><span class="o">::</span><span class="n">kCPU</span> <span class="o">||</span> <span class="n">device</span><span class="p">.</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">at</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">Meta</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">cpu_ready_queue</span><span class="p">;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="c1">// See Note [Allocating GPUs to autograd threads]</span>
    <span class="k">return</span> <span class="n">device_ready_queues_</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">device</span><span class="p">.</span><span class="n">index</span><span class="p">());</span>
  <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>

<p>The <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.h#L245-L283"><code class="language-plaintext highlighter-rouge">ReadyQueue</code></a> object is defined in <code class="language-plaintext highlighter-rouge">torch/csrc/autograd/engine.h</code> and it is a simple wrapper over <code class="language-plaintext highlighter-rouge">std::priority_queue</code> that allows a thread to <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L219">wait for a task</a> if it’s empty. One interesting property of the <code class="language-plaintext highlighter-rouge">ReadyQueue</code> is that it increases the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L195"><code class="language-plaintext highlighter-rouge">GraphTask::outstanding_tasks_</code></a> value used to determine if the execution has completed or not.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">ReadyQueue</span><span class="o">::</span><span class="n">push</span><span class="p">(</span><span class="n">NodeTask</span> <span class="n">item</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">incrementOutstandingTasks</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kt">void</span> <span class="p">{</span>
  <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">incrementOutstandingTasks</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;</span> <span class="n">graph_task</span> <span class="o">=</span> <span class="n">item</span><span class="p">.</span><span class="n">base_</span><span class="p">.</span><span class="n">lock</span><span class="p">();</span>
      <span class="o">++</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">outstanding_tasks_</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">heap_</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">item</span><span class="p">));</span>
  <span class="p">}</span>
  <span class="n">not_empty_</span><span class="p">.</span><span class="n">notify_one</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">auto</span> <span class="n">ReadyQueue</span><span class="o">::</span><span class="n">pop</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">NodeTask</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>
  <span class="n">not_empty_</span><span class="p">.</span><span class="n">wait</span><span class="p">(</span><span class="n">lock</span><span class="p">,</span> <span class="p">[</span><span class="k">this</span><span class="p">]{</span> <span class="k">return</span> <span class="o">!</span><span class="n">heap_</span><span class="p">.</span><span class="n">empty</span><span class="p">();</span> <span class="p">});</span>
  <span class="k">auto</span> <span class="n">task</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">NodeTask</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="n">heap_</span><span class="p">.</span><span class="n">top</span><span class="p">()));</span> <span class="n">heap_</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
  <span class="k">return</span> <span class="n">task</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div></div>

<h3 id="reentrant-backward">Reentrant Backward</h3>

<p>A reentrant backward happens when one of the tasks in a backward pass calls again <code class="language-plaintext highlighter-rouge">backward</code>. It is not a very common case, but it can be used to reduce memory utilization as it could potentially avoid saving intermediate results. For more information, check this <a href="https://discuss.pytorch.org/t/what-is-the-scenario-of-reentrant-backwards-in-pytorch-source-code/19330/2">PyTorch forum post</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ReentrantBackward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">input</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Let's compute the backward by using autograd
</span>        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">enable_grad</span><span class="p">():</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># REENTRANT CALL!!
</span>        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>

</code></pre></div></div>

<p>Here, we call <code class="language-plaintext highlighter-rouge">backward()</code> inside <code class="language-plaintext highlighter-rouge">backward()</code> for a user custom-defined autograd function.
This situation can lead to deadlocks because the first backward needs to wait for the second one to complete. But some internal implementation details can prevent the second backward from completing as it is explained in the dedicated subsection.</p>
<h2 id="thread-initialization">Thread Initialization</h2>

<p><a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L1054-L1126"><code class="language-plaintext highlighter-rouge">execute_with_graph_task</code></a> is in charge of initializing the threads taking care of the computation and placing the <code class="language-plaintext highlighter-rouge">root</code> node in the queue of the device that produced it.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c10</span><span class="o">::</span><span class="n">intrusive_ptr</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ivalue</span><span class="o">::</span><span class="n">Future</span><span class="o">&gt;</span> <span class="n">Engine</span><span class="o">::</span><span class="n">execute_with_graph_task</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;&amp;</span> <span class="n">graph_task</span><span class="p">,</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">graph_root</span><span class="p">,</span>
    <span class="n">InputBuffer</span><span class="o">&amp;&amp;</span> <span class="n">input_buffer</span><span class="p">)</span> <span class="p">{</span>

  <span class="n">initialize_device_threads_pool</span><span class="p">();</span>
  <span class="c1">// Lock mutex for GraphTask.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">mutex_</span><span class="p">);</span>

  <span class="k">auto</span> <span class="n">queue</span> <span class="o">=</span> <span class="n">ready_queue</span><span class="p">(</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">cpu_ready_queue_</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">.</span><span class="n">device</span><span class="p">());</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">worker_device</span> <span class="o">==</span> <span class="n">NO_DEVICE</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">set_device</span><span class="p">(</span><span class="n">CPU_DEVICE</span><span class="p">);</span>
    <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">owner_</span> <span class="o">=</span> <span class="n">worker_device</span><span class="p">;</span>
    <span class="n">queue</span><span class="o">-&gt;</span><span class="n">push</span><span class="p">(</span><span class="n">NodeTask</span><span class="p">(</span><span class="n">graph_task</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">graph_root</span><span class="p">),</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_buffer</span><span class="p">)));</span>
    <span class="n">lock</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
    <span class="n">thread_main</span><span class="p">(</span><span class="n">graph_task</span><span class="p">);</span>
    <span class="n">worker_device</span> <span class="o">=</span> <span class="n">NO_DEVICE</span><span class="p">;</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
     <span class="c1">// This deals with reentrant backwards, we will see it later.</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">future_result_</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div></div>

<p>First, this function initializes several threads (one per device) calling <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L1046-L1052"><code class="language-plaintext highlighter-rouge"> initialize_device_threads_pool()</code></a> where several things happen:
One <code class="language-plaintext highlighter-rouge">ReadyQueue</code> per device is created.
One thread per non-cpu device is created.
A thread local <code class="language-plaintext highlighter-rouge">worker_device</code> variable is set to track the current device associated with the thread.
<code class="language-plaintext highlighter-rouge">thread_main</code> function is called, and threads wait for tasks to be put in their queues.</p>

<p>Then it retrieves the queue to place the root node based on the device that holds the tensors present in the <code class="language-plaintext highlighter-rouge">input_buffer</code> using the <code class="language-plaintext highlighter-rouge">ready_queue</code> function. Now, the main thread (the one also executing the Python interpreter) has its <code class="language-plaintext highlighter-rouge">worker_device</code> set to <code class="language-plaintext highlighter-rouge">NO_DEVICE</code>, and it is in charge of executing functions with all its tensors living in the cpu. If <code class="language-plaintext highlighter-rouge">worker_device</code> is set to any other value, the graph execution is already started, and <code class="language-plaintext highlighter-rouge">.backward()</code> was called inside a running <code class="language-plaintext highlighter-rouge">Node</code>, creating a reentrant backward call. This is explained later. For now, 
the main thread places the task in the queue and call <code class="language-plaintext highlighter-rouge">thread_main</code>.</p>
<h2 id="where-the-magic-happens">Where the Magic Happens</h2>

<p>It’s been a long way, but finally, we are ready to traverse the graph and execute the nodes. Each of the spawned threads, and the main thread call <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L377-L464"><code class="language-plaintext highlighter-rouge">thread_main</code></a>.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">auto</span> <span class="n">Engine</span><span class="o">::</span><span class="n">thread_main</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;&amp;</span> <span class="n">graph_task</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kt">void</span> <span class="p">{</span>

  <span class="k">while</span> <span class="p">(</span><span class="n">graph_task</span> <span class="o">==</span> <span class="nb">nullptr</span> <span class="o">||</span> <span class="o">!</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">future_result_</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">())</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;</span> <span class="n">local_graph_task</span><span class="p">;</span>
    <span class="p">{</span>
      <span class="n">NodeTask</span> <span class="n">task</span> <span class="o">=</span> <span class="n">local_ready_queue</span><span class="o">-&gt;</span><span class="n">pop</span><span class="p">();</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">task</span><span class="p">.</span><span class="n">isShutdownTask_</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">break</span><span class="p">;</span>
      <span class="p">}</span>

      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">local_graph_task</span> <span class="o">=</span> <span class="n">task</span><span class="p">.</span><span class="n">base_</span><span class="p">.</span><span class="n">lock</span><span class="p">()))</span> <span class="p">{</span>
        <span class="c1">// GraphTask for function is no longer valid, skipping further</span>
        <span class="c1">// execution.</span>
        <span class="k">continue</span><span class="p">;</span>
      <span class="p">}</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">task</span><span class="p">.</span><span class="n">fn_</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">has_error_</span><span class="p">.</span><span class="n">load</span><span class="p">())</span> <span class="p">{</span>
        <span class="n">at</span><span class="o">::</span><span class="n">ThreadLocalStateGuard</span> <span class="n">tls_guard</span><span class="p">(</span><span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">thread_locals_</span><span class="p">);</span>

        <span class="k">try</span> <span class="p">{</span>
          <span class="n">GraphTaskGuard</span> <span class="n">guard</span><span class="p">(</span><span class="n">local_graph_task</span><span class="p">);</span>
          <span class="n">NodeGuard</span> <span class="n">ndguard</span><span class="p">(</span><span class="n">task</span><span class="p">.</span><span class="n">fn_</span><span class="p">);</span>
          <span class="p">{</span>
            <span class="n">evaluate_function</span><span class="p">(</span>
                <span class="n">local_graph_task</span><span class="p">,</span>
                <span class="n">task</span><span class="p">.</span><span class="n">fn_</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="n">task</span><span class="p">.</span><span class="n">inputs_</span><span class="p">,</span>
                <span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">cpu_ready_queue_</span><span class="p">);</span>
          <span class="p">}</span>
        <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">e</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">thread_on_exception</span><span class="p">(</span><span class="n">local_graph_task</span><span class="p">,</span> <span class="n">task</span><span class="p">.</span><span class="n">fn_</span><span class="p">,</span> <span class="n">e</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="c1">// Decrement the outstanding tasks.</span>
    <span class="o">--</span><span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">outstanding_tasks_</span><span class="p">;</span>

    <span class="c1">// Check if we've completed execution.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">mark_as_completed_and_run_post_processing</span><span class="p">();</span>
      <span class="k">auto</span> <span class="n">base_owner</span> <span class="o">=</span> <span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">owner_</span><span class="p">;</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">worker_device</span> <span class="o">!=</span> <span class="n">base_owner</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">atomic_thread_fence</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_release</span><span class="p">);</span>
        <span class="n">ready_queue_by_index</span><span class="p">(</span><span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">cpu_ready_queue_</span><span class="p">,</span> <span class="n">base_owner</span><span class="p">)</span>
            <span class="o">-&gt;</span><span class="n">push</span><span class="p">(</span><span class="n">NodeTask</span><span class="p">(</span><span class="n">local_graph_task</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">,</span> <span class="n">InputBuffer</span><span class="p">(</span><span class="mi">0</span><span class="p">)));</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>

<p>The code here is simple, given the <code class="language-plaintext highlighter-rouge">local_ready_queue</code> assigned to each thread in thread-local storage. The threads loop until there are no tasks left to execute in the graph. Note that for device-associated threads, the passed <code class="language-plaintext highlighter-rouge">graph_task</code> argument is <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L326-L327"><code class="language-plaintext highlighter-rouge">nullptr</code></a>, and they block in <code class="language-plaintext highlighter-rouge">local_ready_queue-&gt;pop()</code> until a task is pushed in their queue. After some consistency checks (the task type is shutdown, or the graph is still valid). We get to the actual function invocation in <code class="language-plaintext highlighter-rouge">evaluate_function</code>.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="k">try</span> <span class="p">{</span>
          <span class="n">GraphTaskGuard</span> <span class="n">guard</span><span class="p">(</span><span class="n">local_graph_task</span><span class="p">);</span>
          <span class="n">NodeGuard</span> <span class="n">ndguard</span><span class="p">(</span><span class="n">task</span><span class="p">.</span><span class="n">fn_</span><span class="p">);</span>
          <span class="p">{</span>
            <span class="n">evaluate_function</span><span class="p">(</span>
                <span class="n">local_graph_task</span><span class="p">,</span>
                <span class="n">task</span><span class="p">.</span><span class="n">fn_</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="n">task</span><span class="p">.</span><span class="n">inputs_</span><span class="p">,</span>
                <span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">cpu_ready_queue_</span><span class="p">);</span>
          <span class="p">}</span>
        <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">e</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">thread_on_exception</span><span class="p">(</span><span class="n">local_graph_task</span><span class="p">,</span> <span class="n">task</span><span class="p">.</span><span class="n">fn_</span><span class="p">,</span> <span class="n">e</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="err">}</span>

</code></pre></div></div>

<p>After calling <code class="language-plaintext highlighter-rouge">evaluate_function</code>, we check if the <code class="language-plaintext highlighter-rouge">graph_task</code> execution is complete by looking the <code class="language-plaintext highlighter-rouge">outstanding_tasks_</code> number. This number increases when a task is pushed to a queue and is decreased in <code class="language-plaintext highlighter-rouge">local_graph_task-&gt;completed()</code> when a task is executed. When the execution is done, we return the results that are be in the <code class="language-plaintext highlighter-rouge">captured_vars_</code> in case we called <code class="language-plaintext highlighter-rouge">torch.autograd.grad()</code> instead of <code class="language-plaintext highlighter-rouge">torch.autograd.backward()</code> as this function returns tensors instead of storing them in the <code class="language-plaintext highlighter-rouge">.grad</code> attribute of the inputs. Finally we wake up the main thread if it’s waiting by sending a dummy task.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="c1">// Decrement the outstanding tasks.</span>
    <span class="o">--</span><span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">outstanding_tasks_</span><span class="p">;</span>

    <span class="c1">// Check if we've completed execution.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">completed</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">mark_as_completed_and_run_post_processing</span><span class="p">();</span>
      <span class="k">auto</span> <span class="n">base_owner</span> <span class="o">=</span> <span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">owner_</span><span class="p">;</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">worker_device</span> <span class="o">!=</span> <span class="n">base_owner</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">atomic_thread_fence</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">memory_order_release</span><span class="p">);</span>
        <span class="n">ready_queue_by_index</span><span class="p">(</span><span class="n">local_graph_task</span><span class="o">-&gt;</span><span class="n">cpu_ready_queue_</span><span class="p">,</span> <span class="n">base_owner</span><span class="p">)</span>
            <span class="o">-&gt;</span><span class="n">push</span><span class="p">(</span><span class="n">NodeTask</span><span class="p">(</span><span class="n">local_graph_task</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">,</span> <span class="n">InputBuffer</span><span class="p">(</span><span class="mi">0</span><span class="p">)));</span>
      <span class="p">}</span>
    <span class="p">}</span>

</code></pre></div></div>

<h2 id="calling-the-function-and-unlocking-new-tasks">Calling the Function and Unlocking New Tasks</h2>

<p><a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L786-L920"><code class="language-plaintext highlighter-rouge">evaluate_function</code></a> serves three purposes:</p>

<p>Run the function.
Accumulate its results in the next node <code class="language-plaintext highlighter-rouge">InputBuffers</code>.
Decrease the dependencies counter of the next nodes and enqueues the tasks reaching 0 to be executed.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">Engine</span><span class="o">::</span><span class="n">evaluate_function</span><span class="p">(</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;&amp;</span> <span class="n">graph_task</span><span class="p">,</span>
    <span class="n">Node</span><span class="o">*</span> <span class="n">func</span><span class="p">,</span>
    <span class="n">InputBuffer</span><span class="o">&amp;</span> <span class="n">inputs</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ReadyQueue</span><span class="o">&gt;&amp;</span> <span class="n">cpu_ready_queue</span><span class="p">)</span> <span class="p">{</span>

  <span class="c1">// If exec_info_ is not empty, we have to instrument the execution</span>
  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">exec_info_</span> <span class="o">=</span> <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">exec_info_</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">exec_info_</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
    <span class="c1">// Checks if the function needs to be executed </span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">fn_info</span><span class="p">.</span><span class="n">needed_</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">// Skip execution if we don't need to execute the function.</span>
      <span class="k">return</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">auto</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">call_function</span><span class="p">(</span><span class="n">graph_task</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">);</span>

  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">fn</span> <span class="o">=</span> <span class="o">*</span><span class="n">func</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">keep_graph_</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">fn</span><span class="p">.</span><span class="n">release_variables</span><span class="p">();</span>
  <span class="p">}</span>

</code></pre></div></div>

<p>Initially, we check the <code class="language-plaintext highlighter-rouge">exec_info_</code> map of the <code class="language-plaintext highlighter-rouge">GraphTask</code> structure to determine if the current node needs to be executed. Remember that if this map is empty, all the nodes are executed because we are calculating the grads for all the inputs of the forward pass.</p>

<p>After this check, the function is executed by running <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L735-L784"><code class="language-plaintext highlighter-rouge">call_function</code></a>. Its implementation is very straightforward and calls the actual derivative function and registered hooks if any.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kt">int</span> <span class="n">num_outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">num_outputs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Records leaf stream (if applicable)</span>
    <span class="k">return</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">AnomalyMode</span><span class="o">::</span><span class="n">is_enabled</span><span class="p">())</span> <span class="p">{</span>
    <span class="c1">// check for nan values in result</span>
  <span class="p">}</span>

</code></pre></div></div>

<p>Next, we check the outputs of the function after <code class="language-plaintext highlighter-rouge">call_function</code> is done. If the number of outputs is 0, there are no following nodes to be executed so we can safely return. This is the case of the <code class="language-plaintext highlighter-rouge">AccumulateGrad</code> node associated with the leaf nodes.</p>

<p>Also, the check for <code class="language-plaintext highlighter-rouge">NaN</code> values in the gradients is done here if requested.</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
  <span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">mutex_</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">const</span> <span class="k">auto</span> <span class="n">i</span> <span class="o">:</span> <span class="n">c10</span><span class="o">::</span><span class="n">irange</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">auto</span><span class="o">&amp;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="k">const</span> <span class="k">auto</span><span class="o">&amp;</span> <span class="n">next</span> <span class="o">=</span> <span class="n">fn</span><span class="p">.</span><span class="n">next_edge</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">next</span><span class="p">.</span><span class="n">is_valid</span><span class="p">())</span> <span class="k">continue</span><span class="p">;</span>

   

</code></pre></div></div>

<p>We have now executed a <code class="language-plaintext highlighter-rouge">grad_fn</code> that has returned one gradient per each of the associated forward pass function inputs. As we saw in the <a href="https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/#linking-nodes-together">previous blog post</a>, we have an <code class="language-plaintext highlighter-rouge">Edge</code> object per each of these input tensors, and the <code class="language-plaintext highlighter-rouge">grad_fn</code> of the function producing them in the forward pass. Essentially, Output[0] of the node in the backward pass, corresponds to the first argument of the forward pass associated function. Figure 4 shows how the outputs of a backward function are related to the inputs of the forward function. See that the outputs of <code class="language-plaintext highlighter-rouge">grad_fn C</code> are the gradients of <code class="language-plaintext highlighter-rouge">z</code> w.r.t. the inputs of <code class="language-plaintext highlighter-rouge">Function C</code></p>

<p align="center">
  <img src="/assets/images/forward-backward-function-fig-4.png" width="100%" />
</p>

<p align="center">
Figure 4: Correspondence between forward and backward functions inputs and outputs
</p>

<p>We now iterate through these edges and check if the associated functions are ready to be executed.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// Check if the next function is ready to be computed</span>
    <span class="kt">bool</span> <span class="n">is_ready</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
    <span class="k">auto</span><span class="o">&amp;</span> <span class="n">dependencies</span> <span class="o">=</span> <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">dependencies_</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">it</span> <span class="o">=</span> <span class="n">dependencies</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">next</span><span class="p">.</span><span class="n">function</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">it</span> <span class="o">==</span> <span class="n">dependencies</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span>
      <span class="k">auto</span> <span class="n">name</span> <span class="o">=</span> <span class="n">next</span><span class="p">.</span><span class="n">function</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">();</span>
      <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">"dependency not found for "</span><span class="p">)</span> <span class="o">+</span> <span class="n">name</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="o">--</span><span class="n">it</span><span class="o">-&gt;</span><span class="n">second</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">dependencies</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">it</span><span class="p">);</span>
      <span class="n">is_ready</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">auto</span><span class="o">&amp;</span> <span class="n">not_ready</span> <span class="o">=</span> <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">not_ready_</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">not_ready_it</span> <span class="o">=</span> <span class="n">not_ready</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">next</span><span class="p">.</span><span class="n">function</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>

</code></pre></div></div>

<p>For this, we check the <code class="language-plaintext highlighter-rouge">graph_task-&gt;dependencies_</code> map. We decrement the counter, and if it reaches 0, we mark the function pointed by the edge ready to be executed. Following, we prepare the input buffers of the tasks indicated by the next edges.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">if</span> <span class="p">(</span><span class="n">not_ready_it</span> <span class="o">==</span> <span class="n">not_ready</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">exec_info_</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
        <span class="c1">// Skip functions that aren't supposed to be executed</span>
      <span class="p">}</span>

      <span class="c1">// Creates an InputBuffer and moves the output to the corresponding input position</span>
      <span class="n">InputBuffer</span> <span class="n">input_buffer</span><span class="p">(</span><span class="n">next</span><span class="p">.</span><span class="n">function</span><span class="o">-&gt;</span><span class="n">num_inputs</span><span class="p">());</span>
      <span class="n">input_buffer</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">next</span><span class="p">.</span><span class="n">input_nr</span><span class="p">,</span>
                       <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
                       <span class="n">opt_parent_stream</span><span class="p">,</span>
                       <span class="n">opt_next_stream</span><span class="p">);</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">is_ready</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">auto</span> <span class="n">queue</span> <span class="o">=</span> <span class="n">ready_queue</span><span class="p">(</span><span class="n">cpu_ready_queue</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">.</span><span class="n">device</span><span class="p">());</span>
        <span class="n">queue</span><span class="o">-&gt;</span><span class="n">push</span><span class="p">(</span>
            <span class="n">NodeTask</span><span class="p">(</span><span class="n">graph_task</span><span class="p">,</span> <span class="n">next</span><span class="p">.</span><span class="n">function</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_buffer</span><span class="p">)));</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">not_ready</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">next</span><span class="p">.</span><span class="n">function</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_buffer</span><span class="p">));</span>
      <span class="p">}</span>

</code></pre></div></div>

<p>Here, we look for the task in the <code class="language-plaintext highlighter-rouge">graph_task-&gt;not_ready_</code> map. If it is not present, we create a new <code class="language-plaintext highlighter-rouge">InputBuffer</code> object and set the current output in the <code class="language-plaintext highlighter-rouge">input_nr</code> position of the buffer associated with the edge. If the task is ready to be executed, we enqueue it in the appropriate device <code class="language-plaintext highlighter-rouge">ready_queue</code> and complete the execution. However, if the task is not ready and we have seen it before, it is present in the <code class="language-plaintext highlighter-rouge">not_ready_map_</code>.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="err">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="c1">// The function already has a buffer</span>
      <span class="k">auto</span> <span class="o">&amp;</span><span class="n">input_buffer</span> <span class="o">=</span> <span class="n">not_ready_it</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">;</span>
      <span class="c1">// Accumulates into buffer</span>
      <span class="n">input_buffer</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">next</span><span class="p">.</span><span class="n">input_nr</span><span class="p">,</span>
                       <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
                       <span class="n">opt_parent_stream</span><span class="p">,</span>
                       <span class="n">opt_next_stream</span><span class="p">);</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">is_ready</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">auto</span> <span class="n">queue</span> <span class="o">=</span> <span class="n">ready_queue</span><span class="p">(</span><span class="n">cpu_ready_queue</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">.</span><span class="n">device</span><span class="p">());</span>
        <span class="n">queue</span><span class="o">-&gt;</span><span class="n">push</span><span class="p">(</span><span class="n">NodeTask</span><span class="p">(</span><span class="n">graph_task</span><span class="p">,</span> <span class="n">next</span><span class="p">.</span><span class="n">function</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_buffer</span><span class="p">)));</span>
        <span class="n">not_ready</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">not_ready_it</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="err">}</span>
<span class="err">}</span>

</code></pre></div></div>

<p>In this case, we accumulate the output in the existing <code class="language-plaintext highlighter-rouge">input_buffer</code> instead of creating a new one. Once all the tasks are processed, the worker thread exits the loop and complete.
All this process is summarized in the animation in Figure 5. We see how a thread peeks at the tasks in the ready queue and decrements the next nodes’ dependencies, unlocking them for execution.</p>

<p align="center">
  <img src="/assets/images/computation-animation-fig-5.gif" width="100%" />
</p>

<p align="center">
Figure 5: Animation of the execution of the computational graph
</p>

<h2 id="flow-with-reentrant-backward">Flow with Reentrant Backward</h2>

<p>As we saw above, the reentrant backward problem is when the currently executed function does a nested call to <code class="language-plaintext highlighter-rouge">backward</code>. When this happens, the thread running this function goes all the way down to <code class="language-plaintext highlighter-rouge">execute_with_graph_task</code> as in the non-reentrant case, but here is when things are different.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c10</span><span class="o">::</span><span class="n">intrusive_ptr</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ivalue</span><span class="o">::</span><span class="n">Future</span><span class="o">&gt;</span> <span class="n">Engine</span><span class="o">::</span><span class="n">execute_with_graph_task</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;&amp;</span> <span class="n">graph_task</span><span class="p">,</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">graph_root</span><span class="p">,</span>
    <span class="n">InputBuffer</span><span class="o">&amp;&amp;</span> <span class="n">input_buffer</span><span class="p">)</span> <span class="p">{</span>

  <span class="n">initialize_device_threads_pool</span><span class="p">();</span>
  <span class="c1">// Lock mutex for GraphTask.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">mutex_</span><span class="p">);</span>

  <span class="k">auto</span> <span class="n">queue</span> <span class="o">=</span> <span class="n">ready_queue</span><span class="p">(</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">cpu_ready_queue_</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">.</span><span class="n">device</span><span class="p">());</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">worker_device</span> <span class="o">==</span> <span class="n">NO_DEVICE</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">//Regular case</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="c1">// If worker_device is any devices (i.e. CPU, CUDA): this is a re-entrant</span>
    <span class="c1">//    backward call from that device.</span>
    <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">owner_</span> <span class="o">=</span> <span class="n">worker_device</span><span class="p">;</span>

    <span class="c1">// Now that all the non-thread safe fields of the graph_task have been populated,</span>
    <span class="c1">// we can enqueue it.</span>
    <span class="n">queue</span><span class="o">-&gt;</span><span class="n">push</span><span class="p">(</span><span class="n">NodeTask</span><span class="p">(</span><span class="n">graph_task</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">graph_root</span><span class="p">),</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">input_buffer</span><span class="p">)));</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">current_depth</span> <span class="o">&gt;=</span> <span class="n">max_recursion_depth_</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">// If reached the max depth, switch to a different thread</span>
      <span class="n">add_thread_pool_task</span><span class="p">(</span><span class="n">graph_task</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="o">++</span><span class="n">total_depth</span><span class="p">;</span>
      <span class="o">++</span><span class="n">current_depth</span><span class="p">;</span>
      <span class="n">lock</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
      <span class="n">thread_main</span><span class="p">(</span><span class="n">graph_task</span><span class="p">);</span>
      <span class="o">--</span><span class="n">current_depth</span><span class="p">;</span>
      <span class="o">--</span><span class="n">total_depth</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">future_result_</span><span class="p">;</span>
<span class="p">}</span>

</code></pre></div></div>

<p>Here, <code class="language-plaintext highlighter-rouge">execute_with_graph_task</code> detects this as a reentrant call and then looks for the current number of nested calls. If it exceeds the limit, we create a new thread to take care of the execution of this graph, and if not, we execute this reentrant call regularly.
The limit of nested calls was originally set to avoid stack overflow due to reentrant calls creating very large call stacks. However, the number was further reduced when sanitizer tests were added because of the maximum amount of locks a thread can hold at a given moment. This can be seen in <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.h#L36-L42"><code class="language-plaintext highlighter-rouge">torch/csrc/autograd/engine.h</code></a>.</p>

<p>When this maximum depth is exceeded, a new thread is created with the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L1239-L1255"><code class="language-plaintext highlighter-rouge">add_thread_pool_task</code></a> function.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">Engine</span><span class="o">::</span><span class="n">add_thread_pool_task</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;&amp;</span> <span class="n">graph_task</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lck</span><span class="p">(</span><span class="n">thread_pool_shared_</span><span class="o">-&gt;</span><span class="n">mutex_</span><span class="p">);</span>
  <span class="c1">// if we have pending graph_task objects to be processed, create a worker.</span>
   <span class="kt">bool</span> <span class="n">create_thread</span> <span class="o">=</span> <span class="p">(</span><span class="n">thread_pool_shared_</span><span class="o">-&gt;</span><span class="n">num_workers_</span> <span class="o">&lt;=</span> <span class="n">thread_pool_shared_</span><span class="o">-&gt;</span><span class="n">graphtasks_queue_</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
  <span class="n">thread_pool_shared_</span><span class="o">-&gt;</span><span class="n">graphtasks_queue_</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">graph_task</span><span class="p">);</span>


  <span class="n">lck</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">create_thread</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="kr">thread</span> <span class="n">t</span><span class="p">(</span><span class="o">&amp;</span><span class="n">Engine</span><span class="o">::</span><span class="n">reentrant_thread_init</span><span class="p">,</span> <span class="k">this</span><span class="p">);</span>
    <span class="n">t</span><span class="p">.</span><span class="n">detach</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="n">thread_pool_shared_</span><span class="o">-&gt;</span><span class="n">work_</span><span class="p">.</span><span class="n">notify_one</span><span class="p">();</span>
<span class="p">}</span>



</code></pre></div></div>

<p>Before going in-depth, let’s look at the <code class="language-plaintext highlighter-rouge">thread_pool_shared_</code> object in the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.h#L421"><code class="language-plaintext highlighter-rouge">Engine</code></a> which manages all the information related to the threads associated to the reentrant backward calls.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">struct</span> <span class="nc">ThreadPoolShared</span> <span class="p">{</span>
    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">num_workers_</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">condition_variable</span> <span class="n">work_</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">mutex</span> <span class="n">mutex_</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">queue</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;&gt;</span> <span class="n">graphtasks_queue_</span><span class="p">;</span>

    <span class="c1">// NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)</span>
    <span class="n">ThreadPoolShared</span><span class="p">()</span> <span class="o">:</span> <span class="n">num_workers_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="p">{}</span>
 <span class="p">};</span>



</code></pre></div></div>

<p><a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.h#L398-L414"><code class="language-plaintext highlighter-rouge">ThreadPoolShared</code></a> is a simple container holding a queue of <code class="language-plaintext highlighter-rouge">GraphTask</code> objects with synchronization mechanisms and the number of current workers.</p>

<p>Now it is easy to understand how <code class="language-plaintext highlighter-rouge">add_thread_pool_task</code> creates a thread when there are <code class="language-plaintext highlighter-rouge">graph_task</code> objects enqueued and insufficient workers to process them.</p>

<p><code class="language-plaintext highlighter-rouge">add_thread_pool_task</code> initializes a thread by executing <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L471-L493"><code class="language-plaintext highlighter-rouge">reentrant_thread_init</code></a></p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">Engine</span><span class="o">::</span><span class="n">reentrant_thread_init</span><span class="p">()</span> <span class="p">{</span>
  <span class="n">at</span><span class="o">::</span><span class="n">init_num_threads</span><span class="p">();</span>
  <span class="k">auto</span> <span class="n">tp_shared</span> <span class="o">=</span> <span class="n">thread_pool_shared_</span><span class="p">;</span>
  <span class="k">while</span><span class="p">(</span><span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lk</span><span class="p">(</span><span class="n">tp_shared</span><span class="o">-&gt;</span><span class="n">mutex_</span><span class="p">);</span>
    <span class="o">++</span><span class="n">thread_pool_shared_</span><span class="o">-&gt;</span><span class="n">num_workers_</span><span class="p">;</span>
    <span class="n">tp_shared</span><span class="o">-&gt;</span><span class="n">work_</span><span class="p">.</span><span class="n">wait</span><span class="p">(</span><span class="n">lk</span><span class="p">,</span> <span class="p">[</span><span class="o">&amp;</span><span class="n">tp_shared</span><span class="p">]{</span> <span class="k">return</span> <span class="o">!</span><span class="n">tp_shared</span><span class="o">-&gt;</span><span class="n">graphtasks_queue_</span><span class="p">.</span><span class="n">empty</span><span class="p">();});</span>
    <span class="o">--</span><span class="n">thread_pool_shared_</span><span class="o">-&gt;</span><span class="n">num_workers_</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">task</span> <span class="o">=</span> <span class="n">tp_shared</span><span class="o">-&gt;</span><span class="n">graphtasks_queue_</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
    <span class="n">tp_shared</span><span class="o">-&gt;</span><span class="n">graphtasks_queue_</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
    <span class="n">lk</span><span class="p">.</span><span class="n">unlock</span><span class="p">();</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;</span> <span class="n">graph_task</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">graph_task</span> <span class="o">=</span> <span class="n">task</span><span class="p">.</span><span class="n">lock</span><span class="p">()))</span> <span class="p">{</span>
      <span class="k">continue</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">set_device</span><span class="p">(</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">owner_</span><span class="p">);</span>
    <span class="c1">// set the local_ready_queue to the ready queue on the graph_task-&gt;owner_ device</span>
    <span class="n">local_ready_queue</span> <span class="o">=</span> <span class="n">ready_queue_by_index</span><span class="p">(</span><span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">cpu_ready_queue_</span><span class="p">,</span> <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">owner_</span><span class="p">);</span>
    <span class="n">total_depth</span> <span class="o">=</span> <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">reentrant_depth_</span><span class="p">;</span>
    <span class="n">thread_main</span><span class="p">(</span><span class="n">graph_task</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>



</code></pre></div></div>

<p>The code is straightforward. The newly created thread waits on the <code class="language-plaintext highlighter-rouge">thread_pool_shared-&gt;graphtasks_queue_</code> for reentrant backward graphs to be available and executes them. Notice that this thread uses the task-ready queue associated with the device of the thread that started this call by accessing the <code class="language-plaintext highlighter-rouge">graph_task-&gt;owner_</code> field set in the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L1092"><code class="language-plaintext highlighter-rouge">execute_with_graph_task</code></a> function.</p>

<h2 id="error-handling">Error Handling</h2>

<p>Whenever an error happens in one of the worker threads. It will be propagated to the <code class="language-plaintext highlighter-rouge">backward</code> calling thread.</p>

<p>To achieve this, there is a try/catch block in the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L415-L438"><code class="language-plaintext highlighter-rouge">thread_main</code></a> that catches any exception in the <code class="language-plaintext highlighter-rouge">Node</code> function call and sets it to the associated <code class="language-plaintext highlighter-rouge">GraphTask</code> object.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       <span class="k">try</span> <span class="p">{</span>
          <span class="err">…</span>
          <span class="n">GraphTaskGuard</span> <span class="n">guard</span><span class="p">(</span><span class="n">local_graph_task</span><span class="p">);</span>
          <span class="n">NodeGuard</span> <span class="n">ndguard</span><span class="p">(</span><span class="n">task</span><span class="p">.</span><span class="n">fn_</span><span class="p">);</span>
          <span class="p">{</span>
            <span class="n">evaluate_function</span><span class="p">(</span>
               <span class="err">…</span>
          <span class="p">}</span>
        <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">e</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">thread_on_exception</span><span class="p">(</span><span class="n">local_graph_task</span><span class="p">,</span> <span class="n">task</span><span class="p">.</span><span class="n">fn_</span><span class="p">,</span> <span class="n">e</span><span class="p">);</span>
        <span class="p">}</span>
      <span class="err">}</span>
    <span class="err">}</span>

</code></pre></div></div>

<p><a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L495-L500"><code class="language-plaintext highlighter-rouge">thread_on_exception</code></a> and the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/csrc/autograd/engine.cpp#L605-L621">functions it calls</a> end up setting the exception in the <code class="language-plaintext highlighter-rouge">local_graph_task</code> object.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">Engine</span><span class="o">::</span><span class="n">thread_on_exception</span><span class="p">(</span>
    <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">GraphTask</span><span class="o">&gt;</span> <span class="n">graph_task</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&amp;</span> <span class="n">fn</span><span class="p">,</span>
    <span class="n">std</span><span class="o">::</span><span class="n">exception</span><span class="o">&amp;</span> <span class="n">e</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">graph_task</span><span class="o">-&gt;</span><span class="n">set_exception</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">current_exception</span><span class="p">(),</span> <span class="n">fn</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">GraphTask</span><span class="o">::</span><span class="n">set_exception_without_signal</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&amp;</span> <span class="n">fn</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">has_error_</span><span class="p">.</span><span class="n">exchange</span><span class="p">(</span><span class="nb">true</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">AnomalyMode</span><span class="o">::</span><span class="n">is_enabled</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">fn</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">fn</span><span class="o">-&gt;</span><span class="n">metadata</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">print_stack</span><span class="p">(</span><span class="n">fn</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">());</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">GraphTask</span><span class="o">::</span><span class="n">set_exception</span><span class="p">(</span>
    <span class="n">std</span><span class="o">::</span><span class="n">exception_ptr</span> <span class="n">eptr</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;&amp;</span> <span class="n">fn</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">set_exception_without_signal</span><span class="p">(</span><span class="n">fn</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">future_completed_</span><span class="p">.</span><span class="n">exchange</span><span class="p">(</span><span class="nb">true</span><span class="p">))</span> <span class="p">{</span>
    <span class="c1">// NOLINTNEXTLINE(performance-move-const-arg)</span>
    <span class="n">future_result_</span><span class="o">-&gt;</span><span class="n">setError</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">eptr</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>

<p>In <code class="language-plaintext highlighter-rouge">set_exception</code> it sets the <code class="language-plaintext highlighter-rouge">has_error_</code> flag to <code class="language-plaintext highlighter-rouge">true</code> and it calls the <a href=""><code class="language-plaintext highlighter-rouge">setError</code></a>
function of the <a href="https://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/aten/src/ATen/core/ivalue_inl.h#L770-L1322"><code class="language-plaintext highlighter-rouge">future_result_</code></a> object. This will make the error to be re-thrown at the caller thread when <code class="language-plaintext highlighter-rouge">future_result_-&gt;value()</code> is accessed.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">IValue</span> <span class="nf">value</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span> <span class="n">lock</span><span class="p">(</span><span class="n">mutex_</span><span class="p">);</span>
    <span class="n">AT_ASSERT</span><span class="p">(</span><span class="n">completed</span><span class="p">());</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">eptr_</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">rethrow_exception</span><span class="p">(</span><span class="n">eptr_</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">value_</span><span class="p">;</span>
  <span class="p">}</span>

</code></pre></div></div>

<h1 id="closing-remarks">Closing Remarks</h1>

<p>This has been the last post of this series covering how PyTorch does the auto differentiation. We hope you enjoyed reading it and that now you are familiar enough with PyTorch internals to start contributing in PyTorch development!</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Accelerating Generative AI with PyTorch: Segment Anything 2 - Fast and furious inference with low latency and fast cold starts | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="This post is a follow-up to our first entry in the multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch and a focus on latency and elastic scalability. We use torch.compile and torch.export to create highly optimized low latency versions of SAM2 that can be quickly scaled up on new instances.

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Accelerating Generative AI with PyTorch: Segment Anything 2 - Fast and furious inference with low latency and fast cold starts" />
<meta property="og:description" content="This post is a follow-up to our first entry in the multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch and a focus on latency and elastic scalability. We use torch.compile and torch.export to create highly optimized low latency versions of SAM2 that can be quickly scaled up on new instances.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Accelerating Generative AI with PyTorch: Segment Anything 2 - Fast and furious inference with low latency and fast cold starts" />
<meta name="twitter:description" content="This post is a follow-up to our first entry in the multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch and a focus on latency and elastic scalability. We use torch.compile and torch.export to create highly optimized low latency versions of SAM2 that can be quickly scaled up on new instances.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
            Join us at PyTorch Conference in San Francisco, October 22-23. CFP open now! <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
          <a class="nav-dropdown-item" href="/new">
            <span class="dropdown-title">New to PyTorch Foundation</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="https://github.com/pytorch-fdn/ecosystem" target="_blank">
            <span class="dropdown-title">Join the Ecosystem</span>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2024">
            <span class="dropdown-title">Contributor Awards - 2024</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
          <a class="nav-dropdown-item" target="_blank" href="https://pytorch.org/executorch/stable/index.html">
            <span class="dropdown-title">ExecuTorch Documentation</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/newsletter">
            <span class=dropdown-title>Newsletter</span>
            <p>Stay up-to-date with the latest updates</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
          <a class="nav-dropdown-item" href="/credits">
            <span class=dropdown-title>Cloud Credit Program</span>
          </a>
          <a class="nav-dropdown-item" href="/tac">
            <span class=dropdown-title>Technical Advisory Council</span>
          </a>
          <a class="nav-dropdown-item" href="/staff">
            <span class=dropdown-title>Staff</span>
          </a>
          <a class="nav-dropdown-item" href="/contact-us">
            <span class=dropdown-title>Contact Us</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">February 26, 2025</p>
            <h1>
                <a class="blog-title">Accelerating Generative AI with PyTorch: Segment Anything 2 - Fast and furious inference with low latency and fast cold starts</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <p>This post is a follow-up to our <a href="https://pytorch.org/blog/accelerating-generative-ai/">first entry in the multi-series blog focused on how to accelerate generative AI models</a> with pure, native PyTorch and a focus on latency and elastic scalability. We use torch.compile and torch.export to create highly optimized low latency versions of SAM2 that can be quickly scaled up on new instances.</p>

<p>By utilizing AOTInductor’s (AOTI) ahead-of-time compilation via torch.export, reduced precision, batched prompts and GPU preprocessing we observe up to <strong>13x improvement in p90 execution latency</strong> and <strong>queue times compared to regular eager mode PyTorch</strong>.</p>

<p>We calculate our final results and demonstrate the improvement in a realistic deployment on auto-scaling cloud infrastructure from <a href="https://modal.com">Modal</a>.</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td colspan="2">p50 execution latency
<br />
(ms / improvement)
   </td>
   <td colspan="2">p90 execution latency
<br />
(ms / improvement)
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>eager float32
   </td>
   <td>AOTI float16
   </td>
   <td>eager float32
   </td>
   <td>AOTI float16
   </td>
  </tr>
  <tr>
   <td>AMG
   </td>
   <td>741
   </td>
   <td>112 (6.6x)
   </td>
   <td>1140
   </td>
   <td>176 (6.5x)
   </td>
  </tr>
  <tr>
   <td>SPS
   </td>
   <td>98
   </td>
   <td>20 (4.9x)
   </td>
   <td>130
   </td>
   <td>28 (4.6x)
   </td>
  </tr>
  <tr>
   <td>MPS
   </td>
   <td>269
   </td>
   <td>38 (7.1x)
   </td>
   <td>714
   </td>
   <td>52 (13.7x)
   </td>
  </tr>
</table>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td colspan="2">p50 queue time (ms / improvement)
   </td>
   <td colspan="2">p90 queue time (ms / improvement)
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>eager float32
   </td>
   <td>AOTI float16
   </td>
   <td>eager float32
   </td>
   <td>AOTI float16
   </td>
  </tr>
  <tr>
   <td>AMG
   </td>
   <td>201
   </td>
   <td>41 (4.9x)
   </td>
   <td>815
   </td>
   <td>327 (2.6x)
   </td>
  </tr>
  <tr>
   <td>SPS
   </td>
   <td>31
   </td>
   <td>33 (0.9x)
   </td>
   <td>441
   </td>
   <td>49 (9.0x)
   </td>
  </tr>
  <tr>
   <td>MPS
   </td>
   <td>40
   </td>
   <td>37 (1.1x)
   </td>
   <td>942
   </td>
   <td>75 (12.6x)
   </td>
  </tr>
</table>

<h2 id="the-tasks">The Tasks</h2>

<p>The first post focused on processing a small number of varying prompts (points of interest) per image. These points represented the center points of the ground truth masks. For this post, we’ll now focus on a broader set of tasks. Single prompt segmentation (SPS), multi prompt segmentation (MPS), automatic mask generation (AMG) which generates the full set of masks for the input image without a given set of prompts. The first post focused on MPS only.</p>

<p><img src="/assets/images/accelerating-generative-ai-2.jpg" alt="comparison of 3 images" style="width:100%" /></p>

<p>The little star in the image represents a user prompt. For AMG there are no prompts and masks are filtered down heuristically from a dense grid of initial candidate prompts (guesses). For SPS and MPS user prompts are derived from the center points of AMG masks. For SPS we choose the mask with the largest area.</p>

<p><strong>Note that SAM2 uses a different backbone than SAM1. In particular, we only consider the largest and most accurate sam2.1_hiera_large backbone for this blog.</strong></p>

<p>We aggregate the scripts needed to reproduce the results in <a href="https://github.com/pytorch/ao/tree/main/examples/sam2_amg_server">torchao’s example folder</a> and incrementally upstream the more stable parts of the <a href="https://github.com/pytorch/ao/tree/main/torchao/_models/sam2">changes to the SAM2 model in torchao</a> to the main <a href="https://github.com/facebookresearch/sam2">SAM2</a> repository. So if you are interested in taking a look at the cutting-edge variant or would like to contribute experimental features, please don’t hesitate to reach out to the torchao repository and team. For the more stable and latest model version, please head on over to SAM2 directly.</p>

<h2 id="overview">Overview</h2>

<p>We categorize the changes presented here into two. <strong>Fast</strong> changes constrain themselves to techniques that are not meant to affect model accuracy. <strong>Furious</strong> changes sacrifice some numerical accuracy for additional speed by making use of approximations such as low-precision data types.</p>

<p>Approximations may slightly lower precision metrics in favor of significantly improved performance while still passing an end-to-end check based on mean intersection over union (mIoU).</p>

<p>To measure the performance improvements we processed 1000 images, which were selected at random from the SAM2 validation dataset. We look at the p50 and p90 latency per image. To measure accuracy we consider the mIoU. Most notably for the AMG task we also define a fail count metric. We consider a comparison failed if the <strong>number of masks</strong> differs. This turns out to be a fairly unstable quantity and we can see that the other tasks are not as sensitive to small numeric changes as AMG.</p>

<h2 id="the-setup">The Setup</h2>

<p>We are running the offline experiments on a regular H100 devserver, which is a fairly beefy and performant machine.</p>

<p>However, we try to look at these tasks with realistic constraints. In particular, we would like to emulate a server-side inference environment. That means we don’t use DataLoader to hide the latency of image preprocessing or decoding routines.</p>

<p>For the latency calculations we include decoding, segmentation and conversion of masks to a dictionary of run-length encoded masks. Or put differently, we exclude loading the images into in-memory host bytearrays and storing the resulting dictionaries as json files on disk. This is meant to emulate a more realistic setting.</p>

<p>More concretely, consider the code below for the routines we include in our measurements. For any task <code class="language-plaintext highlighter-rouge">gen_masks</code> produces a batched bool Tensor bitmask that represents the corresponding object masks. We then compress this bitmask into a run length encoded (rle) format that can be used to transfer back the results from a remote server much more efficiently.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image_tensors = decode_img_bytes(...)
masks = gen_masks(image_tensors, ...)
rle_dicts = [rle_dict_from_masks(m) for m in masks]
</code></pre></div></div>

<h2 id="optimizations">Optimizations</h2>

<h3 id="ao-eager-code-optimizations">ao: eager code optimizations</h3>

<p>The most effective tool for this work is the PyTorch autograd profiler combined with <code class="language-plaintext highlighter-rouge">record_function</code>. To build this software, we’ve used the profiler repeatedly to observe the program and confirm the effectiveness of any changes. It’s also important to keep in mind that the profiler itself has overhead. The more data you collect, such as stack traces, the more overhead you introduce, which might skew the collected trace. But it is excellent to find synchronization points, space between kernels and GPU kernels that take a long time.</p>

<p>GPU traces help you understand bottlenecks that are not necessarily easily addressed by compile. We found that AutomaticMaskGeneration in particular is dominated by the data structure used to store the masks and by the routine used to convert the masks to a run-length encoded compressed format. We also found a large part of AMG performance is dominated by the large number of masks created as a single batch. Sometimes candidate masks can be filtered down to fewer candidates earlier in the postprocessing stage by reordering operations. This in turn significantly speeds up the later operations.</p>

<p>In order to confirm the accuracy of our implementation we first compare without any changes in settings and using float32 precision. We see that mIoU is unchanged and the masks match perfectly when using the exact same settings. This means that these eager mode changes did not affect the accuracy of these tasks.</p>

<p>AMG</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU / fail count
   </td>
  </tr>
  <tr>
   <td>Baseline
   </td>
   <td>864
   </td>
   <td>1144
   </td>
   <td>4350
   </td>
   <td>reference
   </td>
  </tr>
  <tr>
   <td>AO
   </td>
   <td>693
   </td>
   <td>786
   </td>
   <td>4010
   </td>
   <td>1 / 0
   </td>
  </tr>
</table>

<h3 id="ao-batching-prompts">ao: batching prompts</h3>

<p>Another lossless performance optimization that we were able to apply is batching the user input prompt calculations. When optimizing for latency at batch size 1 on a server-grade GPU such as an H100 we are often left with a lot of spare memory. We can easily trade off that memory for more performance by processing more points of interest (also called user prompts) at once. Remember that SAM2 is split into two parts: First the backbone (image encoder), second the prediction and decoding of masks based on a set of user prompts / points of interest. It is the second part where we may expect a larger or even varying number of inputs and it is this second part where we apply batching.</p>

<p>This causes a large increase in memory, but also much better latency. The baseline generates one mask per prompt in a loop. For AMG the baseline processes 64 prompts at once and all that is needed is to change it to 1024, which is the number of candidate prompts generated. For SPS we process one prompt at a time, but it’s still included below for completeness.</p>

<p>AMG</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU / fail count
   </td>
  </tr>
  <tr>
   <td>Baseline
   </td>
   <td>864
   </td>
   <td>1144
   </td>
   <td>4350
   </td>
   <td>reference
   </td>
  </tr>
  <tr>
   <td>AO + batching
   </td>
   <td>613
   </td>
   <td>706
   </td>
   <td>33786
   </td>
   <td>0.9999995 / 0
   </td>
  </tr>
</table>

<p>SPS</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
  </tr>
  <tr>
   <td>Baseline
   </td>
   <td>116
   </td>
   <td>181
   </td>
   <td>1337
   </td>
   <td>reference
   </td>
  </tr>
  <tr>
   <td>AO
   </td>
   <td>110
   </td>
   <td>170
   </td>
   <td>1339
   </td>
   <td>1
   </td>
  </tr>
</table>

<p>MPS</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
  </tr>
  <tr>
   <td>Baseline
   </td>
   <td>276
   </td>
   <td>681
   </td>
   <td>1337
   </td>
   <td>reference
   </td>
  </tr>
  <tr>
   <td>AO + batching
   </td>
   <td>126
   </td>
   <td>225
   </td>
   <td>8021
   </td>
   <td>0.9999992
   </td>
  </tr>
</table>

<p>As a technical side note: Most notably to enable batching for MPS, and to avoid a significant manual rewrite of the code base to support multiple prompts at the same time, we used a Tensor subclass we call MapTensor. A MapTensor allows us to pass a batch of N prompts, but have it advertise a batch size of 1. Any operation is then automatically broadcast to the wrapped Tensor and propagated throughout the prediction part of the model. This works because individual prompt predictions are independent of one another. This is very similar to torch.vmap.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>center_points_torch = to_map_tensor(center_points_torch)
center_points_label_torch = to_map_tensor(center_points_label_torch)
masks, scores, _ = mask_generator.predictor.predict(
    point_coords=center_points_torch,
    point_labels=center_points_label_torch,
    multimask_output=True,
    return_logits=False,
    return_type="torch",
)
# Unwrapping MapTensor
masks = masks.elems
scores = scores.elems
</code></pre></div></div>

<h3 id="fast-fullgraph-compilation">fast: fullgraph compilation</h3>

<p>Just as with our first post, we first remove GPU syncs and graph breaks to make use of fullgraph compiled model code with max-autotune kernels where appropriate. After some rewriting, we are able to compile the image encoder and the prediction of masks.</p>

<p>We run the experiments twice to get a sense of the overhead due to compilation. We run it once in an environment with an empty TORCHINDUCTOR_CACHE_DIR and then again while ingesting the artifacts from the previous run. In particular, auto-tuning can take a long time and happens on the first call in a pristine environment. We call the second run “warm”. The first iteration is typically expected to be slow due to various other related initialization processes, but compile increases it significantly, even if an existing cache is used and the same exact shapes are fed again. Having said that, an overhead of a few seconds in a warm environment is often still stomachable on the very first call.</p>

<p>Most of these drawbacks can be mitigated and compiling causes a significant improvement in latency and reduction in memory.</p>

<p>AMG</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU / 
<br />
fail count
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>AO + batching
   </td>
   <td>613
   </td>
   <td>706
   </td>
   <td>33786
   </td>
   <td>0.9999995 / 0
   </td>
   <td>1125
   </td>
  </tr>
  <tr>
   <td>+ compile (cold)
   </td>
   <td>423
   </td>
   <td>513
   </td>
   <td>29349
   </td>
   <td>skipped
   </td>
   <td>404866
   </td>
  </tr>
  <tr>
   <td>+ compile (warm)
   </td>
   <td>439
   </td>
   <td>530
   </td>
   <td>29349
   </td>
   <td>0.994 / 190
   </td>
   <td>8544
   </td>
  </tr>
</table>

<p>The number of masks produced per mask can vary slightly when using automatic mask segmentation. There is ambiguity in the number of masks per object the model may produce. For example, a car may be subdivided into frames, windows and doors or treated as a whole. When a modification causes the number of masks to change, we consider the comparison failed and we only calculate the mIoU on masks with an exact match. This does not apply to the other tasks. We found that the number of masks generated is very sensitive to small numerical changes. The other tasks use the same code and MPS in particular can help us further verify correctness.</p>

<p>SPS</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>AO
   </td>
   <td>110
   </td>
   <td>170
   </td>
   <td>1339
   </td>
   <td>1
   </td>
   <td>562
   </td>
  </tr>
  <tr>
   <td>+ compile (cold)
   </td>
   <td>102
   </td>
   <td>158
   </td>
   <td>1343
   </td>
   <td>skipped
   </td>
   <td>319954
   </td>
  </tr>
  <tr>
   <td>+ compile (warm)
   </td>
   <td>100
   </td>
   <td>160
   </td>
   <td>1302
   </td>
   <td>0.9999
   </td>
   <td>8947
   </td>
  </tr>
</table>

<p>MPS</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>AO + batching
   </td>
   <td>126
   </td>
   <td>225
   </td>
   <td>8021
   </td>
   <td>0.9999992
   </td>
   <td>504
   </td>
  </tr>
  <tr>
   <td>+ compile (cold)
   </td>
   <td>129
   </td>
   <td>215
   </td>
   <td>8021
   </td>
   <td>skipped
   </td>
   <td>333308
   </td>
  </tr>
  <tr>
   <td>+ compile (warm)
   </td>
   <td>113
   </td>
   <td>213
   </td>
   <td>8021
   </td>
   <td>0.998
   </td>
   <td>8617
   </td>
  </tr>
</table>

<h3 id="furious-tf32-float16-and-gpu-preprocessing">furious: TF32, float16 and GPU preprocessing</h3>

<p>We found that using float16 is the right level of precision for a few significant subcomponents of the model. In particular, the image encoder and mask decoder weights can be converted entirely to float16. We can also use TensorFloat32 precision for the remaining float32 matrix operations. It should be possible to further reduce the precision and we may address this in a future post. We also move image preprocessing such as image normalization onto the GPU with the furious mode. We can’t use GPU decoding (nvJPEG) routines, because the differences are too significant and the model suffers from significant degradation in mIoU, so image decoding still happens on the CPU.</p>

<p>AMG</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU / 
<br />
fail count
   </td>
  </tr>
  <tr>
   <td>AO 
<br />
+ batching 
<br />
+ compile (warm)
   </td>
   <td>439
   </td>
   <td>530
   </td>
   <td>29349
   </td>
   <td>0.994 / 190
   </td>
  </tr>
  <tr>
   <td>+ furious
   </td>
   <td>165
   </td>
   <td>240
   </td>
   <td>28335
   </td>
   <td>0.978 / 306
   </td>
  </tr>
</table>

<p>This causes a significant degradation in mIoU for the AMG task, but doesn’t affect the other tasks. After an in-depth investigation, we still chalk this up to numerical instability and reordering of operations. More work is needed to further investigate this and it may not be interesting to run the AMG task in lower precision. The other tasks, however, benefit drastically in latency with minimal changes in mIoU.</p>

<p>SPS</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
  </tr>
  <tr>
   <td>AO 
<br />
+ compile (warm)
   </td>
   <td>100
   </td>
   <td>160
   </td>
   <td>1302
   </td>
   <td>0.9999
   </td>
  </tr>
  <tr>
   <td>+ furious
   </td>
   <td>32
   </td>
   <td>63
   </td>
   <td>861
   </td>
   <td>0.9997
   </td>
  </tr>
</table>

<p>MPS</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
  </tr>
  <tr>
   <td>AO 
   <br />
+ batching
<br />
+ compile (warm)
   </td>
   <td>113
   </td>
   <td>213
   </td>
   <td>8021
   </td>
   <td>0.998
   </td>
  </tr>
  <tr>
   <td>+ furious
   </td>
   <td>36
   </td>
   <td>64
   </td>
   <td>4222
   </td>
   <td>0.997
   </td>
  </tr>
</table>

<h3 id="aotinductors-aoti-ahead-of-time-compilation-via-torchexport">AOTInductor’s (AOTI) ahead-of-time compilation via torch.export</h3>

<p>When scaling elastically it often is not possible to accommodate long startup times. That means the first iteration cannot be slow, but we must quickly deliver results. This is when torch.compile’s current compilation overhead can get in the way. To address this we can use AOTInductor’s (AOTI) ahead-of-time compilation via torch.export. AOTI lets us compile the model on a representative input and store the resulting code in a binary that is quick to load and run.</p>

<p>AOTI via torch.export is a new feature and we currently can’t export everything that is compilable. We’ve been able to export the image encoder for all tasks but have only been able to export the mask prediction for the AMG and SPS tasks due to varying prompts. torch.export also supports dynamic shapes, but we need to invest a bit more time to prepare the code for it.</p>

<p>AMG: AO + batching + furious</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU / 
<br />
fail count
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>+ compile (warm)
   </td>
   <td>165
   </td>
   <td>240
   </td>
   <td>28335
   </td>
   <td>0.978 / 306
   </td>
   <td>10341
   </td>
  </tr>
  <tr>
   <td>+ load export
<br />
(cold)
   </td>
   <td>162
   </td>
   <td>233
   </td>
   <td>27927
   </td>
   <td>0.974 / 308
   </td>
   <td>906
   </td>
  </tr>
</table>

<p>SPS: AO + furious</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>+ compile (warm)
   </td>
   <td>32
   </td>
   <td>63
   </td>
   <td>861
   </td>
   <td>0.9997
   </td>
   <td>7989
   </td>
  </tr>
  <tr>
   <td>+ load export
<br />
(cold)
   </td>
   <td>35
   </td>
   <td>66
   </td>
   <td>1686
   </td>
   <td>0.9997
   </td>
   <td>763
   </td>
  </tr>
</table>

<p>Note that loading the exported model significantly increases memory. It likely only increases peak memory utilization, because initialization really needs to be delayed before loading up an exported model to avoid having twice the weights in memory at once. This is something we could address, but the memory consumption is nowhere near the limit. We don’t see an increase in the other tasks, because AMG and MPS peak memory is dominated by processing batches of masks. One way to reduce that could be to operate on masks in the rle format (or some other sparse format) earlier on, but for now, there is no reason for this given the current memory consumption and focus on latency.</p>

<p>MPS: AO + batching + furious</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>+ compile (warm)
   </td>
   <td>36
   </td>
   <td>64
   </td>
   <td>4222
   </td>
   <td>0.997
   </td>
   <td>9626
   </td>
  </tr>
  <tr>
   <td>+ load export
<br />
(cold)
   </td>
   <td>43
   </td>
   <td>72
   </td>
   <td>3813
   </td>
   <td>0.997
   </td>
   <td>747
   </td>
  </tr>
</table>

<p>Using export by itself doesn’t seem to benefit from extensive warmup and can be run in a pristine new inductor cache directory. But again, we do not evict the CUDA cache or other caches. In the section on Modal, we are running some of these experiments in a pristine environment.</p>

<p>When only processing 1000 images in a new process, using export can really be worth it to save out on compile and other cold start overhead.</p>

<h3 id="bonus-more-gpu-preprocessing">bonus: More GPU preprocessing</h3>

<p>At this point, the latency is fairly low. In particular, for the SPS and MPS tasks we are processing at around 30ms to 40ms. Let’s bring back the pseudo-code from the setup section again.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image_tensors = decode_img_bytes(...)
masks = gen_masks(image_tensors, ...)
rle_dicts = [rle_dict_from_masks(m) for m in masks]
</code></pre></div></div>

<p>Further profiling showed that at this point <code class="language-plaintext highlighter-rouge">decode_img_bytes</code> takes about 10ms. In particular, it uses torchvision’s ToTensor transform to convert from a numpy Tensor to a scaled, float32 torch.Tensor. The bytes passed to ToTensor have already been decoded and converted to an numpy ndarray. By slightly rewriting ToTensor, using torchvision’s v2 API and moving the uint8 decoded smaller integer Tensor to GPU first before scaling, we can gain another 10ms in latency. Without including <code class="language-plaintext highlighter-rouge">decode_img_bytes</code> in our analysis we would have missed this opportunity that has real-world impact on server-side inference.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image_tensor = torch.from_numpy(image_tensor)
image_tensor = image_tensor.permute((2, 0, 1))
image_tensor = image_tensor.cuda()
image_tensor = v2.ToDtype(torch.float32, scale=True)( image_tensor)
</code></pre></div></div>

<p>Note in particular that using pinned memory to perform asynchronous data transfers doesn’t apply, since the time it takes to move the Tensor into pinned memory isn’t worth the gain in asynchronicity for this data movement. For future work, we might want to explore further improvements here by using more advanced direct memory transfer techniques.</p>

<p>AMG: AO + batching + furious</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU / 
<br />
fail count
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>+ load export
<br />
(cold)
   </td>
   <td>162
   </td>
   <td>233
   </td>
   <td>27927
   </td>
   <td>0.974 / 308
   </td>
   <td>906
   </td>
  </tr>
  <tr>
   <td>+ load export (warm)
   </td>
   <td>157
   </td>
   <td>230
   </td>
   <td>27927
   </td>
   <td>0.974 / 308
   </td>
   <td>799
   </td>
  </tr>
  <tr>
   <td>+ load export (warm)
<br />
+ preproc
   </td>
   <td>136
   </td>
   <td>208
   </td>
   <td>27950
   </td>
   <td>0.977 / 311
   </td>
   <td>908
   </td>
  </tr>
</table>

<p>SPS: AO + furious</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>+ load export
<br />
(cold)
   </td>
   <td>35
   </td>
   <td>66
   </td>
   <td>1686
   </td>
   <td>0.9997
   </td>
   <td>763
   </td>
  </tr>
  <tr>
   <td>+ load export (warm)
   </td>
   <td>31
   </td>
   <td>63
   </td>
   <td>1686
   </td>
   <td>0.9997
   </td>
   <td>683
   </td>
  </tr>
  <tr>
   <td>+ load export (warm)
<br />
+ preproc
   </td>
   <td>19
   </td>
   <td>25
   </td>
   <td>1711
   </td>
   <td>0.9997
   </td>
   <td>658
   </td>
  </tr>
</table>

<p>MPS: AO + batching + furious</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td>p50 latency (ms)
   </td>
   <td>p90 latency (ms)
   </td>
   <td>memory (MiB)
   </td>
   <td>mIoU
   </td>
   <td>first iteration
<br />
(ms)
   </td>
  </tr>
  <tr>
   <td>+ load export
<br />
(cold)
   </td>
   <td>43
   </td>
   <td>72
   </td>
   <td>3813
   </td>
   <td>0.997
   </td>
   <td>747
   </td>
  </tr>
  <tr>
   <td>+ load export (warm)
   </td>
   <td>53
   </td>
   <td>81
   </td>
   <td>3813
   </td>
   <td>0.997
   </td>
   <td>807
   </td>
  </tr>
  <tr>
   <td>+ load export (warm)
<br />
+ preproc
   </td>
   <td>31
   </td>
   <td>41
   </td>
   <td>3837
   </td>
   <td>0.997
   </td>
   <td>671
   </td>
  </tr>
</table>

<p>This small change has a significant impact on the SPS and MPS task.</p>

<h2 id="deploying-on-modal">Deploying on Modal</h2>

<p>Finally, we deployed our optimized inference onto <a href="https://modal.com">Modal</a>, a serverless infrastructure provider, to demonstrate that the benefits of these optimizations can be realized in a more realistic deployment setting.</p>

<p>In particular, compilation and AOTI via torch.export requires extra work. In a naïve deployment that work might be added to every single inference execution, adding latency that dwarfs any improvements from a faster model. This is particularly challenging with elastic or autoscaling infrastructure, where replicas of our inference service need to be regularly and automatically created and destroyed.</p>

<p>We share a deployment script in the torchao repository (<a href="https://github.com/pytorch/ao/tree/main/examples/sam2_amg_server">cli_on_modal.py</a>) to demonstrate one pattern for an elastic deployment. We build the exported models ahead of time and then upload them to <a href="https://modal.com/docs/guide/volumes">distributed storage</a>. Relative to eager execution, this adds a bit of extra work when replicas spin up since they need to read this data over a network, but this is far less costly than compilation or export.</p>

<p>We benchmarked this deployment with a large batch inference workload: sending 1000 images for concurrent processing. The deployment scales up to ten replicas on ten GPUs at peak and scales down to zero GPUs when inactive.</p>

<p>First, let’s look at the execution latencies.</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td colspan="3">p50 execution latency
<br />
(ms / improvement)
   </td>
   <td colspan="3">p90 execution latency
<br />
(ms / improvement)
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>eager float32
   </td>
   <td colspan="2">AOTI float16
   </td>
   <td>eager float32
   </td>
   <td colspan="2">AOTI float16
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>
   </td>
   <td>Modal
   </td>
   <td>Offline
   </td>
   <td>
   </td>
   <td>Modal
   </td>
   <td>Offline
   </td>
  </tr>
  <tr>
   <td>AMG
   </td>
   <td>741
   </td>
   <td>112 (6.6x)
   </td>
   <td>136 (5.4x)
   </td>
   <td>1140
   </td>
   <td>176 (6.5x)
   </td>
   <td>208 (5.5x)
   </td>
  </tr>
  <tr>
   <td>SPS
   </td>
   <td>98
   </td>
   <td>20 (4.9x)
   </td>
   <td>19 (5.2x)
   </td>
   <td>130
   </td>
   <td>28 (4.6x)
   </td>
   <td>25 (5.2x)
   </td>
  </tr>
  <tr>
   <td>MPS
   </td>
   <td>269
   </td>
   <td>38 (7.1x)
   </td>
   <td>31 (8.7x)
   </td>
   <td>714
   </td>
   <td>52 (13.7x)
   </td>
   <td>41 (17.4x)
   </td>
  </tr>
</table>

<p>We notice that execution latencies on Modal and Offline are fairly close, especially relative to the baseline, indicating that optimizing the deployment offline was a reasonable proxy for optimizing the deployment directly.</p>

<p>In addition to execution latency, our batch workload has queueing time, since there are fewer replicas than there are inputs, and so some inputs have to wait in line.</p>

<table class="table table-bordered">
  <tr>
   <td>
   </td>
   <td colspan="2">p50 queue time (ms)
   </td>
   <td colspan="2">p90 queue time (ms)
   </td>
  </tr>
  <tr>
   <td>
   </td>
   <td>eager float32
   </td>
   <td>AOTI float16
   </td>
   <td>eager float32
   </td>
   <td>AOTI float16
   </td>
  </tr>
  <tr>
   <td>AMG
   </td>
   <td>201
   </td>
   <td>41 (4.9x)
   </td>
   <td>815
   </td>
   <td>327 (2.6x)
   </td>
  </tr>
  <tr>
   <td>SPS
   </td>
   <td>31
   </td>
   <td>33 (0.9x)
   </td>
   <td>441
   </td>
   <td>49 (9.0x)
   </td>
  </tr>
  <tr>
   <td>MPS
   </td>
   <td>40
   </td>
   <td>37 (1.1x)
   </td>
   <td>942
   </td>
   <td>75 (12.6x)
   </td>
  </tr>
</table>

<p>Even though the queueing system provided by the infrastructure is unchanged, the queue latencies also decrease when we use our optimized model – in the p90 case by a factor of 2 to 12. That’s because when we finish previous inputs faster (from reduced execution latency) we can pull our next inputs sooner (reducing their queueing time).</p>

<p>If you’re interested in optimizing SAM2 inference or deployments further, don’t hesitate to reach out to us at the <a href="https://github.com/pytorch/ao">torchao repository</a>!</p>

<h2 id="conclusions">Conclusions</h2>

<p>We rewrote Meta’s original SAM2 in pure PyTorch with little loss of accuracy and a strong focus on latency. We deployed our optimized inference onto <a href="https://modal.com">Modal</a>, a serverless infrastructure provider, to demonstrate that the benefits of these optimizations can be realized in a more realistic deployment setting.</p>

<p>By utilizing AOTInductor’s (AOTI) ahead-of-time compilation via torch.export, reduced precision, batched prompts and GPU preprocessing we observe up to 13x improvement in p90 execution latency and queue times compared to regular eager mode PyTorch.</p>

<p>With elastic or autoscaling infrastructure, where replicas of our inference service need to be regularly and automatically created and destroyed, a naïve deployment of torch.compile can add work to inference execution that dwarfs any improvements from a faster model. By utilizing AOTInductor’s (AOTI) ahead-of-time compilation via torch.export, we are able to upload exported models ahead of time and read this data over a network, which enables us to get the benefits of compilation without significantly increased work.</p>

<p>For more details on how to reproduce the data in this blog post, <a href="https://github.com/pytorch/ao/tree/main/examples/sam2_amg_server">check out the experiments folder of torchao</a>. Please don’t hesitate to contact us or <a href="https://github.com/pytorch/ao/issues/new">open an issue</a> if you run into any technical issues.</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p
        class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and the latest news</p>
    
    
        <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
        <script>
          hbspt.forms.create({
            region: "na1",
            portalId: "8112310",
            formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
          });
        </script>
        
    
      <p
        class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. <a href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>
        
    </div>
    


    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://join.slack.com/t/pytorch/shared_invite/zt-2j2la612p-miUinTTaxXczKOJw48poHA" target="_blank" title="PyTorch Slack">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack"><path fill="currentColor" d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z"></path></svg>
        </a></li>
        <li><a href="/wechat" title="PyTorch on WeChat">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat"><path fill="currentColor" d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z"></path><path fill="currentColor" d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z"></path></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
          <li>
            <a href="/new">New to PyTorch Foundation</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Tools</a>
          </li>
          <li>
            <a href="https://github.com/pytorch-fdn/ecosystem">Join the Ecosystem</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2024">Contributor Awards - 2024</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
          <li>
            <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
          <li>
            <a href="/newsletter">Newsletter</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="/credits">Cloud Credit Program</a>
          </li>
          <li>          
            <a href="/tac">Technical Advisory Council</a>
          </li>
          <li>
            <a href="/staff">Staff</a>
          </li>
          <li>
            <a href="/contact-us">Contact Us</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

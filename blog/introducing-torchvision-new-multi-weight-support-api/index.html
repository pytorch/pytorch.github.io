<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-52DXT37');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Introducing TorchVision’s New Multi-Weight Support API | PyTorch
    
  </title>
  
  <meta property="og:title" content="Introducing TorchVision’s New Multi-Weight Support API" />
  <meta property="og:description" content="TorchVision has a new backwards compatible API for building models with multi-weight support. The new API allows loading different pre-trained weights on the same model variant, keeps track of vital meta-data such as the classification labels and includes the preprocessing transforms necessary for using the models. In this blog post, we plan to review the prototype API, show-case its features and highlight key differences with the existing one.

" />
  <meta property="og:image" content="https://pytorch.org/assets/images/torchvision_featured.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Introducing TorchVision’s New Multi-Weight Support API">
  <meta name="twitter:description" content="TorchVision has a new backwards compatible API for building models with multi-weight support. The new API allows loading different pre-trained weights on the same model variant, keeps track of vital meta-data such as the classification labels and includes the preprocessing transforms necessary for using the models. In this blog post, we plan to review the prototype API, show-case its features and highlight key differences with the existing one.

" />


<meta property="og:type" content="website" />
<meta name="robots" content="index, follow" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-52DXT37"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow" href="/ecosystem">
          Ecosystem
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/pted/2021">
            <span class="dropdown-title">Ecosystem Day - 2021</span>
            <p>See the posters presented at ecosystem day 2021</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/ptdd/2021">
            <span class="dropdown-title">Developer Day - 2021</span>
            <p>See the posters presented at developer day 2021</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="doc-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">December 22, 2021</p>
            <h1>
                <a class="blog-title">Introducing TorchVision’s New Multi-Weight Support API</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Vasilis Vryniotis
                      
                    </p>
                    <p>TorchVision has a new backwards compatible API for building models with multi-weight support. The new API allows loading different pre-trained weights on the same model variant, keeps track of vital meta-data such as the classification labels and includes the preprocessing transforms necessary for using the models. In this blog post, we plan to review the prototype API, show-case its features and highlight key differences with the existing one.</p>

<div class="text-center">
  <img src="/assets/images/torchvision_gif.gif" width="100%" />
</div>

<p>We are hoping to get your thoughts about the API prior finalizing it. To collect your feedback, we have created a <a href="https://github.com/pytorch/vision/issues/5088">Github issue</a> where you can post your thoughts, questions and comments.</p>

<h2 id="limitations-of-the-current-api">Limitations of the current API</h2>

<p>TorchVision currently provides pre-trained models which could be a starting point for transfer learning or used as-is in Computer Vision applications. The typical way to instantiate a pre-trained model and make a prediction is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span> <span class="k">as</span> <span class="n">M</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">transforms</span> <span class="k">as</span> <span class="n">T</span>


<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"test/assets/encode_jpeg/grace_hopper_517x606.jpg"</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">M</span><span class="p">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Define and initialize the inference transforms
</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">T</span><span class="p">.</span><span class="n">Resize</span><span class="p">([</span><span class="mi">256</span><span class="p">,</span> <span class="p">]),</span>
    <span class="n">T</span><span class="p">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
    <span class="n">T</span><span class="p">.</span><span class="n">PILToTensor</span><span class="p">(),</span>
    <span class="n">T</span><span class="p">.</span><span class="n">ConvertImageDtype</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">T</span><span class="p">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
<span class="p">])</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms
</span><span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category
</span><span class="n">class_id</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">.</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">class_id</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"imagenet_classes.txt"</span><span class="p">,</span> <span class="s">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="n">readlines</span><span class="p">()]</span>
    <span class="n">category_name</span> <span class="o">=</span> <span class="n">categories</span><span class="p">[</span><span class="n">class_id</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">score</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>

</code></pre></div></div>

<p>There are a few limitations with the above approach:</p>

<ol>
  <li><strong>Inability to support multiple pre-trained weights:</strong> Since the <code class="language-plaintext highlighter-rouge">pretrained</code> variable is boolean, we can only offer one set of weights. This poses a severe limitation when we significantly <a href="https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/">improve the accuracy of existing models</a> and we want to make those improvements available to the community. It also stops us from offering pre-trained weights of the same model variant on different datasets.</li>
  <li><strong>Missing inference/preprocessing transforms:</strong> The user is forced to define the necessary transforms prior using the model. The inference transforms are usually linked to the training process and dataset used to estimate the weights. Any minor discrepancies in these transforms (such as interpolation value, resize/crop sizes etc) can lead to major reductions in accuracy or unusable models.</li>
  <li><strong>Lack of meta-data:</strong> Critical pieces of information in relation to the weights are unavailable to the users. For example, one needs to look into external sources and the documentation to find things like the <a href="https://github.com/pytorch/vision/issues/1946">category labels</a>, the training recipe, the accuracy metrics etc.</li>
</ol>

<p>The new API addresses the above limitations and reduces the amount of boilerplate code needed for standard tasks.</p>

<h2 id="overview-of-the-prototype-api">Overview of the prototype API</h2>

<p>Let’s see how we can achieve exactly the same results as above using the new API:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision.prototype</span> <span class="kn">import</span> <span class="n">models</span> <span class="k">as</span> <span class="n">PM</span>


<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"test/assets/encode_jpeg/grace_hopper_517x606.jpg"</span><span class="p">)</span>

<span class="c1"># Step 1: Initialize model
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">PM</span><span class="p">.</span><span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V1</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PM</span><span class="p">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># Step 2: Initialize the inference transforms
</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">weights</span><span class="p">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Step 3: Apply inference preprocessing transforms
</span><span class="n">batch</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">img</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">softmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Use the model and print the predicted category
</span><span class="n">class_id</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">.</span><span class="n">argmax</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">class_id</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>
<span class="n">category_name</span> <span class="o">=</span> <span class="n">weights</span><span class="p">.</span><span class="n">meta</span><span class="p">[</span><span class="s">"categories"</span><span class="p">][</span><span class="n">class_id</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">category_name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">score</span><span class="si">}</span><span class="s">*%*"</span><span class="p">)</span>
</code></pre></div></div>

<p>As we can see the new API eliminates the aforementioned limitations. Let’s explore the new features in detail.</p>

<h3 id="multi-weight-support">Multi-weight support</h3>

<p>At the heart of the new API, we have the ability to define multiple different weights for the same model variant. Each model building method (eg <code class="language-plaintext highlighter-rouge">resnet50</code>) has an associated Enum class (eg <code class="language-plaintext highlighter-rouge">ResNet50_Weights</code>) which has as many entries as the number of pre-trained weights available. Additionally, each Enum class has a <code class="language-plaintext highlighter-rouge">DEFAULT</code> alias which points to the best available weights for the specific model. This allows the users who want to always use the best available weights to do so without modifying their code.</p>

<p>Here is an example of initializing models with different weights:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.prototype.models</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">ResNet50_Weights</span>

<span class="c1"># Legacy weights with accuracy 76.130%
</span><span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V1</span><span class="p">)</span>

<span class="c1"># New weights with accuracy 80.858%
</span><span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span><span class="p">)</span>

<span class="c1"># Best available weights (currently alias for IMAGENET1K_V2)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">DEFAULT</span><span class="p">)</span>

<span class="c1"># No weights - random initialization
</span><span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="associated-meta-data---preprocessing-transforms">Associated meta-data &amp;  preprocessing transforms</h3>

<p>The weights of each model are associated with meta-data. The type of information we store depends on the task of the model (Classification, Detection, Segmentation etc). Typical information includes a link to the training recipe, the interpolation mode, information such as the categories and validation metrics. These values are programmatically accessible via the <code class="language-plaintext highlighter-rouge">meta</code> attribute:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.prototype.models</span> <span class="kn">import</span> <span class="n">ResNet50_Weights</span>

<span class="c1"># Accessing a single record
</span><span class="n">size</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span><span class="p">.</span><span class="n">meta</span><span class="p">[</span><span class="s">"size"</span><span class="p">]</span>

<span class="c1"># Iterating the items of the meta-data dictionary
</span><span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span><span class="p">.</span><span class="n">meta</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<p>Additionally, each weights entry is associated with the necessary preprocessing transforms. All current preprocessing transforms are JIT-scriptable and can be accessed via the <code class="language-plaintext highlighter-rouge">transforms</code> attribute. Prior using them with the data, the transforms need to be initialized/constructed. This lazy initialization scheme is done to ensure the solution is memory efficient. The input of the transforms can be either a <code class="language-plaintext highlighter-rouge">PIL.Image</code> or a <code class="language-plaintext highlighter-rouge">Tensor</code> read using <code class="language-plaintext highlighter-rouge">torchvision.io</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.prototype.models</span> <span class="kn">import</span> <span class="n">ResNet50_Weights</span>

<span class="c1"># Initializing preprocessing at standard 224x224 resolution
</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span><span class="p">.</span><span class="n">transforms</span><span class="p">()</span>

<span class="c1"># Initializing preprocessing at 400x400 resolution
</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span><span class="p">.</span><span class="n">transforms</span><span class="p">(</span><span class="n">crop_size</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">resize_size</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>

<span class="c1"># Once initialized the callable can accept the image data:
# img_preprocessed = preprocess(img)
</span></code></pre></div></div>

<p>Associating the weights with their meta-data and preprocessing will boost transparency, improve reproducibility and make it easier to document how a set of weights was produced.</p>

<h3 id="get-weights-by-name">Get weights by name</h3>

<p>The ability to link directly the weights with their properties (meta data, preprocessing callables etc) is the reason why our implementation uses Enums instead of Strings. Nevertheless for cases when only the name of the weights is available, we offer a method capable of linking Weight names to their Enums:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.prototype.models</span> <span class="kn">import</span> <span class="n">get_weight</span>

<span class="c1"># Weights can be retrieved by name:
</span><span class="k">assert</span> <span class="n">get_weight</span><span class="p">(</span><span class="s">"ResNet50_Weights.IMAGENET1K_V1"</span><span class="p">)</span> <span class="o">==</span> <span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V1</span>
<span class="k">assert</span> <span class="n">get_weight</span><span class="p">(</span><span class="s">"ResNet50_Weights.IMAGENET1K_V2"</span><span class="p">)</span> <span class="o">==</span> <span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span>

<span class="c1"># Including using the DEFAULT alias:
</span><span class="k">assert</span> <span class="n">get_weight</span><span class="p">(</span><span class="s">"ResNet50_Weights.DEFAULT"</span><span class="p">)</span> <span class="o">==</span> <span class="n">ResNet50_Weights</span><span class="p">.</span><span class="n">IMAGENET1K_V2</span>
</code></pre></div></div>

<h2 id="deprecations">Deprecations</h2>

<p>In the new API the boolean <code class="language-plaintext highlighter-rouge">pretrained</code> and <code class="language-plaintext highlighter-rouge">pretrained_backbone</code> parameters, which were previously used to load weights to the full model or to its backbone, are deprecated. The current implementation is fully backwards compatible as it seamlessly maps the old parameters to the new ones. Using the old parameters to the new builders emits the following deprecation warnings:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">prototype</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 <span class="nb">UserWarning</span><span class="p">:</span> <span class="n">The</span> <span class="n">parameter</span> <span class="s">'pretrained'</span> <span class="ow">is</span> <span class="n">deprecated</span><span class="p">,</span> <span class="n">please</span> <span class="n">use</span> <span class="s">'weights'</span> <span class="n">instead</span><span class="p">.</span>
<span class="nb">UserWarning</span><span class="p">:</span> 
<span class="n">Arguments</span> <span class="n">other</span> <span class="n">than</span> <span class="n">a</span> <span class="n">weight</span> <span class="n">enum</span> <span class="ow">or</span> <span class="sb">`None`</span> <span class="k">for</span> <span class="s">'weights'</span> <span class="n">are</span> <span class="n">deprecated</span><span class="p">.</span> 
<span class="n">The</span> <span class="n">current</span> <span class="n">behavior</span> <span class="ow">is</span> <span class="n">equivalent</span> <span class="n">to</span> <span class="n">passing</span> <span class="sb">`weights=ResNet50_Weights.IMAGENET1K_V1`</span><span class="p">.</span> 
<span class="n">You</span> <span class="n">can</span> <span class="n">also</span> <span class="n">use</span> <span class="sb">`weights=ResNet50_Weights.DEFAULT`</span> <span class="n">to</span> <span class="n">get</span> <span class="n">the</span> <span class="n">most</span> <span class="n">up</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">date</span> <span class="n">weights</span><span class="p">.</span>
</code></pre></div></div>

<p>Additionally the builder methods require using keyword parameters. The use of positional parameter is deprecated and using them emits the following warning:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">prototype</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">resnet50</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
<span class="nb">UserWarning</span><span class="p">:</span> 
<span class="n">Using</span> <span class="s">'weights'</span> <span class="k">as</span> <span class="n">positional</span> <span class="n">parameter</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="ow">is</span> <span class="n">deprecated</span><span class="p">.</span> 
<span class="n">Please</span> <span class="n">use</span> <span class="n">keyword</span> <span class="n">parameter</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">instead</span><span class="p">.</span>
</code></pre></div></div>

<h2 id="testing-the-new-api">Testing the new API</h2>

<p>Migrating to the new API is very straightforward. The following method calls between the 2 APIs are all equivalent:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Using pretrained weights:
torchvision.prototype.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
torchvision.models.resnet50(pretrained=True)
torchvision.models.resnet50(True)

# Using no weights:
torchvision.prototype.models.resnet50(weights=None)
torchvision.models.resnet50(pretrained=False)
torchvision.models.resnet50(False)
</code></pre></div></div>

<p>Note that the prototype features are available only on the nightly versions of TorchVision, so to use it you need to install it as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install torchvision -c pytorch-nightly
</code></pre></div></div>

<p>For alternative ways to install the nightly have a look on the PyTorch <a href="https://pytorch.org/get-started/locally/">download page</a>. You can also install TorchVision from source from the latest main; for more information have a look on our <a href="https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md">repo</a>.</p>

<h2 id="accessing-state-of-the-art-model-weights-with-the-new-api">Accessing state-of-the-art model weights with the new API</h2>

<p>If you are still unconvinced about giving a try to the new API, here is one more reason to do so. We’ve recently refreshed our <a href="https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/">training recipe</a> and achieved SOTA accuracy from many of our models. The improved weights can easily be accessed via the new API. Here is a quick overview of the model improvements:</p>

<div class="text-center">
  <img src="/assets/images/torchvision_chart1.png" width="100%" />
</div>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Old Acc@1</th>
      <th>New Acc@1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>EfficientNet B1</td>
      <td>78.642</td>
      <td>79.838</td>
    </tr>
    <tr>
      <td>MobileNetV3 Large</td>
      <td>74.042</td>
      <td>75.274</td>
    </tr>
    <tr>
      <td>Quantized ResNet50</td>
      <td>75.92</td>
      <td>80.282</td>
    </tr>
    <tr>
      <td>Quantized ResNeXt101 32x8d</td>
      <td>78.986</td>
      <td>82.574</td>
    </tr>
    <tr>
      <td>RegNet X 400mf</td>
      <td>72.834</td>
      <td>74.864</td>
    </tr>
    <tr>
      <td>RegNet X 800mf</td>
      <td>75.212</td>
      <td>77.522</td>
    </tr>
    <tr>
      <td>RegNet X 1 6gf</td>
      <td>77.04</td>
      <td>79.668</td>
    </tr>
    <tr>
      <td>RegNet X 3 2gf</td>
      <td>78.364</td>
      <td>81.198</td>
    </tr>
    <tr>
      <td>RegNet X 8gf</td>
      <td>79.344</td>
      <td>81.682</td>
    </tr>
    <tr>
      <td>RegNet X 16gf</td>
      <td>80.058</td>
      <td>82.72</td>
    </tr>
    <tr>
      <td>RegNet X 32gf</td>
      <td>80.622</td>
      <td>83.018</td>
    </tr>
    <tr>
      <td>RegNet Y 400mf</td>
      <td>74.046</td>
      <td>75.806</td>
    </tr>
    <tr>
      <td>RegNet Y 800mf</td>
      <td>76.42</td>
      <td>78.838</td>
    </tr>
    <tr>
      <td>RegNet Y 1 6gf</td>
      <td>77.95</td>
      <td>80.882</td>
    </tr>
    <tr>
      <td>RegNet Y 3 2gf</td>
      <td>78.948</td>
      <td>81.984</td>
    </tr>
    <tr>
      <td>RegNet Y 8gf</td>
      <td>80.032</td>
      <td>82.828</td>
    </tr>
    <tr>
      <td>RegNet Y 16gf</td>
      <td>80.424</td>
      <td>82.89</td>
    </tr>
    <tr>
      <td>RegNet Y 32gf</td>
      <td>80.878</td>
      <td>83.366</td>
    </tr>
    <tr>
      <td>ResNet50</td>
      <td>76.13</td>
      <td>80.858</td>
    </tr>
    <tr>
      <td>ResNet101</td>
      <td>77.374</td>
      <td>81.886</td>
    </tr>
    <tr>
      <td>ResNet152</td>
      <td>78.312</td>
      <td>82.284</td>
    </tr>
    <tr>
      <td>ResNeXt50 32x4d</td>
      <td>77.618</td>
      <td>81.198</td>
    </tr>
    <tr>
      <td>ResNeXt101 32x8d</td>
      <td>79.312</td>
      <td>82.834</td>
    </tr>
    <tr>
      <td>Wide ResNet50 2</td>
      <td>78.468</td>
      <td>81.602</td>
    </tr>
    <tr>
      <td>Wide ResNet101 2</td>
      <td>78.848</td>
      <td>82.51</td>
    </tr>
  </tbody>
</table>

<p>Please spare a few minutes to provide your feedback on the new API, as this is crucial for graduating it from prototype and including it in the next release. You can do this on the dedicated <a href="https://github.com/pytorch/vision/issues/5088">Github Issue</a>. We are looking forward to reading your comments!</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-tos-privacy-policy.pdf" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-oss-privacy-policy.pdf" target="_blank">Privacy</a></li>
      </ul>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/ecosystem">Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/pted/2021">Ecosystem Day 2021</a>
          </li>
          <li>
            <a href="/ecosystem/ptdd/2021">Developer Day 2021</a>
          </li>
        </ul>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/elastic">TorchElastic</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/#community-module">Community</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

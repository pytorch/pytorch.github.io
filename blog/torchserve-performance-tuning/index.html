<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Torchserve Performance Tuning, Animated Drawings Case-Study | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="In this post we discuss performance tuning of Torchserve for serving your models in production. One of the biggest challenges in the life cycle of a ML project is deploying models in production.  This requires a reliable serving solution along with solutions that address the MLOps needs. A robust serving solution needs to provide support for multi model serving, model versioning, metric logging, monitoring and scaling to serve the peak traffic. In this post, we will have an overview of Torchserve and how to tune its performance for production use-cases. We discuss the Animated Drawings app from Meta that can turn your human figure sketches to animations and how it could serve the peak traffic with Torchserve. The Animated Drawing’s workflow is below.

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/sketch_animator.png" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/sketch_animator.png" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Torchserve Performance Tuning, Animated Drawings Case-Study" />
<meta property="og:description" content="In this post we discuss performance tuning of Torchserve for serving your models in production. One of the biggest challenges in the life cycle of a ML project is deploying models in production.  This requires a reliable serving solution along with solutions that address the MLOps needs. A robust serving solution needs to provide support for multi model serving, model versioning, metric logging, monitoring and scaling to serve the peak traffic. In this post, we will have an overview of Torchserve and how to tune its performance for production use-cases. We discuss the Animated Drawings app from Meta that can turn your human figure sketches to animations and how it could serve the peak traffic with Torchserve. The Animated Drawing’s workflow is below.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Torchserve Performance Tuning, Animated Drawings Case-Study" />
<meta name="twitter:description" content="In this post we discuss performance tuning of Torchserve for serving your models in production. One of the biggest challenges in the life cycle of a ML project is deploying models in production.  This requires a reliable serving solution along with solutions that address the MLOps needs. A robust serving solution needs to provide support for multi model serving, model versioning, metric logging, monitoring and scaling to serve the peak traffic. In this post, we will have an overview of Torchserve and how to tune its performance for production use-cases. We discuss the Animated Drawings app from Meta that can turn your human figure sketches to animations and how it could serve the peak traffic with Torchserve. The Animated Drawing’s workflow is below.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">December 28, 2022</p>
            <h1>
                <a class="blog-title">Torchserve Performance Tuning, Animated Drawings Case-Study</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Hamid Shojanazeri, Geeta Chauhan, Mark Saroufim, Jesse Smith
                      
                    </p>
                    <p>In this post we discuss performance tuning of Torchserve for serving your models in production. One of the biggest challenges in the life cycle of a ML project is deploying models in production.  This requires a reliable serving solution along with solutions that address the MLOps needs. A robust serving solution needs to provide support for multi model serving, model versioning, metric logging, monitoring and scaling to serve the peak traffic. In this post, we will have an overview of Torchserve and how to tune its performance for production use-cases. We discuss the <a href="https://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life/">Animated Drawings app</a> from Meta that can turn your human figure sketches to animations and how it could serve the peak traffic with Torchserve. The Animated Drawing’s workflow is below.</p>

<p>
<img src="/assets/images/sketch_animator.png" width="90%" />
</p>

<p><a href="https://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life/">https://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life/</a></p>

<p>Many AI systems and tools are designed to handle realistic images of humans, children’s drawings add a level of complexity and unpredictability as they are often constructed in abstract, fanciful ways. These types of morphological and stylistic variations can confuse even state-of-the-art AI systems that excel at spotting objects in photorealistic images and drawings.
Meta AI researchers are working to overcome this challenge so that AI systems will be better able to recognize drawings of human figures in the wildly varied ways that children create them. This great blog post provides more details about the Animated Drawings and the approach taken.</p>

<h2 id="torchserve">Torchserve</h2>

<p>
<img src="/assets/images/Tuning-flow-chart.png" width="90%" />
<center><i>Fig1. Overall flow of Torchserve performance tuning</i> </center>
</p>

<p>Once you have trained your model, it needs to be integrated into a larger system to have a full-fledged application, we use the term “model serving” to refer to this integration. Basically model serving is making your trained model available to run inferences and subsequent use of the model.</p>

<p>Torchserve is the Pytorch preferred solution for serving models in production. It is a performant and scalable tool that wraps your model in a HTTP or HTTPS API. It has a frontend implemented in Java that handles multiple tasks from assigning workers for serving models to handling the connection between client and server. Torchserve has a Python backend that is responsible for handling the inference service.</p>

<p>Torchserve supports multi model serving and versioning for AB test, dynamic batching, logging and metrics. It exposes four APIs for <a href="https://github.com/pytorch/serve/blob/master/docs/inference_api.md">inference</a>, <a href="https://github.com/pytorch/serve/blob/master/docs/inference_api.md#explanations-api">explanations</a>, <a href="https://github.com/pytorch/serve/blob/master/docs/management_api.md">management</a> and <a href="https://github.com/pytorch/serve/blob/master/docs/metrics_api.md">metrics</a>.</p>

<p><a href="https://github.com/pytorch/serve/blob/master/docs/inference_api.md">Inference</a> API is listening on port 8080 and accessible through localhost by default, this can be configured in <a href="https://github.com/pytorch/serve/blob/master/docs/configuration.md">Torchserve configuration</a> and enable getting predictions from the model.</p>

<p><a href="https://github.com/pytorch/serve/blob/master/docs/inference_api.md#explanations-api">Explanation</a> API uses  Captum under the hood to provide explanations of the model that is being served and listens to the port 8080 as well.</p>

<p><a href="https://github.com/pytorch/serve/blob/master/docs/management_api.md#management-api">Management</a> API allows to register or unregister and describe a model. It also enables users to  scale up or down the number of workers that serve the model.</p>

<p><a href="https://github.com/pytorch/serve/blob/master/docs/metrics_api.md">Metric</a> API by default listens to port 8082 and enables us to monitor the model that is being served.</p>

<p>Torchserve let you scale your model serving and handle the peak traffic by supporting <a href="https://github.com/pytorch/serve/blob/master/docs/batch_inference_with_ts.md">batch inference</a> and multiple  workers that serve your model. Scaling can be done through <a href="https://github.com/pytorch/serve/blob/master/docs/management_api.md">management</a>  API and settings through a <a href="https://github.com/pytorch/serve/blob/master/docs/configuration.md">configuration</a> file. Also, metric API helps you to monitor your model serving through default and customizable metrics.</p>

<p>Other advanced settings such as the length of the queue for the received requests, maximum wait time for a batch of inputs and many other properties are configurable through a<a href="https://github.com/pytorch/serve/blob/master/docs/configuration.md"> config file</a> that can be passed to Torchserve when it is started.</p>

<p><strong>Steps to serve your model with Torchserve</strong></p>

<ol>
  <li><a href="https://github.com/pytorch/serve/blob/master/docs/getting_started.md#install-torchserve-and-torch-model-archiver">Install Torchserve, model archiver</a> and its requirements.</li>
  <li>Choose a default handler that fits your task (e.g image classification, etc) or author a <a href="https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers">custom handler</a>.</li>
  <li><a href="https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers#create-model-archive-eager-mode">Package your model</a> artifacts (trained model checkpoint and all other necessary files for loading and running your model) and the handler into a “.mar” file using <a href="https://github.com/pytorch/serve/blob/master/model-archiver/README.md">Torcharchive</a> and place it in the model store.</li>
  <li><a href="https://github.com/pytorch/serve/blob/master/docs/getting_started.md">Start serving your model</a>.</li>
  <li><a href="https://github.com/pytorch/serve/blob/master/docs/getting_started.md#get-predictions-from-a-model">Run inference</a>.
We will discuss model handlers and metrics in more detail here.</li>
</ol>

<h2 id="model-handlers">Model handlers</h2>

<p>Torchserve uses a handler in the backend to load the models, preprocess the received data, run inference and post-process the response. Handler in torchserve is a <strong>python script</strong> that all the model initialization, preprocessing, inference and post processing logic goes into.</p>

<p>Torchserve provides an out of the box handler for a number of applications like image classification, segmentation, object detection and text classification. It also supports custom handlers, in case your use case is not supported in default handlers.</p>

<p>It provides a great flexibility in custom handlers, this potentially make Torchserve as <strong>multi-framework</strong> serving tool. Custom handlers let you define your custom logic to initialize a model that can be used also to load models from other frameworks such as ONNX.</p>

<p>Torchserve <strong>handler</strong> is made of four main <strong>functions</strong>, <strong>initialize</strong>, <strong>preprocess</strong>, <strong>inference</strong> and <strong>postprocess</strong> that each return a list. The code snippet below shows an example of a custom handler.<strong>Custom handlers inherit</strong> from <strong>BaseHandler</strong> in Torchserve and can <strong>overwrite</strong> any of the <strong>main</strong> <strong>functions</strong>.  Here is an example of the handler used for loading the <a href="https://github.com/facebookresearch/detectron2">Detectron2</a> model for figure detection, this model has been exported to Torchscript and uses model.half() to run the inference with FP16, details are explained in another <a href="">section</a> in this post.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">MyModelHandler</span><span class="p">(</span><span class="n">BaseHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">manifest</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">manifest</span>
        <span class="n">properties</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">system_properties</span>
        <span class="n">model_dir</span> <span class="o">=</span> <span class="n">properties</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"model_dir"</span><span class="p">)</span>
        <span class="n">serialized_file</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">manifest</span><span class="p">[</span><span class="s">"model"</span><span class="p">][</span><span class="s">"serializedFile"</span><span class="p">]</span>
        <span class="n">model_pt_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">serialized_file</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span>
        <span class="s">"cuda:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">properties</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"gpu_id"</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">properties</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"gpu_id"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="k">else</span> <span class="s">"cpu"</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_pt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">half</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">request</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>

            <span class="n">request_body</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"body"</span><span class="p">)</span>

            <span class="n">input_</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">request_body</span><span class="p">)</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">imdecode</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">input_</span><span class="p">.</span><span class="n">read</span><span class="p">(),</span> <span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">image</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">half</span><span class="p">()</span>
            <span class="n">inputs</span><span class="p">.</span><span class="n">append</span><span class="p">({</span><span class="s">"image"</span><span class="p">:</span> <span class="nb">input</span><span class="p">})</span>

        <span class="k">return</span> <span class="n">inputs</span>

    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">predictions</span>

    <span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">inference_output</span> <span class="ow">in</span> <span class="n">inference_outputs</span><span class="p">:</span>
            <span class="n">responses_json</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">'classes'</span><span class="p">:</span> <span class="n">inference_output</span><span class="p">[</span><span class="s">'pred_classes'</span><span class="p">].</span><span class="n">tolist</span><span class="p">(),</span>
            <span class="s">'scores'</span><span class="p">:</span> <span class="n">inference_output</span><span class="p">[</span><span class="s">'scores'</span><span class="p">].</span><span class="n">tolist</span><span class="p">(),</span>
            <span class="s">"boxes"</span><span class="p">:</span> <span class="n">inference_output</span><span class="p">[</span><span class="s">'pred_boxes'</span><span class="p">].</span><span class="n">tolist</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="n">responses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">responses_json</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">responses</span>
</code></pre></div></div>

<h2 id="metrics">Metrics</h2>

<p>An essential component in serving models in production is the ability to monitor them. <strong>Torchserve</strong> <strong>collects</strong> <strong>system level</strong> <a href="https://github.com/pytorch/serve/blob/master/docs/metrics.md">metrics</a> regularly and <strong>allows</strong> adding <strong>custom metrics</strong> as well.</p>

<p><strong><a href="https://github.com/pytorch/serve/blob/master/docs/metrics.md#system-metrics">System level metrics</a></strong> consist of CPU utilization, available and used disk space and memory on the host machine along with number of requests with different response codes (e.g 200-300, 400-500 and above 500). <strong>Custom metrics</strong> can be <strong>added</strong> to the metrics as explained <a href="https://github.com/pytorch/serve/blob/master/docs/metrics.md#custom-metrics-api">here</a>. TorchServe logs these two sets of metrics to different log files. Metrics are collected by default at:</p>

<ul>
  <li>System metrics - log_directory/ts_metrics.log</li>
  <li>Custom metrics - log directory/model_metrics.log</li>
</ul>

<p>As mentioned before, Torchserve also exposes <a href="https://github.com/pytorch/serve/blob/master/docs/metrics_api.md">metric API</a>, that by default listens to port 8082 and enables users to query and monitor the collected metrics.  The default metrics endpoint returns Prometheus formatted metrics. You can query metrics using curl requests or point a <a href="https://github.com/pytorch/serve/blob/master/docs/metrics_api.md#prometheus-server">Prometheus Server</a> to the endpoint and use <a href="https://github.com/pytorch/serve/blob/master/docs/metrics_api.md#grafana">Grafana</a> for dashboards.</p>

<p>While serving a model you can query metrics using curl request as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl http://127.0.0.1:8082/metrics
</code></pre></div></div>

<p>In case you are looking into exporting the logged metrics, please refer to this <a href="https://github.com/google/mtail">example</a> that uses mtail to export metrics to Prometheus. Tracking these metrics in a dashboard allows you to monitor performance regressions that may have been sporadic or hard to spot during an offline benchmark run.</p>

<h2 id="what-to-consider-for-tuning-performance-of-a-model-in-production">What to consider for tuning performance of a model in production</h2>

<p>The workflow suggested in Fig 1, is the general idea on how to approach model deployment in production with Torchserve.</p>

<p>In many cases serving models in production is <strong>optimized</strong> <strong>based</strong> on <strong>throughput</strong> or <strong>latency</strong> service level agreement (<strong>SLA)s</strong>. Usually <strong>real-time</strong> <strong>applications</strong> are more concerned about <strong>latency</strong> whereas <strong>off-line applications</strong> may care more about higher <strong>throughput</strong>.</p>

<p>There are a number of main factors contributing to the performance of a serving model in production. In particular, we are focusing on serving Pytorch models with Torchserve here, however most of these factors generalize to all models from other frameworks as well.</p>

<ul>
  <li><strong>Model optimizations</strong>: this is a pre-step for deploying models into production. This is a very broad discussion that we will get into in a series of future blogs. This includes techniques like quantization, pruning to decrease the size of the model, using Intermediate representations (IR graphs) such as Torchscript in Pytorch, fusing kernels and many others. Currently <a href="https://github.com/msaroufim/torchprep">torchprep</a> provides many of these techniques as a CLI tool.</li>
  <li><strong>Batch inference:</strong> it refers to feeding multiple inputs into a model, while it is essential during training, it can be very helpful to manage the cost at inference time as well. Hardware accelerators are optimized for parallelism and batching helps to saturate the compute capacity and often leads to higher throughput. The main difference in inference is you can’t wait too long to get a batch filled from clients, something we call dynamic batching</li>
  <li>
    <p><strong>Number of Workers :</strong> Torchserve uses workers to serve models. Torchserve workers are Python processes that hold a copy of the model weights for running inference. Too few workers means you’re not benefitting from enough parallelism but too many can cause worker contention and degrade end to end performance.</p>
  </li>
  <li><strong>Hardware :</strong> choosing the appropriate hardware based on the model, application and latency, throughput budget. This could be one of the <strong>supported</strong> hardwares in Torchserve, <strong>CPU, GPU, AWS Inferentia</strong>. Some hardware configurations are intended for best in class performance and others are better suited for cost effective inference. From our experiments we’ve found that GPUs shine best at larger batch sizes whereas the right CPUs and AWS Inferentia can be far more cost effective for lower batch sizes and low latency.</li>
</ul>

<h2 id="best-practices-for-performance-tuning-on-torchserve">Best Practices for Performance tuning on Torchserve</h2>

<p>To get the best performance out of your model while serving it with Torchserve, we are sharing some of the best practices here. Torchserve provides a  <a href="https://github.com/pytorch/serve/tree/c87bfec8916d340de5de5810b14a016049b0e395/benchmarks#benchmarking-with-apache-bench">benchmark</a> suite that provides helpful insight to make informed decisions on different choices as detailed below.</p>

<ul>
  <li><strong>Optimize your model</strong> as the first step, Pytorch model optimization <a href="https://pytorch.org/tutorials/">tutorials</a>. <strong>Model optimization</strong> choices are also closely <strong>tied</strong> to the <strong>hardware</strong> of choice. We will discuss it in more detail in another blog post.</li>
  <li><strong>Deciding</strong> the <strong>hardware</strong> for model deployment can be closely related to the latency and throughput budget and cost per inference. Depending on the size of model and application it can vary, for some models like computer vision models it has been historically not affordable to run in production on CPU.  However, by having optimizations such <a href="https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/examples/intel_extension_for_pytorch/README.md">IPEX</a> as recently added to Torchserve this has been much more affordable and cost beneficial and you can learn more in this investigative <a href="https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html">case study</a></li>
  <li>
    <p><strong>Workers</strong> in Torchserve are Python processes that provide parallelism, setting the number of workers should be done carefully. By default Torchserve launch number of workers equal to VCPUs or available GPUs on the host, this can add a considerable amount of time to the Torchserve start.</p>

    <p>Torchserve exposes a <a href="https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/docs/configuration.md#config-model">config property</a> to set the number of workers. To provide an <strong>efficient parallelism</strong> through <strong>multiple workers</strong> and avoiding them to compete over resources, as a baseline we <strong>recommend</strong> following setting on CPU and GPU:</p>

    <p><strong>CPU</strong> : In the handler,  <code class="language-plaintext highlighter-rouge">torch.set_num_threads(1) </code>then set the number of workers to <code class="language-plaintext highlighter-rouge">num physical cores / 2. </code>But the the best threading configurations can be achieved by leveraging the Intel CPU launcher script.</p>

    <p><strong>GPU</strong>: number of available GPUs can be set through<a href="https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/docs/configuration.md#limit-gpu-usage"> number_gpus</a> in config.properties. Torchserve uses round robin to assign workers to GPUs. We recommend setting the number of workers as follows.  <code class="language-plaintext highlighter-rouge">Number of worker = (Number of available GPUs) / (Number of Unique Models). </code>Note that GPUs that are pre-Ampere do not provide any resource isolation with Multi Instance GPUs.</p>
  </li>
  <li><strong>Batch size</strong> can directly affect the latency and the throughput. To better utilize the compute resources batch size needs to be increased. However, there is a tradeoff between latency and throughput. <strong>Larger batch sizes</strong> can <strong>increase</strong> the <strong>throughput but results in a higher latency</strong> as well.  Batch size can be set in Torchserve in two ways, either through<a href="https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/docs/configuration.md#config-model"> model config</a> in config.properties or while registering the model using <a href="https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/docs/management_api.md#scale-workers">Management API</a>.</li>
</ul>

<p>In the next section, we are going to use Torchserve benchmark suite to decide the best combination of model optimization,  hardware, workers, and batch size.</p>

<h2 id="animated-drawings-performance-tuning">Animated Drawings Performance Tuning</h2>

<p>To use the Torchserve benchmark suite, first we need to have an archived file, “.mar” file as discussed above, that contains the model, handler and all other artifacts to load and run inference. Animated Drawings uses Detectron2’s implementation of Mask-RCNN for an object detection model.</p>

<h3 id="how-to-run-benchmark-suite">How to run benchmark suite</h3>

<p>The <a href="https://github.com/pytorch/serve/tree/master/benchmarks#auto-benchmarking-with-apache-bench">Automated benchmark suite</a> in Torchserve let you benchmark multiple models with different setting including batch size and number of worker and finally generate a report for you. To get started:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/pytorch/serve.git

cd serve/benchmarks

pip install -r requirements-ab.txt

apt-get install apache2-utils
</code></pre></div></div>

<p>Model level settings can be configured in a yaml file similar to</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="na">Model_name</span><span class="pi">:</span>
    <span class="na">eager_mode</span><span class="pi">:</span>
        <span class="na">benchmark_engine</span><span class="pi">:</span> <span class="s2">"</span><span class="s">ab"</span>
        <span class="na">url</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Path</span><span class="nv"> </span><span class="s">to</span><span class="nv"> </span><span class="s">.mar</span><span class="nv"> </span><span class="s">file"</span>
        <span class="na">workers</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="m">1</span>
            <span class="pi">-</span> <span class="m">4</span>
        <span class="na">batch_delay</span><span class="pi">:</span> <span class="m">100</span>
        <span class="na">batch_size</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="m">1</span>
            <span class="pi">-</span> <span class="m">2</span>
            <span class="pi">-</span> <span class="m">4</span>
            <span class="pi">-</span> <span class="m">8</span>
        <span class="na">requests</span><span class="pi">:</span> <span class="m">10000</span>
        <span class="na">concurrency</span><span class="pi">:</span> <span class="m">10</span>
        <span class="na">input</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Path</span><span class="nv"> </span><span class="s">to</span><span class="nv"> </span><span class="s">model</span><span class="nv"> </span><span class="s">input"</span>
        <span class="na">backend_profiling</span><span class="pi">:</span> <span class="s">False</span>
        <span class="na">exec_env</span><span class="pi">:</span> <span class="s2">"</span><span class="s">local"</span>
        <span class="na">processors</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">cpu"</span>
            <span class="pi">-</span> <span class="s2">"</span><span class="s">gpus"</span><span class="pi">:</span> <span class="s2">"</span><span class="s">all"</span>

</code></pre></div></div>

<p>This yaml file will be referenced in the <a href="https://github.com/pytorch/serve/blob/master/benchmarks/benchmark_config_template.yaml#L12">benchmark_config_template</a>.yaml file that includes other settings for generating reports, this can optionally work with AWS cloud watch for logs as well.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python benchmarks/auto_benchmark.py --input benchmark_config_template.yaml
</code></pre></div></div>

<p>Running the <strong>benchmarks</strong>, results will be written in “csv” file that can be found in “_ /tmp/benchmark/ab_report.csv_” and full report “/tmp/ts_benchmark/report.md”. It will include items such as Torchserve average latency, model P99 latency, throughput, number of concurrency, number of requests, handler time, and some other metrics. Here we focus on some of the important ones that we track to tune the performance which are, <strong>concurrency</strong>, <strong>model P99</strong> latency, <strong>throughput</strong>. We look at these numbers specifically in <strong>combination</strong> with <strong>batch size</strong>, the used <strong>device, number of workers</strong> and if any <strong>model optimization</strong> has been done.</p>

<p>The <strong>latency SLA</strong> for this model has been set to <strong>100 ms,</strong> this is real-time application and as we discussed earlier, latency is more of a concern and <strong>throughput</strong> ideally should be as high as possible while it does <strong>not violate</strong> the <strong>latency SLA.</strong></p>

<p>Through searching the space, over different batch sizes (1-32), number of workers (1-16) and devices (CPU,GPU), we have run a set of experiments that summarized the best ones in the table below.</p>

<table>
  <tr>
   <td>Device 
   </td>
   <td>Concurrency 
   </td>
   <td># Requests
   </td>
   <td>#workers
   </td>
   <td>Batch size
   </td>
   <td>Payload/image
   </td>
   <td>Optimization 
   </td>
   <td>Throughput 
   </td>
   <td>Latency P99
   </td>
  </tr>
  <tr>
   <td>CPU
   </td>
   <td>10
   </td>
   <td>1000
   </td>
   <td>1
   </td>
   <td>1
   </td>
   <td>small
   </td>
   <td>N/A
   </td>
   <td>3.45
   </td>
   <td>305.3 ms
   </td>
  </tr>
  <tr>
   <td>CPU
   </td>
   <td>1
   </td>
   <td>1000
   </td>
   <td>1
   </td>
   <td>1
   </td>
   <td>small
   </td>
   <td>N/A
   </td>
   <td>3.45
   </td>
   <td>291.8 ms
   </td>
  </tr>
  <tr>
   <td>GPU
   </td>
   <td>10
   </td>
   <td>1000
   </td>
   <td>1
   </td>
   <td>1
   </td>
   <td>small
   </td>
   <td>N/A
   </td>
   <td>41.05
   </td>
   <td>25.48  ms
   </td>
  </tr>
  <tr>
   <td>GPU
   </td>
   <td>1
   </td>
   <td>1000
   </td>
   <td>1
   </td>
   <td>1
   </td>
   <td>small
   </td>
   <td>N/A
   </td>
   <td>42.21
   </td>
   <td>23.6  ms
   </td>
  </tr>
  <tr>
   <td>GPU
   </td>
   <td>10
   </td>
   <td>1000
   </td>
   <td>1
   </td>
   <td>4
   </td>
   <td>small
   </td>
   <td>N/A
   </td>
   <td>54.78
   </td>
   <td>73.62 ms
   </td>
  </tr>
  <tr>
   <td>GPU
   </td>
   <td>10
   </td>
   <td>1000
   </td>
   <td>1
   </td>
   <td>4
   </td>
   <td>small
   </td>
   <td>model.half()
   </td>
   <td>78.62
   </td>
   <td>50.69 ms
   </td>
  </tr>
  <tr>
   <td>GPU
   </td>
   <td>10
   </td>
   <td>1000
   </td>
   <td>1
   </td>
   <td>8
   </td>
   <td>small
   </td>
   <td>model.half()
   </td>
   <td>85.29
   </td>
   <td>94.4 ms
   </td>
  </tr>
</table>

<p>The latency of this model on CPU with all of the tried settings in terms of batch size, concurrency and number of workers did not meet the SLA, in fact ~13x higher.</p>

<p><strong>Moving</strong> the model serving <strong>to GPU</strong>, immediately could <strong>improve</strong> the <strong>latency</strong> ~**13x **from 305 ms down to 23.6 ms.</p>

<p>One of the <strong>simplest</strong> <strong>optimizations</strong> that we could do for the model was lowering its precision to <strong>fp16</strong>, it is one liner (<strong>model.half()</strong>)  and could reduce the <strong>model P99 latency **by  **32%</strong> and increase the throughput by almost the same amount.</p>

<p>There could be other optimization done by Torchscripting the model and using  <a href="https://github.com/pytorch/pytorch/blob/master/torch/jit/_freeze.py#L168">optimize_for_inference</a> or other tricks including onnx or tensorrt runtime optimizations which leverage aggressive fusions are out of the scope of this post. We will discuss model optimizations in a separate post.</p>

<p>We found both on CPU and GPU , setting **number of workers=1 **worked the best in this case.</p>

<ul>
  <li>Moving the model to GPU, using <strong>number of workers = 1</strong>, and <strong>batch size = 1</strong> increased the <strong>Throughput ~12x compared</strong> to <strong>CPU and latency ~13x.</strong></li>
  <li>Moving the model to GPU, using <strong>model.half()</strong>, <strong>number of workers = 1</strong>, and <strong>batch size = 8</strong> yielded <strong>best</strong> results in terms of <strong>Throughput</strong> and tolerable latency. <strong>Throughput</strong> increased <strong>~25x compared</strong> to <strong>CPU with latency still meeting the SLA (94.4ms).</strong></li>
</ul>

<p><em>Note: if you are running the benchmark suite, make sure you are setting a proper <code class="language-plaintext highlighter-rouge">batch_delay</code> and set the concurrency of the request to a number proportional to your batch size. Concurrency here means the number of concurrent requests being sent to the server.</em></p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, we have discussed the considerations and knobs that Torchserve expose to tune the performance in production. We have discussed the Torchserve benchmark suite as a means to tune the performance and get insights on possible choices for model optimizations, hardware choice and cost in general. We used Animated Drawings app which uses Detectron2’s Mask-RCNN model as a case-study to showcase the performance tuning with benchmark suite.</p>

<p>For more details on Performance tuning in Torchserve please refer to our documentation <a href="https://github.com/pytorch/serve/blob/master/docs/performance_guide.md">here</a>.
Also feel free to open a ticket on <a href="https://github.com/pytorch/serve/issues">Torchserve repo</a> for any further questions and feedback.</p>

<h3 id="acknowledgement">Acknowledgement</h3>

<p>We would like to thank Somya Jain (Meta), Christopher Gustave (Meta) for their great support and guidance throughout many steps of this blog and providing insights to Sketch Animator workflow. Also, special thanks to<a href="https://www.linkedin.com/in/li-ning-7274604/"> Li Ning</a> from AWS for the great efforts to make performance tuning much easier on Torchserve with automated benchmark suite.</p>

<style>

    td{
        border: 1px solid black;
    }
    
    /* article.pytorch-article table tr td:first-of-type{
        padding: 0.3125rem;
    }

    article.pytorch-article table td {
    padding: 0.3125rem;
    } */

   li a:focus, li a:hover, li a:active{
    cursor: pointer;
    text-decoration: underline;
    }

    ol a:hover, ol a:active{
    cursor: pointer;
    text-decoration: underline;
    }

}

</style>


                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Introduction to Quantization on PyTorch | PyTorch
    
  </title>
  <meta property="og:title" content="PyTorch"/>
<meta property="og:description" content="An open source deep learning platform that provides a seamless path from research prototyping to production deployment."/>
<meta property="og:url" content="https://www.pytorch.org"/>
<meta property="og:type" content="website"/>
<meta property="og:image" content="https://pytorch.org/assets/images/pytorch-logo.png"/>

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('create', 'UA-117752657-2', 'auto', 'newCampaignTracker');
  ga('send', 'pageview');

</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>

  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');

</script>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
</head>


<body class="blog">
    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item ">

      <div class="ecosystem-dropdown">
        <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
          Ecosystem
        </a>
        <div class="ecosystem-dropdown-menu">
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class=dropdown-title>Tools & Libraries</span>
            <p>Explore the ecosystem of tools and libraries</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <a href="/docs">Docs</a>
    </li>

    <li class="main-menu-item ">

      <div class="resources-dropdown">
        <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">March 26, 2020</p>
            <h1>
                <a class="blog-title">Introduction to Quantization on PyTorch</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath, and Seth Weidman
                      
                    </p>
                    <p>It’s important to make efficient use of both server-side and on-device compute resources when developing machine learning applications. To support more efficient deployment on servers and edge devices, PyTorch added a support for model quantization using the familiar eager mode Python API.</p>

<p>Quantization leverages 8bit integer (int8) instructions to reduce the model size and run the inference faster (reduced latency) and can be the difference between a model achieving quality of service goals or even fitting into the resources available on a mobile device. Even when resources aren’t quite so constrained it may enable you to deploy a larger and more accurate model. Quantization is available in PyTorch starting in version 1.3 and with the release of PyTorch 1.4 we published quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2 in the PyTorch torchvision 0.5 library.</p>

<p>This blog post provides an overview of the quantization support on PyTorch and its incorporation with the TorchVision domain library.</p>

<h2 id="what-is-quantization"><strong>What is Quantization?</strong></h2>

<p>Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually int8 compared to floating point implementations. This enables performance gains in several important areas:</p>
<ul>
  <li>4x reduction in model size;</li>
  <li>2-4x reduction in memory bandwidth;</li>
  <li>2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).</li>
</ul>

<p>Quantization does not however come without additional cost. Fundamentally quantization means introducing approximations and the resulting networks have slightly less accuracy. These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy.</p>

<p>We designed quantization to fit into the PyTorch framework. The means that:</p>
<ol>
  <li>PyTorch has data types corresponding to <a href="https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor">quantized tensors</a>, which share many of the features of tensors.</li>
  <li>One can write kernels with quantized tensors, much like kernels for floating point tensors to customize their implementation. PyTorch supports quantized modules for common operations as part of the <code class="highlighter-rouge">torch.nn.quantized</code> and <code class="highlighter-rouge">torch.nn.quantized.dynamic</code> name-space.</li>
  <li>Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. The quantization method is virtually identical for both server and mobile backends. One can easily mix quantized and floating point operations in a model.</li>
  <li>Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.</li>
</ol>

<div class="text-center">
  <img src="https://pytorch.org/assets/images/torch_stack1.png" width="100%" />
</div>

<p>We developed three techniques for quantizing neural networks in PyTorch as part of quantization tooling in the <code class="highlighter-rouge">torch.quantization</code> name-space.</p>

<h2 id="the-three-modes-of-quantization-supported-in-pytorch-starting-version-13"><strong>The Three Modes of Quantization Supported in PyTorch starting version 1.3</strong></h2>

<ol>
  <li>
    <h3 id="dynamic-quantization"><strong>Dynamic Quantization</strong></h3>
    <p>The easiest method of quantization PyTorch supports is called <strong>dynamic quantization</strong>. This involves not just converting the weights to int8 - as happens in all quantization variants - but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”). The computations will thus be performed using efficient int8 matrix multiplication and convolution implementations, resulting in faster compute. However, the activations are read and written to memory in floating point format.</p>
    <ul>
      <li><strong>PyTorch API</strong>: we have a simple API for dynamic quantization in PyTorch. <code class="highlighter-rouge">torch.quantization.quantize_dynamic</code> takes in a model, as well as a couple other arguments, and produces a quantized model! Our <a href="https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html">end-to-end tutorial</a> illustrates this for a BERT model; while the tutorial is long and contains sections on loading pre-trained models and other concepts unrelated to quantization, the part the quantizes the BERT model is simply:</li>
    </ul>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.quantization</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize_dynamic</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">)</span>
</code></pre></div>    </div>
    <ul>
      <li>See the documentation for the function <a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic">here</a> an end-to-end example in our tutorials <a href="https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html">here</a> and <a href="https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html">here</a>.</li>
    </ul>
  </li>
  <li>
    <h3 id="post-training-static-quantization"><strong>Post-Training Static Quantization</strong></h3>

    <p>One can further improve the performance (latency) by converting networks to use both integer arithmetic and int8 memory accesses. Static quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting “observer” modules at different points that record these distributions). This information is used to determine how specifically the different activations should be quantized at inference time (a simple technique would be to simply divide the entire range of activations into 256 levels, but we support more sophisticated methods as well). Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.</p>

    <p>With this release, we’re supporting several features that allow users to optimize their static quantization:</p>
    <ol>
      <li>Observers: you can customize observer modules which specify how statistics are collected prior to quantization to try out more advanced methods to quantize your data.</li>
      <li>Operator fusion: you can fuse multiple operations into a single operation, saving on memory access while also improving the operation’s numerical accuracy.</li>
      <li>Per-channel quantization: we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed.</li>
    </ol>

    <ul>
      <li>
        <h3 id="pytorch-api"><strong>PyTorch API</strong>:</h3>
        <ul>
          <li>To fuse modules, we have <code class="highlighter-rouge">torch.quantization.fuse_modules</code></li>
          <li>Observers are inserted using <code class="highlighter-rouge">torch.quantization.prepare</code></li>
          <li>Finally, quantization itself is done using <code class="highlighter-rouge">torch.quantization.convert</code></li>
        </ul>
      </li>
    </ul>

    <p>We have a tutorial with an end-to-end example of quantization (this same tutorial also covers our third quantization method, quantization-aware training), but because of our simple API, the three lines that perform post-training static quantization on the pre-trained model <code class="highlighter-rouge">myModel</code> are:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># set quantization config for server (x86)</span>
<span class="n">deploymentmyModel</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">(</span><span class="s">'fbgemm'</span><span class="p">)</span>

<span class="c"># insert observers</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">myModel</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c"># Calibrate the model and collect statistics</span>

<span class="c"># convert to quantized version</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">myModel</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="quantization-aware-training"><strong>Quantization Aware Training</strong></h3>
<p><strong>Quantization-aware training(QAT)</strong> is the third method, and the one that typically results in highest accuracy of these three. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.</p>
<ul>
  <li>
    <h3 id="pytorch-api-1"><strong>PyTorch API</strong>:</h3>
    <ul>
      <li><code class="highlighter-rouge">torch.quantization.prepare_qat</code> inserts fake quantization modules to model quantization.</li>
      <li>Mimicking the static quantization API, <code class="highlighter-rouge">torch.quantization.convert</code> actually quantizes the model once training is complete.</li>
    </ul>
  </li>
</ul>

<p>For example, in <a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">the end-to-end example</a>, we load in a pre-trained model as <code class="highlighter-rouge">qat_model</code>, then we simply perform quantization-aware training using:</p>
<ul>
  <li>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># specify quantization config for QAT</span>
<span class="n">qat_model</span><span class="o">.</span><span class="n">qconfig</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s">'fbgemm'</span><span class="p">)</span>

<span class="c"># prepare QAT</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare_qat</span><span class="p">(</span><span class="n">qat_model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c"># convert to quantized version, removing dropout, to check for accuracy on each</span>
<span class="n">epochquantized_model</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">qat_model</span><span class="o">.</span><span class="nb">eval</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="device-and-operator-support"><strong>Device and Operator Support</strong></h3>
<p>Quantization support is restricted to a subset of available operators, depending on the method being used, for a list of supported operators, please see the documentation at <a href="https://pytorch.org/docs/stable/quantization.html">https://pytorch.org/docs/stable/quantization.html</a>.</p>

<p>The set of available operators and the quantization numerics also depend on the backend being used to run quantized models. Currently quantized operators are supported only for CPU inference in the following backends: x86 and ARM. Both the quantization configuration (how tensors should be quantized and the quantized kernels (arithmetic with quantized tensors) are backend dependent. One can specify the backend by doing:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torchbackend</span><span class="o">=</span><span class="s">'fbgemm'</span>
<span class="c"># 'fbgemm' for server, 'qnnpack' for mobile</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
<span class="c"># prepare and convert model</span>
<span class="c"># Set the backend on which the quantized kernels need to be run</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">engine</span><span class="o">=</span><span class="n">backend</span>
</code></pre></div></div>

<p>However, quantization aware training occurs in full floating point and can run on either GPU or CPU. Quantization aware training is typically only used in CNN models when post training static or dynamic quantization doesn’t yield sufficient accuracy. This can occur with models that are highly optimized to achieve small size (such as Mobilenet).</p>

<h4 id="integration-in-torchvision"><strong>Integration in torchvision</strong></h4>
<p>We’ve also enabled quantization for some of the most popular models in <a href="https://github.com/pytorch/vision/tree/master/torchvision/models/quantization">torchvision</a>: Googlenet, Inception, Resnet, ResNeXt, Mobilenet and Shufflenet. We have upstreamed these changes to torchvision in three forms:</p>
<ol>
  <li>Pre-trained quantized weights so that you can use them right away.</li>
  <li>Quantization ready model definitions so that you can do post-training quantization or quantization aware training.</li>
  <li>A script for doing quantization aware training — which is available for any of these model though, as you will learn below, we only found it necessary for achieving accuracy with Mobilenet.</li>
  <li>We also have a <a href="https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html">tutorial</a> showing how you can do transfer learning with quantization using one of the torchvision models.</li>
</ol>

<h3 id="choosing-an-approach"><strong>Choosing an approach</strong></h3>
<p>The choice of which scheme to use depends on multiple factors:</p>
<ol>
  <li>Model/Target requirements: Some models might be sensitive to quantization, requiring quantization aware training.</li>
  <li>Operator/Backend support: Some backends require fully quantized operators.</li>
</ol>

<p>Currently, operator coverage is limited and may restrict the choices listed in the table below:
The table below provides a guideline.</p>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border-color:black;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;font-weight:bold;color:black;}
article.pytorch-article table tr th:first-of-type, article.pytorch-article table tr td:first-of-type{padding-left:5px}
</style>

<table class="tg">
  <tr>
    <th class="tg-0pky">Model Type</th>
    <th class="tg-0pky">Preferred scheme</th>
    <th class="tg-0pky">Why</th>
  </tr>
  <tr>
    <td class="tg-lboi">LSTM/RNN</td>
    <td class="tg-lboi">Dynamic Quantization</td>
    <td class="tg-lboi">Throughput dominated by compute/memory bandwidth for weights</td>
  </tr>
  <tr>
    <td class="tg-lboi">BERT/Transformer</td>
    <td class="tg-lboi">Dynamic Quantization</td>
    <td class="tg-lboi">Throughput dominated by compute/memory bandwidth for weights</td>
  </tr>
  <tr>
    <td class="tg-lboi">CNN</td>
    <td class="tg-lboi">Static Quantization</td>
    <td class="tg-lboi">Throughput limited by memory bandwidth for activations</td>
  </tr>
  <tr>
    <td class="tg-lboi">CNN</td>
    <td class="tg-lboi">Quantization Aware Training</td>
    <td class="tg-lboi">In the case where accuracy can't be achieved with static quantization</td>
  </tr>
</table>

<h3 id="performance-results"><strong>Performance Results</strong></h3>
<p>Quantization provides a 4x reduction in the model size and a speedup of 2x to 3x compared to floating point implementations depending on the hardware platform and the model being benchmarked. Some sample results are:</p>

<div class="table-responsive">
  <table class="tg">
    <tr>
      <td class="tg-0pky">Model</td>
      <td class="tg-0pky">Float Latency (ms)</td>
      <td class="tg-0pky">Quantized Latency (ms)</td>
      <td class="tg-0pky">Inference Performance Gain</td>
      <td class="tg-0pky">Device</td>
      <td class="tg-0pky">Notes</td>
    </tr>
    <tr>
      <td class="tg-lboi">BERT</td>
      <td class="tg-lboi">581</td>
      <td class="tg-lboi">313</td>
      <td class="tg-lboi">1.8x</td>
      <td class="tg-lboi">Xeon-D2191 (1.6GHz)</td>
      <td class="tg-lboi">Batch size = 1, Maximum sequence length= 128, Single thread, x86-64, Dynamic quantization</td>
    </tr>
    <tr>
      <td class="tg-lboi">Resnet-50</td>
      <td class="tg-lboi">214</td>
      <td class="tg-lboi">103</td>
      <td class="tg-lboi">2x</td>
      <td class="tg-lboi">Xeon-D2191 (1.6GHz)</td>
      <td class="tg-lboi">Single thread, x86-64, Static quantization</td>
    </tr>
    <tr>
      <td class="tg-lboi">Mobilenet-v2</td>
      <td class="tg-lboi">97</td>
      <td class="tg-lboi">17</td>
      <td class="tg-lboi">5.7x</td>
      <td class="tg-lboi">Samsung S9</td>
      <td class="tg-lboi">Static quantization, Floating point numbers are based on Caffe2 run-time and are not optimized</td>
    </tr>
  </table>
</div>

<h3 id="accuracy-results"><strong>Accuracy results</strong></h3>
<p>We also compared the accuracy of static quantized models with the floating point models on Imagenet. For dynamic quantization, we <a href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py">compared</a> the F1 score of BERT on the GLUE benchmark for MRPC.</p>

<h4 id="computer-vision-model-accuracy"><strong>Computer Vision Model accuracy</strong></h4>

<table class="tg">
  <tr>
    <td class="tg-0pky">Model</td>
    <td class="tg-0pky">Top-1 Accuracy (Float)</td>
    <td class="tg-0pky">Top-1 Accuracy (Quantized)</td>
    <td class="tg-0pky">Quantization scheme</td>
  </tr>
  <tr>
    <td class="tg-cly1">Googlenet</td>
    <td class="tg-cly1">69.8</td>
    <td class="tg-cly1">69.7</td>
    <td class="tg-cly1">Static post training quantization</td>
  </tr>
  <tr>
    <td class="tg-cly1">Inception-v3</td>
    <td class="tg-cly1">77.5</td>
    <td class="tg-cly1">77.1</td>
    <td class="tg-cly1">Static post training quantization</td>
  </tr>
  <tr>
    <td class="tg-0lax">ResNet-18</td>
    <td class="tg-0lax">69.8</td>
    <td class="tg-0lax">69.4</td>
    <td class="tg-0lax">Static post training quantization</td>
  </tr>
  <tr>
    <td class="tg-0lax">Resnet-50</td>
    <td class="tg-0lax">76.1</td>
    <td class="tg-0lax">75.9</td>
    <td class="tg-0lax">Static post training quantization</td>
  </tr>
  <tr>
    <td class="tg-0lax">ResNext-101 32x8d</td>
    <td class="tg-0lax">79.3</td>
    <td class="tg-0lax">79</td>
    <td class="tg-0lax">Static post training quantization</td>
  </tr>
  <tr>
    <td class="tg-0lax">Mobilenet-v2</td>
    <td class="tg-0lax">71.9</td>
    <td class="tg-0lax">71.6</td>
    <td class="tg-0lax">Quantization Aware Training</td>
  </tr>
  <tr>
    <td class="tg-0lax">Shufflenet-v2</td>
    <td class="tg-0lax">69.4</td>
    <td class="tg-0lax">68.4</td>
    <td class="tg-0lax">Static post training quantization</td>
  </tr>
</table>

<h4 id="speech-and-nlp-model-accuracy"><strong>Speech and NLP Model accuracy</strong></h4>

<div class="table-responsive">
  <table class="tg">
    <tr>
      <td class="tg-0pky">Model</td>
      <td class="tg-0pky">F1 (GLUEMRPC) Float</td>
      <td class="tg-0pky">F1 (GLUEMRPC) Quantized</td>
      <td class="tg-0pky">Quantization scheme</td>
    </tr>
    <tr>
      <td class="tg-cly1">BERT</td>
      <td class="tg-cly1">0.902</td>
      <td class="tg-cly1">0.895</td>
      <td class="tg-cly1">Dynamic quantization</td>
    </tr>
  </table>
</div>

<h3 id="conclusion"><strong>Conclusion</strong></h3>
<p>To get started on quantizing your models in PyTorch, start with <a href="https://pytorch.org/tutorials/#model-optimization">the tutorials on the PyTorch website</a>. If you are working with sequence data start with <a href="https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html">dynamic quantization for LSTM</a>, or <a href="https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html">BERT</a>. If you are working with image data then we recommend starting with the <a href="https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html">transfer learning with quantization</a> tutorial. Then you can explore <a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">static post training quantization</a>. If you find that the accuracy drop with post training quantization is too high, then try <a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">quantization aware training</a>.</p>

<p>If you run into issues you can get community help by posting in at <a href="https://discuss.pytorch.org/">discuss.pytorch.org</a>, use the quantization category for quantization related issues.</p>

<p><em>This post is authored by Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath and Seth Weidman. Special thanks to Jianyu Huang, Lingyi Liu and Haixin Liu for producing quantization metrics included in this post.</em></p>

<h3 id="further-reading"><strong>Further reading</strong>:</h3>
<ol>
  <li>PyTorch quantization presentation at Neurips: <a href="https://research.fb.com/wp-content/uploads/2019/12/2.-Quantization.pptx">(https://research.fb.com/wp-content/uploads/2019/12/2.-Quantization.pptx)</a></li>
  <li>Quantized Tensors <a href="https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor">(https://github.com/pytorch/pytorch/wiki/
Introducing-Quantized-Tensor)</a></li>
  <li>Quantization RFC on Github <a href="https://github.com/pytorch/pytorch/issues/18318">(https://github.com/pytorch/pytorch/
issues/18318)</a></li>
</ol>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col follow-us-col">
        <ul>
          <li class="list-title">Stay Connected</li>
          <li>
            
            <div id="mc_embed_signup">
  <form
    action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
    method="post"
    id="mc-embedded-subscribe-form"
    name="mc-embedded-subscribe-form"
    class="email-subscribe-form validate"
    target="_blank"
    novalidate>
    <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
      <div class="mc-field-group">
        <label for="mce-EMAIL" style="display:none;">Email Address</label>
        <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
      </div>

      <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
      </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

      <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

      <div class="clear">
        <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
      </div>
    </div>
  </form>
</div>

          </li>
        </ul>

        <div class="footer-social-icons">
          <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
          <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
        </div>
      </div>
    </div>
    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-tos-privacy-policy.pdf" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-oss-privacy-policy.pdf" target="_blank">Privacy</a></li>
      </ul>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="">
          <a href="/features">Features</a>
        </li>

        <li class="">
          <a href="/ecosystem">Ecosystem</a>
        </li>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="">
          <a href="/hub">PyTorch Hub</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li>
          <a href="/docs">Docs</a>
        </li>

        <li class="">
          <a href="/resources">Resources</a>
        </li>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top === 0) {
        $(".header-holder").css({"backgroundColor": "transparent"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>



</body>

</html>

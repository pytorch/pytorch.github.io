<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models | PyTorch
    
  </title>
  
  <meta property="og:title" content="PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models" />
  <meta property="og:description" content="In this blog post, we describe the first peer-reviewed research paper that explores accelerating the hybrid of PyTorch DDP (torch.nn.parallel.DistributedDataParallel) [1] and Pipeline (torch.distributed.pipeline) - PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models (Transformers such as BERT [2]  and ViT [3]), published at ICML 2021.

" />
  <meta property="og:image" content="https://pytorch.org/assets/images/pipetransformer_overview.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models">
  <meta name="twitter:description" content="In this blog post, we describe the first peer-reviewed research paper that explores accelerating the hybrid of PyTorch DDP (torch.nn.parallel.DistributedDataParallel) [1] and Pipeline (torch.distributed.pipeline) - PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models (Transformers such as BERT [2]  and ViT [3]), published at ICML 2021.

" />


<meta property="og:type" content="website" />
<meta name="robots" content="index, follow" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
</head>


<body class="blog">
    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow" href="/ecosystem">
          Ecosystem
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/pted/2021">
            <span class="dropdown-title">Ecosystem Day - 2021</span>
            <p>See the posters presented at ecosystem day 2021</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item ">
      <a href="/mobile">Mobile</a>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="doc-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">August 18, 2021</p>
            <h1>
                <a class="blog-title">PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr
                      
                    </p>
                    <p>In this blog post, we describe the first peer-reviewed research paper that explores accelerating the hybrid of PyTorch DDP (<code class="language-plaintext highlighter-rouge">torch.nn.parallel.DistributedDataParallel</code>) [1] and Pipeline (<code class="language-plaintext highlighter-rouge">torch.distributed.pipeline</code>) - <a href="http://proceedings.mlr.press/v139/he21a.html">PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models</a> (Transformers such as BERT [2]  and ViT [3]), published at ICML 2021.</p>

<p>PipeTransformer leverages automated elastic pipelining for efficient distributed training of Transformer models. In PipeTransformer, we designed an adaptive on-the-fly freeze algorithm that can identify and freeze some layers gradually during training and an elastic pipelining system that can dynamically allocate resources to train the remaining active layers. More specifically, PipeTransformer automatically excludes frozen layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width. We evaluate PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on SQuAD and GLUE datasets. Our results show that compared to the state-of-the-art baseline, PipeTransformer attains up to 2.83-fold speedup without losing accuracy. We also provide various performance analyses for a more comprehensive understanding of our algorithmic and system-wise design.</p>

<p>Next, we will introduce the background, motivation, our idea, design, and how we implement the algorithm and system with PyTorch Distributed APIs.</p>

<ul>
  <li>Paper: <a href="http://proceedings.mlr.press/v139/he21a.html">http://proceedings.mlr.press/v139/he21a.html</a></li>
  <li>Source Code: <a href="https://distml.ai">https://DistML.ai</a>.</li>
  <li>Slides: <a href="https://docs.google.com/presentation/d/1t6HWL33KIQo2as0nSHeBpXYtTBcy0nXCoLiKd0EashY/edit?usp=sharing">https://docs.google.com/presentation/d/1t6HWL33KIQo2as0nSHeBpXYtTBcy0nXCoLiKd0EashY/edit?usp=sharing</a></li>
</ul>

<h1 id="introduction">Introduction</h1>
<p align="center">
<img src="https://pytorch.org/assets/images/model_size.png" alt="Model Size" width="500" />
<br />
Figure 1: the Parameter Number of Transformer Models Increases Dramatically.
</p>

<p>Large Transformer models [4][5] have powered accuracy breakthroughs in both natural language processing and computer vision. GPT-3 [4] hit a new record high accuracy for nearly all NLP tasks. Vision Transformer (ViT) [3] also achieved 89\% top-1 accuracy in ImageNet, outperforming state-of-the-art convolutional networks ResNet-152 and EfficientNet. To tackle the growth in model sizes, researchers have proposed various distributed training techniques, including parameter servers [6][7][8], pipeline parallelism [9][10][11][12], intra-layer parallelism [13][14][15], and zero redundancy data-parallel [16].</p>

<p>Existing distributed training solutions, however, only study scenarios where all model weights are required to be optimized throughout the training (i.e., computation and communication overhead remains relatively static over different iterations). Recent works on <em>progressive training</em> suggest that parameters in neural networks can be trained dynamically:</p>

<ul>
  <li>Freeze Training: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. NeurIPS 2017</li>
  <li>Efficient Training of BERT by Progressively Stacking. ICML 2019</li>
  <li>Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. NeurIPS 2020.</li>
  <li>On the Transformer Growth for Progressive BERT Training. NACCL 2021</li>
</ul>

<p align="center">
<img src="https://pytorch.org/assets/images/freeze_training.png" alt="Freeze Training" width="560" />
<br />
</p>
<p>Figure 2. Interpretable Freeze Training: DNNs converge bottom-up (Results on CIFAR10 using ResNet). Each pane shows layer-by-layer similarity using SVCCA [17][18]</p>

<p>For example, in freeze training [17][18], neural networks usually converge from the bottom-up (i.e., not all layers need to be trained all the way through training). Figure 2 shows an example of how weights gradually stabilize during training in this approach. This observation motivates us to utilize freeze training for distributed training of Transformer models to accelerate training by dynamically allocating resources to focus on a shrinking set of active layers. Such a layer freezing strategy is especially pertinent to pipeline parallelism, as excluding consecutive bottom layers from the pipeline can reduce computation, memory, and communication overhead.</p>

<p align="center">
<img src="https://pytorch.org/assets/images/PipeTransformer.png" width="50%" />
<br />
Figure 3. The process of PipeTransformer’s automated and elastic pipelining to accelerate distributed training of Transformer models
</p>

<p>We propose PipeTransformer, an elastic pipelining training acceleration framework that automatically reacts to frozen layers by dynamically transforming the scope of the pipelined model and the number of pipeline replicas. To the best of our knowledge, this is the first paper that studies layer freezing in the context of both pipeline and data-parallel training. Figure 3 demonstrates the benefits of such a combination. First, by excluding frozen layers from the pipeline, the same model can be packed into fewer GPUs, leading to both fewer cross-GPU communications and smaller pipeline bubbles. Second, after packing the model into fewer GPUs, the same cluster can accommodate more pipeline replicas, increasing the width of data parallelism. More importantly, the speedups acquired from these two benefits are multiplicative rather than additive, further accelerating the training.</p>

<p>The design of PipeTransformer faces four major challenges. First, the freeze algorithm must make on-the-fly and adaptive freezing decisions; however, existing work [17][18] only provides a posterior analysis tool. Second, the efficiency of pipeline re-partitioning results is influenced by multiple factors, including partition granularity, cross-partition activation size, and the chunking (the number of micro-batches) in mini-batches, which require reasoning and searching in a large solution space. Third, to dynamically introduce additional pipeline replicas, PipeTransformer must overcome the static nature of collective communications and avoid potentially complex cross-process messaging protocols when onboarding new processes (one pipeline is handled by one process). Finally, caching can save time for repeated forward propagation of frozen layers, but it must be shared between existing pipelines and newly added ones, as the system cannot afford to create and warm up a dedicated cache for each replica.</p>

<p align="center">
<img src="https://pytorch.org/assets/images/PipeTransformer-Animation.gif" alt="Freeze Training" />
<br />
Figure 4: An Animation to Show the Dynamics of PipeTransformer
</p>

<p>As shown in the animation (Figure 4), PipeTransformer is designed with four core building blocks to address the aforementioned challenges. First, we design a tunable and adaptive algorithm to generate signals that guide the selection of layers to freeze over different iterations (Freeze Algorithm). Once triggered by these signals, our elastic pipelining module (AutoPipe), then packs the remaining active layers into fewer GPUs by taking both activation sizes and variances of workloads across heterogeneous partitions (frozen layers and active layers) into account. It then splits a mini-batch into an optimal number of micro-batches based on prior profiling results for different pipeline lengths. Our next module, AutoDP, spawns additional pipeline replicas to occupy freed-up GPUs and maintains hierarchical communication process groups to attain dynamic membership for collective communications. Our final module, AutoCache, efficiently shares activations across existing and new data-parallel processes and automatically replaces stale caches during transitions.</p>

<p>Overall, PipeTransformer combines the Freeze Algorithm, AutoPipe, AutoDP, and AutoCache modules to provide a significant training speedup.
We evaluate PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on GLUE and SQuAD datasets. Our results show that PipeTransformer attains up to 2.83-fold speedup without losing accuracy. We also provide various performance analyses for a more comprehensive understanding of our algorithmic and system-wise design.
Finally, we have also developed open-source flexible APIs for PipeTransformer, which offer a clean separation among the freeze algorithm, model definitions, and training accelerations, allowing for transferability to other algorithms that require similar freezing strategies.</p>

<h1 id="overall-design">Overall Design</h1>

<p>Suppose we aim to train a massive model in a distributed training system where the hybrid of pipelined model parallelism and data parallelism is used to target scenarios where either the memory of a single GPU device cannot hold the model, or if loaded, the batch size is small enough to avoid running out of memory. More specifically, we define our settings as follows:</p>

<p><strong>Training task and model definition.</strong> We train Transformer models (e.g., Vision Transformer, BERT on large-scale image or text datasets. The Transformer model <img src="https://render.githubusercontent.com/render/math?math=mathcal{F}" /> has <img src="https://render.githubusercontent.com/render/math?math=L" /> layers, in which the <img src="https://render.githubusercontent.com/render/math?math=i" /> th layer is composed of a forward computation function <img src="https://render.githubusercontent.com/render/math?math=f_i" /> and a corresponding set of parameters.</p>

<p><strong>Training infrastructure.</strong> Assume the training infrastructure contains a GPU cluster that has <img src="https://render.githubusercontent.com/render/math?math=N" /> GPU servers (i.e. nodes). Each node has <img src="https://render.githubusercontent.com/render/math?math=I" /> GPUs. Our cluster is homogeneous, meaning that each GPU and server have the same hardware configuration. Each GPU’s memory capacity is <img src="https://render.githubusercontent.com/render/math?math=M_\text{GPU}" />. Servers are connected by a high bandwidth network interface such as InfiniBand interconnect.</p>

<p><strong>Pipeline parallelism.</strong> In each machine, we load a model <img src="https://render.githubusercontent.com/render/math?math=\mathcal{F}" /> into a pipeline <img src="https://render.githubusercontent.com/render/math?math=\mathcal{P}" /> which has <img src="https://render.githubusercontent.com/render/math?math=K" />partitions (<img src="https://render.githubusercontent.com/render/math?math=K" /> also represents the pipeline length). The <img src="https://render.githubusercontent.com/render/math?math=k" />th partition <img src="https://render.githubusercontent.com/render/math?math=p_k" /> consists of consecutive layers. We assume each partition is handled by a single GPU device. <img src="https://render.githubusercontent.com/render/math?math=1 \leq K \leq I" />, meaning that we can build multiple pipelines for multiple model replicas in a single machine. We assume all GPU devices in a pipeline belonging to the same machine. Our pipeline is a synchronous pipeline, which does not involve stale gradients, and the number of micro-batches is <img src="https://render.githubusercontent.com/render/math?math=M" />. In the Linux OS, each pipeline is handled by a single process. We refer the reader to GPipe [10] for more details.</p>

<p><strong>Data parallelism.</strong> DDP is a cross-machine distributed data-parallel process group within <img src="https://render.githubusercontent.com/render/math?math=R" /> parallel workers. Each worker is a pipeline replica (a single process). The <img src="https://render.githubusercontent.com/render/math?math=r" />th worker’s index (ID) is rank <img src="https://render.githubusercontent.com/render/math?math=r" />. For any two pipelines in DDP, they can belong to either the same GPU server or different GPU servers, and they can exchange gradients with the AllReduce algorithm.</p>

<p>Under these settings, our goal is to accelerate training by leveraging freeze training, which does not require all layers to be trained throughout the duration of the training. Additionally, it may help save computation, communication, memory cost, and potentially prevent overfitting by consecutively freezing layers. However, these benefits can only be achieved by overcoming the four challenges of designing an adaptive freezing algorithm, dynamical pipeline re-partitioning, efficient resource reallocation, and cross-process caching, as discussed in the introduction.</p>

<p align="center">
<img src="https://pytorch.org/assets/images/pipetransformer_overview.png" alt="Overview" width="560" />
<br />
Figure 5. Overview of PipeTransformer Training System
</p>

<p>PipeTransformer co-designs an on-the-fly freeze algorithm and an automated elastic pipelining training system that can dynamically transform the scope of the pipelined model and the number of pipeline replicas. The overall system architecture is illustrated in Figure 5.  To support PipeTransformer’s elastic pipelining, we maintain a customized version of PyTorch Pipeline. For data parallelism, we use PyTorch DDP as a baseline. Other libraries are standard mechanisms of an operating system (e.g.,multi-processing) and thus avoid specialized software or hardware customization requirements. To ensure the generality of our framework, we have decoupled the training system into four core components: <strong>freeze algorithm</strong>, <strong>AutoPipe</strong>, <strong>AutoDP</strong>, and <strong>AutoCache</strong>. The <strong>freeze algorithm</strong> (grey) samples indicators from the training loop and makes layer-wise freezing decisions, which will be shared with <strong>AutoPipe</strong> (green). AutoPipe is an elastic pipeline module that speeds up training by excluding frozen layers from the pipeline and packing the active layers into fewer GPUs (pink), leading to both fewer cross-GPU communications and smaller pipeline bubbles. Subsequently, <strong>AutoPipe</strong> passes pipeline length information to <strong>AutoDP</strong> (purple), which then spawns more pipeline replicas to increase data-parallel width, if possible. The illustration also includes an example in which AutoDP introduces a new replica (purple). <strong>AutoCache</strong> (orange edges) is a cross-pipeline caching module, as illustrated by connections between pipelines. The source code architecture is aligned with Figure 5 for readability and generality.</p>

<h1 id="implementation-using-pytorch-apis">Implementation Using PyTorch APIs</h1>

<p>As can be seen from Figure 5, PipeTransformers contain four components: Freeze Algorithm, AutoPipe, AutoDP, and AutoCache. Among them, AutoPipe and AutoDP relies on PyTorch DDP (<code class="language-plaintext highlighter-rouge">torch.nn.parallel.DistributedDataParallel</code>) [1] and Pipeline (<code class="language-plaintext highlighter-rouge">torch.distributed.pipeline</code>), respectively. In this blog, we only highlight the key implementation details of AutoPipe and AutoDP. For details of Freeze Algorithm and AutoCache, please refer to our paper.</p>

<h2 id="autopipe-elastic-pipelining">AutoPipe: Elastic Pipelining</h2>

<p>AutoPipe can accelerate training by excluding frozen layers from the pipeline and packing the active layers into fewer GPUs. This section elaborates on the key components of AutoPipe that dynamically 1) partition pipelines, 2) minimize the number of pipeline devices, and 3) optimize mini-batch chunk size accordingly.</p>

<h3 id="basic-usage-of-pytorch-pipeline">Basic Usage of PyTorch Pipeline</h3>

<p>Before diving into details of AutoPipe, let us warm up the basic usage of PyTorch Pipeline (<code class="language-plaintext highlighter-rouge">torch.distributed.pipeline.sync.Pipe</code>, see <a href="https://pytorch.org/docs/stable/pipeline.html">this tutorial</a>). More specially, we present a simple example to understand the design of Pipeline in practice:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: build a model including two linear layers
</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">).</span><span class="n">cuda</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">).</span><span class="n">cuda</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Step 2: wrap the two layers with nn.Sequential
</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">fc1</span><span class="p">,</span> <span class="n">fc2</span><span class="p">)</span>

<span class="c1"># Step 3: build Pipe (torch.distributed.pipeline.sync.Pipe)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Pipe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># do training/inference
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">).</span><span class="n">cuda</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">output_rref</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div></div>

<p>In this basic example, we can see that before initializing <code class="language-plaintext highlighter-rouge">Pipe</code>, we need to partition the model <code class="language-plaintext highlighter-rouge">nn.Sequential</code> into multiple GPU devices and set optimal chunk number (<code class="language-plaintext highlighter-rouge">chunks</code>). Balancing computation time across partitions is critical to pipeline training speed, as skewed workload distributions across stages can lead to stragglers and forcing devices with lighter workloads to wait. The chunk number may also have a non-trivial influence on the throughput of the pipeline.</p>

<h3 id="balanced-pipeline-partitioning">Balanced Pipeline Partitioning</h3>

<p>In dynamic training system such as PipeTransformer, maintaining optimally balanced partitions in terms of parameter numbers does not guarantee the fastest training speed because other factors also play a crucial role:</p>

<p align="center">
<img src="https://pytorch.org/assets/images/balancing_partition.png" width="560" />
<br />
Figure 6. The partition boundary is in the middle of a skip connection
</p>

<ol>
  <li>
    <p><strong>Cross-partition communication overhead.</strong> Placing a partition boundary in the middle of a skip connection leads to additional communications since tensors in the skip connection must now be copied to a different GPU. For example, with BERT partitions in Figure 6, partition <img src="https://render.githubusercontent.com/render/math?math=k" /> must take intermediate outputs from both partition <img src="https://render.githubusercontent.com/render/math?math=k-2" /> and partition <img src="https://render.githubusercontent.com/render/math?math=k-1" />. In contrast, if the boundary is placed after the addition layer, the communication overhead between partition <img src="https://render.githubusercontent.com/render/math?math=k-1" /> and <img src="https://render.githubusercontent.com/render/math?math=k" /> is visibly smaller. Our measurements show that having cross-device communication is more expensive than having slightly imbalanced partitions (see the Appendix in our paper). Therefore, we do not consider breaking skip connections (highlighted separately as an entire attention layer and MLP layer in green color at line 7 in Algorithm 1.</p>
  </li>
  <li>
    <p><strong>Frozen layer memory footprint.</strong> During training, AutoPipe must recompute partition boundaries several times to balance two distinct types of layers: frozen layers and active layers. The frozen layer’s memory cost is a fraction of that inactive layer, given that the frozen layer does not need backward activation maps, optimizer states, and gradients. Instead of launching intrusive profilers to obtain thorough metrics on memory and computational cost, we define a tunable cost factor <img src="https://render.githubusercontent.com/render/math?math=lambda_{\text{frozen}}" /> to estimate the memory footprint ratio of a frozen layer over the same active layer. Based on empirical measurements in our experimental hardware, we set it to <img src="https://render.githubusercontent.com/render/math?math=\frac{1}{6}" />.</p>
  </li>
</ol>

<p align="center">
<img src="https://pytorch.org/assets/images/AutoPipe_algorithm.png" width="100%" />
<br />
</p>

<p>Based on the above two considerations, AutoPipe balances pipeline partitions based on parameter sizes. More specifically, AutoPipe uses a greedy algorithm to allocate all frozen and active layers to evenly distribute partitioned sublayers into <img src="https://render.githubusercontent.com/render/math?math=K" /> GPU devices. Pseudocode is described as the <code class="language-plaintext highlighter-rouge">load\_balance()</code> function in Algorithm 1. The frozen layers are extracted from the original model and kept in a separate model instance <img src="https://render.githubusercontent.com/render/math?math=\mathcal{F}_{\text{frozen}}" /> in the first device of a pipeline.</p>

<p>Note that the partition algorithm employed in this paper is not the only option; PipeTransformer is modularized to work with any alternatives.</p>

<h3 id="pipeline-compression">Pipeline Compression</h3>

<p>Pipeline compression helps to free up GPUs to accommodate more pipeline replicas and reduce the number of cross-device communications between partitions. To determine the timing of compression, we can estimate the memory cost of the largest partition after compression, and then compare it with that of the largest partition of a pipeline at timestep <img src="https://render.githubusercontent.com/render/math?math=T=0" />. To avoid extensive memory profiling, the compression algorithm uses the parameter size as a proxy for the training memory footprint. Based on this simplification, the criterion of pipeline compression is as follows:</p>

<p align="center">
<img src="https://pytorch.org/assets/images/memory_reduction.png" width="320" />
<br />
</p>

<p>Once the freeze notification is received, AutoPipe will always attempt to divide the pipeline length <img src="https://render.githubusercontent.com/render/math?math=K" /> by 2 (e.g., from 8 to 4, then 2). By using <img src="https://render.githubusercontent.com/render/math?math=\frac{K}{2}" /> as the input, the compression algorithm can verify if the result satisfies the criterion in Equation (1). Pseudocode is shown in lines 25-33 in Algorithm 1. Note that this compression makes the acceleration ratio exponentially increase during training, meaning that if a GPU server has a larger number of GPUs (e.g., more than 8), the acceleration ratio will be further amplified.</p>

<p align="center">
<img src="https://pytorch.org/assets/images/pipe_buble.png" width="560" />
<br />
Figure 7. Pipeline Bubble: <img src="https://render.githubusercontent.com/render/math?math=F_{d,b}" />, and <img src="https://render.githubusercontent.com/render/math?math=U_d" /> denote forward, backward, and the optimizer update of micro-batch <img src="https://render.githubusercontent.com/render/math?math=b" /> on device <img src="https://render.githubusercontent.com/render/math?math=d" />, respectively. The total bubble size in each iteration is <img src="https://render.githubusercontent.com/render/math?math=K-1" /> times per micro-batch forward and backward cost.
</p>

<p>Additionally, such a technique can also speed up training by shrinking the size of pipeline bubbles. To explain bubble sizes in a pipeline, Figure 7 depicts how 4 micro-batches run through a 4-device pipeline <img src="https://render.githubusercontent.com/render/math?math=K = 4" />. In general, the total bubble size is <img src="https://render.githubusercontent.com/render/math?math=(K-1)" /> times per micro-batch forward and backward cost. Therefore, it is clear that shorter pipelines have smaller bubble sizes.</p>

<h3 id="dynamic-number-of-micro-batches">Dynamic Number of Micro-Batches</h3>

<p>Prior pipeline parallel systems use a fixed number of micro-batches per mini-batch (<img src="https://render.githubusercontent.com/render/math?math=M" /> ). GPipe suggests <img src="https://render.githubusercontent.com/render/math?math=M \geq 4 \times K" />, where <img src="https://render.githubusercontent.com/render/math?math=K" /> is the number of partitions (pipeline length). However, given that PipeTransformer dynamically configures <img src="https://render.githubusercontent.com/render/math?math=K" />, we find it to be sub-optimal to maintain a static <img src="https://render.githubusercontent.com/render/math?math=M" /> during training. Moreover, when integrated with DDP, the value of <img src="https://render.githubusercontent.com/render/math?math=M" /> also has an impact on the efficiency of DDP gradient synchronizations. Since DDP must wait for the last micro-batch to finish its backward computation on a parameter before launching its gradient synchronization, finer micro-batches lead to a smaller overlap between computation and communication. Hence, instead of using a static value, PipeTransformer searches for optimal <img src="https://render.githubusercontent.com/render/math?math=M" /> on the fly in the hybrid of DDP environment by enumerating <img src="https://render.githubusercontent.com/render/math?math=M" /> values ranging from <img src="https://render.githubusercontent.com/render/math?math=K" /> to <img src="https://render.githubusercontent.com/render/math?math=6K" />. For a specific training environment, the profiling needs only to be done once (see Algorithm 1 line 35).</p>

<p>For the complete source code, please refer to <code class="language-plaintext highlighter-rouge">https://github.com/Distributed-AI/PipeTransformer/blob/master/pipe_transformer/pipe/auto_pipe.py</code>.</p>

<h2 id="autodp-spawning-more-pipeline-replicas">AutoDP: Spawning More Pipeline Replicas</h2>
<p>As AutoPipe compresses the same pipeline into fewer GPUs, AutoDP can automatically spawn new pipeline replicas to increase data-parallel width.</p>

<p>Despite the conceptual simplicity, subtle dependencies on communications and states require careful design. The challenges are threefold:</p>

<ol>
  <li>
    <p><strong>DDP Communication</strong>: Collective communications in PyTorch DDP requires static membership, which prevents new pipelines from connecting with existing ones;</p>
  </li>
  <li>
    <p><strong>State Synchronization</strong>: newly activated processes must be consistent with existing pipelines in the training progress (e.g., epoch number and learning rate), weights and optimizer states, the boundary of frozen layers, and pipeline GPU range;</p>
  </li>
  <li>
    <p><strong>Dataset Redistribution</strong>: the dataset should be re-balanced to match a dynamic number of pipelines. This not only avoids stragglers but also ensures that gradients from all DDP processes are equally weighted.</p>
  </li>
</ol>

<p align="center">
<img src="https://pytorch.org/assets/images/AutoDP.png" width="560" />
<br />
Figure 8. AutoDP: handling dynamical data-parallel with messaging between double process groups (Process 0-7 belong to machine 0, while process 8-15 belong to machine 1)
</p>

<p>To tackle these challenges, we create double communication process groups for DDP. As in the example shown in Figure 8, the message process group (purple) is responsible for light-weight control messages and covers all processes, while the active training process group (yellow) only contains active processes and serves as a vehicle for heavy-weight tensor communications during training. The message group remains static, whereas the training group is dismantled and reconstructed to match active processes.
In T0, only processes 0 and 8 are active. During the transition to T1, process 0 activates processes 1 and 9 (newly added pipeline replicas) and synchronizes necessary information mentioned above using the message group. The four active processes then form a new training group, allowing static collective communications adaptive to dynamic memberships.
To redistribute the dataset, we implement a variant of DistributedSampler that can seamlessly adjust data samples to match the number of active pipeline replicas.</p>

<p>The above design also naturally helps to reduce DDP communication overhead. More specifically, when transitioning from T0 to T1, processes 0 and 1 destroy the existing DDP instances, and active processes construct a new DDP training group using a cached pipelined model (AutoPipe stores frozen model and cached model separately).</p>

<p>We use the following APIs to implement the design above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="c1"># initialize the process group (this must be called in the initialization of PyTorch DDP)
</span><span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">init_method</span><span class="o">=</span><span class="s">'tcp://'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">master_addr</span><span class="p">)</span> <span class="o">+</span> <span class="s">':'</span> <span class="o">+</span>
<span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">master_port</span><span class="p">),</span> <span class="n">backend</span><span class="o">=</span><span class="n">Backend</span><span class="p">.</span><span class="n">GLOO</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">global_rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">world_size</span><span class="p">)</span>
<span class="p">...</span>

<span class="c1"># create active process group (yellow color)
</span><span class="bp">self</span><span class="p">.</span><span class="n">active_process_group</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">active_ranks</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">Backend</span><span class="p">.</span><span class="n">NCCL</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">365</span><span class="p">))</span>
<span class="p">...</span>

<span class="c1"># create message process group (yellow color)
</span><span class="bp">self</span><span class="p">.</span><span class="n">comm_broadcast_group</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">ranks</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">world_size</span><span class="p">)],</span> <span class="n">backend</span><span class="o">=</span><span class="n">Backend</span><span class="p">.</span><span class="n">GLOO</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">365</span><span class="p">))</span>
<span class="p">...</span>

<span class="c1"># create DDP-enabled model when the number of data-parallel workers is changed. Note:
# 1. The process group to be used for distributed data all-reduction.
</span><span class="n">If</span> <span class="bp">None</span><span class="p">,</span> <span class="n">the</span> <span class="n">default</span> <span class="n">process</span> <span class="n">group</span><span class="p">,</span> <span class="n">which</span> <span class="ow">is</span> <span class="n">created</span> <span class="n">by</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">,</span> <span class="n">will</span> <span class="n">be</span> <span class="n">used</span><span class="p">.</span>
<span class="n">In</span> <span class="n">our</span> <span class="n">case</span><span class="p">,</span> <span class="n">we</span> <span class="nb">set</span> <span class="n">it</span> <span class="k">as</span> <span class="bp">self</span><span class="p">.</span><span class="n">active_process_group</span>
<span class="c1"># 2. device_ids should be set when the pipeline length = 1 (the model resides on a single CUDA device).
</span>
<span class="bp">self</span><span class="p">.</span><span class="n">pipe_len</span> <span class="o">=</span> <span class="n">gpu_num_per_process</span>
<span class="k">if</span> <span class="n">gpu_num_per_process</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">active_process_group</span><span class="p">,</span> <span class="n">find_unused_parameters</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">local_rank</span><span class="p">],</span> <span class="n">process_group</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">active_process_group</span><span class="p">,</span> <span class="n">find_unused_parameters</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># to broadcast message among processes, we use dist.broadcast_object_list
</span><span class="k">def</span> <span class="nf">dist_broadcast</span><span class="p">(</span><span class="n">object_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
    <span class="s">"""Broadcasts a given object to all parties."""</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">object_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">object_list</span>
</code></pre></div></div>
<p>For the complete source code, please refer to <code class="language-plaintext highlighter-rouge">https://github.com/Distributed-AI/PipeTransformer/blob/master/pipe_transformer/dp/auto_dp.py</code>.</p>

<h1 id="experiments">Experiments</h1>

<p>This section first summarizes experiment setups and then evaluates PipeTransformer using computer vision and natural language processing tasks.</p>

<p><strong>Hardware.</strong> Experiments were conducted on 2 identical machines connected by InfiniBand CX353A (<img src="https://render.githubusercontent.com/render/math?math=5" />GB/s), where each machine is equipped with 8 NVIDIA Quadro RTX 5000 (16GB GPU memory). GPU-to-GPU bandwidth within a machine (PCI 3.0, 16 lanes) is <img src="https://render.githubusercontent.com/render/math?math=15.754" />GB/s.</p>

<p><strong>Implementation.</strong> We used PyTorch Pipe as a building block. The BERT model definition, configuration, and related tokenizer are from HuggingFace 3.5.0. We implemented Vision Transformer using PyTorch by following its TensorFlow implementation. More details can be found in our source code.</p>

<p><strong>Models and Datasets.</strong> Experiments employ two representative Transformers in CV and NLP: Vision Transformer (ViT) and BERT. ViT was run on an image classification task, initialized with pre-trained weights on ImageNet21K and fine-tuned on ImageNet and CIFAR-100. BERT was run on two tasks, text classification on the SST-2 dataset from the General Language Understanding Evaluation (GLUE) benchmark, and question answering on the SQuAD v1.1 Dataset (Stanford Question Answering), which is a collection of 100k crowdsourced question/answer pairs.</p>

<p><strong>Training Schemes.</strong> Given that large models normally would require thousands of GPU-days {\emph{e.g.}, GPT-3) if trained from scratch, fine-tuning downstream tasks using pre-trained models has become a trend in CV and NLP communities. Moreover, PipeTransformer is a complex training system that involves multiple core components. Thus, for the first version of PipeTransformer system development and algorithmic research, it is not cost-efficient to develop and evaluate from scratch using large-scale pre-training. Therefore, the experiments presented in this section focuses on pre-trained models. Note that since the model architectures in pre-training and fine-tuning are the same, PipeTransformer can serve both. We discussed pre-training results in the Appendix.</p>

<p><strong>Baseline.</strong> Experiments in this section compare PipeTransformer to the state-of-the-art framework, a hybrid scheme of PyTorch Pipeline (PyTorch’s implementation of GPipe) and PyTorch DDP. Since this is the first paper that studies accelerating distributed training by freezing layers, there are no perfectly aligned counterpart solutions yet.</p>

<p><strong>Hyper-parameters.</strong> Experiments use ViT-B/16 (12 transformer layers, <img src="https://render.githubusercontent.com/render/math?math=16 \times 16" /> input patch size) for ImageNet and CIFAR-100, BERT-large-uncased (24 layers) for SQuAD 1.1, and BERT-base-uncased (12 layers) for SST-2. With PipeTransformer, ViT and BERT training can set the per-pipeline batch size to around 400 and 64, respectively. Other hyperparameters (e.g., epoch, learning rate) for all experiments are presented in Appendix.</p>

<h2 id="overall-training-acceleration">Overall Training Acceleration</h2>
<p align="center">
<img src="https://pytorch.org/assets/images/experiments_table1.png" width="560" />
<br />
</p>

<p>We summarize the overall experimental results in the table above. Note that the speedup we report is based on a conservative <img src="https://render.githubusercontent.com/render/math?math=\alpha" /> <img src="https://render.githubusercontent.com/render/math?math=\frac{1}{3}" /> value that can obtain comparable or even higher accuracy. A more aggressive <img src="https://render.githubusercontent.com/render/math?math=\alpha" /> (<img src="https://render.githubusercontent.com/render/math?math=\frac{2}{5}" />, <img src="https://render.githubusercontent.com/render/math?math=\frac{1}{2}" />) can obtain a higher speedup but may lead to a slight loss in accuracy. Note that the model size of BERT (24 layers) is larger than ViT-B/16 (12 layers), thus it takes more time for communication.</p>

<h2 id="performance-analysis">Performance Analysis</h2>

<h3 id="speedup-breakdown">Speedup Breakdown</h3>

<p>This section presents evaluation results and analyzes the performance of different components in \autopipe. More experimental results can be found in the Appendix.</p>

<p align="center">
<img src="https://pytorch.org/assets/images/experiments_throughput.png" width="560" />
<br />
Figure 9. Speedup Breakdown (ViT on ImageNet)
</p>

<p>To understand the efficacy of all four components and their impacts on training speed, we experimented with different combinations and used their training sample throughput (samples/second) and speedup ratio as metrics. Results are illustrated in Figure 9. Key takeaways from these experimental results are:</p>

<ol>
  <li>the main speedup is the result of elastic pipelining which is achieved through the joint use of AutoPipe and AutoDP;</li>
  <li>AutoCache’s contribution is amplified by AutoDP;</li>
  <li>freeze training alone without system-wise adjustment even downgrades the training speed.</li>
</ol>

<h3 id="tuning--in-freezing-algorithm">Tuning <img src="https://render.githubusercontent.com/render/math?math=\alpha" /> in Freezing Algorithm</h3>

<p align="center">
<img src="https://pytorch.org/assets/images/experiments_tuning_alpha.png" width="460" />
<br />
Figure 10. Tuning <img src="https://render.githubusercontent.com/render/math?math=\alpha" /> in Freezing Algorithm
</p>

<p>We ran experiments to show how the <img src="https://render.githubusercontent.com/render/math?math=\alpha" />  in the freeze algorithms influences training speed. The result clearly demonstrates that a larger <img src="https://render.githubusercontent.com/render/math?math=\alpha" /> (excessive freeze) leads to a greater speedup but suffers from a slight performance degradation. In the case shown in Figure 10, where <img src="https://render.githubusercontent.com/render/math?math=\alpha=1/5" />, freeze training outperforms normal training and obtains a <img src="https://render.githubusercontent.com/render/math?math=2.04" />-fold speedup. We provide more results in the Appendix.</p>

<h3 id="optimal-chunks-in-the-elastic-pipeline">Optimal Chunks in the elastic pipeline</h3>

<p align="center">
<img src="https://pytorch.org/assets/images/experiments_optimal_k.png" width="450" />
<br />
Figure 11. Optimal chunk number in the elastic pipeline
</p>

<p>We profiled the optimal number of micro-batches <img src="https://render.githubusercontent.com/render/math?math=M" /> for different pipeline lengths <img src="https://render.githubusercontent.com/render/math?math=K" />. Results are summarized in Figure 11. As we can see, different <img src="https://render.githubusercontent.com/render/math?math=K" /> values lead to different optimal <img src="https://render.githubusercontent.com/render/math?math=M" />, and the throughput gaps across different M values are large (as shown when <img src="https://render.githubusercontent.com/render/math?math=K=8" />), which confirms the necessity of an anterior profiler in elastic pipelining.</p>

<h3 id="understanding-the-timing-of-caching">Understanding the Timing of Caching</h3>

<p align="center">
<img src="https://pytorch.org/assets/images/experiment_autocache.png" width="320" />
<br />
Figure 12. the timing of caching
</p>

<p>To evaluate AutoCache, we compared the sample throughput of training that activates AutoCache from epoch <img src="https://render.githubusercontent.com/render/math?math=0" /> (blue) with the training job without AutoCache (red). Figure 12 shows that enabling caching too early can slow down training, as caching can be more expensive than the forward propagation on a small number of frozen layers. After more layers are frozen, caching activations clearly outperform the corresponding forward propagation. As a result, AutoCache uses a profiler to determine the proper timing to enable caching. In our system, for ViT (12 layers), caching starts from 3 frozen layers, while for BERT (24 layers), caching starts from 5 frozen layers.</p>

<p>For more detailed experimental analysis, please refer to our paper.</p>

<h1 id="summarization">Summarization</h1>
<p>This blog introduces PipeTransformer, a holistic solution that combines elastic pipeline-parallel and data-parallel for distributed training using PyTorch Distributed APIs. More specifically, PipeTransformer incrementally freezes layers in the pipeline, packs remaining active layers into fewer GPUs, and forks more pipeline replicas to increase the data-parallel width. Evaluations on ViT and BERT models show that compared to the state-of-the-art baseline, PipeTransformer attains up to 2.83× speedups without accuracy loss.</p>

<h1 id="reference">Reference</h1>

<p>[1] Li, S., Zhao, Y., Varma, R., Salpekar, O., Noordhuis, P., Li,T., Paszke, A., Smith, J., Vaughan, B., Damania, P., et al. Pytorch Distributed:  Experiences on Accelerating Dataparallel Training. Proceedings of the VLDB Endowment,13(12), 2020</p>

<p>[2] Devlin, J., Chang, M. W., Lee, K., and Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT, 2019</p>

<p>[3] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is Worth 16x16 words: Transformers for Image Recognition at Scale.</p>

<p>[4] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language Models are Few-shot Learners.</p>

<p>[5] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling Giant Models with Conditional Computation and Automatic Sharding.</p>

<p>[6] Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., Long, J., Shekita, E. J., and Su, B. Y. Scaling Distributed Machine Learning with the Parameter Server. In 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14), pp. 583–598, 2014.</p>

<p>[7] Jiang, Y., Zhu, Y., Lan, C., Yi, B., Cui, Y., and Guo, C. A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pp. 463–479. USENIX Association, November 2020. ISBN 978-1-939133-19- 9.</p>

<p>[8] Kim, S., Yu, G. I., Park, H., Cho, S., Jeong, E., Ha, H., Lee, S., Jeong, J. S., and Chun, B. G. Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks. In Proceedings of the Fourteenth EuroSys Conference 2019, pp. 1–15, 2019.</p>

<p>[9] Kim, C., Lee, H., Jeong, M., Baek, W., Yoon, B., Kim, I., Lim, S., and Kim, S. TorchGPipe: On-the-fly Pipeline Parallelism for Training Giant Models.</p>

<p>[10] Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism.</p>

<p>[11] Park, J. H., Yun, G., Yi, C. M., Nguyen, N. T., Lee, S., Choi, J., Noh, S. H., and ri Choi, Y. Hetpipe: Enabling Large DNN Training on (whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism. In 2020 USENIX Annual Technical Conference (USENIX ATC 20), pp. 307–321. USENIX Association, July 2020. ISBN 978-1-939133- 14-4.</p>

<p>[12] Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N. R., Ganger, G. R., Gibbons, P. B., and Zaharia, M. Pipedream: Generalized Pipeline Parallelism for DNN Training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP ’19, pp. 1–15, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450368735. doi: 10.1145/3341301.3359646.</p>

<p>[13] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling Giant Models with Conditional Computation and Automatic Sharding.</p>

<p>[14] Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., Sepassi, R., and Hechtman, B. Mesh-Tensorflow: Deep Learning for Supercomputers. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31, pp. 10414–10423. Curran Associates, Inc., 2018.</p>

<p>[15] Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-LM: Training Multi-billion Parameter Language Models using Model Parallelism.</p>

<p>[16] Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. ZERO: Memory Optimization towards Training a Trillion Parameter Models.</p>

<p>[17] Raghu, M., Gilmer, J., Yosinski, J., and Sohl Dickstein, J. Svcca: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. In NIPS, 2017.</p>

<p>[18] Morcos, A., Raghu, M., and Bengio, S. Insights on Representational Similarity in Neural Networks with Canonical Correlation. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31, pp. 5732–5741. Curran Associates, Inc., 2018.</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-tos-privacy-policy.pdf" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="/assets/tos-oss-privacy-policy/fb-oss-privacy-policy.pdf" target="_blank">Privacy</a></li>
      </ul>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/ecosystem">Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/ecosystem/pted/2021">Ecosystem Day 2021</a>
          </li>
        </ul>

        <li class="">
          <a href="/mobile">Mobile</a>
        </li>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/docs/stable/torchvision">torchvision</a>
          </li>

          <li class="">
            <a href="/elastic">TorchElastic</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/#community-module">Community</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

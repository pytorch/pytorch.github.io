<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      INT4 Decoding GQA CUDA Optimizations for LLM Inference | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="An efficient decoding Grouped-Query Attention with low-precision KV cache

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="INT4 Decoding GQA CUDA Optimizations for LLM Inference" />
<meta property="og:description" content="An efficient decoding Grouped-Query Attention with low-precision KV cache

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="INT4 Decoding GQA CUDA Optimizations for LLM Inference" />
<meta name="twitter:description" content="An efficient decoding Grouped-Query Attention with low-precision KV cache

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>
    <div class="hello-bar">
        <div class="container">
          Join us in Silicon Valley September 18-19 at the 2024 PyTorch Conference. <a target="_blank" href="https://events.linuxfoundation.org/pytorch-conference/">Learn more</a>.
        </div>
      </div>
      
    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Learn
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/get-started">
            <span class=dropdown-title>Get Started</span>
            <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/">
            <span class="dropdown-title">Tutorials</span>
            <p>Whats new in PyTorch tutorials</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
            <span class="dropdown-title">Learn the Basics</span>
            <p>Familiarize yourself with PyTorch concepts and modules</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
            <span class="dropdown-title">PyTorch Recipes</span>
            <p>Bite-size, ready-to-deploy PyTorch code examples</p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
            <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
            <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem/contributor-awards-2023">
            <span class="dropdown-title">Contributor Awards - 2023</span>
            <p>Award winners announced at this year's PyTorch Conference</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
            <p>Build innovative and privacy-aware AI experiences for edge devices</p>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
            <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://pytorch.org/docs">
            <span class="dropdown-title">PyTorch</span>
            <p>Explore the documentation for comprehensive guidance on how to use PyTorch.</p>
          </a>
          <a class="nav-dropdown-item" href="/pytorch-domains">
            <span class="dropdown-title">PyTorch Domains</span>
            <p> Read the PyTorch Domains documentation to learn more about domain-specific libraries.</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          Blog & News
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/blog">
            <span class="dropdown-title">PyTorch Blog</span>
            <p>Catch up on the latest technical news and happenings</p>
          </a>
          <a class="nav-dropdown-item" href="/community-blog">
            <span class="dropdown-title">Community Blog</span>
            <p>Stories from the PyTorch ecosystem</p>
          </a>
          <a class="nav-dropdown-item" href="/videos">
            <span class="dropdown-title">Videos</span>
            <p>Learn about the latest PyTorch tutorials, new, and more </p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class="dropdown-title">Community Stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="with-down-arrow">
          About
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/governing-board">
            <span class=dropdown-title>Governing Board</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <a href="/join" data-cta="join">
        Become a Member
      </a>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub">
        <div id="topnav-gh-icon"></div>
      </a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">June 06, 2024</p>
            <h1>
                <a class="blog-title">INT4 Decoding GQA CUDA Optimizations for LLM Inference</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Sarunya Pumma, Jongsoo Park, Jianyu Huang, Amy Yang, Jaewon Lee, Daniel Haziza, Grigory Sizov, Jeremy Reizenstein, Jeff Johnson, Ying Zhang
                      
                    </p>
                    <h4 id="an-efficient-decoding-grouped-query-attention-with-low-precision-kv-cache">An efficient decoding Grouped-Query Attention with low-precision KV cache</h4>

<h2 id="introduction">Introduction</h2>

<p>Generative AI has taken the world by storm with its ability to generate content like humans.  Many of these generative AI tools are powered by large language models (LLMs), like Meta <a href="https://llama.meta.com/llama3/">Llama</a> models and OpenAI’s <a href="https://openai.com/gpt-4">ChatGPT</a>.  One of the main challenges of LLMs is supporting large “context lengths” (also known as “sequence lengths”).  The context length refers to the number of tokens that the model uses to understand the input context and generate responses.  Longer context lengths generally translate into higher precision and quality in the responses.  However, long context lengths are compute and memory intensive.  This is mainly due to the following reasons:</p>

<ul>
  <li>The computational complexity of attention layers increases proportionally with the context length (the growth rate depends on the attention algorithm).  As a result, when using long context lengths, the attention layers can become a bottleneck, particularly during the prefill phase where attentions are compute bound.</li>
  <li>The KV cache size grows linearly with the context length, thus, putting higher pressure on the memory requirement and consequently slowing down the already memory-bound attention decoding.  Moreover, since the memory capacity is limited, the batch size reduces when the KV cache gets bigger, which generally results in a drop in throughput.</li>
</ul>

<p>The computational complexity growth is difficult to solve compared to the other problem mentioned above.  One way to address the KV cache size growth problem is to use low precision KV cache.  From our experiments, group-wise INT4 quantization provides comparable results in terms of accuracy compared to BF16 KV cache during the decode phase in Meta Llama 2 inference.  However, we did not observe any latency improvement, despite reading 4x lesser data in attention decoding layers.  This means that the INT4 attention is 4x less efficient at utilizing precious HBM bandwidth than BF16 attention.</p>

<p>In this note, we discuss the CUDA optimizations that we applied to INT4 GQA (grouped-query attention – the attention layer that we use in the LLM inference phase) to improve its performance by up to <strong>1.8x on the NVIDIA A100 GPU</strong> and <strong>1.9x on the NVIDIA H100 GPU</strong>.</p>

<ul>
  <li>The <strong>optimized CUDA INT4 GQA</strong> outperformed <a href="https://pytorch.org/blog/flash-decoding/">INT4 Flash-Decoding GQA</a> (the best performing INT4 GQA that we used in the experiment mentioned above) by <strong>1.4x-1.7x on A100</strong> and <strong>1.09x-1.3x on H100.</strong></li>
  <li>The <strong>optimized CUDA INT4 GQA</strong> performs better than <strong>BF16 Flash-Decoding GQA</strong> by <strong>1.5x-1.7x on A100 and 1.4x-1.7x on H100.</strong></li>
</ul>

<h2 id="background">Background</h2>

<h3 id="gqa-for-llm-inference">GQA for LLM Inference</h3>

<p><a href="https://arxiv.org/abs/2305.13245">Grouped-Query Attention (GQA)</a> is a variant of multi-head attention (MHA) where each KV cache head is shared across a group of query heads.  Our LLM inference adopts GQA as an attention layer in both the prefill and decode phases in order to reduce the capacity requirement for the KV cache.  We use multiple GPUs in inference where the KV cache and query heads are distributed across GPUs.  Each GPU runs an attention layer with a single KV head and a group of Q heads.  Therefore, when viewed from a single GPU perspective, the GQA component can also be described as <a href="https://arxiv.org/abs/1911.02150">MQA (Multi-Query Attention)</a>.</p>

<p>The simplified workflow of decoding GQA is illustrated in Figure 1.  GQA takes three main inputs: input query (denoted <code class="language-plaintext highlighter-rouge">Q</code>), K cache (denoted <code class="language-plaintext highlighter-rouge">K</code>), and V cache (denoted <code class="language-plaintext highlighter-rouge">V</code>).  Our current GQA inference uses BF16 for <code class="language-plaintext highlighter-rouge">Q</code>, <code class="language-plaintext highlighter-rouge">K</code>, and <code class="language-plaintext highlighter-rouge">V</code>.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Q</code> is a 4D BF16 tensor of shape (<code class="language-plaintext highlighter-rouge">B</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">H<sub>Q</sub></code>, <code class="language-plaintext highlighter-rouge">D</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">K</code> is a 4D BF16 tensor of shape (<code class="language-plaintext highlighter-rouge">B</code>, <code class="language-plaintext highlighter-rouge">T<sub>max</sub></code>, <code class="language-plaintext highlighter-rouge">H<sub>KV</sub></code>, <code class="language-plaintext highlighter-rouge">D</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">V</code> is a 4D BF16 tensor of shape (<code class="language-plaintext highlighter-rouge">B</code>, <code class="language-plaintext highlighter-rouge">T<sub>max</sub></code>, <code class="language-plaintext highlighter-rouge">H<sub>KV</sub></code>, <code class="language-plaintext highlighter-rouge">D</code>)</li>
</ul>

<p><em>where</em></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">B</code> is the batch size (the number of input prompts)</li>
  <li><code class="language-plaintext highlighter-rouge">H<sub>Q</sub></code> is the number of query heads</li>
  <li><code class="language-plaintext highlighter-rouge">H<sub>KV</sub></code> is the number of KV heads (<code class="language-plaintext highlighter-rouge">H<sub>Q</sub></code> must be divisible by <code class="language-plaintext highlighter-rouge">H<sub>KV</sub></code>)</li>
  <li><code class="language-plaintext highlighter-rouge">T<sub>max</sub></code> is the maximum context length</li>
  <li><code class="language-plaintext highlighter-rouge">D</code> is the head dimension (fixed to 128)</li>
</ul>

<p>GQA is simply <code class="language-plaintext highlighter-rouge">bmm(softmax(bmm(Q, K<sup>T</sup>) / sqrt(D)), V)</code>.  This yields a single output tensor (denoted as <code class="language-plaintext highlighter-rouge">O</code>) which is a 4D BF16 tensor that has the same shape as <code class="language-plaintext highlighter-rouge">Q</code>.  Note that matrix multiplications are performed using BF16, however, accumulation and <code class="language-plaintext highlighter-rouge">softmax</code> are carried out in FP32.  We call this “BF16 GQA” as the KV cache is BF16.</p>

<p><img src="/assets/images/int4-decoding/fg1.png" alt="Figure 1: The simplified workflow of BF16 GQA for LLM inference" style="width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;" /></p>

<p><strong>Figure 1</strong> The simplified workflow of BF16 GQA for LLM inference</p>

<h3 id="int4-gqa">INT4 GQA</h3>

<p>To further reduce the size of the KV cache, we explore the possibility of using INT4 for KV cache instead of BF16.  We estimate the potential performance improvement by calculating the computational intensity (CI) of INT4 GQA and comparing it to that of BF16 GQA, as CI represents FLOPS per byte.  We compute the CI for <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> and <code class="language-plaintext highlighter-rouge">PV</code> (as shown in Equation 1) as they take KV cache as an operand.  Note that we disregard the <code class="language-plaintext highlighter-rouge">Q</code> load as it is negligible compared to the KV cache.  We also ignore any intermediate data loads/stores that are not on global memory.  Thus, the CI only takes into account the computation FLOPS and KV cache loads.</p>

<p><img src="/assets/images/int4-decoding/eq.jpg" alt="Equation 1" style="width:100%;display:block;max-width:400px;margin-left:auto;margin-right:auto;" /></p>

<p><strong>Equation (1)</strong></p>

<p>Assuming that <code class="language-plaintext highlighter-rouge">H<sub>Q</sub></code> = 8 and <code class="language-plaintext highlighter-rouge">H<sub>KV</sub></code> = 1, CI for BF16 KV cache is 8 while CI for INT4 KV cache is 32.  The CIs indicate that both BF16 and INT4 GQAs are memory bound (the peak CIs for BF16 tensor cores for A100 and H100 are <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/a100-80gb-datasheet-update-nvidia-us-1521051-r2-web.pdf">312 TF / 2 TB/s = 141</a> and <a href="https://www.nvidia.com/en-us/data-center/h100/">990 TF / 3.35 TB/s = 269</a>; note that these TF numbers are without sparsity).  Moreover, with INT4 KV cache, we should expect up to 4x performance improvement compared to BF16 GQA.</p>

<p>To enable INT4 KV cache support in GQA, we can dequantize the KV cache from INT4 to BF16 before passing it to the BF16 GQA operator.  However, since KV cache is typically large, copying it from/to global memory can be costly.  Moreover, decoding GQA is a memory bound operation (the memory unit is utilized much more heavily than the compute unit).  Figure 2 shows the NCU profile of the <a href="https://github.com/facebookresearch/xformers/blob/9f6abadabdec17cd4b5c301632a44bf8216a7f35/xformers/csrc/attention/cuda/fmha/autogen/impl/cutlassF_bf16_aligned.cu#L33">FMHA CUTLASS BF16 GQA kernel in xFormers</a>, which is one of the state of the art implementations of GQA.  From the figure, it is obvious that memory is a bottleneck.</p>

<p><img src="/assets/images/int4-decoding/fg2.png" alt="Figure 2: The NCU profile of the FMHA CUTLASS BF16 kernel in xFormers" style="width:100%" /></p>

<p><strong>Figure 2</strong> The NCU profile of the <a href="https://github.com/facebookresearch/xformers/blob/9f6abadabdec17cd4b5c301632a44bf8216a7f35/xformers/csrc/attention/cuda/fmha/autogen/impl/cutlassF_bf16_aligned.cu#L33">FMHA CUTLASS BF16 kernel in xFormers</a></p>

<p>A more efficient alternative is to fuse INT4 dequantization with the GQA operation (shown in Figure 3).  In other words, having GQA read INT4 KV cache directly and perform the INT4 to BF16 conversion within the kernel.  This change can potentially reduce the amount of global memory reads required for the KV cache, which could lead to a decrease in latency.  We call this “INT4 GQA.”</p>

<p><img src="/assets/images/int4-decoding/fg3.png" alt="Figure 3: The workflow of fused INT4 GQA" style="width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;" /></p>

<p><strong>Figure 3</strong> The workflow of fused INT4 GQA</p>

<p>We list the state of the art implementations of GQA in the table below along with their features in Table 1.</p>

<p><strong>Table 1</strong> State of the art GQA implementations</p>

<table class="table table-bordered">
  <tr>
   <td><strong>Implementation</strong>
   </td>
   <td><strong>Denote</strong>
   </td>
   <td><strong>BF16 GQA</strong>
   </td>
   <td><strong>Fused INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><a href="https://pytorch.org/blog/flash-decoding/">Flash-Decoding</a> (Triton implementation)
   </td>
   <td>FD
   </td>
   <td>Yes
   </td>
   <td>Yes
   </td>
  </tr>
  <tr>
   <td><a href="https://github.com/Dao-AILab/flash-attention">Flash Attention (v2.3.3)</a>
   </td>
   <td>FA
   </td>
   <td>Yes
   </td>
   <td>No
   </td>
  </tr>
  <tr>
   <td>CUDA baseline
   </td>
   <td>CU
   </td>
   <td>Yes
   </td>
   <td>Yes
   </td>
  </tr>
</table>

<p>All implementations, except for CU, support both split-K and non split-K.  CU only has the split-K implementation.  Only FA has a heuristic in the backend to determine whether to run the split-K or non split-K kernel.  For other implementations, users must explicitly choose which version to run.  In this note, we focus on long context lengths (in our experiments, we use a context length of 8192) and therefore opt for the split-K version wherever possible.</p>

<p>As the baseline, we measured the performance of the state of the art GQA implementations on NVIDIA A100 and H100 GPUs.  The latency (time in microseconds) and achieved bandwidth (GB/s) are reported in Table 2.  Note that we ran a range of split-Ks (from 2 to 128 splits) and reported the best performance for each implementation.  For all experiments, we use a context length of 8192.  For INT4 GQA, we used row-wise quantization (i.e., num quantized groups = 1).</p>

<p><strong>Table 2</strong> Baseline GQA performance</p>

<p>On A100</p>

<table class="table table-bordered">
  <tr>
   <td><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>BF16 GQA</strong>
   </td>
   <td colspan="3"><strong>INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Batch size</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>139
   </td>
   <td>133
   </td>
   <td>183
   </td>
   <td>137
   </td>
   <td>-
   </td>
   <td>143
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>245
   </td>
   <td>229
   </td>
   <td>335
   </td>
   <td>234
   </td>
   <td>-
   </td>
   <td>257
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>433
   </td>
   <td>555
   </td>
   <td>596
   </td>
   <td>432
   </td>
   <td>-
   </td>
   <td>455
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>826
   </td>
   <td>977
   </td>
   <td>1127
   </td>
   <td>815
   </td>
   <td>-
   </td>
   <td>866
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1607
   </td>
   <td>1670
   </td>
   <td>2194
   </td>
   <td>1581
   </td>
   <td>-
   </td>
   <td>1659
   </td>
  </tr>
</table>

<table class="table table-bordered">
  <tr>
   <td><strong>Effective Bandwidth (GB/s)</strong>
   </td>
   <td colspan="3"><strong>BF16 GQA</strong>
   </td>
   <td colspan="3"><strong>INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Batch size</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>965
   </td>
   <td>1012
   </td>
   <td>736
   </td>
   <td>262
   </td>
   <td>-
   </td>
   <td>250
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>1097
   </td>
   <td>1175
   </td>
   <td>802
   </td>
   <td>305
   </td>
   <td>-
   </td>
   <td>278
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>1240
   </td>
   <td>968
   </td>
   <td>901
   </td>
   <td>331
   </td>
   <td>-
   </td>
   <td>314
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>1301
   </td>
   <td>1100
   </td>
   <td>954
   </td>
   <td>351
   </td>
   <td>-
   </td>
   <td>331
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1338
   </td>
   <td>1287
   </td>
   <td>980
   </td>
   <td>362
   </td>
   <td>-
   </td>
   <td>345
   </td>
  </tr>
</table>

<p>On H100</p>

<table class="table table-bordered">
  <tr>
   <td><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>BF16 GQA</strong>
   </td>
   <td colspan="3"><strong>INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Batch size</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>91
   </td>
   <td>90
   </td>
   <td>114
   </td>
   <td>70
   </td>
   <td>-
   </td>
   <td>96
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>148
   </td>
   <td>146
   </td>
   <td>200
   </td>
   <td>113
   </td>
   <td>-
   </td>
   <td>162
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>271
   </td>
   <td>298
   </td>
   <td>361
   </td>
   <td>205
   </td>
   <td>-
   </td>
   <td>294
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>515
   </td>
   <td>499
   </td>
   <td>658
   </td>
   <td>389
   </td>
   <td>-
   </td>
   <td>558
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1000
   </td>
   <td>1011
   </td>
   <td>1260
   </td>
   <td>756
   </td>
   <td>-
   </td>
   <td>1066
   </td>
  </tr>
</table>

<table class="table table-bordered">
  <tr>
   <td><strong>Effective Bandwidth (GB/s)</strong>
   </td>
   <td colspan="3"><strong>BF16 GQA</strong>
   </td>
   <td colspan="3"><strong>INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Batch size</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>1481
   </td>
   <td>1496
   </td>
   <td>1178
   </td>
   <td>511
   </td>
   <td>-
   </td>
   <td>371
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>1815
   </td>
   <td>1840
   </td>
   <td>1345
   </td>
   <td>631
   </td>
   <td>-
   </td>
   <td>443
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>1982
   </td>
   <td>1802
   </td>
   <td>1487
   </td>
   <td>699
   </td>
   <td>-
   </td>
   <td>487
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>2087
   </td>
   <td>2156
   </td>
   <td>1634
   </td>
   <td>736
   </td>
   <td>-
   </td>
   <td>513
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>2150
   </td>
   <td>2127
   </td>
   <td>1706
   </td>
   <td>757
   </td>
   <td>-
   </td>
   <td>537
   </td>
  </tr>
</table>

<p>First, let’s discuss the BF16 GQA performance: CU ranks last in terms of performance among all implementations.  FD and FA have comparable performance.  When the batch size is less than or equal to 64, FA utilizes the split-K kernel and performs slightly better than FD.  However, when the batch size is greater than 64, FD performs better.</p>

<p>The same trend holds true for INT4 GQAs. However, we did not measure the performance of FA as it does not support INT4 KV cache. FD outperforms CU for all cases.</p>

<p>When comparing the latencies of FD between BF16 and INT4 GQAs, we find that they are almost identical.  This suggests that <em>INT4 GQA is highly inefficient</em>, which can be further confirmed by the significantly lower achievable bandwidth for INT4 GQA compared to BF16 GQA.  The same trend is also true when looking at the performance of CU.</p>

<h3 id="cuda-with-tensor-cores-int4-gqa-implementation">CUDA with Tensor Cores INT4 GQA Implementation</h3>

<p>In this section, we briefly describe our baseline implementation which is CUDA with tensor cores INT4 GQA (CU).  Each thread block processes only one KV head and a group of query heads from one input prompt.  Therefore, each thread block performs <code class="language-plaintext highlighter-rouge">mm(softmax(mm(Q, K<sup>T</sup>) / sqrt(D)), V)</code>; notice that <code class="language-plaintext highlighter-rouge">mm</code> is being performed not <code class="language-plaintext highlighter-rouge">bmm</code>.  Moreover, since this is a split-K implementation, tokens in the KV cache are split among different thread blocks.  Note that each thread block contains 4 warps (each warp contains 32 threads for NVIDIA A100 and H100 GPUs).  Work in each thread block is split among warps.  Within each warp, we use the <a href="https://bruce-lee-ly.medium.com/nvidia-tensor-core-introduction-to-wmma-api-programming-21bcfee4ec45">WMMA</a> API to compute matrix multiplication on tensor cores.  Figure 4 demonstrates the work partitioning in CU.</p>

<p><img src="/assets/images/int4-decoding/fg4.jpg" alt="Figure 4: CU work partitioning" style="width:100%" /></p>

<p><strong>Figure 4</strong> CU work partitioning</p>

<h2 id="optimizing-cuda-with-tensor-cores-kernel-of-int4-gqa">Optimizing CUDA with Tensor Cores Kernel of INT4 GQA</h2>

<p>In this note, we discuss the optimizations that we have applied to the CUDA with tensor cores implementation of INT4 GQA (CU).  The ideal goal is to improve the INT4 GQA performance by 4 times based on the CI analysis in the previous section.  Note that the query size is negligible compared to the KV cache size when the context length is long.</p>

<p>In our analysis, we used the <a href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html">NVIDIA Nsight Compute (NCU)</a> as the main profiler.  Our general bottleneck elimination approach is to minimize the stall cycles.  We applied 10 optimizations to INT4 GQA, three of which are specific for NVIDIA A100/H100 GPUs.  These optimizations are well known CUDA optimization techniques which can be generalized to many applications.</p>

<p>It is worth noting that the reason that we choose to optimize the CUDA implementation rather than the Flash-Decoding implementation (FD) (which is Triton based) is because with CUDA, we have a better control of how the low-level instructions are being generated.  Many optimization techniques that we apply such as, operating on tensor core fragments directly (Optimizations 7-9), cannot be done through Triton since it does not expose low-level details to developers.  However, these optimizations can be integrated into the compiler-based solution to make the optimizations available to broader operators, which is indeed a part of our future plan.</p>

<h3 id="optimization-1-unroll-k-loads">Optimization 1: Unroll <code class="language-plaintext highlighter-rouge">K</code> Loads</h3>

<p><strong>Problem Analysis:</strong></p>

<p>The NCU profile shows that during <code class="language-plaintext highlighter-rouge">K</code> loading, there are only 2 global loads followed by <em>memory stalls</em> at <code class="language-plaintext highlighter-rouge">dequantize_permuted_int4</code>.  The memory stalls are the long scoreboard stalls which indicates the waits for global memory access.  This suggests that the kernel does not issue sufficient memory loads</p>

<p>to hide the global load latency.  The kernel issues data loading, and then waits to consume the data immediately causing the global load latency to be exposed.  The stalls are shown in Figure 5.</p>

<p><img src="/assets/images/int4-decoding/fg5.png" alt="Figure 5: K loading before unrolling" style="width:100%" /></p>

<p><strong>Figure 5</strong> K loading before unrolling (the numbers that the arrows point to are stall cycles caused by global memory wait)</p>

<p><strong>Solution:</strong></p>

<p>In the baseline implementation, we use <code class="language-plaintext highlighter-rouge">uint32_t</code> to load 8 INT4 <code class="language-plaintext highlighter-rouge">K</code> values in a single load and we perform 2 <code class="language-plaintext highlighter-rouge">uint32_t</code> loads in each iteration, which is 16 INT4 K values.  To allow for a better global load latency hiding, we issue 8 <code class="language-plaintext highlighter-rouge">uint32_t</code> loads instead of two before consuming the <code class="language-plaintext highlighter-rouge">K</code> values in <code class="language-plaintext highlighter-rouge">dequantize_permuted_int4</code>.  This allows the compiler to unroll the loads as well as reorder the instructions to hide the global load latency better.  Figure 6 shows the NCU profile of <code class="language-plaintext highlighter-rouge">K</code> loading after unrolling.  Comparing Figure 5 and Figure 6, we effectively reduce the stall cycles by unrolling the <code class="language-plaintext highlighter-rouge">K</code> loads.</p>

<p><img src="/assets/images/int4-decoding/fg6.png" alt="Figure 6: K loading after unrolling" style="width:100%" /></p>

<p><strong>Figure 6</strong> K loading after unrolling (the numbers that the arrows point to are stall cycles caused by global memory wait)</p>

<p><strong>Results:</strong></p>

<p><strong>Table 3</strong> Performance of Optimization 1 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 1</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 1</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>134
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>267
   </td>
   <td><strong>1.02</strong>
   </td>
   <td><strong>1.07</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>237
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>302
   </td>
   <td><strong>0.99</strong>
   </td>
   <td><strong>1.09</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>422
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>339
   </td>
   <td><strong>1.02</strong>
   </td>
   <td><strong>1.08</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>806
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>355
   </td>
   <td><strong>1.01</strong>
   </td>
   <td><strong>1.07</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1550
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>369
   </td>
   <td><strong>1.02</strong>
   </td>
   <td><strong>1.07</strong>
   </td>
  </tr>
</table>

<h3 id="optimization-2-improve-p-type-casting-fp32-bf16">Optimization 2: Improve <code class="language-plaintext highlighter-rouge">P</code> Type Casting (FP32-&gt;BF16)</h3>

<p><strong>Problem Analysis:</strong></p>

<p>Since the product of <code class="language-plaintext highlighter-rouge">softmax(bmm(Q, K<sup>T</sup>) / sqrt(D))</code> is FP32 (denoted as <code class="language-plaintext highlighter-rouge">P</code> in Figure 3), the kernel has to convert <code class="language-plaintext highlighter-rouge">P</code> from FP32 to BF16 before feeding it to the next <code class="language-plaintext highlighter-rouge">bmm</code> computation.  The kernel performs the FP32 to BF16 conversion of <code class="language-plaintext highlighter-rouge">P</code> by copying the FP32 data from one location in shared memory to another location in shared memory.  This causes stalls during the shared memory access (shown in Figure 7) which might be caused by (1) the shared memory indirection; and (2) the shared memory bank conflict since each thread accesses an 16-bit element (because of this, two threads can access the same memory bank simultaneously).</p>

<p><img src="/assets/images/int4-decoding/fg7.png" alt="Figure 7: P type casting before Optimization 2" style="width:100%" /></p>

<p><strong>Figure 7</strong> <code class="language-plaintext highlighter-rouge">P</code> type casting before Optimization 2 (the number that the arrow points to is stall cycles caused by shared memory wait)</p>

<p><strong>Solution:</strong></p>

<p>We use all threads in the thread block to do in-place type conversion.  Each thread operates on two consecutive elements in order to avoid the shared memory bank conflict when storing BF16.  All threads work on the same head (<code class="language-plaintext highlighter-rouge">h</code>) at the same time to guarantee correctness of the conversion.  The in-place conversion steps are as follows:</p>

<ol>
  <li>Each thread loads 2 FP32 token elements from the same head from the shared memory into registers</li>
  <li>Call <code class="language-plaintext highlighter-rouge">__syncthreads()</code> to make sure that every thread finishes reading the data</li>
  <li>Each thread converts its data to 2 BF16 token elements and then stores the results to the same shared memory</li>
</ol>

<p>Some optimizations that we apply to the implementation:</p>

<ul>
  <li>Use vector types (especially <code class="language-plaintext highlighter-rouge">nv_bfloat2</code>)</li>
  <li>Unroll data loading/storing, i.e., performing multiple loads before calling <code class="language-plaintext highlighter-rouge">__syncthreads()</code> and performing multiple stores after <code class="language-plaintext highlighter-rouge">__syncthreads()</code></li>
</ul>

<p>After this optimization, long stalls are not observed during <code class="language-plaintext highlighter-rouge">P</code> type casting as shown in Figure 8.</p>

<p><img src="/assets/images/int4-decoding/fg8.png" alt="Figure 8: P type casting after Optimization 2" style="width:100%" /></p>

<p><strong>Figure 8</strong> <code class="language-plaintext highlighter-rouge">P</code> type casting after Optimization 2 (the numbers that the arrow points to are stall cycles caused by shared memory wait)</p>

<p><strong>Culprits:</strong></p>

<p>Since we unroll data loading/storing by using registers as an intermediate storage, the number of registers per thread increases resulting in reduced occupancy.</p>

<p><strong>Results:</strong></p>

<p><strong>Table 4</strong> Performance of Optimization 2 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 2</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 2</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>126
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>285
   </td>
   <td><strong>1.09</strong>
   </td>
   <td><strong>1.14</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>221
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>324
   </td>
   <td><strong>1.06</strong>
   </td>
   <td><strong>1.16</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>395
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>362
   </td>
   <td><strong>1.09</strong>
   </td>
   <td><strong>1.15</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>749
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>382
   </td>
   <td><strong>1.09</strong>
   </td>
   <td><strong>1.16</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1435
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>399
   </td>
   <td><strong>1.10</strong>
   </td>
   <td><strong>1.16</strong>
   </td>
  </tr>
</table>

<h3 id="optimization-3-remove-local-memory-usage-for-max-qkt-computation">Optimization 3: Remove Local Memory Usage for max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> computation</h3>

<p><strong>Problem Analysis:</strong></p>

<p>During the softmax computation, the kernel has to compute max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> for each head. It uses a temporary “thread-local” storage for storing per-thread max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> results (one float value for each head).  Depending on the compiler, the thread-local storage can be allocated on registers (on chip) or the local memory (off chip == global memory).  Unfortunately, in the baseline, the thread-local storage resides in the local memory which is much slower than the registers (shown in Figure 9).  We suspect that this is because the compiler cannot determine the indices of thread-local storage at compile time (since the number of heads (<code class="language-plaintext highlighter-rouge">H</code>) in the kernel is a runtime variable).  Accessing local memory as if accessing registers can hurt the performance of the kernel.</p>

<p><img src="/assets/images/int4-decoding/fg9.png" alt="Figure 9: Local memory access during max QKT computation" style="width:100%" /></p>

<p><strong>Figure 9</strong> Local memory access during max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> computation</p>

<p><strong>Solution:</strong></p>

<p>We realize that we do not need <code class="language-plaintext highlighter-rouge">H</code> (number of heads) floats as temporary storage per thread since each thread can compute max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> for only one head instead of all the heads.  Thus, we only need one float per thread, which can be easily stored in a register.  To accumulate the max results among warps, we use shared memory.  This optimization eliminates the local memory usage during max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> computation.</p>

<p><strong>Results:</strong></p>

<p><strong>Table 5</strong> Performance of Optimization 3 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 3</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 3</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>119
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>300
   </td>
   <td><strong>1.14</strong>
   </td>
   <td><strong>1.20</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>206
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>348
   </td>
   <td><strong>1.14</strong>
   </td>
   <td><strong>1.25</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>368
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>389
   </td>
   <td><strong>1.17</strong>
   </td>
   <td><strong>1.24</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>696
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>411
   </td>
   <td><strong>1.17</strong>
   </td>
   <td><strong>1.24</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1338
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>428
   </td>
   <td><strong>1.18</strong>
   </td>
   <td><strong>1.24</strong>
   </td>
  </tr>
</table>

<h3 id="optimization-4-remove-local-memory-usage-for-row-sum">Optimization 4: Remove local memory usage for row sum</h3>

<p><strong>Problem Analysis:</strong></p>

<p>Similar to<a href="https://www.internalfb.com/diff/D50183201"> </a>Optimization 3, the local memory usage problem is also observed during the row sum computation in the <code class="language-plaintext highlighter-rouge">softmax</code> computation.  Since local memory is off chip, accessing it as if accessing registers can hurt the performance of the kernel.</p>

<p><strong>Solution</strong>:</p>

<p>We apply the same solution as the max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> computation for the row sum computation.  That is to have each thread compute a row sum of only one head, which requires only one float per thread.  This eliminates the need for local memory.</p>

<p><strong>Results:</strong></p>

<p><strong>Table 6</strong> Performance of Optimization 4 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 4</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 4</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>118
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>302
   </td>
   <td><strong>1.15</strong>
   </td>
   <td><strong>1.21</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>204
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>351
   </td>
   <td><strong>1.15</strong>
   </td>
   <td><strong>1.26</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>364
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>393
   </td>
   <td><strong>1.19</strong>
   </td>
   <td><strong>1.25</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>688
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>416
   </td>
   <td><strong>1.18</strong>
   </td>
   <td><strong>1.26</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1328
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>431
   </td>
   <td><strong>1.19</strong>
   </td>
   <td><strong>1.25</strong>
   </td>
  </tr>
</table>

<h3 id="optimization-5-add-prefetch-for-v-load">Optimization 5: Add prefetch for <code class="language-plaintext highlighter-rouge">V</code> load</h3>

<p><strong>Problem Analysis:</strong></p>

<p>The same issue as <code class="language-plaintext highlighter-rouge">K</code> loading is observed when loading <code class="language-plaintext highlighter-rouge">V</code>.  That is, the kernel issues data loading, and then waits to consume the data immediately causing the global load latency to be exposed.  However, when using the unrolling technique mentioned above, the compiler allocates the temporary buffer on local memory instead of registers causing a large slow down.</p>

<p><strong>Solution:</strong></p>

<p>We adopt the data prefetching technique for <code class="language-plaintext highlighter-rouge">V</code> loading.  We load the next iteration <code class="language-plaintext highlighter-rouge">V</code> values immediately after the current iteration values are consumed.  This allows the data loading to be overlapped with the <code class="language-plaintext highlighter-rouge">PK</code> computation resulting in better kernel performance.</p>

<p><strong>Results:</strong></p>

<p><strong>Table 7</strong> Performance of Optimization 5 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 5</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 5</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>109
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>327
   </td>
   <td><strong>1.25</strong>
   </td>
   <td><strong>1.31</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>194
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>370
   </td>
   <td><strong>1.21</strong>
   </td>
   <td><strong>1.33</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>345
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>414
   </td>
   <td><strong>1.25</strong>
   </td>
   <td><strong>1.32</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>649
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>441
   </td>
   <td><strong>1.26</strong>
   </td>
   <td><strong>1.33</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1244
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>460
   </td>
   <td><strong>1.27</strong>
   </td>
   <td><strong>1.33</strong>
   </td>
  </tr>
</table>

<h3 id="optimization-6-add-group-wise-int4-groups--4-with-vector-load">Optimization 6: Add Group-Wise INT4 (Groups = 4) with Vector Load</h3>

<p><strong>Problem Analysis:</strong></p>

<p>Prior to this optimization, CU only supported row-wise INT4 quantization.  That is, every column in each row shares the same scales.  The scales of each row are stored in the first 4 bytes of each row as shown in Figure 10.  In the kernel, each thread loads only one row at a time.  Since each row contains 68 bytes (4 bytes for scales and 64 bytes for data), it cannot guarantee that every row aligns with a size of any vector type.  Thus, vector loads cannot be used for loading the KV cache.</p>

<p><img src="/assets/images/int4-decoding/fg10.jpg" alt="Figure 10: The layout of each row of INT4 KV cache with row-wise quantization" style="width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;" /></p>

<p><strong>Figure 10</strong> The layout of each row of INT4 KV cache with row-wise quantization</p>

<p><strong>Solution:</strong></p>

<p>We have implemented support for group-wise INT4 quantization with num groups = 4.  In this case, columns in each row in the KV cache tensor are divided into 4 equal groups.  Columns within the same group share the same scales for quantization/dequantization.  The data layout for INT4 KV cache is shown in Figure 11.   The scales for all groups are serialized and stored at the beginning of each row.  The INT4 data is also serialized and laid out next to the scales.</p>

<p>Because the number of bytes in each row now becomes 80 bytes, we can use a vector type, i.e., <code class="language-plaintext highlighter-rouge">uint2</code> in our case, to load data.  (We <strong>do not</strong> use <code class="language-plaintext highlighter-rouge">uint4</code> since each thread loads only 16 INT4s at a time due to the tensor core fragment size.)  Vector load is generally better than scalar load since it does not cause extra byte loads.</p>

<p><img src="/assets/images/int4-decoding/fg11.jpg" alt="Figure 11: The layout of each row of INT4 KV cache with row-wise quantization" style="width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;" /></p>

<p><strong>Figure 11</strong> The layout of each row of INT4 KV cache with row-wise quantization</p>

<p><strong>Results:</strong></p>

<p><strong>Table 8</strong> Performance of Optimization 6 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 6</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>111
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>322
   </td>
   <td><strong>1.23</strong>
   </td>
   <td><strong>1.29</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>192
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>372
   </td>
   <td><strong>1.22</strong>
   </td>
   <td><strong>1.34</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>346
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>414
   </td>
   <td><strong>1.25</strong>
   </td>
   <td><strong>1.32</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>642
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>446
   </td>
   <td><strong>1.27</strong>
   </td>
   <td><strong>1.35</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1244
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>460
   </td>
   <td><strong>1.27</strong>
   </td>
   <td><strong>1.33</strong>
   </td>
  </tr>
</table>

<p><strong>Table 9</strong> Performance of Optimization 6 for INT4 GQA (group-wise quantization with num groups = 4)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="2"><strong>Time (us)</strong>
   </td>
   <td colspan="2"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 6</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>129
   </td>
   <td>116
   </td>
   <td>325
   </td>
   <td>364
   </td>
   <td>1.31
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>219
   </td>
   <td>195
   </td>
   <td>385
   </td>
   <td>431
   </td>
   <td>1.36
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>392
   </td>
   <td>347
   </td>
   <td>429
   </td>
   <td>484
   </td>
   <td>1.39
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>719
   </td>
   <td>638
   </td>
   <td>468
   </td>
   <td>527
   </td>
   <td>1.41
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1375
   </td>
   <td>1225
   </td>
   <td>489
   </td>
   <td>550
   </td>
   <td>1.43
   </td>
  </tr>
</table>

<h3 id="optimization-7-compute-max-qkt-from-wmma-fragment-directly-a100h100-specific">Optimization 7: Compute max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> From WMMA Fragment Directly (A100/H100 specific)</h3>

<p><strong>Problem Analysis:</strong></p>

<p>We observe large stalls due to shared memory accessing during the max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> computation (showing as large short scoreboard stalls) as shown in Figure 12.</p>

<p><img src="/assets/images/int4-decoding/fg12.png" alt="Figure 12: Stalls due to shared memory access during max QKT computation" style="width:100%" /></p>

<p><strong>Figure 12</strong> Stalls due to shared memory access during max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> computation (the number that the arrow points to is stall cycles caused by shared memory wait)</p>

<p><strong>Solution:</strong></p>

<p>We bypass shared memory when computing max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> by computing it from the WMMA fragment (i.e., the tensor core fragment) directly.  The layout of the WMMA fragment is specific to the GPU architecture.  In this optimization, we only enabled this optimization for the NVIDIA A100/H100 GPUs. Other GPUs will still use shared memory for the max <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> computation. By bypassing shared memory, we effectively eliminate the stalls caused by shared memory access.  The tensor core layout of the <code class="language-plaintext highlighter-rouge">C</code> fragment which is used for storing the <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> results is shown in Figure 13.</p>

<p><img src="/assets/images/int4-decoding/fg13.jpg" alt="Figure 13: C fragment (QKT storage) tensor core layout on A100/H100" style="width:100%" /></p>

<p><strong>Figure 13</strong> <code class="language-plaintext highlighter-rouge">C</code> fragment (<code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> storage) tensor core layout on A100/H100</p>

<p><strong>Table 10</strong> Performance of Optimization 7 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 7</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 7</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>107
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>333
   </td>
   <td><strong>1.27</strong>
   </td>
   <td><strong>1.33</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>183
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>391
   </td>
   <td><strong>1.28</strong>
   </td>
   <td><strong>1.40</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>333
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>430
   </td>
   <td><strong>1.30</strong>
   </td>
   <td><strong>1.37</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>620
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>461
   </td>
   <td><strong>1.31</strong>
   </td>
   <td><strong>1.40</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1206
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>475
   </td>
   <td><strong>1.31</strong>
   </td>
   <td><strong>1.38</strong>
   </td>
  </tr>
</table>

<p><strong>Table 11</strong> Performance of Optimization 7 for INT4 GQA (group-wise quantization with num groups = 4)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CUDA_WMMA Opt 6</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 7</strong>
   </td>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 7</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>129
   </td>
   <td>116
   </td>
   <td>111
   </td>
   <td>325
   </td>
   <td>364
   </td>
   <td>380
   </td>
   <td><strong>1.17</strong>
   </td>
   <td><strong>1.04</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>219
   </td>
   <td>195
   </td>
   <td>187
   </td>
   <td>385
   </td>
   <td>431
   </td>
   <td>449
   </td>
   <td><strong>1.17</strong>
   </td>
   <td><strong>1.04</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>392
   </td>
   <td>347
   </td>
   <td>333
   </td>
   <td>429
   </td>
   <td>484
   </td>
   <td>506
   </td>
   <td><strong>1.18</strong>
   </td>
   <td><strong>1.04</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>719
   </td>
   <td>638
   </td>
   <td>615
   </td>
   <td>468
   </td>
   <td>527
   </td>
   <td>547
   </td>
   <td><strong>1.17</strong>
   </td>
   <td><strong>1.04</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1375
   </td>
   <td>1225
   </td>
   <td>1184
   </td>
   <td>489
   </td>
   <td>550
   </td>
   <td>569
   </td>
   <td><strong>1.16</strong>
   </td>
   <td><strong>1.03</strong>
   </td>
  </tr>
</table>

<h3 id="optimization-8-write-fp32-bf16-results-to-p-fragment-directly-a100h100-specific">Optimization 8: Write FP32-&gt;BF16 Results to <code class="language-plaintext highlighter-rouge">P</code> Fragment Directly (A100/H100 specific)</h3>

<p><strong>Problem Analysis:</strong></p>

<p>During the FP32-BF16 conversion for the <code class="language-plaintext highlighter-rouge">P</code> fragment, the kernel loads the FP32 data from shared memory, does the conversion and then stores the BF16 data back to shared memory.  Moreover, the conversion requires many thread block synchronizations (<code class="language-plaintext highlighter-rouge">__syncthreads()</code>).</p>

<p><strong>Solution:</strong></p>

<p>Due to the data partitioning design of the kernel, each warp performs only one pass through the <code class="language-plaintext highlighter-rouge">P</code> fragment.  Thus, we do not have to write the conversion results back to the shared memory for future usage.  To avoid writing the BF16 data to the shared memory and thread block synchronizations, we have each warp load the FP32 data of the <code class="language-plaintext highlighter-rouge">P</code> WMMA fragment from the shared memory, do the conversion and then write the BF16 data directly to the <code class="language-plaintext highlighter-rouge">P</code> fragment.</p>

<p>Note that this optimization is applied to only the NVIDIA A100 and H100 GPUs because the WMMA fragment layout is architecture dependent.  For non-A100/H100 GPUs, the kernel will fallback to the original path.</p>

<p>The <code class="language-plaintext highlighter-rouge">P</code> fragment tensor core layout is shown in Figure 14.  Note that this layout is specific to the NVIDIA A100/H100 GPU.</p>

<p><img src="/assets/images/int4-decoding/fg14.jpg" alt="Figure 14: P fragment tensor core layout on A100/H100" style="width:100%" /></p>

<p><strong>Figure 14</strong> <code class="language-plaintext highlighter-rouge">P</code> fragment tensor core layout on A100/H100</p>

<p><strong>Table 12</strong> Performance of Optimization 8 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 8</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 8</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>101
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>353
   </td>
   <td><strong>1.35</strong>
   </td>
   <td><strong>1.41</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>174
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>410
   </td>
   <td><strong>1.34</strong>
   </td>
   <td><strong>1.47</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>317
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>451
   </td>
   <td><strong>1.36</strong>
   </td>
   <td><strong>1.43</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>590
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>485
   </td>
   <td><strong>1.38</strong>
   </td>
   <td><strong>1.47</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1143
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>501
   </td>
   <td><strong>1.38</strong>
   </td>
   <td><strong>1.45</strong>
   </td>
  </tr>
</table>

<p><strong>Table 13</strong> Performance of Optimization 8 for INT4 GQA (group-wise quantization with num groups = 4)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CUDA_WMMA Opt 6</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 8</strong>
   </td>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 8</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>129
   </td>
   <td>116
   </td>
   <td>106
   </td>
   <td>325
   </td>
   <td>364
   </td>
   <td>396
   </td>
   <td><strong>1.22</strong>
   </td>
   <td><strong>1.09</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>219
   </td>
   <td>195
   </td>
   <td>180
   </td>
   <td>385
   </td>
   <td>431
   </td>
   <td>467
   </td>
   <td><strong>1.21</strong>
   </td>
   <td><strong>1.08</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>392
   </td>
   <td>347
   </td>
   <td>319
   </td>
   <td>429
   </td>
   <td>484
   </td>
   <td>528
   </td>
   <td><strong>1.23</strong>
   </td>
   <td><strong>1.09</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>719
   </td>
   <td>638
   </td>
   <td>596
   </td>
   <td>468
   </td>
   <td>527
   </td>
   <td>565
   </td>
   <td><strong>1.21</strong>
   </td>
   <td><strong>1.07</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1375
   </td>
   <td>1225
   </td>
   <td>1138
   </td>
   <td>489
   </td>
   <td>550
   </td>
   <td>591
   </td>
   <td><strong>1.21</strong>
   </td>
   <td><strong>1.08</strong>
   </td>
  </tr>
</table>

<h3 id="optimization-9-swizzle-p-shared-memory-layouts-a100h100-specific">Optimization 9: Swizzle P Shared Memory Layouts (A100/H100 specific)</h3>

<p><strong>Problem Analysis:</strong></p>

<p>We observe large shared memory bank conflicts during <code class="language-plaintext highlighter-rouge">P</code> loading.  The amount of bank conflict depends on the memory access stride.  For instance, for split-Ks = 32 and max seq length = 8192, we observed that only 4 out of 32 banks are being accessed in parallel (memory access stride = 256).  From Figure 14, when all threads access element 0, threads that have the same <code class="language-plaintext highlighter-rouge">threadIdx.x % 4</code> access the same bank.</p>

<p><img src="/assets/images/int4-decoding/fg15.jpg" alt="Figure 15: P fragment in shared memory before swizzling" style="width:100%" /></p>

<p><strong>Figure 15</strong> P fragment in shared memory before swizzling</p>

<p><strong>Solution:</strong></p>

<p>We shuffle the layout of <code class="language-plaintext highlighter-rouge">P</code> load/store in the shared memory in such a way that avoids bank conflicts.  In other words, we store the <code class="language-plaintext highlighter-rouge">QK<sup>T</sup></code> results (<code class="language-plaintext highlighter-rouge">C</code> fragment) and load them (<code class="language-plaintext highlighter-rouge">P</code> fragment) using the swizzled layout.  Moreover, instead of using the original memory access stride which is dependent on the number of tokens per thread block, we use the fragment’s column size as the stride which is constant.  Thus, the load and store of the <code class="language-plaintext highlighter-rouge">P</code> fragment is always contiguous.</p>

<p>The new layouts for the C and P fragments are shown in Figure 16.  With the new layout, it is guaranteed that 16 banks are being accessed in parallel as shown in Figure 17.</p>

<p><img src="/assets/images/int4-decoding/fg16.jpg" alt="Figure 16: The swizzled layouts of C and P fragments" style="width:100%" /></p>

<p><strong>Figure 16</strong> The swizzled layouts of C and P fragments</p>

<p><img src="/assets/images/int4-decoding/fg17.jpg" alt="Figure 17: P fragment in shared memory after swizzling" style="width:100%" /></p>

<p><strong>Figure 17</strong> P fragment in shared memory after swizzling</p>

<p><strong>Table 14</strong> Performance of Optimization 9 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 9</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 9</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>98
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>365
   </td>
   <td><strong>1.39</strong>
   </td>
   <td><strong>1.46</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>167
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>429
   </td>
   <td><strong>1.41</strong>
   </td>
   <td><strong>1.54</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>299
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>479
   </td>
   <td><strong>1.45</strong>
   </td>
   <td><strong>1.52</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>549
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>521
   </td>
   <td><strong>1.48</strong>
   </td>
   <td><strong>1.58</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>1060
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>540
   </td>
   <td><strong>1.49</strong>
   </td>
   <td><strong>1.56</strong>
   </td>
  </tr>
</table>

<p><strong>Table 15</strong> Performance of Optimization 9 for INT4 GQA (group-wise quantization with num groups = 4)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CUDA_WMMA Opt 6</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 9</strong>
   </td>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 9</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>129
   </td>
   <td>116
   </td>
   <td>105
   </td>
   <td>325
   </td>
   <td>364
   </td>
   <td>400
   </td>
   <td><strong>1.23</strong>
   </td>
   <td><strong>1.10</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>219
   </td>
   <td>195
   </td>
   <td>174
   </td>
   <td>385
   </td>
   <td>431
   </td>
   <td>484
   </td>
   <td><strong>1.26</strong>
   </td>
   <td><strong>1.12</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>392
   </td>
   <td>347
   </td>
   <td>302
   </td>
   <td>429
   </td>
   <td>484
   </td>
   <td>558
   </td>
   <td><strong>1.30</strong>
   </td>
   <td><strong>1.15</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>719
   </td>
   <td>638
   </td>
   <td>560
   </td>
   <td>468
   </td>
   <td>527
   </td>
   <td>601
   </td>
   <td><strong>1.28</strong>
   </td>
   <td><strong>1.14</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1375
   </td>
   <td>1225
   </td>
   <td>1065
   </td>
   <td>489
   </td>
   <td>550
   </td>
   <td>632
   </td>
   <td><strong>1.29</strong>
   </td>
   <td><strong>1.15</strong>
   </td>
  </tr>
</table>

<h3 id="optimization-10-pad-shared-memory-for-int4-dequantization">Optimization 10: Pad Shared Memory for INT4 Dequantization</h3>

<p><strong>Problem Analysis:</strong></p>

<p>Once the kernel reads the INT4 <code class="language-plaintext highlighter-rouge">K</code> or <code class="language-plaintext highlighter-rouge">V</code> cache from global memory, it performs dequantization and stores the results (BF16) in the shared memory.  Then, the BF16 data is loaded to the WMMA fragment from shared memory (via the WMMA interface).  We observed a large number of bank conflicts for both <code class="language-plaintext highlighter-rouge">K</code> and <code class="language-plaintext highlighter-rouge">V</code> accesses.  For instance, for <code class="language-plaintext highlighter-rouge">K</code> stores, only 4 out of 32 banks are being accessed in parallel.  For <code class="language-plaintext highlighter-rouge">K</code> loads, 16 banks are being accessed in parallel.  The same also occurs for <code class="language-plaintext highlighter-rouge">V</code> stores and loads.  See the figures in the solution section.</p>

<p><strong>Solution:</strong></p>

<p>We pad the shared memory to reduce the bank conflict.  Specifically, we pad each row by 2.  That is, the row stride of <code class="language-plaintext highlighter-rouge">K</code> becomes <code class="language-plaintext highlighter-rouge">F_K</code> + 2 and the row stride of V becomes <code class="language-plaintext highlighter-rouge">F_N</code> + 2 (<code class="language-plaintext highlighter-rouge">F_K</code> and <code class="language-plaintext highlighter-rouge">F_N</code> are the fixed widths of the <code class="language-plaintext highlighter-rouge">K</code> and <code class="language-plaintext highlighter-rouge">V</code> WMMA fragments, respectively).  With this optimization, we are able to reduce the bank conflict by 1.8x as shown in Figure 18.</p>

<p><img src="/assets/images/int4-decoding/fg18.png" alt="Figure 18: Bank conflicts before and after Optimization 10" style="width:100%" /></p>

<p><strong>Figure 18</strong> Bank conflicts before and after Optimization 10</p>

<p>After Optimization 10, for <code class="language-plaintext highlighter-rouge">K</code> stores, 32 banks are being accessed in parallel (shown in Figure 19), while for <code class="language-plaintext highlighter-rouge">K</code> loads, 29 banks are accessed in parallel (shown in Figure 20).</p>

<p><img src="/assets/images/int4-decoding/fg19.jpg" alt="Figure 19: K fragment store shared memory layout without and with padding" style="width:100%" /></p>

<p><strong>Figure 19</strong> K fragment store shared memory layout without and with padding</p>

<p><img src="/assets/images/int4-decoding/fg20.jpg" alt="Figure 20: K fragment load shared memory layout without and with padding" style="width:100%" /></p>

<p><strong>Figure 20</strong> K fragment load shared memory layout without and with padding</p>

<p><strong>Table 16</strong> Performance of Optimization 10 for INT4 GQA (row-wise quantization)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CU</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CU baseline</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 10</strong>
   </td>
   <td><strong>Baseline</strong>
   </td>
   <td><strong>Opt 10</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>137
   </td>
   <td>143
   </td>
   <td>94
   </td>
   <td>262
   </td>
   <td>250
   </td>
   <td>380
   </td>
   <td><strong>1.45</strong>
   </td>
   <td><strong>1.52</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>234
   </td>
   <td>257
   </td>
   <td>151
   </td>
   <td>305
   </td>
   <td>278
   </td>
   <td>475
   </td>
   <td><strong>1.55</strong>
   </td>
   <td><strong>1.71</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>432
   </td>
   <td>455
   </td>
   <td>266
   </td>
   <td>331
   </td>
   <td>314
   </td>
   <td>538
   </td>
   <td><strong>1.63</strong>
   </td>
   <td><strong>1.71</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>815
   </td>
   <td>866
   </td>
   <td>489
   </td>
   <td>351
   </td>
   <td>331
   </td>
   <td>586
   </td>
   <td><strong>1.67</strong>
   </td>
   <td><strong>1.77</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1581
   </td>
   <td>1659
   </td>
   <td>930
   </td>
   <td>362
   </td>
   <td>345
   </td>
   <td>616
   </td>
   <td><strong>1.70</strong>
   </td>
   <td><strong>1.79</strong>
   </td>
  </tr>
</table>

<p><strong>Table 17</strong> Performance of Optimization 10 for INT4 GQA (group-wise quantization with num groups = 4)</p>

<table class="table table-bordered">
  <tr>
   <td rowspan="3"><strong>Batch size</strong>
   </td>
   <td colspan="3"><strong>Time (us)</strong>
   </td>
   <td colspan="3"><strong>Bandwidth (GB/s)</strong>
   </td>
   <td colspan="2"><strong>Speed up</strong>
   </td>
  </tr>
  <tr>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>FD</strong>
   </td>
   <td colspan="2"><strong>CUDA_WMMA</strong>
   </td>
   <td rowspan="2"><strong>vs FD</strong>
   </td>
   <td rowspan="2"><strong>vs CUDA_WMMA Opt 6</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 10</strong>
   </td>
   <td><strong>Opt 6</strong>
   </td>
   <td><strong>Opt 10</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>129
   </td>
   <td>116
   </td>
   <td>99
   </td>
   <td>325
   </td>
   <td>364
   </td>
   <td>425
   </td>
   <td><strong>1.31</strong>
   </td>
   <td><strong>1.17</strong>
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>219
   </td>
   <td>195
   </td>
   <td>161
   </td>
   <td>385
   </td>
   <td>431
   </td>
   <td>523
   </td>
   <td><strong>1.36</strong>
   </td>
   <td><strong>1.21</strong>
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>392
   </td>
   <td>347
   </td>
   <td>282
   </td>
   <td>429
   </td>
   <td>484
   </td>
   <td>598
   </td>
   <td><strong>1.39</strong>
   </td>
   <td><strong>1.23</strong>
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>719
   </td>
   <td>638
   </td>
   <td>509
   </td>
   <td>468
   </td>
   <td>527
   </td>
   <td>662
   </td>
   <td><strong>1.41</strong>
   </td>
   <td><strong>1.25</strong>
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1375
   </td>
   <td>1225
   </td>
   <td>965
   </td>
   <td>489
   </td>
   <td>550
   </td>
   <td>698
   </td>
   <td><strong>1.43</strong>
   </td>
   <td><strong>1.27</strong>
   </td>
  </tr>
</table>

<h2 id="performance-evaluation">Performance Evaluation</h2>

<h3 id="microbenchmark-results">Microbenchmark results</h3>

<p>We also evaluated BF16 GQA performance using our optimized kernel (as shown in Table 19).  CU still performs generally worse than FD and FA for BF16.  This is expected since our optimizations are INT4 focused.</p>

<p>While INT4 GQA is still not as efficient as BF16 GQA (see the achieved bandwidths), it is important to note that when comparing FD BF16 GQA performance against CU INT4 GQA performance, <strong>we can see that the latency of INT4 is smaller than that of BF16</strong>.</p>

<p><strong>Table 19</strong> Performance of BF16 GQA and INT GQA after CU optimizations</p>

<p><strong>On A100</strong></p>

<table class="table table-bordered">
  <tr>
   <td><strong>Time (us)</strong>
   </td>
   <td colspan="4"><strong>BF16 GQA</strong>
   </td>
   <td colspan="4"><strong>INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Batch size</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU before</strong>
   </td>
   <td><strong>CU after</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU before</strong>
   </td>
   <td><strong>CU after</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>139
   </td>
   <td>133
   </td>
   <td>183
   </td>
   <td>163
   </td>
   <td>137
   </td>
   <td>-
   </td>
   <td>143
   </td>
   <td>94
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>245
   </td>
   <td>229
   </td>
   <td>335
   </td>
   <td>276
   </td>
   <td>234
   </td>
   <td>-
   </td>
   <td>257
   </td>
   <td>151
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>433
   </td>
   <td>555
   </td>
   <td>596
   </td>
   <td>517
   </td>
   <td>432
   </td>
   <td>-
   </td>
   <td>455
   </td>
   <td>266
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>826
   </td>
   <td>977
   </td>
   <td>1127
   </td>
   <td>999
   </td>
   <td>815
   </td>
   <td>-
   </td>
   <td>866
   </td>
   <td>489
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1607
   </td>
   <td>1670
   </td>
   <td>2194
   </td>
   <td>1879
   </td>
   <td>1581
   </td>
   <td>-
   </td>
   <td>1659
   </td>
   <td>930
   </td>
  </tr>
</table>

<table class="table table-bordered">
  <tr>
   <td><strong>Effective Bandwidth (GB/s)</strong>
   </td>
   <td colspan="4"><strong>BF16 GQA</strong>
   </td>
   <td colspan="4"><strong>INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Batch size</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU before</strong>
   </td>
   <td><strong>CU after</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU before</strong>
   </td>
   <td><strong>CU after</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>965
   </td>
   <td>1012
   </td>
   <td>736
   </td>
   <td>824
   </td>
   <td>262
   </td>
   <td>-
   </td>
   <td>250
   </td>
   <td>380
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>1097
   </td>
   <td>1175
   </td>
   <td>802
   </td>
   <td>972
   </td>
   <td>305
   </td>
   <td>-
   </td>
   <td>278
   </td>
   <td>475
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>1240
   </td>
   <td>968
   </td>
   <td>901
   </td>
   <td>1039
   </td>
   <td>331
   </td>
   <td>-
   </td>
   <td>314
   </td>
   <td>538
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>1301
   </td>
   <td>1100
   </td>
   <td>954
   </td>
   <td>1075
   </td>
   <td>351
   </td>
   <td>-
   </td>
   <td>331
   </td>
   <td>586
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1338
   </td>
   <td>1287
   </td>
   <td>980
   </td>
   <td>1144
   </td>
   <td>362
   </td>
   <td>-
   </td>
   <td>345
   </td>
   <td>616
   </td>
  </tr>
</table>

<p><strong>On H100</strong></p>

<table class="table table-bordered">
  <tr>
   <td><strong>Time (us)</strong>
   </td>
   <td colspan="4"><strong>BF16 GQA</strong>
   </td>
   <td colspan="4"><strong>INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Batch size</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU before</strong>
   </td>
   <td><strong>CU after</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU before</strong>
   </td>
   <td><strong>CU after</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>91
   </td>
   <td>90
   </td>
   <td>114
   </td>
   <td>100
   </td>
   <td>70
   </td>
   <td>-
   </td>
   <td>96
   </td>
   <td>64
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>148
   </td>
   <td>146
   </td>
   <td>200
   </td>
   <td>183
   </td>
   <td>113
   </td>
   <td>-
   </td>
   <td>162
   </td>
   <td>101
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>271
   </td>
   <td>298
   </td>
   <td>361
   </td>
   <td>308
   </td>
   <td>205
   </td>
   <td>-
   </td>
   <td>294
   </td>
   <td>170
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>515
   </td>
   <td>499
   </td>
   <td>658
   </td>
   <td>556
   </td>
   <td>389
   </td>
   <td>-
   </td>
   <td>558
   </td>
   <td>306
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>1000
   </td>
   <td>1011
   </td>
   <td>1260
   </td>
   <td>1066
   </td>
   <td>756
   </td>
   <td>-
   </td>
   <td>1066
   </td>
   <td>575
   </td>
  </tr>
</table>

<table class="table table-bordered">
  <tr>
   <td><strong>Effective Bandwidth (GB/s)</strong>
   </td>
   <td colspan="4"><strong>BF16 GQA</strong>
   </td>
   <td colspan="4"><strong>INT4 GQA</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Batch size</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU before</strong>
   </td>
   <td><strong>CU after</strong>
   </td>
   <td><strong>FD</strong>
   </td>
   <td><strong>FA</strong>
   </td>
   <td><strong>CU before</strong>
   </td>
   <td><strong>CU after</strong>
   </td>
  </tr>
  <tr>
   <td>32
   </td>
   <td>1481
   </td>
   <td>1496
   </td>
   <td>1178
   </td>
   <td>1341
   </td>
   <td>511
   </td>
   <td>-
   </td>
   <td>371
   </td>
   <td>560
   </td>
  </tr>
  <tr>
   <td>64
   </td>
   <td>1815
   </td>
   <td>1840
   </td>
   <td>1345
   </td>
   <td>1470
   </td>
   <td>631
   </td>
   <td>-
   </td>
   <td>443
   </td>
   <td>710
   </td>
  </tr>
  <tr>
   <td>128
   </td>
   <td>1982
   </td>
   <td>1802
   </td>
   <td>1487
   </td>
   <td>1743
   </td>
   <td>699
   </td>
   <td>-
   </td>
   <td>487
   </td>
   <td>844
   </td>
  </tr>
  <tr>
   <td>256
   </td>
   <td>2087
   </td>
   <td>2156
   </td>
   <td>1634
   </td>
   <td>1934
   </td>
   <td>736
   </td>
   <td>-
   </td>
   <td>513
   </td>
   <td>935
   </td>
  </tr>
  <tr>
   <td>512
   </td>
   <td>2150
   </td>
   <td>2127
   </td>
   <td>1706
   </td>
   <td>2015
   </td>
   <td>757
   </td>
   <td>-
   </td>
   <td>537
   </td>
   <td>996
   </td>
  </tr>
</table>

<h3 id="e2e-results">E2E results</h3>

<p>We evaluated our optimized INT4 GQA kernel in Llama 2 70B on 8 H100 GPUs. We ran the model end-to-end, but only reported the decode latency.  We use FP8 FFN (feed forward network) to emphasize the attention performance in the decoding phase.  We vary the batch size from 1 to 256 and the context length from 2,048 (2K) to 16,384 (16K).  The E2E performance results are shown in the figure below.</p>

<p><img src="/assets/images/int4-decoding/fg21.png" alt="Figure 21: Meta Llama 2 decode latency (ms) comparison" style="width:100%" /></p>

<p><strong>Figure 21</strong> Meta Llama 2 decode latency (ms) comparison (BF16 GQA runs out of memory in large batch size configurations)</p>

<h2 id="code">Code</h2>

<p>If you are interested, please checkout our code <a href="https://github.com/pytorch/FBGEMM/tree/main/fbgemm_gpu/experimental/gen_ai">here</a>.  If you have any questions, please feel free to open an issue on GitHub, and we will be happy to help.  Your contributions are welcome!</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="lf-grid">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org" class="footer-logo">
          <img src="/assets/images/logo-icon.svg" alt="PyTorch logo" width="40">
        </a>
      </div>

      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook"><path fill="currentColor" d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z"/></svg>	
        </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X"><path fill="currentColor" d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66"/></svg>
        </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube"><path fill="currentColor" d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z"/></svg>	
        </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn"><rect width="512" height="512" rx="0" fill="currentColor"/><circle fill="#000" cx="142" cy="138" r="37"/><path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198"/><path fill="#000" d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
        </a></li>
        <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank" title="PyTorch on Mastodon">
          <svg fill="currentColor" aria-label="Mastodon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"/></svg>
        </a></li>
      </ul>
    </div>

    <div class="privacy-policy">
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">Linux Foundation Policies</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">LF Projects, LLC Policies</a>. <a href="https://www.linuxfoundation.org/privacy">Privacy Policy</a> and <a href="https://www.linuxfoundation.org/terms">Terms of Use</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Learn</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/get-started">Get Started</a> 
          </li>
          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
          </li>
          <li>
            <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
          <li>
            <a href="/#community-module">Community</a>
          </li>
          <li>
            <a href="https://discuss.pytorch.org">Forums</a>
          </li>
          <li>
            <a href="/resources">Developer Resources</a>
          </li>
          <li>
            <a href="/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>Docs</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/docs">PyTorch</a>
          </li>
          <li>
            <a href="/pytorch-domains">PyTorch Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/blog">PyTorch Blog</a>
          </li>
          <li>
            <a href="/community-blog">Community Blog</a>
          </li>
          <li>
            <a href="/videos">Videos</a>
          </li>
          <li>
            <a href="/community-stories">Community Stories</a>
          </li>
          <li>
            <a href="/events">Events</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="/governing-board">Governing Board</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a href="/join">Become a Member</a>
        </li>
        <li class="resources-mobile-menu-title">
          <a href="https://github.com/pytorch/pytorch" title="Go to PyTorch GitHub"><div id="topnav-gh-icon"></div></a>
        </li>

      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header, .announcement-header, .comm-stories-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>
